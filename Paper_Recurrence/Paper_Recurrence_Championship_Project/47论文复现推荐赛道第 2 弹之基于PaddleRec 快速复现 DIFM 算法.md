### ä¸€ã€é¡¹ç›®èƒŒæ™¯

   æœ¬é¡¹ç›®æ˜¯ [é£æ¡¨è®ºæ–‡å¤ç°æŒ‘æˆ˜èµ›ï¼ˆç¬¬å››æœŸï¼‰](https://aistudio.baidu.com/aistudio/competition/detail/106) æ¨èèµ›é“, æˆ‘æ‰€é€‰åšçš„ç¬¬ 2 ç¯‡. ç¬¬ä¸€ç¯‡, å¯ä»¥å‚è€ƒ [è®ºæ–‡å¤ç°èµ›: åŸºäºPaddleRec 24å°æ—¶å¿«é€Ÿå¤ç°ç»å…¸ CTR é¢„ä¼°ç®—æ³•](https://aistudio.baidu.com/aistudio/projectdetail/2263714?contributionType=1&shared=1), æ¬¢è¿ fork, æ¬¢è¿æå‡ºæ‰¹è¯„å»ºè®®~ ğŸ‰ğŸ‰

æœ¬é¡¹ç›®ä»ç„¶æ˜¯åŸºäº PaddleRec æ¡†æ¶å¯¹è®ºæ–‡è¿›è¡Œå¤ç°, PaddleRec æ¡†æ¶ç›¸å…³ä»‹ç»åŠä½¿ç”¨æ–¹æ³•å¯ä»¥å‚è€ƒä¸Šä¸€ç¯‡ç›¸å…³ç« èŠ‚. ä¸‹é¢ğŸ‘‡ æˆ‘ä»¬å¤ç°æŒ‘æˆ˜èµ›çš„ 94 å·é¢˜ç›® [DIFM:A Dual Input-aware Factorization Machine for CTR Prediction](https://www.ijcai.org/Proceedings/2020/0434.pdf) å¤ç°è¿›è¡Œä»‹ç». ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†:

- **DIFM ç®—æ³•åŸç†**
- **å¦‚ä½•åŸºäº PaddleRec å¿«é€Ÿå¤ç°**
- **æ€»ç»“**
- **å‚è€ƒèµ„æ–™**
- **å†™åœ¨æœ€å**


### äºŒã€DIFM ç®—æ³•åŸç†

#### 1ã€ç®€ä»‹

![DIFM](https://tva1.sinaimg.cn/large/008i3skNly1gtffgzgk1bj30kq0e8wfz.jpg)

åŸè®ºæ–‡ï¼š[A Dual Input-aware Factorization Machine for CTR Prediction](https://www.ijcai.org/Proceedings/2020/0434.pdf)

DIFM æ˜¯ç»å…¸ CTR é¢„ä¼°ç®—æ³• FM çš„ä¸€ä¸ªå˜ç§, ä¸Šå›¾ä¸º DIFM çš„ç½‘ç»œç»“æ„å›¾. Paper æ ‡é¢˜ä¸­æ‰€æŒ‡çš„ Dual-FEN ä¸º `vector-wise` å’Œ `bit-wise`æ˜¯ä¸¤ä¸ª Input-aware Factorization æ¨¡å—, ä¸€ä¸ªæ˜¯ `bit-wise`, ä¸€ä¸ªæ˜¯ `vector-wise`ã€‚åªæ˜¯ç»´åº¦ä¸Šä¸åŒï¼Œå®ç°çš„ç›´è§‰æ˜¯ä¸€æ ·çš„ã€‚bit-wise ç»´åº¦ä¼šå¯¹æŸä¸€ä¸ª sparse embedding å‘é‡å†…éƒ¨å½¼æ­¤è¿›è¡Œäº¤å‰ï¼Œè€Œ vector-wise ä»…ä»…å¤„ç† embedding å‘é‡å±‚æ¬¡äº¤å‰ã€‚

æŠŠè¿™ä¸¤ä¸ª FEN æ¨¡å—æ‹¿æ‰, DIFM å°±é€€åŒ–ä¸º FM ç®—æ³•äº†. æŠŠ vector-wise FEN æ¨¡å—å»æ‰ï¼ŒDIFM å°±é€€åŒ–ä¸º IFM æ¨¡å‹ï¼Œè¯¥ç®—æ³•ä¹Ÿæ˜¯è®ºæ–‡ä½œè€…å®éªŒç»„çš„å¤§ä½œï¼Œå…¶ç»“æ„å›¾å¦‚ä¸‹ï¼š

![IFM](https://tva1.sinaimg.cn/large/008i3skNly1gtffi72287j60ez0cwq3p02.jpg)

ä¸¤ç±»ä¸åŒç»´åº¦çš„ FEN(Factor Estimating Net) ä½œç”¨éƒ½æ˜¯ä¸€è‡´çš„ï¼Œå³è¾“å‡º Embedding Layer ç›¸åº”å‘é‡çš„æƒé‡ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œå‡è®¾ä¸Šæ¸¸æœ‰ n ä¸ª sparse featuresï¼Œ åˆ™ FEN è¾“å‡ºç»“æœä¸º [a1, a2, ..., an]. åœ¨ Reweighting Layer ä¸­ï¼Œå¯¹åŸå§‹è¾“å…¥è¿›è¡Œæƒé‡è°ƒæ•´ã€‚æœ€åè¾“å…¥åˆ° FM å±‚è¿›è¡Œç‰¹å¾äº¤å‰ï¼Œè¾“å‡ºé¢„æµ‹ç»“æœã€‚

å› æ­¤ï¼Œæ€»ç»“ä¸¤ç¯‡è®ºæ–‡ IFM å’Œ DIFM æ­¥éª¤å¦‚ä¸‹ï¼š

- sparse features ç»ç”± Embedding Layer æŸ¥è¡¨å¾—åˆ° embedding å‘é‡ï¼Œdense features ç‰¹å¾å¦‚ä½•å¤„ç†ä¸¤ç¯‡è®ºæ–‡éƒ½æ²¡æåŠï¼›
- sparse features å¯¹åº”çš„ä¸€é˜¶æƒé‡ä¹Ÿå¯ä»¥é€šè¿‡ 1 ç»´ Embedding Layer æŸ¥æ‰¾ï¼›
- sparse embeddings è¾“å…¥ FEN (bit-wise or vector-wise)ï¼Œå¾—åˆ°ç‰¹å¾å¯¹åº”çš„æƒé‡ [a1, a2, ..., an]ï¼›
- Reweighting Layer æ ¹æ®ä¸Šä¸€æ­¥éª¤ä¸­çš„ç‰¹å¾æƒé‡ï¼Œå¯¹ sparse embeddings è¿›ä¸€æ­¥è°ƒæ•´ï¼›
- FM Layer è¿›è¡Œç‰¹å¾äº¤å‰ï¼Œè¾“å‡ºé¢„æµ‹æ¦‚ç‡ï¼›


#### 2ã€å¤ç°ç²¾åº¦

æœ¬é¡¹ç›®å®ç°äº† IFMã€ DIFM ä»¥åŠåœ¨ IFM åŸºç¡€ä¸Šå¢åŠ äº† deep layer ç”¨äºå¤„ç† dense features, è®°ä½œ IFM-Plus çš„ä¸‰ç§æ¨¡å‹.
åœ¨ DIFM è®ºæ–‡ä¸­ï¼Œä¸¤ç§ç®—æ³•åœ¨ Criteo æ•°æ®é›†çš„è¡¨ç°å¦‚ä¸‹ï¼š

![](https://tva1.sinaimg.cn/large/008i3skNly1gtfg698y4nj30bo06tdgp.jpg)

æœ¬æ¬¡ PaddlePaddle è®ºæ–‡å¤ç°èµ›è¦æ±‚åœ¨ PaddleRec Criteo æ•°æ®é›†ä¸Šï¼ŒDIFM çš„å¤ç°ç²¾åº¦ä¸º AUC > 0.799.

å®é™…æœ¬é¡¹ç›®å¤ç°ç²¾åº¦ä¸ºï¼š
- IFMï¼šAUC = 0.8016
- IFM-Plus: AUC = 0.8010
- DIFM: AUC = 0.799941


#### 3ã€æ•°æ®é›†

åŸè®ºæ–‡é‡‡ç”¨ Kaggle Criteo æ•°æ®é›†ï¼Œä¸ºå¸¸ç”¨çš„ CTR é¢„ä¼°ä»»åŠ¡åŸºå‡†æ•°æ®é›†ã€‚å•æ¡æ ·æœ¬åŒ…æ‹¬ 13 åˆ— dense featuresã€ 26 åˆ— sparse featuresåŠ label.

[Kaggle Criteo æ•°æ®é›†](https://www.kaggle.com/c/criteo-display-ad-challenge)
- train set: 4584, 0617 æ¡
- test set:   604, 2135 æ¡ ï¼ˆno label)

[PaddleRec Criteo æ•°æ®é›†](https://github.com/PaddlePaddle/PaddleRec/blob/release/2.1.0/datasets/criteo/run.sh)
- train set: 4400, 0000 æ¡
- test set:   184, 0617 æ¡

P.S. åŸè®ºæ–‡æ‰€æåŠ Criteo æ•°æ®é›†ä¸º Terabyte Criteo æ•°æ®é›†(å³åŒ…å« 1 äº¿æ¡æ ·æœ¬)ï¼Œä½†ä½œè€…å¹¶æœªä½¿ç”¨å…¨é‡æ•°æ®ï¼Œè€Œæ˜¯é‡‡æ ·äº†è¿ç»­ 8 å¤©æ•°æ®è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚
è¿™ä¸ªé‡çº§æ˜¯å’Œ PaddleRec Criteo æ•°æ®é›†æ˜¯ä¸€æ ·çš„ï¼Œå› æ­¤å¤ç°è¿‡ç¨‹ä¸­ç›´æ¥é€‰æ‹©äº† PaddleRec æä¾›çš„æ•°æ®ã€‚ åŸæ–‡è¡¨è¿°å¦‚ä¸‹ï¼š

![æ•°æ®é›†ä»‹ç»](https://tva1.sinaimg.cn/large/008i3skNly1gtgdgteholj61g40e6af502.jpg)


#### 4. æœ€ä¼˜å¤ç°å‚æ•°

```
  # åŸæ–‡å¤ç°ç›¸å…³å‚æ•°
  att_factor_dim: 80
  att_head_num: 16
  fen_layers_size:  [256, 256, 27]
  class: Adam
  learning_rate: 0.001
  train_batch_size: 2000
  epochs: 2

  # ç®€å•è°ƒèŠ‚ train_batch_size åˆ° 1024ï¼ŒAUC å¯ä»¥ç”± 0.799941 æå‡åˆ° 0.801587
```

#### 5ã€å¤ç°è®°å½•
1. å‚è€ƒ PaddleRec ä¸­ FMï¼Œ å®ç° IFM æ¨¡å‹ï¼Œå…¨é‡ Criteo æµ‹è¯•é›†ä¸Š AUC = 0.8016ï¼›
2. åœ¨ IFM æ¨¡å‹åŸºç¡€ä¸Šï¼Œå¢åŠ  dnn layer å¤„ç† dense features, å…¨é‡ Criteo æµ‹è¯•é›†ä¸Š AUC = 0.8010ï¼›
3. åœ¨ IFM æ¨¡å‹åŸºç¡€ä¸Šï¼Œå¢åŠ  Multi-Head Self Attentionï¼Œå®ç° DIFMï¼›0.799941ï¼›
4. å¢åŠ  Multi-Head Self Attention æ¨¡å—åï¼Œä¼šå¯¼è‡´æ¨¡å‹æ˜¾è‘—è¿‡æ‹Ÿåˆï¼Œéœ€è¦è¿›ä¸€æ­¥ç»†è‡´è°ƒå‚ï¼Œæœ¬é¡¹ç›®å‚æ•°ç›´æ¥å‚è€ƒè®ºæ–‡é»˜è®¤å‚æ•°ï¼Œå¹¶æœªè¿›è¡Œç»†ç²’åº¦å‚æ•°è°ƒä¼˜ï¼›

### ä¸‰ã€å¦‚ä½•åŸºäº PaddleRec å¿«é€Ÿå¤ç°

*P.S. å¦‚æœè¯»è€…ä¸äº†è§£ PaddleRec åŠ¨æ€å›¾è®­ç»ƒåŠæ¨æ–­è¿‡ç¨‹, å¼ºçƒˆå»ºè®®é˜…è¯»ä¸Šä¸€ç¯‡é¡¹ç›®çš„ç›¸å…³éƒ¨åˆ†ä¹‹åå†ç»§ç»­.*

åŸºäº PaddleRec å¿«é€Ÿå¤ç°çš„å…³é”®æ˜¯ net.py æ¨¡å‹ç»„ç½‘. è¿™é‡Œä»‹ç»ä¸€ä¸‹ net.py ä»£ç :

#### 1. MLP

ä¸‹é¢å®ç° MLP å±‚, å¯ä»¥çœ‹åˆ°å’Œ PyTorchã€Tensorflow2 çš„è¯­æ³•éå¸¸æ¥è¿‘, å‡ ä¹å¯ä»¥æ— ç¼åˆ‡æ¢åˆ° PaddlePaddle. å®˜ç½‘ API æ–‡æ¡£ä¸­æœ‰ä¸€å¼ æ˜ å°„è¡¨, å¯ä»¥å‚è€ƒ, [PyTorch-PaddlePaddle APIæ˜ å°„è¡¨](https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/08_api_mapping/pytorch_api_mapping_cn.html)

```Python
class MLPLayer(nn.Layer):
    def __init__(self, input_shape, units_list=None, l2=0.01, last_action=None, **kwargs):
        super(MLPLayer, self).__init__(**kwargs)

        if units_list is None:
            units_list = [128, 128, 64]
        units_list = [input_shape] + units_list

        self.units_list = units_list
        self.l2 = l2
        self.mlp = []
        self.last_action = last_action

        for i, unit in enumerate(units_list[:-1]):
            if i != len(units_list) - 1:
                dense = paddle.nn.Linear(in_features=unit,
                                         out_features=units_list[i + 1],
                                         weight_attr=paddle.ParamAttr(
                                             initializer=paddle.nn.initializer.Normal(std=1.0 / math.sqrt(unit))))
                self.mlp.append(dense)

                relu = paddle.nn.ReLU()
                self.mlp.append(relu)

                norm = paddle.nn.BatchNorm1D(units_list[i + 1])
                self.mlp.append(norm)
            else:
                dense = paddle.nn.Linear(in_features=unit,
                                         out_features=units_list[i + 1],
                                         weight_attr=paddle.nn.initializer.Normal(std=1.0 / math.sqrt(unit)))
                self.mlp.append(dense)

                if last_action is not None:
                    relu = paddle.nn.ReLU()
                    self.mlp.append(relu)

    def forward(self, inputs):
        outputs = inputs
        for n_layer in self.mlp:
            outputs = n_layer(outputs)
        return outputs
```

#### 2. Multi-Head Attention Layer (Vector-wise FEN)

![vector-wise part](https://tva1.sinaimg.cn/large/008i3skNly1guai5unvhwj60fj0dv3zt02.jpg)

è¯»è¿‡è®ºæ–‡çš„åŒå­¦çŸ¥é“, ä¸Šè¿°çš„ vector-wise æ˜¯é€šè¿‡ä¸€ä¸ª Multi-Head Attention æ¨¡å—å’Œ Res-Net æ®‹å·®ç½‘ç»œå®ç°çš„. å…·ä½“å®ç°å¯ä»¥ç›´æ¥çœ‹ä¸‹é¢ğŸ‘‡ä»£ç , è¯¥æ¨¡å—çš„è¾“å‡ºæ˜¯ input embedding å‘é‡çš„æƒé‡.

```python
class MultiHeadAttentionLayer(nn.Layer):
    def __init__(self,
                 att_factor_dim,
                 att_head_num,
                 sparse_feature_dim,
                 sparse_field_num,
                 batch_size):
        super(MultiHeadAttentionLayer, self).__init__()
        self.att_factor_dim = att_factor_dim
        self.att_head_num = att_head_num
        self.sparse_feature_dim = sparse_feature_dim
        self.sparse_field_num = sparse_field_num
        self.batch_size = batch_size

        self.W_Query = paddle.create_parameter(default_initializer=nn.initializer.TruncatedNormal(),
                                               shape=[self.sparse_feature_dim, self.att_factor_dim * self.att_head_num],
                                               dtype='float32')
        self.W_Key = paddle.create_parameter(default_initializer=nn.initializer.TruncatedNormal(),
                                             shape=[self.sparse_feature_dim, self.att_factor_dim * self.att_head_num],
                                             dtype='float32')
        self.W_Value = paddle.create_parameter(default_initializer=nn.initializer.TruncatedNormal(),
                                               shape=[self.sparse_feature_dim, self.att_factor_dim * self.att_head_num],
                                               dtype='float32')
        self.W_Res = paddle.create_parameter(default_initializer=nn.initializer.TruncatedNormal(),
                                             shape=[self.sparse_feature_dim, self.att_factor_dim * self.att_head_num],
                                             dtype='float32')
        self.dnn_layer = MLPLayer(input_shape=(self.sparse_field_num + 1) * self.att_factor_dim * self.att_head_num,
                                  units_list=[self.sparse_field_num + 1],
                                  last_action="relu")

    def forward(self, combined_features):
        """
        combined_features: (batch_size, (sparse_field_num + 1), embedding_size)
        W_Query: (embedding_size, factor_dim * att_head_num)
        (b, f, e) * (e, d*h) -> (b, f, d*h)
        """
        # (b, f, d*h)
        querys = paddle.matmul(combined_features, self.W_Query)
        keys = paddle.matmul(combined_features, self.W_Key)
        values = paddle.matmul(combined_features, self.W_Value)

        # (h, b, f, d) <- (b, f, d)
        querys = paddle.stack(paddle.split(querys, self.att_head_num, axis=2))
        keys = paddle.stack(paddle.split(keys, self.att_head_num, axis=2))
        values = paddle.stack(paddle.split(values, self.att_head_num, axis=2))

        # (h, b, f, f)
        inner_product = paddle.matmul(querys, keys, transpose_y=True)
        inner_product /= self.att_factor_dim ** 0.5
        normalized_att_scores = Fun.softmax(inner_product)

        # (h, b, f, d)
        result = paddle.matmul(normalized_att_scores, values)
        result = paddle.concat(paddle.split(result, self.att_head_num, axis=0), axis=-1)

        # (b, f, h * d)
        result = paddle.squeeze(result, axis=0)
        result += paddle.matmul(combined_features, self.W_Res)

        # (b, f * h * d)
        result = paddle.reshape(result, shape=(self.batch_size, -1))
        m_vec = self.dnn_layer(result)
        return m_vec
```

#### 3. Bit-wise FEN

Bit-wise FEN çš„å®ç°ç›¸å¯¹ç®€å•ä¸€äº›, ç›´æ¥æ‹¼æ¥ input embedding è¾“å…¥åˆ° MLP ä¸­è¿›è¡Œéšå¼äº¤å‰, æœ€åä¸€å±‚ç»´åº¦è®¾ç½®ä¸ºè¾“å…¥ embedding çš„ä¸ªæ•°. å…·ä½“ç›´æ¥æŸ¥çœ‹ `FENLayer` ä¸­çš„ç›¸å…³éƒ¨åˆ†:

```python
# -------------------- fen layer ------------------------------------
    # (batch_size, embedding_size)
    dense_embedding = self.dense_mlp(dense_inputs)
    dnn_logits = self.dnn_mlp(dense_embedding)
    dense_embedding = paddle.unsqueeze(dense_embedding, axis=1)

    # (batch_size, sparse_field_num, embedding_size)
    sparse_embedding = self.sparse_embedding(sparse_inputs_concat)

    # (batch_size, (sparse_field_num + 1), embedding_size)
    feat_embeddings = paddle.concat([dense_embedding, sparse_embedding], axis=1)

    # (batch_size, (sparse_field_num + 1))
    # m_x è¿™é‡Œè¾“å‡ºçš„æ˜¯ç›¸åº”æƒé‡å‘é‡
    m_x = self.fen_mlp(paddle.reshape(feat_embeddings, shape=(self.batch_size, -1)))
```

#### 4. IFM

è¯¥éƒ¨åˆ†ä»‹ç»ä¸€ä¸‹, å¦‚ä½•ç»„ç½‘æ„å»º IFM æ¨¡å‹. input embedding ç»ç”±ä¸Šä¸€éƒ¨åˆ† Bit-wise FEN æ¨¡å—å¾—åˆ°ç›¸åº”çš„æƒé‡å‘é‡, å†å¯¹è¾“å…¥ embedding å‘é‡è¿›è¡Œè°ƒæ•´. æ¥ç€è¾“å…¥åˆ° FM æ¨¡å—ä¸­è¿›è¡Œç‰¹å¾äº¤å‰, æœ€åè¾“å‡ºé¢„æµ‹å€¼.

```
class IFM(nn.Layer):
    def __init__(self,
                 sparse_field_num,
                 sparse_feature_num,
                 sparse_feature_dim,
                 dense_feature_dim,
                 fen_layers_size,
                 dense_layers_size,
                 batch_size):
        super(IFM, self).__init__()
        self.sparse_field_num = sparse_field_num
        self.sparse_feature_num = sparse_feature_num
        self.sparse_feature_dim = sparse_feature_dim
        self.dense_feature_dim = dense_feature_dim
        self.fen_layers_size = fen_layers_size
        self.dense_layers_size = dense_layers_size
        self.batch_size = batch_size
        self.fen_layer = FENLayer(sparse_field_num=self.sparse_field_num,
                                  sparse_feature_num=self.sparse_feature_num,
                                  sparse_feature_dim=self.sparse_feature_dim,
                                  dense_feature_dim=self.dense_feature_dim,
                                  fen_layers_size=self.fen_layers_size,
                                  dense_layers_size=self.dense_layers_size,
                                  batch_size=self.batch_size)
        self.fm_layer = FMLayer()

    def forward(self, sparse_inputs, dense_inputs):
        dnn_logits, feat_emb_one, feat_embeddings, m_x = self.fen_layer(sparse_inputs, dense_inputs)

        m_x = Fun.softmax(m_x)

        # (batch_size, (sparse_field_num + 1))
        feat_emb_one = feat_emb_one * m_x
        # (batch_size, (sparse_field_num + 1), embedding_size)
        feat_embeddings = feat_embeddings * paddle.unsqueeze(m_x, axis=-1)

        # (batch_size, 1)
        first_order = paddle.sum(feat_emb_one, axis=1, keepdim=True)

        return self.fm_layer(dnn_logits, first_order, feat_embeddings)
```

#### 5. DIFM
è¯¥éƒ¨åˆ†ä»‹ç» DIFM çš„ç»„ç½‘æ–¹å¼, ä¸ IFM ç±»ä¼¼, åªæ˜¯ forward ä¸­è¾“å…¥å‘é‡æƒé‡å˜æˆäº† vector-wise å’Œ bit-wise çš„å’Œ, å³ `m = Fun.softmax(m_vec + m_bit)`. å…·ä½“æºç å‚è€ƒ DIFM ä¸­ forward éƒ¨åˆ†, å¦‚ä¸‹:

```python
    def forward(self, sparse_inputs, dense_inputs):
        dnn_logits, feat_emb_one, feat_embeddings, m_bit = self.fen_layer(sparse_inputs, dense_inputs)
        m_vec = self.mha_layer(feat_embeddings)
        # èåˆ vector-wise å’Œ bit-wise ä¸¤éƒ¨åˆ† FEN æƒé‡
        m = Fun.softmax(m_vec + m_bit)

        feat_emb_one = feat_emb_one * m
        feat_embeddings = feat_embeddings * paddle.unsqueeze(m, axis=-1)

        first_order = paddle.sum(feat_emb_one, axis=1, keepdim=True)

        return self.fm_layer(dnn_logits, first_order, feat_embeddings)
```

å…¶ä»–æ¨¡å—, å¦‚ loss ã€metrics ç­‰è®¡ç®—é›†æˆåœ¨ PaddleRec æ¡†æ¶ä¸­, å¯ä»¥ç›´æ¥æŸ¥çœ‹æºç .

#### 6. ç¯å¢ƒä¾èµ–

- ç¡¬ä»¶ï¼šCPUã€GPU
- æ¡†æ¶ï¼š
  - PaddlePaddle >= 2.1.2
  - Python >= 3.7

#### 7ã€ä»£ç ç»“æ„ä¸è¯¦ç»†è¯´æ˜

ä»£ç ç»“æ„éµå¾ª PaddleRec æ¡†æ¶ç»“æ„
```
|--models
  |--rank
    |--difm                   # æœ¬é¡¹ç›®æ ¸å¿ƒä»£ç 
      |--data                 # é‡‡æ ·å°æ•°æ®é›†
      |--config.yaml          # é‡‡æ ·å°æ•°æ®é›†æ¨¡å‹é…ç½®
      |--config_bigdata.yaml  # Kaggle Criteo å…¨é‡æ•°æ®é›†æ¨¡å‹é…ç½®
      |--criteo_reader.py     # datasetåŠ è½½ç±»  
      |--dygraph_model.py     # PaddleRec åŠ¨æ€å›¾æ¨¡å‹è®­ç»ƒç±»
      |--net.py               # difm æ ¸å¿ƒç®—æ³•ä»£ç ï¼ŒåŒ…æ‹¬ difm ç»„ç½‘ã€ifm ç»„ç½‘ç­‰
|--tools                      # PaddleRec å·¥å…·ç±»
|--LICENSE                    # é¡¹ç›® LICENSE
|--README.md                  # readme
|--run.sh                     # é¡¹ç›®æ‰§è¡Œè„šæœ¬(éœ€åœ¨ aistudio notebook ä¸­è¿è¡Œ)
```

#### 8. å¿«é€Ÿå¼€å§‹

ä¹Ÿå¯ä»¥åœ¨ AI-Studio çš„ NoteBook ä¸Š clone ä»£ç , ç›´æ¥ä¸Šæ‰‹è·‘è·‘çœ‹, æ­¥éª¤å¦‚ä¸‹:

- Step 1, git clone code
- Step 2, download data
- Step 3, train model & infer

```
################# Step 1, git clone code ################
# å½“å‰å¤„äº /home/aistudio ç›®å½•, ä»£ç å­˜æ”¾åœ¨ /home/work/rank/DIFM-Paddle ä¸­

import os
if not os.path.isdir('work/rank/DIFM-Paddle'):
    if not os.path.isdir('work/rank'):
        !mkdir work/rank
    !cd work/rank && git clone https://hub.fastgit.org/Andy1314Chen/DIFM-Paddle.git

################# Step 2, download data ################
# å½“å‰å¤„äº /home/aistudio ç›®å½•ï¼Œæ•°æ®å­˜æ”¾åœ¨ /home/data/criteo ä¸­

import os
os.makedirs('data/criteo', exist_ok=True)

# Download  data
if not os.path.exists('data/criteo/slot_test_data_full.tar.gz') or not os.path.exists('data/criteo/slot_train_data_full.tar.gz'):
    !cd data/criteo && wget https://paddlerec.bj.bcebos.com/datasets/criteo/slot_test_data_full.tar.gz
    !cd data/criteo && tar xzvf slot_test_data_full.tar.gz

    !cd data/criteo && wget https://paddlerec.bj.bcebos.com/datasets/criteo/slot_train_data_full.tar.gz
    !cd data/criteo && tar xzvf slot_train_data_full.tar.gz

################## Step 3, train model ##################
# å¯åŠ¨è®­ç»ƒè„šæœ¬ (éœ€æ³¨æ„å½“å‰æ˜¯å¦æ˜¯ GPU ç¯å¢ƒï¼‰
!cd work/rank/DIFM-Paddle && sh run.sh config_bigdata

```

#### 2. criteo slot_test_data_full éªŒè¯é›†ç»“æœ
```
...
2021-08-14 11:53:10,026 - INFO - epoch: 0 done, auc: 0.799622, epoch time: 261.84 s
2021-08-14 11:57:32,841 - INFO - epoch: 1 done, auc: 0.799941, epoch time: 262.81 s
```

#### 3. ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹
- åœ¨ notebook ä¸­åˆ‡æ¢åˆ° `V1.2è°ƒå‚æ•°` ç‰ˆæœ¬ï¼ŒåŠ è½½é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ï¼Œå¯å¿«é€ŸéªŒè¯æµ‹è¯•é›† AUCï¼›
- ï¼ï¼æ³¨æ„ config_bigdata.yaml çš„ `use_gpu` é…ç½®éœ€è¦ä¸å½“å‰è¿è¡Œç¯å¢ƒä¿å­˜ä¸€è‡´
```
!cd /home/aistudio/work/rank/DIFM-Paddle && python -u tools/infer.py -m models/rank/difm/config_bigdata.yaml
```




### å››ã€æ€»ç»“

ç›´æ¥çœ‹ DIFM è®ºæ–‡, å‘ç°è¯¥æ¨¡å‹å…¶å®æ˜¯æ¯”è¾ƒå¤æ‚çš„, åˆæ˜¯ Multi-Head Attention åˆæ˜¯ Res-Net ç­‰, ğŸ˜µâ€ğŸ’«çœ¼èŠ±ç¼­ä¹±çš„. ä½†åªè¦ä»”ç»†æ‹†è§£, ä¼šå‘ç°è®ºæ–‡ä½œè€…å®éªŒå®¤ä¹‹å‰æœ‰ç¯‡ç±»ä¼¼å·¥ä½œ, å¯ä»¥çœ‹ä½œæ˜¯è¯¥ç¯‡è®ºæ–‡çš„å‰ä¼ , IFM.

å°† FMã€IFMã€DIFM ä¸‰ç¯‡è®ºæ–‡ç½‘ç»œç»“æ„æ”¾åœ¨ä¸€èµ·, ä½ ä¼šå‘ç°å¯ä»¥æƒ³å’ç§¯æœ¨ä¸€æ ·, ä¸€ç‚¹ç‚¹è¿›è¡Œæ¨¡å‹ç»„ç½‘. æ¯”å¦‚, FM åŠ ä¸Š bit-wise FEN(å…¶å®æ˜¯ MLP æ¨¡å—) æ„æˆäº† IFM, åœ¨ IFM åŸºç¡€ä¸Š, åŠ ä¸Š vector-wise FEN(å…¶å®æ˜¯ Multi-Head Attention æ¨¡å—)æ„æˆäº† DIFM. è€Œ PaddleRec æ¡†æ¶ä¸­å·²ç»é›†æˆäº†å¾ˆå¤šç±»ä¼¼ FM ä¸€æ ·çš„åŸºç¡€ç»å…¸ CTR æ¨¡å‹.


æ‰€ä»¥, å¤ç°è®ºæ–‡å’Œé˜…è¯»è®ºæ–‡ä¹Ÿæœ‰ä¸€ç‚¹å…±é€šä¹‹å¤„, å³å–„äºæŒ–æ˜è®ºæ–‡çš„åˆ›æ–°ç‚¹(ç›¸å¯¹ baseline, æ”¹è¿›ä¹‹å¤„åœ¨å“ª), åœ¨å‰äººçš„åŸºç¡€ä¸Šå¿«é€Ÿå¤ç°.

### äº”ã€å‚è€ƒèµ„æ–™

1. DIFM [A Dual Input-aware Factorization Machine for CTR Prediction](https://www.ijcai.org/Proceedings/2020/0434.pdf)
2. IFM [An Input-aware Factorization Machine for Sparse Prediction](extension://oikmahiipjniocckomdccmplodldodja/pdf-viewer/web/viewer.html?file=https%3A%2F%2Fwww.ijcai.org%2Fproceedings%2F2019%2F0203.pdf#=&zoom=100)
3. DIFM çš„ Tensorflow å®ç° [https://github.com/shenweichen/DeepCTR/blob/master/deepctr/models/difm.py](https://github.com/shenweichen/DeepCTR/blob/master/deepctr/models/difm.py)
4. IFM çš„ Tensorflow å®ç° [https://github.com/gulyfish/Input-aware-Factorization-Machine/blob/master/code/IFM.py](https://github.com/gulyfish/Input-aware-Factorization-Machine/blob/master/code/IFM.py)
5. PaddleRec æ¡†æ¶ [https://github.com/PaddlePaddle/PaddleRec](https://github.com/PaddlePaddle/PaddleRec)
6. [é£æ¡¨è®ºæ–‡å¤ç°æ‰“å¡è¥](https://aistudio.baidu.com/aistudio/education/group/info/24681)

### å…­ã€å†™åœ¨æœ€å

æœ€å... å¦‚æœå„ä½å¤§ä½¬æ„Ÿè§‰ä¸Šè¿°å†…å®¹æœ‰ç‚¹å„¿å¸®åŠ©, åŠ³çƒ¦å» [github](https://github.com/Andy1314Chen/DIFM-Paddle) ç»™ä¸ª star, çˆ±ä½ å‘¦~ â™¥ï¸


```python

```
