# CBOW训练词向量原理

### 一.整体框架

其核心为**根据上下文的词向量推测中心词**

该框架的神经网络表示如下图：
<div align="center">
    <a>
    	<img src="images\1.png"style="zoom:80%";>  
    </a>
</div>

由图可以看出，该网络包含三层：输入层，隐藏层和输出层。

**1.输入层**：为一个形状为C×V的one-hot张量，其中C代表上下文中词的个数，通常是一个偶；V表示词表大小，该张量的每一行都是一个上下文词的one-hot向量表示。

**2.隐藏层**：一个形状为V×N的参数张量W1，一般称为word-embedding，N表示每个词的词向量长度。输入张量和word embedding W1进行矩阵乘法，就会得到一个形状为C×N的张量。综合考虑上下文中所有词的信息去推理中心词，因此将上下文中C个词相加得一个1×N的向量，是整个上下文的一个隐含表示。

**3.输出层**：创建另一个形状为N×V的参数张量，将隐藏层得到的1×N的向量乘以该N×V的参数张量，得到了一个形状为1×V的向量。最终，1×V的向量代表了使用上下文去推理中心词，每个候选词的打分，再经过softmax函数的归一化，即得到了对中心词的推理概率





### 二.算法流程

#### 1.数据处理

（1）原始数据经过解压读取后，根据词出现的频率为其分配一个独有的id，并构造出三个字典，分别为每个词到id的映射关系，每个id出现的频率和每个id到词的映射关系；

（2）通过二次采样去掉那些出现频率较高的单词，这些一般是冠词，代词等等，基本不能作为中心词；

（3）每个中心词在其前后分别选择两个词作为上下文词，上下文词与中心词构成正样本；进行负采样，除中心词外任选一词为非中心词，上下文词与非中心词构成负样本；

（4）每batch_size个样本构成一组训练数据;这样就构造了多组训练数据

#### 2.网络搭建

（1）初始化网络参数，该参数为两个V×N的参数矩阵W0和W1，分别代表输入上下文词和中心词与词向量的关系矩阵；

（2）用输入的上下文词去查询W0，得到上下文词对应的词向量，并将这几个词向量相加求平均值，得到一个张量；

（3）用最终结果（可能是中心词可能是非中心词）去查询W1，得到中心词（或非中心词）的词向量，其为另一个张量；

（4）将这两个张量进行点乘，并将结果输入到sigmoid函数中进行归一化，便得到预测概率，并结合预先设定的标签进行损失的计算；

（5）最后反向传播并不断进行更新参数便得到我们最后需要的参数矩阵。

#### 3.网络测试

将训练好的网络参数带入其中，并尝试送入几个上下文词，从而得到预测的中心词

例如将上下文词['four','two','five','nine']带入其中，可以得到其预测的中心词为‘one';

将上下文词['economist','son','alfonso','throne']带入其中，可以得到预测的中心词为’king‘。





### 三.与Skip_gram算法区别

CBOW算法与Skip_gram算法均为训练词向量的算法，只不过一个是通过**上下文的词向量推理中心词**，一个是根据中**心词推理上下文**。两种算法可以算是一个相反的过程。

其推理过程的区别如下：

**（1）CBOW：**先在句子中选定一个中心词，并把其它词作为这个中心词的上下文。例如，把“Spiked”作为中心词，把“Pineapples、are、and、yellow”作为中心词的上下文。在学习过程中，使用上下文的词向量推理中心词，这样中心词的语义就被传递到上下文的词向量中，如“Spiked → pineapple”，从而达到学习语义信息的目的。

**（2）Skip-gram**：同样先选定一个中心词，并把其他词作为这个中心词的上下文。把“Spiked”作为中心词，把“Pineapples、are、and、yellow”作为中心词的上下文。不同的是，在学习过程中，使用中心词的词向量去推理上下文，这样上下文定义的语义被传入中心词的表示中，如“pineapple → Spiked”， 从而达到学习语义信息的目的。

另外，一般来说，CBOW比Skip-gram训练速度快，训练过程更加稳定，原因是CBOW使用上下文average的方式进行训练，每个训练step会见到更多样本。而在生僻字（出现频率低的字）处理上，skip-gram比CBOW效果更好，原因是skip-gram不会刻意回避生僻字(CBOW结构中输入中存在生僻字时，生僻字会被其它非生僻字的权重冲淡)。



