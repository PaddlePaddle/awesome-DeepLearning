# Yolov5


## 背景知识集锦

### 目标检测

目标检测（Object Detection）的任务是找出图像中所有感兴趣的目标（物体），确定它们的类别和位置，是计算机视觉领域的核心问题之一。由于各类物体有不同的外观、形状和姿态，加上成像时的光照、遮挡等因素的干扰，目标检测一直是计算机视觉领域最具有挑战性的问题。

1)计算机视觉中关于图像识别有四大类任务：

①分类-Classification：给定一张图片或一段视频判断里面包含什么类别的目标。

②定位-Location：定位出这个目标的的位置。

③检测-Detection：定位出这个目标的位置并且知道目标物是什么。

④分割-Segmentation：分为实例的分割（Instance-level）和场景分割（Scene-level），解决“每一个像素属于哪个目标物或场景”的问题。

2)基本的检测流程为：

①生成候选区域：确定搜索范围；

②提取区域特征：通过卷积神经网络提取特征，将候选区域的特征表示为定长向量；

③对区域进行分类：用候选区域的特征作为输入进行分类，确定是否包含物体及其所属的类别；

④后处理：对重叠较多的框进行合并——希望同一个物体上有且只有一个检测框。


### 重要策略概念

1)**Region Proposal**边界框提案（感兴趣区域, 区域提案, 矩形框提案）

这是一个可能含有一个物体的相对于原图的边界框。

这些边界框可以由一些启发式的搜索算法给出，如：objectness,selective search。也可以由一个区域提案网络（RPN）结合特征图给出。

一个矩形边界框有两种方式表示，一种是给出左上角和右下角的坐标（x0,y0,x1,y1），另一种（更普遍）是给出中心点坐标以及宽度和高度（x,y,w,h）。一个矩形框通常都包含该矩形框中包含一个物体的可能性。

两个矩形框之间的差异通常用代表它们两者的向量的L2距离来表示。其中w和h在计算距离前会先被对数化。
    
2)Intersection over Union(**IoU**, Jaccard similarity)
![](https://ai-studio-static-online.cdn.bcebos.com/e41655e595a643c7a14779209a7b7c6dfc00b81f7d0848b68255a06eaffb98ca)

交并比（Intersection-over-Union，IoU），目标检测中使用的一种度量两个矩形框的相似性的方法，是产生的候选框（candidate bound）与原标记框（ground truth bound）的交叠率，即它们的交集与并集的比值。

3)非极大值抑制 (Non-maximum suppression，**NMS**)

一种去除非极大值的算法，用来合并重叠的矩形框（提案或者是识别结果）。与更高置信度的矩形框高度重叠（IoU>预设的阈值）的矩形框将被抑制（移除）。
![](https://ai-studio-static-online.cdn.bcebos.com/8b02f7fcd285440684a6b05263f2c08d5c57034593ea4cfdacc5951c845cb532)

4)**Bounding box regression**边界框回归 (边界框优化)

即使推荐的矩形框比较小，没有把物体全部包含在里面，但是我们仍然能够通过观察矩形框对应的图案，推测出能更好的包围物体的矩形框。因此，我们可以训练一个回归器，它接收矩形框对应的特征并且预测该矩形框与实际矩形框之间的差异，进而优化矩形框。

回归器有两种，一种是针对特定物体类别的回归器，还有一种是所有类别通用的回归器。矩形框回归器通常伴随着一个矩形框分类器，该分类器用来预测这个矩形框中含有物体的置信度。分类器同样有针对特定类别的和类别通用的两种形式。如果没有定义prior boxes，那么输入的矩形框将扮演它的角色。
![](https://ai-studio-static-online.cdn.bcebos.com/6fdd1edf9e29485499b1e063841d98b0f083d6466e0f4c84b7020f52c5d0b2f4)

5)预设矩形框**Prior box** (default box, anchor box)

我们可以训练多个矩形框回归器，每个回归器都对应自己独立的prior box（预设矩形框），它们以相同的矩形框区域作为输入，然后它们学习预测各自的prior box和ground truth box（真实矩形框）的偏移。

通过这种方式，对应不同prior box的回归器可以学会预测具有不同属性的矩形框（长宽比、尺寸、位置）。prior box可以预先设置好（相对于输入矩形框），也可以通过聚类的方式进行学习。合适的矩形框回归策略对训练的收敛起到非常重要的作用。

6)**Box Matching Strategy**
一个矩形框回归器无法预测一个距离输入区域（或者是prior box）很远的物体的矩形框。因此我们需要一个矩形框匹配策略以确定哪个prior box和ground truth box的重叠度更高。每个成功的匹配都是回归的训练样本。可行的策略有：

·Multibox：将每个ground truth box和它对应的具有最高IoU的prior box匹配（匹配数量=ground truth box的数量）

·SSD,Faster R-CNN：将每个prior box和与它的IoU超过0.5的ground truth box进行匹配（匹配数量>ground truth box的数量,一个ground truth box可能与多个prior box匹配）

7)Hard negative example mining（**负样本挖掘**）

核心思想：用分类器对样本进行分类，把其中错误分类的样本(hard negative)放入负样本集合再继续训练分类器。

对于每一个prior box都有一个边界框分类器用来分析该矩形框中含有物体的可能性。在矩形框匹配之后，每个成功匹配的prior box都是这个分类器的正样本，所有其它prior box都是负样本。

如果我们使用所有这些负样本，那么在正样本和负样本之间存在明显的不平衡。有效的解决方法是：随机选择一些负样本（Faster R-CNN），或者选择那些分类器分类效果最差的样本（SSD），通过这些手段让负样本和正样本之间的比例大概是3:1。

8)选择搜索（**selective search**，简称SS）

从原始图像中获得region proposal的方法。

主要思想：图像中物体可能存在的区域应该是有某些相似性或者连续性区域的。因此，选择搜索基于这一想法采用子区域合并的方法进行提取bounding boxes。首先，对输入图像进行分割算法产生许多小的子区域。其次，根据这些子区域之间相似性(相似性标准主要有颜色、纹理、大小等等)进行区域合并，不断的进行区域迭代合并。每次迭代过程中对这些合并的子区域做bounding boxes(外切矩形)，这些子区域外切矩形就是通常所说的候选框。
![](https://ai-studio-static-online.cdn.bcebos.com/ea04281e57ef49329e67240f3f070b95c6525898aa714b5fa2559d5231862307)

### 算法分类：

基于深度学习的目标检测算法主要分为两类：Two stage和One stage。

1）**Tow Stage**

①先生成可能包含物体的候选区域（Region Proposal）；

②通过卷积神经网络进行对候选区域进一步分类和校准，得到最后的检测结果。

任务流程：特征提取 --> 生成RP --> 分类/定位回归。

常见tow stage目标检测算法有：R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN和R-FCN等。

2）**One Stage**

①直接给出最终的检测结果，直接在网络中提取特征来预测物体分类和位置；

②没有显式地生成候选区域的步骤。

任务流程：特征提取–> 分类/定位回归。

常见的one stage目标检测算法有：OverFeat、YOLOv1、YOLOv2、YOLOv3、YOLOv4、YOLOv5、SSD和RetinaNet等。

## YOLO以往版本回顾

**略讲部分：**

1）**YOLOv1**（2016）

① 基本思想

YOLO（You Only Look Once ）是继RCNN，fast-RCNN和faster-RCNN之后，Ross Girshick针对DL目标检测速度问题提出的另一种框架，其核心思想是**生成RoI+目标检测两阶段（two-stage）算法用一套网络的一阶段（one-stage）算法替代，直接在输出层回归bounding box的位置和所属类别**。

![](https://ai-studio-static-online.cdn.bcebos.com/1439c70e3cb74da1b126a8f93c6363c477dd9fd8261a4e98b2cbf5db9e44c9e0)

之前的物体检测方法（two stages）首先需要产生大量可能包含待检测物体的先验框, 然后用分类器判断每个先验框对应的边界框里是否包含待检测物体，以及物体所属类别的概率或者置信度，同时需要后处理修正边界框，最后基于一些准则过滤掉置信度不高和重叠度较高的边界框，进而得到检测结果。这种基于先产生候选区再检测的方法虽然有相对较高的检测准确率，但运行速度较慢。

YOLO创造性的将物体检测任务直接当作回归问题（regression problem）来处理，将候选区和检测两个阶段合二为一。只需一眼就能知道每张图像中有哪些物体以及物体的位置。

实际上，YOLO并没有真正去掉候选区，而是采用了预定义候选区的方法，也就是将图片划分为7*7个网格，每个网格允许预测出2个边框，总共49*2个bounding box，可以理解为98个候选区域，它们很粗略地覆盖了图片的整个区域。YOLO以降低mAP为代价，大幅提升了时间效率。

![](https://ai-studio-static-online.cdn.bcebos.com/9ba298422673408fa24552614a156d436422afcbe54a4c618af238fb1424a79e)


② 优点

·YOLO检测物体速度非常快，其增强版GPU中能跑45fps（frame per second），简化版155fps
·YOLO在训练和测试时都能看到一整张图的信息（而不像其它算法看到局部图片信息），因此YOLO在检测物体是能很好利用上下文信息，从而不容易在背景上预测出错误的物体信息
·YOLO可以学到物体泛化特征

③ 缺点

·精度低于其它state-of-the-art的物体检测系统
·容易产生定位错误
·对小物体检测效果不好，尤其是密集的小物体，因为一个栅格只能检测2个物体
·由于损失函数的问题，定位误差是影响检测效果的主要原因，尤其是大小物体处理上还有待加强

2）**YOLOv2**（2016）

Ross Girshick吸收fast-RCNN和SSD算法，设计了YOLOv2（论文原名《YOLO9000: Better, Faster, Stronger 》），在精度上利用一些列训练技巧，在速度上应用了新的网络模型DarkNet19，在分类任务上采用联合训练方法，结合wordtree等方法，使YOLOv2的检测种类扩充到了上千种，作者在论文中称可以检测超过9000个目标类别，所以也称YOLO9000. YOLOv2模型可以以不同的尺寸运行，从而在速度和准确性之间提供了一个简单的折中，在67FPS时，YOLOv2在VOC 2007上获得了76.8 mAP。在40FPS时，YOLOv2获得了78.6 mAP，比使用ResNet的Faster R-CNN和SSD等先进方法表现更出色，同时仍然运行速度显著更快。

**精讲部分：**

### YOLOv3（2018）

YOLOv3总结了自己在YOLOv2的基础上做的一些尝试性改进，有的尝试取得了成功，而有的尝试并没有提升模型性能。其中有两个值得一提的亮点，一个是使用残差模型，进一步加深了网络结构；另一个是使用FPN架构实现多尺度检测。

残差概念简要回顾：
https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/classification/ResNet.html

YOLOv3算法的**基本思想**可以分成两部分：

按一定规则在图片上产生一系列的候选区域，然后根据这些候选区域与图片上物体真实框之间的位置关系对候选区域进行标注。跟真实框足够接近的那些候选区域会被标注为正样本，同时将真实框的位置作为正样本的位置目标。偏离真实框较大的那些候选区域则会被标注为负样本，负样本不需要预测位置或者类别。

使用卷积神经网络提取图片特征并对候选区域的位置和类别进行预测。这样每个预测框就可以看成是一个样本，根据真实框相对它的位置和类别进行了标注而获得标签值，通过网络模型预测其位置和类别，将网络预测值和标签值进行比较，就可以建立起损失函数。

![](https://ai-studio-static-online.cdn.bcebos.com/1426ae6116bc45b8901e0aa024dfeeaad11fee30e00d47039038b9dd6e3ddc8e)

上图三个蓝色方框内表示Yolov3的三个**基本组件**：

（1）**CBL**：Yolov3网络结构中的最小组件，由Conv+Bn+Leaky_relu激活函数三者组成。

（2）**Res unit**：借鉴Resnet网络中的残差结构，让网络可以构建的更深。

（3）**ResX**：由一个CBL和X个残差组件构成，是Yolov3中的大组件。每个Res模块前面的CBL都起到下采样的作用，因此经过5次Res模块后，得到的特征图是608->304->152->76->38->19大小。

**基础操作**：

（1）**Concat**： 张量拼接，会扩充两个张量的维度，例如26×26×256和26×26×512两个张量拼接，结果是26×26×768。Concat和cfg文件中的route功能一样。

（2）**Add**：张量相加，张量直接相加，不会扩充维度，例如104×104×128和104×104×128相加，结果还是104×104×128。add和cfg文件中的shortcut功能一样。


### YOLO4

YOLO4和YOLO3相比，其在概念思维上的突破并不大，但是，由于使用了当时新推出的多个成果，并成功的嵌入在网络结构的各个细节中，在模型性能上完成了很大的提升，故该成果也得到广泛认同。YOLO V4是YOLO系列一个重大的更新，其在COCO数据集上的平均精度(AP)和帧率精度(FPS)分别提高了10% 和12%，并得到了Joseph Redmon的官方认可，被认为是当前最强的实时对象检测模型之一。

作者用了的调优手段：加权残差连接（WRC）；跨阶段部分连接（CSP）；跨小批量标准化（CmBN）；自对抗训练（SAT）；Mish激活；马赛克数据增强；CmBN；DropBlock；正则化CIoU Loss等

这篇文章的贡献如下:

·开发了一个高效、强大的目标检测模型。它使每个人都可以使用1080 Ti或2080 TiGPU来训练一个超级快速和准确的目标探测器。

·验证了在检测器训练过程中，最先进的Bag-of-Freebies和Bag-of-Specials 的目标检测方法的影响。

·修改了最先进的方法，使其更有效，更适合于单GPU训练，包括CBN、PAN、SAM等。

![](https://ai-studio-static-online.cdn.bcebos.com/12344e37fb92496b9b9f16719a263c17f76cb1cc565941f0918bfd6dcc08963b)

Yolov4的结构图和Yolov3相比，因为多了CSP结构，PAN结构,整体架构和Yolov3是相同的，不过使用各种新的算法思想对各个子结构都进行了改进。

**五个基本组件**

1. **CBM**：Yolov4网络结构中的最小组件，由Conv+Bn+Mish激活函数三者组成。

2. **CBL**：由Conv+Bn+Leaky_relu激活函数三者组成。

3. **Res unit**：借鉴Resnet网络中的残差结构，让网络可以构建的更深。

4. **CSPX**：借鉴CSPNet网络结构，由卷积层和X个Res unint模块Concat组成。

5. **SPP**：采用1×1，5×5，9×9，13×13的最大池化的方式，进行多尺度融合。

**基础操作**：

（1）**Concat**： 张量拼接，会扩充两个张量的维度，例如26×26×256和26×26×512两个张量拼接，结果是26×26×768。Concat和cfg文件中的route功能一样。

（2）**Add**：张量相加，张量直接相加，不会扩充维度，例如104×104×128和104×104×128相加，结果还是104×104×128。add和cfg文件中的shortcut功能一样。

**YoloV4的创新之处**

（1）**输入端**：这里指的创新主要是训练时对输入端的改进，主要包括Mosaic数据增强、cmBN、SAT自对抗训练。

（2）**BackBone主干网络**：将各种新的方式结合起来，包括：CSPDarknet53、Mish激活函数、Dropblock

（3）**Neck**：目标检测网络在BackBone和最后的输出层之间往往会插入一些层，比如Yolov4中的SPP模块、FPN+PAN结构

（4）**Prediction**：输出层的锚框机制和Yolov3相同，主要改进的是训练时的损失函数CIOU_Loss，以及预测框筛选的nms变为DIOU_nms

## YOLO5具体介绍

YOLO4基于YOLO3的思维方式和框架建设，通过对网络的各个细节使用先进方法进行调优。YOLO5也是在YOLO4之上进行某一方向特性的优化得到的。YOLO V5 在性能上稍弱于YOLO V4，但是在灵活性与速度上远强于YOLO V4，在模型的快速部署上具有极强优势。

从上面对以往版本的回顾可以知道，YOLO系列的网络优化的突破点在于对其主要结构采用不同的处理方法，不同的对接方法。

其优化点在于：

	输入端，即数据增强的方法，不同的数据增强的方法能够有针对性地对模型的训练过程产生帮助；

	骨干网络Backbone：即特征提取过程中发挥作用的主干网络部分，比如YOLO3中使用DarkNet53来作为主干提取目标特征；

	Neck：提取特征后在应用到预测功能之前，需要对特征图所进行特殊处理，提取融合特征以达成更好的预测效果。比如Yolov4中的SPP模块、FPN+PAN结构；

	Prediction：优化过程中对模型进行参数学习和调整，以得到最终的预测模型。然而，不同的损失计算方法会带来对优化过程的不同效果，即影响参数调整，影响最终的预测效果。在这个阶段，需要对损失函数进行全面的考虑。同时，目标框的筛选直接的对于检测结果造成影响，所以这也是需要进行优化的点。

以下是YOLO5的创新点一览（在后面进行具体介绍）：

	（1）输入端：Mosaic数据增强、自适应锚框计算

	（2）Backbone：Focus结构，CSP结构

	（3）Neck：FPN+PAN结构

	（4）Prediction：GIOU_Loss 

###  一、输入端：

#### Mosaic数据增强

Yolov4中使用的Mosaic是参考2019年底提出的CutMix数据增强的方式，但CutMix只使用了两张图片进行拼接，而Mosaic数据增强则采用了4张图片，随机缩放、随机裁剪、随机排布的方式进行拼接。
![](https://ai-studio-static-online.cdn.bcebos.com/e025f0b934e04b8097bedb3049c60a326980aa107a3a4d8ca3e09766e02c86e1)

这里首先要了解为什么要进行Mosaic数据增强呢？

在平时项目训练时，小目标的AP一般比中目标和大目标低很多。而Coco数据集中也包含大量的小目标，但比较麻烦的是**小目标的分布并不均匀**。

Coco数据集中小目标占比达到41.4%，数量比中目标和大目标都要多。但在所有的训练集图片中，只有52.3%的图片有小目标，而中目标和大目标的分布相对来说更加均匀一些。

针对这种状况，Yolov4的作者采用了Mosaic数据增强的方式。

主要有几个优点：

a. **丰富数据集**：随机使用4张图片，随机缩放，再随机分布进行拼接，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好。

b. **减少GPU使用**：可能会有人说，随机缩放，普通的数据增强也可以做，但作者考虑到很多人可能只有一个GPU。因此Mosaic增强训练时，可以直接计算4张图片的数据，使得Mini-batch大小并不需要很大，一个GPU就可以达到比较好的效果。


#### 自适应锚框计算
在Yolo算法中，针对不同的数据集，都会有初始设定长宽的锚框。

在网络训练中，网络在初始锚框的基础上输出预测框，进而和真实框groundtruth进行比对，计算两者差距，再反向更新，迭代网络参数。因此初始锚框也是比较重要的一部分。

在Yolov3、Yolov4中，训练不同的数据集时，计算初始锚框的值是通过单独的程序运行的。但Yolov5中将此功能嵌入到代码中，每次训练时，自适应的计算不同训练集中的最佳锚框值。


#### 自适应图片缩放

在常用的目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。

但Yolov5代码中对此进行了改进，作者认为，在项目实际使用时，很多图片的长宽比不同。
因此缩放填充后，两端的黑边大小都不同，而如果填充的比较多，则存在信息冗余，影响推理速度。
因此在Yolov5代码中datasets.py的letterbox函数中进行了修改，对原始图像自适应的添加最少的黑边。

### 二、Backbone

#### Focus结构

在Yolov3&Yolov4中并没有这个结构，其中比较关键是切片操作。
![](https://ai-studio-static-online.cdn.bcebos.com/1144dead38124582a3444d4422625ea073dbfbf1d9ff4bc8b151bd3085a3751f)

在切片示意图，4×4×3的图像切片后变成2×2×12的特征图。


#### CSP结构

CSPNet全称是Cross Stage Paritial Network，主要从网络结构设计的角度解决推理中从计算量很大的问题。（论文可见：https://arxiv.org/pdf/1911.11929.pdf）

**CSPNet的作者认为推理计算过高的问题是由于网络优化中的梯度信息重复导致的。**

CSPNet解决了其他大型卷积神经网络框架Backbone中网络优化的梯度信息重复问题，将梯度的变化从头到尾地集成到特征图中，因此减少了模型的参数量和FLOPS数值，既保证了推理速度和准确率，又减小了模型尺寸。

CSPNet实际上是基于Densnet的思想，复制基础层的特征映射图，通过dense block 发送副本到下一个阶段，从而将基础层的特征映射图分离出来。这样可以有效缓解梯度消失问题(通过非常深的网络很难去反推丢失信号) ，支持特征传播，鼓励网络重用特征，从而减少网络参数数量。

因此采用CSP模块先将基础层的特征映射划分为两部分，然后通过跨阶段层次结构将它们合并，在减少了计算量的同时可以保证准确率。
因此Yolov4在主干网络Backbone采用CSPDarknet53网络结构，主要有三个方面的优点：

优点一：增强CNN的学习能力，使得在轻量化的同时保持准确性。
优点二：降低计算瓶颈
优点三：降低内存成本

Yolov5与Yolov4不同点在于，Yolov4中只有主干网络使用了CSP结构，而Yolov5中设计了两种CSP结构，以Yolov5s网络为例，以CSP1_X结构应用于Backbone主干网络，另一种CSP2_X结构则应用于Neck中。


###  三、Neck——FPN+PAN
Neck主要用于生成特征金字塔。特征金字塔会增强模型对于不同缩放尺度对象的检测，从而能够识别不同大小和尺度的同一个物体。在PANET出来之前，FPN一直是对象检测框架特征聚合层的State of the art，直到PANET的出现。在YOLO V4的研究中，PANET被认为是最适合YOLO的特征融合网络，因此YOLO V5和V4都使用PANET作为Neck来聚合特征。

![](https://ai-studio-static-online.cdn.bcebos.com/a6937651b72a4be3b54e15ff2d4a9809a9c3cc205c3a4bdea978f772ee5a6cc6)

**FPN：**

在下采样过程中不断提取和精简特征图，

FPN是自顶向下地将而特征信息通过上采样的方式进行传递融合，得到进行预测的特征图

**PAN：**

在FPN层的后面还添加了一个自底向上的特征金字塔，其中包含两个PAN结构。FPN层自顶向下传达强语义特征，而特征金字塔则自底向上传达强定位特征，两两联手，从不同的主干层对不同的检测层进行特征聚合。

Yolov5和Yolov4的不同点在于，Yolov4的Neck中，采用的都是普通的卷积操作，而Yolov5的Neck结构中，采用借鉴CSPNet设计的CSP2结构，加强网络特征融合的能力。

### 四、Prediction

好的目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比。

![](https://ai-studio-static-online.cdn.bcebos.com/293cf887c71749ab807c481d0339019f3aa257238e844c5cbb53219ced6adde9)

Yolov5中采用其中的GIOU_Loss做Bounding box的损失函数。

### 五、优点
	YOLO V5具有以下显著的优点：

	使用Pytorch框架，对用户非常友好，能够方便地训练自己的数据集，相对于YOLO V4采用的Darknet框架，Pytorch框架更容易投入生产

	代码易读，整合了大量的计算机视觉技术，非常有利于学习和借鉴

	不仅易于配置环境，模型训练也非常快速，并且批处理推理产生实时结果

	能够直接对单个图像，批处理图像，视频甚至网络摄像头端口输入进行有效推理

	能够轻松的将Pytorch权重文件转化为安卓使用的ONXX格式，然后可以转换为OPENCV的使用格式，或者通过CoreML转化为IOS格式，直接部署到手机应用端

	YOLO V5s高达140FPS的对象识别速度令人印象非常深刻，使用体验非常棒

## 参考：

【1】You Only Look Once:Unified, Real-Time Object Detection

【2】YOLO9000:Better, Faster, Stronger

【3】YOLOv3: An Incremental Improvement

【4】YOLOv4: Optimal Speed and Accuracy of Object Detection

【5】 https://github.com/ultralytics/yolov5

【6】https://aistudio.baidu.com/aistudio/projectdetail/2188940

【7】https://blog.csdn.net/nan355655600/article/details/106246625

【8】https://blog.csdn.net/nan355655600/article/details/107852353

【9】https://zhuanlan.zhihu.com/p/161083602
