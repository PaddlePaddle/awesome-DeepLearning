# 简介

## 1. 任务说明
我们的目标使用RNN来建立一个语言模型，例如一个句子有m个单词，一个语言模型能够预测观测到句子的可能性，也就是这句话成立的可能性：
![avatar](https://www.zhihu.com/equation?tex=P%28w_%7B1%7D%2Cw_%7B2%7D%2Cw_%7B3%7D%2C...w_%7Bm%7D%29%3D%5Cprod_%7Bi%3D1%7D%5E%7Bm%7DP%28w_%7Bi%7D%7Cw_%7B1%7D%2C...w_%7Bi-1%7D%29+%5Ctag%7B1%7D)
可以看出就是每个词在已有短语的前提下出现的概率，例如一句话“He went to buy some chocolate”的概率就是在“chocolate”在给定“He went to buy some”下出现的概率，乘以“some”在给定“He went to buy”下出现的概率，依次类推。

## 2. 数据集
为了训练语言模型我们需要文本来进行学习，这里我们的数据集是自建语料库构成的，存放在train.txt文件中


## 3. 预处理
* 根据语料库构建字典，其中字典的容量最大为1000个字
* 对一句语料进行分词，并根据字典将文本转换为向量
* 拆分语料，构建训练集和相应的标签，训练语句的开头是特殊字符<START>
* 对数据集进行batch拆分


## 4.训练
任务训练启动命令如下：
```
!python main.py
```


# 模型原理介绍

## 1. RNN语言模型
RNN是一个序列模型，基本思路是：在时刻t，将前一时刻t−1的隐藏层输出和t时刻的词向量一起输入到隐藏层从而得到时刻t的特征表示，然后用这个特征表示得到t时刻的预测输出，如此在时间维上递归下去。可以看出RNN善于使用上文信息、历史知识，具有“记忆”功能。理论上RNN能实现“长依赖”（即利用很久之前的知识），但在实际应用中发现效果并不理想，研究提出了LSTM和GRU等变种，通过引入门机制对传统RNN的记忆单元进行了改进，弥补了传统RNN在学习长序列时遇到的难题。本例模型使用了LSTM或GRU，可通过配置进行修改。下图是RNN（广义上包含了LSTM、GRU等）语言模型“循环”思想的示意图：
![avatar](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9sNk1qU0ZUaWNpYllMQXZuZW5DSWtpYUxwcng0UmljZWlhZlU5dHZPTWdFQUduM3NMdEpKa2ZpY3RTV0szTTlxVGlhRFRSR3JYUmxtZE5OYzlCQTNPdVZpYkxpY3BOdy82NDA?x-oss-process=image/format,png)



## 2. 模型实现
 
**定义模型结构：**

* 输入层：将输入的词（或字）序列映射成向量，即词向量层： embedding。

* 中间层：根据配置实现RNN层，将上一步得到的embedding向量序列作为输入。

* 输出层：使用softmax归一化计算单词的概率。

* loss：定义多类交叉熵作为模型的损失函数。

**训练模型：**

* 准备输入数据：建立并保存词典、构建train和test数据的reader。

* 初始化模型：包括模型的结构、参数。

* 构建训练器：demo中使用的是Adam优化算法。

* 定义回调函数：构建event_handler来跟踪训练过程中loss的变化，并在每轮训练结束时保存模型的参数。

* 训练：使用trainer训练模型。

**生成文本：**

* 加载训练好的模型和词典文件。

* 读取一个句子，并在句子的开头加上特殊字符<START>

* 生成的文本的下个字