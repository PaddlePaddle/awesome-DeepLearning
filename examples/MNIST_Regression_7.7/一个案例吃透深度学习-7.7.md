# 一个案例吃透深度学习-7.7

## 损失函数方法
一般来说，我们在进行机器学习任务时，使用的每一个算法都有一个目标函数，算法便是对这个目标函数进行优化，特别是在分类或者回归任务中，便是使用损失函数（Loss Function）作为其目标函数，又称为代价函数(Cost Function)。
损失函数是用来评价模型的预测值 $\hat{Y}=f(X)$ 与真实值 $Y$ 的不一致程度，它是一个非负实值函数。通常 使用 $L(Y, f(x))$ 来表示，损失函数越小, 模型的性能就越好。 设总有 $N$ 个样本的样本集为 $(X, Y)=\left(x_{i}, y_{i}\right), y_{i}, i \in[1, N]$ 为样本 $i$ 的真实值, $\hat{y}_{i}=f\left(x_{i}\right), i \in[1, N]$ 为样本 $i$ 的预测值， $f$ 为分类或者回归函数。 那么总的损失函数为：
$$
L=\sum_{i=1}^{N} \ell\left(y_{i}, \hat{y}_{i}\right)
$$
### KLdiv Loss
```
nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)
```
$\mathrm{KL}$ 散度，又叫做相对商，算的是两个分布之间的距离，越相似则越接近零。
$$
\operatorname{loss}(\mathbf{x}, \mathbf{y})=\frac{1}{N} \sum_{i=1}^{N}\left[\mathbf{y}_{i} *\left(\log \mathbf{y}_{i}-\mathbf{x}_{i}\right)\right]
$$
### Hinge Loss
```
nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')
```
Hinge损失可以用来解决间隔最大化问题，如在SVM中解决几何间隔最大化问题，其定义如下：
$$
\begin{gathered}
\ell\left(y_{i}, \hat{y}_{i}\right)=\max \left\{0,1-y_{i} \cdot \hat{y}_{i}\right\} \\
y_{i} \in\{-1,+1\}
\end{gathered}
$$
### 正则
```
nn.L1Loss(size_average=None, reduce=None, reduction='mean')
nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=1.0)
```
一般来说，对分类或者回归模型进行评估时，需要使得模型在训练数据上使得损失函数值最小， 即使得经验风险函数最小化, 但是如果只考虑经验风险(Empirical risk)，容易过拟合, 因此还需要考虑模型的泛化能力，一般常用的方法便是在目标函数中加上正则项，由损失项(Loss term)加上正则 项(Regularization term)构成结构风险(Structural risk)，那么损失函数变为:
$$
L=\sum_{i=1}^{N} \ell\left(y_{i}, \hat{y}_{i}\right)+\lambda \cdot R(\omega)
$$
其中 $\lambda$ 是正则项超参数，常用的正则方法包括： L1正则与L2正则。

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/d77e0ff2817a4ad5aebefe3872e1203f882286f9bb5f4c21a255f050b5c975f4" width="900" hegiht="" ></center>
<center><br>图1：损失函数图形</br></center>
<br></br>

## 池化方法补充
### 随机池化(Stochastic Pooling)
Stochastic pooling是一种简单有效的正则化CNN的方法，能够降低max pooling的过拟合现象，提高泛化能力。对于pooling层的输入，根据输入的多项式分布随机选择一个值作为输出。训练阶段和测试阶段的操作略有不同。
* 训练阶段：
1）前向传播：先将池化窗口中的元素全部除以它们的和，得到概率矩阵；再按照概率随机选中的方格的值，作为该区域池化后的值。
2）反向传播：求导时，只需保留前向传播中已经被选中节点的位置的值，其它值都为0，类似max-pooling的反向传播。
* 测试阶段：
在测试时也使用Stochastic Pooling会对预测值引入噪音，降低性能。取而代之的是使用概率矩阵加权平均。比使用Average Pooling表现要好一些。在平均意义上，与Average Pooling近似，在局部意义上，服从Max Pooling准则。

### 重叠池化(Overlapping Pooling)
重叠池化，即相邻池化窗口之间会有重叠区域。如果定义池化窗口的大小为sizeX，定义两个相邻池化窗口的水平位移 / 竖直位移为stride，此时sizeX>stride。
Alexnet中提出和使用，不仅可以提升预测精度，同时一定程度上可以减缓过拟合。相比于正常池化（步长s=2，窗口x=2），重叠池化(步长s=2，窗口x=3) 可以减少top-1, top-5的错误率分别为0.4% 和0.3%。

### 全局池化(Global Pooling)
Global Pooling就是池化窗口的大小 = 整张特征图的大小。这样，每个 W×H×C 的特征图输入就会被转化为 1×1×C 的输出，也等同于每个位置权重都为 1/(W×H) 的全连接层操作。

## 数据增强方法修改和补充
### 裁剪
做裁剪操作主要是考虑原始图像的宽高扰动，在大多数图像分类网络中，样本在输入网络前必须要统一大小，所以通过调整图像的尺寸可以大量的扩展数据。
通过裁剪有两种扩种方式，一种是对大尺寸的图像直接按照需要送入网络的尺寸进行裁剪，比如原始图像的分辨率大小是256x256，现在网络需要输入的图像像素尺寸是224x224，这样可以直接在原始图像上进行随机裁剪224x224像素大小的图像即可，这样一张图可以扩充32x32张图片；
另外一种是将随机裁剪固定尺寸大小的图片，然后再将图像通过插值算法调整到网络需要的尺寸大小。由于数据集中通常数据大小不一，后者通常使用的较多。
### 翻转和旋转
翻转和旋转都是将原始的图像像素在位置空间上做变换，图像的翻转是将原始的图像进行镜像操作，镜像操作在数据增强中会经常被使用，并且起了非常重要的作用，它主要包括水平镜像翻转，垂直镜像翻转和原点镜像翻转，具体在使用中，需要结合数据形式选择相应翻转操作，比如数据集是汽车图像数据，训练集合测试集都是正常拍摄的图片，此时只使用水平镜像操作，如果加入垂直或者原点镜像翻转，会对原始图像产生干扰。
角度旋转操作和图像镜像相对，它主要是沿着画面的中心进行任意角度的变换，该变换是通过将原图像和仿射变换矩阵相乘实现的。为了实现图像的中心旋转，除了要知道旋转角度，还要计算平移的量才能能让仿射变换的效果等效于旋转轴的画面中心。
### 缩放
图像可以向外或向内缩放。向外缩放时，最终图像尺寸将大于原始图像尺寸，为了保持原始图像的大小，通常需要结合裁剪，从缩放后的图像中裁剪出和原始图像大小一样的图像。另一种方法是向内缩放，它会缩小图像大小，缩小到预设的大小。缩放也会带了一些问题，如缩放后的图像尺寸和原始图像尺寸的长宽比差异较大，会出现图像失帧的现象，如果在实验中对最终的结果有一定的影响，需要做等比例缩放，对不足的地方进行边缘填充。
### 移位
移位只涉及沿X或Y方向（或两者）移动图像，如果图像的背景是单色被背景或者是纯的黑色背景，使用该方法可以很有效的增强数据数量。
### 高斯噪声
当神经网络试图学习可能无用的高频特征（即图像中大量出现的模式）时，通常会发生过度拟合。具有零均值的高斯噪声基本上在所有频率中具有数据点，从而有效地扭曲高频特征。这也意味着较低频率的组件也会失真，但神经网络可以学会超越它，添加适量的噪音可以增强学习能力。
基于噪声的数据增强就是在原来的图片的基础上，随机叠加一些噪声，最常见的做法就是高斯噪声。更复杂一点的就是在面积大小可选定、位置随机的矩形区域上丢弃像素产生黑色矩形块，从而产生一些彩色噪声，以Coarse Dropout方法为代表，甚至还可以对图片上随机选取一块区域并擦除图像信息。

## 图像分类方法综述
### 图像分类概念
从给定的分类集合中给图像分配一个标签的任务。
### 图像分类的应用

* 网络图像检索
* 视频分析与检索
* 医学图像分类
* 医学图像数据挖掘
* 图像检测
* 遥感图像分类

### 传统图像分类算法流程
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/2ec6430e4a8c480c86aa6c59f82169b17a833e3f227345bcbdd80248f57aba98" width="900" hegiht="" ></center>
<center><br>图2：传统图像分类算法流程</br></center>
<br></br>

1. 底层特征提取: 通常从图像中按照固定步长、尺度提取大量局部特征描述。常用的局部特征包括SIFT(Scale-Invariant Feature Transform, 尺度不变特征转换) 、HOG(Histogram of Oriented Gradient, 方向梯度直方图) 、LBP(Local Bianray Pattern, 局部二值模式)等，一般也采用多种特征描述，防止丢失过多的有用信息。

1. 特征编码: 底层特征中包含了大量冗余与噪声，为了提高特征表达的鲁棒性，需要使用一种特征变换算法对底层特征进行编码，称作特征编码。常用的特征编码方法包括向量量化编码、稀疏编码、局部线性约束编码、Fisher向量编码等。

1. 空间特征约束: 特征编码之后一般会经过空间特征约束，也称作特征汇聚。特征汇聚是指在一个空间范围内，对每一维特征取最大值或者平均值，可以获得一定特征不变形的特征表达。金字塔特征匹配是一种常用的特征汇聚方法，这种方法提出将图像均匀分块，在分块内做特征汇聚。

1. 通过分类器分类: 经过前面步骤之后一张图像可以用一个固定维度的向量进行描述，接下来就是经过分类器对图像进行分类。通常使用的分类器包括SVM(Support Vector Machine, 支持向量机)、随机森林等。而使用核方法的SVM是最为广泛的分类器，在传统图像分类任务上性能很好。

### 深度学习算法
* CNN
* VGG
* GoogLeNet
* ResNet
