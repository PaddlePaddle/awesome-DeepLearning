# <center>**深度学习基础题**</center>

**<p align="right">王峰、闫雨泽、张森源</p>** 


---

## 层次softmax

1、基于Hierarchical Softmax的模型概述

传统的神经网络词向量语言模型，里面一般有三层，输入层（词向量），隐藏层和输出层（softmax层）。里面最大的问题在于从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值。这个模型如下图所示。其中V是词汇表的大小，

word2vec对这个模型做了改进，首先，对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。比如输入的是三个4维词向量：(1,2,3,4),(9,6,11,8),(5,10,7,12),那么我们word2vec映射后的词向量就是(5,6,7,8)。由于这里是从多个词向量变成了一个词向量。

第二个改进就是从隐藏层到输出的softmax层这里的计算量个改进。为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。我们在上一节已经介绍了霍夫曼树的原理。如何映射呢？这里就是理解word2vec的关键所在了。

由于我们把之前所有都要计算的从输出softmax层的概率计算变成了一颗二叉霍夫曼树，那么我们的softmax概率计算只需要沿着树形结构进行就可以了。如下图所示，我们可以沿着霍夫曼树从根节点一直走到我们的叶子节点的词w2。

和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种softmax取名为"Hierarchical Softmax"。

如何“沿着霍夫曼树一步步完成”呢？在word2vec中，我们采用了二元逻辑回归的方法，即规定沿着左子树走，那么就是负类(霍夫曼树编码1)，沿着右子树走，那么就是正类(霍夫曼树编码0)。判别正类和负类的方法是使用sigmoid函数，即：
P(+)=σ(xTwθ)=11+e−xTwθ
其中xw是当前内部节点的词向量，而θ则是我们需要从训练样本求出的逻辑回归的模型参数。

使用霍夫曼树有什么好处呢？首先，由于是二叉树，之前计算量为V,现在变成了log2V。第二，由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到，这符合我们的贪心优化思想。

容易理解，被划分为左子树而成为负类的概率为P(−)=1−P(+)。在某一个内部节点，要判断是沿左子树还是右子树走的标准就是看P(−),P(+)谁的概率值大。而控制P(−),P(+)谁的概率值大的因素一个是当前节点的词向量，另一个是当前节点的模型参数θ。

对于上图中的w2，如果它是一个训练样本的输出，那么我们期望对于里面的隐藏节点n(w2,1)的P(−)概率大，n(w2,2)的P(−)概率大，n(w2,3)的P(+)概率大。

回到基于Hierarchical Softmax的word2vec本身，我们的目标就是找到合适的所有节点的词向量和所有内部节点θ, 使训练样本达到最大似然。那么如何达到最大似然呢？

2. 基于Hierarchical Softmax的模型梯度计算
我们使用最大似然法来寻找所有节点的词向量和所有内部节点θ。先拿上面的w2例子来看，我们期望最大化下面的似然函数：
∏i=13P(n(wi),i)=(1−11+e−xTwθ1)(1−11+e−xTwθ2)11+e−xTwθ3
对于所有的训练样本，我们期望最大化所有样本的似然函数乘积。

为了便于我们后面一般化的描述，我们定义输入的词为w,其从输入层词向量求和平均后的霍夫曼树根节点词向量为xw, 从根节点到w所在的叶子节点，包含的节点总数为lw, w在霍夫曼树中从根节点开始，经过的第i个节点表示为pwi,对应的霍夫曼编码为dwi∈{0,1},其中i=2,3,...lw。而该节点对应的模型参数表示为θwi, 其中i=1,2,...lw−1，没有i=lw是因为模型参数仅仅针对于霍夫曼树的内部节点。

定义w经过的霍夫曼树某一个节点j的逻辑回归概率为P(dwj|xw,θwj−1)，其表达式为：

P(dwj|xw,θwj−1)={σ(xTwθwj−1)1−σ(xTwθwj−1)dwj=0dwj=1
那么对于某一个目标输出词w,其最大似然为：
∏j=2lwP(dwj|xw,θwj−1)=∏j=2lw[σ(xTwθwj−1)]1−dwj[1−σ(xTwθwj−1)]dwj
在word2vec中，由于使用的是随机梯度上升法，所以并没有把所有样本的似然乘起来得到真正的训练集最大似然，仅仅每次只用一个样本更新梯度，这样做的目的是减少梯度计算量。这样我们可以得到w的对数似然函数L如下：

L=log∏j=2lwP(dwj|xw,θwj−1)=∑j=2lw((1−dwj)log[σ(xTwθwj−1)]+dwjlog[1−σ(xTwθwj−1)])
要得到模型中w词向量和内部节点的模型参数θ, 我们使用梯度上升法即可。首先我们求模型参数θwj−1的梯度：

∂L∂θwj−1=(1−dwj)(σ(xTwθwj−1)(1−σ(xTwθwj−1)σ(xTwθwj−1)xw−dwj(σ(xTwθwj−1)(1−σ(xTwθwj−1)1−σ(xTwθwj−1)xw=(1−dwj)(1−σ(xTwθwj−1))xw−dwjσ(xTwθwj−1)xw=(1−dwj−σ(xTwθwj−1))xw(1)(2)(3)
这里的梯度推导过程基本类似。
同样的方法，可以求出xw的梯度表达式如下：
∂L∂xw=∑j=2lw(1−dwj−σ(xTwθwj−1))θwj−1
有了梯度表达式，我们就可以用梯度上升法进行迭代来一步步的求解我们需要的所有的θwj−1和xw。



##NLP领域任务分类
　　通常，NLP问题可以划分为四类任务：序列标注、分类任务、句子关系判断、生成式任务。
１.序列标注：典型的NLP任务，比如分词、词性标注、命名体识别、语义角色标注……，序列标注任务的特点是句子中每个单词都要求模型根据上下文给出一个分类类别。
２.分类任务：比如文本分类、情感计算……，分类任务的特点是不管文章长度，总体上能给出一个分类类别。
３.句子关系判断：比如蕴含（entailment）、QA、语义改写、自然语言推理……，句子关系判断任务的特点是给定两个句子，判断两个句子是否具备某种语义关系。
４.生成式任务：比如机器翻译、文本摘要、写诗造句、看图说话……，生成式任务的特点是输入文本内容，自主生成另外的一段文字。
　　而利用ＬＳＴＭ除了情感分析之外还可以实现文本生成以及分类任务、序列标注问题等。
    人类天生具备的一个能力就是记忆的持久性，可以根据过往经验，从而推断出当前看到的内容的实际含义。如看电影的时候可以通过先前时间去推断后续事件；看一篇文章的时候，同样可以通过过往的知识积累去推断文章中每个词语的含义。而传统的神经网络并没有“持久性”，每一个神经元不能通过前面神经元的结果进行推断，为了解决这一问题科学家提出了递归神经网络（Recurrent Neural Networks，RNN）。RNN是包含循环的神经网络（如图所示），允许信息的持久化。其中A可以看作神经网络的一个缩影，接受某时刻的输入。
    ![](https://ai-studio-static-online.cdn.bcebos.com/cb4612d76e1b4ff88bb225ee0d2498c3f82603a2eb93427c9b03aeec2a14f3d3)
    为了更直观的展示，将回路拆分开来，用一个连续的序列进行表示（如图所示）。一个循环神经网络可以看作是若干个相同的基本单元连接起来，每一个基本单元都可以将信息传递到下一个基本单元。
    ![](https://ai-studio-static-online.cdn.bcebos.com/015cd225083f406093f6dfe89a7b87952427e18090bd4cafb64e49a7b574b5c9)
    常规的RNN存在一个问题就是，无法解决“长期依赖”（long-term dependency）问题，即有用信息和预测点相隔较远。以词语预测为例，“我来自中国，我会讲中文”，这句话里面有用信息与预测点相隔较近，RNN可以很轻易的推断出下一个词语应该是中文，但假如有用信息与预测点相隔较远，如“我来自中国……我会讲中文”，此时RNN便无法推断出接下来的词语。换句话说，RNN的信息持久性不够高，不能保持几十甚至上百步。
  为了弥补传统RNN的这个缺点，人们引入了LSTM（long short-term memory）这个模型。LSTM可以看作是一种特殊的RNN，相较于传统RNN，LSTM天生就对长期依赖有着很好的支持。LSTM模型的核心思想主要有两个，分别为记忆元组（memory cell）和非线性的门单元（nonlinear gating unit），其中记忆元组用于保持系统的状态，非线性的门单元用于在每一个时间点调节流入和流出记忆元组的信息。每个递归的神经网络都可以分解成无数个基本重复单元，传统的RNN是这样，LSTM也是如此。在传统的RNN里面，基本重复单元内部结构十分简单，通常只有一个简单的神经网络层（通常为一个tanh模块，如图所示）；在LSTM中，使用了四个神经网络层并且彼此之间以一种特殊的关系进行交互（如图所示）。
    ![](https://ai-studio-static-online.cdn.bcebos.com/f790b396df904eb2a9721902ecfddf7a237e09974f0d47608971c2376ce589ec)
    ![](https://ai-studio-static-online.cdn.bcebos.com/40df0800e1ae4e28b721895d2cd1131933e2d120e093481f9d33f205adf32931)
