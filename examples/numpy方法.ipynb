{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data49224\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/external-libraries’: File exists\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 17.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/beautifulsoup4-4.9.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/bs4 already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve-2.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import json\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import paddle.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    datafile = './data/data49224/housing.data'\r\n",
    "    data = np.fromfile(datafile, sep=' ')\r\n",
    "\r\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\r\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\r\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\r\n",
    "    feature_num = len(feature_names)\r\n",
    "\r\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\r\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\r\n",
    "\r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算训练集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\r\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\r\n",
    "\r\n",
    "    global max_values\r\n",
    "    global min_values\r\n",
    "    global avg_values\r\n",
    "    max_values=maximums\r\n",
    "    min_values=minimums\r\n",
    "    avg_values=avgs\r\n",
    "\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        #print(maximums[i], minimums[i], avgs[i])\r\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义隐藏层使用的激活函数\r\n",
    "def Sigmoid(x):\r\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建一个两层神经网络的类\r\n",
    "\r\n",
    "class Network(object):\r\n",
    "    def __init__(self, num_of_weights):\r\n",
    "        # 随机产生w的初始值\r\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\r\n",
    "        np.random.seed(0)\r\n",
    "        #初始化参数\r\n",
    "        self.w1 = np.random.randn(num_of_weights, 128)\r\n",
    "        self.b1 = 0.\r\n",
    "        self.w2=np.random.randn(128,1)\r\n",
    "        self.b2=0.\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        #正向传播\r\n",
    "        z1 = np.dot(x, self.w1) + self.b1\r\n",
    "        o1=Sigmoid(z1)\r\n",
    "        o2= np.dot(o1,self.w2)+self.b2\r\n",
    "        return z1,o1,o2\r\n",
    "    \r\n",
    "    #计算损失\r\n",
    "    def loss(self, z, y):\r\n",
    "        error = z - y\r\n",
    "        num_samples = error.shape[0]\r\n",
    "        cost = error * error\r\n",
    "        cost = np.sum(cost) / num_samples\r\n",
    "        return cost\r\n",
    "    \r\n",
    "    def gradient(self, x, y, z1,o1,o2):\r\n",
    "\r\n",
    "        #此处略复杂，经过手工计算梯度反向传播公式，然后分别计算相应的梯度\r\n",
    "        N = o1.shape[0]\r\n",
    "        gradient_w2 = 1. / N * np.sum((o2-y) * o1, axis=0)\r\n",
    "        gradient_w2 = gradient_w2[:, np.newaxis]\r\n",
    "        gradient_b2 = 1. / N * np.sum(o2-y)\r\n",
    "\r\n",
    "        M=x.shape[0]\r\n",
    "        gradient_w1=1. /M*np.sum(np.dot(self.w2.T,np.dot(o1.T,(o2-y)))*np.dot(x.T,(1-o1)),axis=0)\r\n",
    "        gradient_w1 = gradient_w1[:, np.newaxis]\r\n",
    "        gradient_b1=1. /M*np.sum(np.dot(self.w2.T,np.dot(o1.T,(o2-y)))*(1-o1),axis=0)\r\n",
    "\r\n",
    "        return gradient_w1, gradient_b1,gradient_w2, gradient_b2\r\n",
    "    \r\n",
    "    #根据计算出的梯度分别对两层的参数进行更新\r\n",
    "    def update(self, gradient_w1, gradient_b1, gradient_w2, gradient_b2, eta = 0.01):\r\n",
    "        self.w2 = self.w2 - eta * gradient_w2\r\n",
    "        self.b2 = self.b2 - eta * gradient_b2\r\n",
    "        self.w1 = self.w1 - eta * gradient_w1.T\r\n",
    "        self.b1 = self.b1 - eta * gradient_b1\r\n",
    "            \r\n",
    "    # 训练过程\r\n",
    "    def train(self, training_data, num_epochs, batch_size=10, eta=0.01):\r\n",
    "        n = len(training_data)\r\n",
    "        losses = []\r\n",
    "        for epoch_id in range(num_epochs):\r\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\r\n",
    "            # 然后再按每次取batch_size条数据的方式取出\r\n",
    "            np.random.shuffle(training_data)\r\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\r\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\r\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "                #print(self.w.shape)\r\n",
    "                #print(self.b)\r\n",
    "                x = mini_batch[:, :-1]\r\n",
    "                y = mini_batch[:, -1:]\r\n",
    "                z1,o1,o2 = self.forward(x)\r\n",
    "                loss = self.loss(o2, y)\r\n",
    "                gradient_w1, gradient_b1, gradient_w2, gradient_b2= self.gradient(x, y, z1,o1,o2)\r\n",
    "                self.update(gradient_w1, gradient_b1, gradient_w2, gradient_b2, eta)\r\n",
    "                losses.append(loss)\r\n",
    "                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\r\n",
    "                                 format(epoch_id, iter_id, loss))\r\n",
    "        \r\n",
    "        return losses\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / iter   0, loss = 86.1997\n",
      "Epoch   0 / iter   1, loss = 0.0605\n",
      "Epoch   0 / iter   2, loss = 0.0694\n",
      "Epoch   0 / iter   3, loss = 0.0720\n",
      "Epoch   0 / iter   4, loss = 0.1004\n",
      "Epoch   1 / iter   0, loss = 0.0493\n",
      "Epoch   1 / iter   1, loss = 0.0869\n",
      "Epoch   1 / iter   2, loss = 0.0658\n",
      "Epoch   1 / iter   3, loss = 0.0890\n",
      "Epoch   1 / iter   4, loss = 0.0466\n",
      "Epoch   2 / iter   0, loss = 0.0709\n",
      "Epoch   2 / iter   1, loss = 0.0730\n",
      "Epoch   2 / iter   2, loss = 0.0644\n",
      "Epoch   2 / iter   3, loss = 0.0824\n",
      "Epoch   2 / iter   4, loss = 0.0076\n",
      "Epoch   3 / iter   0, loss = 0.0828\n",
      "Epoch   3 / iter   1, loss = 0.0593\n",
      "Epoch   3 / iter   2, loss = 0.0642\n",
      "Epoch   3 / iter   3, loss = 0.0819\n",
      "Epoch   3 / iter   4, loss = 0.0635\n",
      "Epoch   4 / iter   0, loss = 0.0674\n",
      "Epoch   4 / iter   1, loss = 0.0911\n",
      "Epoch   4 / iter   2, loss = 0.0599\n",
      "Epoch   4 / iter   3, loss = 0.0731\n",
      "Epoch   4 / iter   4, loss = 0.0263\n",
      "Epoch   5 / iter   0, loss = 0.0836\n",
      "Epoch   5 / iter   1, loss = 0.0804\n",
      "Epoch   5 / iter   2, loss = 0.0632\n",
      "Epoch   5 / iter   3, loss = 0.0617\n",
      "Epoch   5 / iter   4, loss = 0.0358\n",
      "Epoch   6 / iter   0, loss = 0.0936\n",
      "Epoch   6 / iter   1, loss = 0.0567\n",
      "Epoch   6 / iter   2, loss = 0.0795\n",
      "Epoch   6 / iter   3, loss = 0.0572\n",
      "Epoch   6 / iter   4, loss = 0.1027\n",
      "Epoch   7 / iter   0, loss = 0.0584\n",
      "Epoch   7 / iter   1, loss = 0.0773\n",
      "Epoch   7 / iter   2, loss = 0.0697\n",
      "Epoch   7 / iter   3, loss = 0.0837\n",
      "Epoch   7 / iter   4, loss = 0.0505\n",
      "Epoch   8 / iter   0, loss = 0.0741\n",
      "Epoch   8 / iter   1, loss = 0.0854\n",
      "Epoch   8 / iter   2, loss = 0.0808\n",
      "Epoch   8 / iter   3, loss = 0.0501\n",
      "Epoch   8 / iter   4, loss = 0.0295\n",
      "Epoch   9 / iter   0, loss = 0.0727\n",
      "Epoch   9 / iter   1, loss = 0.0543\n",
      "Epoch   9 / iter   2, loss = 0.0815\n",
      "Epoch   9 / iter   3, loss = 0.0738\n",
      "Epoch   9 / iter   4, loss = 0.2063\n",
      "Epoch  10 / iter   0, loss = 0.1024\n",
      "Epoch  10 / iter   1, loss = 0.0580\n",
      "Epoch  10 / iter   2, loss = 0.0786\n",
      "Epoch  10 / iter   3, loss = 0.0444\n",
      "Epoch  10 / iter   4, loss = 0.1638\n",
      "Epoch  11 / iter   0, loss = 0.0967\n",
      "Epoch  11 / iter   1, loss = 0.0560\n",
      "Epoch  11 / iter   2, loss = 0.0550\n",
      "Epoch  11 / iter   3, loss = 0.0760\n",
      "Epoch  11 / iter   4, loss = 0.1751\n",
      "Epoch  12 / iter   0, loss = 0.0693\n",
      "Epoch  12 / iter   1, loss = 0.0582\n",
      "Epoch  12 / iter   2, loss = 0.0568\n",
      "Epoch  12 / iter   3, loss = 0.1043\n",
      "Epoch  12 / iter   4, loss = 0.0331\n",
      "Epoch  13 / iter   0, loss = 0.0751\n",
      "Epoch  13 / iter   1, loss = 0.0497\n",
      "Epoch  13 / iter   2, loss = 0.0897\n",
      "Epoch  13 / iter   3, loss = 0.0748\n",
      "Epoch  13 / iter   4, loss = 0.0100\n",
      "Epoch  14 / iter   0, loss = 0.0627\n",
      "Epoch  14 / iter   1, loss = 0.0918\n",
      "Epoch  14 / iter   2, loss = 0.0714\n",
      "Epoch  14 / iter   3, loss = 0.0630\n",
      "Epoch  14 / iter   4, loss = 0.0047\n",
      "Epoch  15 / iter   0, loss = 0.0840\n",
      "Epoch  15 / iter   1, loss = 0.0546\n",
      "Epoch  15 / iter   2, loss = 0.0857\n",
      "Epoch  15 / iter   3, loss = 0.0635\n",
      "Epoch  15 / iter   4, loss = 0.0186\n",
      "Epoch  16 / iter   0, loss = 0.0751\n",
      "Epoch  16 / iter   1, loss = 0.0852\n",
      "Epoch  16 / iter   2, loss = 0.0486\n",
      "Epoch  16 / iter   3, loss = 0.0766\n",
      "Epoch  16 / iter   4, loss = 0.0651\n",
      "Epoch  17 / iter   0, loss = 0.0721\n",
      "Epoch  17 / iter   1, loss = 0.0722\n",
      "Epoch  17 / iter   2, loss = 0.0755\n",
      "Epoch  17 / iter   3, loss = 0.0662\n",
      "Epoch  17 / iter   4, loss = 0.0609\n",
      "Epoch  18 / iter   0, loss = 0.0836\n",
      "Epoch  18 / iter   1, loss = 0.0808\n",
      "Epoch  18 / iter   2, loss = 0.0670\n",
      "Epoch  18 / iter   3, loss = 0.0547\n",
      "Epoch  18 / iter   4, loss = 0.0480\n",
      "Epoch  19 / iter   0, loss = 0.0709\n",
      "Epoch  19 / iter   1, loss = 0.0682\n",
      "Epoch  19 / iter   2, loss = 0.0812\n",
      "Epoch  19 / iter   3, loss = 0.0681\n",
      "Epoch  19 / iter   4, loss = 0.0132\n",
      "Epoch  20 / iter   0, loss = 0.0848\n",
      "Epoch  20 / iter   1, loss = 0.0782\n",
      "Epoch  20 / iter   2, loss = 0.0605\n",
      "Epoch  20 / iter   3, loss = 0.0629\n",
      "Epoch  20 / iter   4, loss = 0.0333\n",
      "Epoch  21 / iter   0, loss = 0.0762\n",
      "Epoch  21 / iter   1, loss = 0.0610\n",
      "Epoch  21 / iter   2, loss = 0.0848\n",
      "Epoch  21 / iter   3, loss = 0.0630\n",
      "Epoch  21 / iter   4, loss = 0.0551\n",
      "Epoch  22 / iter   0, loss = 0.0933\n",
      "Epoch  22 / iter   1, loss = 0.0732\n",
      "Epoch  22 / iter   2, loss = 0.0620\n",
      "Epoch  22 / iter   3, loss = 0.0584\n",
      "Epoch  22 / iter   4, loss = 0.0241\n",
      "Epoch  23 / iter   0, loss = 0.0667\n",
      "Epoch  23 / iter   1, loss = 0.0684\n",
      "Epoch  23 / iter   2, loss = 0.0682\n",
      "Epoch  23 / iter   3, loss = 0.0796\n",
      "Epoch  23 / iter   4, loss = 0.1124\n",
      "Epoch  24 / iter   0, loss = 0.0763\n",
      "Epoch  24 / iter   1, loss = 0.0656\n",
      "Epoch  24 / iter   2, loss = 0.0798\n",
      "Epoch  24 / iter   3, loss = 0.0648\n",
      "Epoch  24 / iter   4, loss = 0.0387\n",
      "Epoch  25 / iter   0, loss = 0.0816\n",
      "Epoch  25 / iter   1, loss = 0.0515\n",
      "Epoch  25 / iter   2, loss = 0.0691\n",
      "Epoch  25 / iter   3, loss = 0.0835\n",
      "Epoch  25 / iter   4, loss = 0.0131\n",
      "Epoch  26 / iter   0, loss = 0.0731\n",
      "Epoch  26 / iter   1, loss = 0.0615\n",
      "Epoch  26 / iter   2, loss = 0.0904\n",
      "Epoch  26 / iter   3, loss = 0.0598\n",
      "Epoch  26 / iter   4, loss = 0.0408\n",
      "Epoch  27 / iter   0, loss = 0.0929\n",
      "Epoch  27 / iter   1, loss = 0.0661\n",
      "Epoch  27 / iter   2, loss = 0.0545\n",
      "Epoch  27 / iter   3, loss = 0.0702\n",
      "Epoch  27 / iter   4, loss = 0.0997\n",
      "Epoch  28 / iter   0, loss = 0.0709\n",
      "Epoch  28 / iter   1, loss = 0.0546\n",
      "Epoch  28 / iter   2, loss = 0.0741\n",
      "Epoch  28 / iter   3, loss = 0.0863\n",
      "Epoch  28 / iter   4, loss = 0.0107\n",
      "Epoch  29 / iter   0, loss = 0.0849\n",
      "Epoch  29 / iter   1, loss = 0.0654\n",
      "Epoch  29 / iter   2, loss = 0.0701\n",
      "Epoch  29 / iter   3, loss = 0.0632\n",
      "Epoch  29 / iter   4, loss = 0.0469\n",
      "Epoch  30 / iter   0, loss = 0.0684\n",
      "Epoch  30 / iter   1, loss = 0.0774\n",
      "Epoch  30 / iter   2, loss = 0.0844\n",
      "Epoch  30 / iter   3, loss = 0.0473\n",
      "Epoch  30 / iter   4, loss = 0.2044\n",
      "Epoch  31 / iter   0, loss = 0.0684\n",
      "Epoch  31 / iter   1, loss = 0.0753\n",
      "Epoch  31 / iter   2, loss = 0.0601\n",
      "Epoch  31 / iter   3, loss = 0.0799\n",
      "Epoch  31 / iter   4, loss = 0.0444\n",
      "Epoch  32 / iter   0, loss = 0.0822\n",
      "Epoch  32 / iter   1, loss = 0.0557\n",
      "Epoch  32 / iter   2, loss = 0.0747\n",
      "Epoch  32 / iter   3, loss = 0.0631\n",
      "Epoch  32 / iter   4, loss = 0.2351\n",
      "Epoch  33 / iter   0, loss = 0.0618\n",
      "Epoch  33 / iter   1, loss = 0.0722\n",
      "Epoch  33 / iter   2, loss = 0.0892\n",
      "Epoch  33 / iter   3, loss = 0.0600\n",
      "Epoch  33 / iter   4, loss = 0.0307\n",
      "Epoch  34 / iter   0, loss = 0.0597\n",
      "Epoch  34 / iter   1, loss = 0.0643\n",
      "Epoch  34 / iter   2, loss = 0.0792\n",
      "Epoch  34 / iter   3, loss = 0.0805\n",
      "Epoch  34 / iter   4, loss = 0.0360\n",
      "Epoch  35 / iter   0, loss = 0.0717\n",
      "Epoch  35 / iter   1, loss = 0.0831\n",
      "Epoch  35 / iter   2, loss = 0.0660\n",
      "Epoch  35 / iter   3, loss = 0.0593\n",
      "Epoch  35 / iter   4, loss = 0.1215\n",
      "Epoch  36 / iter   0, loss = 0.0611\n",
      "Epoch  36 / iter   1, loss = 0.0822\n",
      "Epoch  36 / iter   2, loss = 0.0573\n",
      "Epoch  36 / iter   3, loss = 0.0830\n",
      "Epoch  36 / iter   4, loss = 0.0197\n",
      "Epoch  37 / iter   0, loss = 0.0558\n",
      "Epoch  37 / iter   1, loss = 0.0719\n",
      "Epoch  37 / iter   2, loss = 0.0678\n",
      "Epoch  37 / iter   3, loss = 0.0863\n",
      "Epoch  37 / iter   4, loss = 0.0439\n",
      "Epoch  38 / iter   0, loss = 0.0729\n",
      "Epoch  38 / iter   1, loss = 0.0445\n",
      "Epoch  38 / iter   2, loss = 0.0921\n",
      "Epoch  38 / iter   3, loss = 0.0719\n",
      "Epoch  38 / iter   4, loss = 0.0732\n",
      "Epoch  39 / iter   0, loss = 0.0471\n",
      "Epoch  39 / iter   1, loss = 0.0839\n",
      "Epoch  39 / iter   2, loss = 0.0640\n",
      "Epoch  39 / iter   3, loss = 0.0880\n",
      "Epoch  39 / iter   4, loss = 0.0343\n",
      "Epoch  40 / iter   0, loss = 0.0771\n",
      "Epoch  40 / iter   1, loss = 0.0715\n",
      "Epoch  40 / iter   2, loss = 0.0687\n",
      "Epoch  40 / iter   3, loss = 0.0663\n",
      "Epoch  40 / iter   4, loss = 0.0056\n",
      "Epoch  41 / iter   0, loss = 0.0928\n",
      "Epoch  41 / iter   1, loss = 0.0695\n",
      "Epoch  41 / iter   2, loss = 0.0635\n",
      "Epoch  41 / iter   3, loss = 0.0574\n",
      "Epoch  41 / iter   4, loss = 0.0114\n",
      "Epoch  42 / iter   0, loss = 0.0823\n",
      "Epoch  42 / iter   1, loss = 0.0469\n",
      "Epoch  42 / iter   2, loss = 0.0943\n",
      "Epoch  42 / iter   3, loss = 0.0581\n",
      "Epoch  42 / iter   4, loss = 0.0487\n",
      "Epoch  43 / iter   0, loss = 0.0716\n",
      "Epoch  43 / iter   1, loss = 0.0503\n",
      "Epoch  43 / iter   2, loss = 0.0602\n",
      "Epoch  43 / iter   3, loss = 0.0919\n",
      "Epoch  43 / iter   4, loss = 0.2383\n",
      "Epoch  44 / iter   0, loss = 0.0602\n",
      "Epoch  44 / iter   1, loss = 0.0629\n",
      "Epoch  44 / iter   2, loss = 0.0824\n",
      "Epoch  44 / iter   3, loss = 0.0762\n",
      "Epoch  44 / iter   4, loss = 0.0442\n",
      "Epoch  45 / iter   0, loss = 0.0499\n",
      "Epoch  45 / iter   1, loss = 0.0708\n",
      "Epoch  45 / iter   2, loss = 0.0815\n",
      "Epoch  45 / iter   3, loss = 0.0731\n",
      "Epoch  45 / iter   4, loss = 0.1914\n",
      "Epoch  46 / iter   0, loss = 0.0506\n",
      "Epoch  46 / iter   1, loss = 0.0841\n",
      "Epoch  46 / iter   2, loss = 0.0688\n",
      "Epoch  46 / iter   3, loss = 0.0748\n",
      "Epoch  46 / iter   4, loss = 0.0996\n",
      "Epoch  47 / iter   0, loss = 0.0869\n",
      "Epoch  47 / iter   1, loss = 0.0658\n",
      "Epoch  47 / iter   2, loss = 0.0587\n",
      "Epoch  47 / iter   3, loss = 0.0702\n",
      "Epoch  47 / iter   4, loss = 0.0305\n",
      "Epoch  48 / iter   0, loss = 0.0916\n",
      "Epoch  48 / iter   1, loss = 0.0570\n",
      "Epoch  48 / iter   2, loss = 0.0662\n",
      "Epoch  48 / iter   3, loss = 0.0662\n",
      "Epoch  48 / iter   4, loss = 0.0161\n",
      "Epoch  49 / iter   0, loss = 0.0615\n",
      "Epoch  49 / iter   1, loss = 0.0745\n",
      "Epoch  49 / iter   2, loss = 0.0616\n",
      "Epoch  49 / iter   3, loss = 0.0751\n",
      "Epoch  49 / iter   4, loss = 0.2319\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data, test_data = load_data()\r\n",
    "\r\n",
    "# 创建网络\r\n",
    "net = Network(13)\r\n",
    "# 启动训练\r\n",
    "losses = net.train(train_data, num_epochs=50, batch_size=100, eta=0.001)\r\n",
    "\r\n",
    "# 画出损失函数的变化趋势\r\n",
    "plot_x = np.arange(len(losses))\r\n",
    "plot_y = np.array(losses)\r\n",
    "plt.plot(plot_x, plot_y)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_one_example():\r\n",
    "    # 从上边已加载的测试集中，随机选择一条作为测试数据\r\n",
    "    idx = np.random.randint(0, test_data.shape[0])\r\n",
    "    one_data, label = test_data[idx, :-1], test_data[idx, -1]\r\n",
    "    # 修改该条数据shape为[1,13]\r\n",
    "    one_data =  one_data.reshape([1,-1])\r\n",
    "\r\n",
    "    return one_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference result is [[31.73786137]], the corresponding label is 32.77574257425745\n"
     ]
    }
   ],
   "source": [
    "one_data, label = load_one_example()\r\n",
    "_, _, predict=net.forward(one_data)\r\n",
    "\r\n",
    "# 对结果做反归一化处理\r\n",
    "predict = predict * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "# 对label数据做反归一化处理\r\n",
    "label = label * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "\r\n",
    "print(\"Inference result is {}, the corresponding label is {}\".format(predict, label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
