**1.深度学习发展历史**

**1943年**

由神经科学家麦卡洛克(W.S.McCilloch) 和数学家皮兹（W.Pitts）在《数学生物物理学公告》上发表论文《神经活动中内在思想的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity）。建立了神经网络和数学模型，称为MCP模型。所谓MCP模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，也就诞生了所谓的“模拟大脑”，人工神经网络的大门由此开启。

MCP当时是希望能够用计算机来模拟人的神经元反应的过程，该模型将神经元简化为了三个过程：输入信号线性加权，求和，非线性激活（阈值法）。如下图所示

**1958年**

计算机科学家罗森布拉特（ Rosenblatt）提出了两层神经元组成的神经网络，称之为“感知器”(Perceptrons)。第一次将MCP用于机器学习（machine learning）分类(classification)。“感知器”算法算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年,该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。

**1969年**

纵观科学发展史，无疑都是充满曲折的，深度学习也毫不例外。 1969年，美国数学家及人工智能先驱 Marvin Minsky 在其著作中证明了感知器本质上是一种线性模型（linear model），只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了将近20年的停滞。

**1986年**

由神经网络之父 Geoffrey Hinton 在1986年发明了适用于多层感知器（MLP）的BP（Backpropagation）算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。

注：Sigmoid 函数是一个在生物学中常见的S型的函数，也称为S型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间。

**90年代时期**

1991年BP算法被指出存在梯度消失问题，也就是说在误差梯度后项传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该问题直接阻碍了深度学习的进一步发展。

此外90年代中期，支持向量机算法诞生（SVM算法）等各种浅层机器学习模型被提出，SVM也是一种有监督的学习模型，应用于模式识别，分类以及回归分析等。支持向量机以统计学为基础，和神经网络有明显的差异，支持向量机等算法的提出再次阻碍了深度学习的发展。

**发展期 2006年 - 2012年**

2006年，加拿大多伦多大学教授、机器学习领域泰斗、神经网络之父—— Geoffrey Hinton 和他的学生 Ruslan Salakhutdinov 在顶尖学术刊物《科学》上发表了一篇文章，该文章提出了深层网络训练中梯度消失问题的解决方案：无监督预训练对权值进行初始化+有监督训练微调。斯坦福大学、纽约大学、加拿大蒙特利尔大学等成为研究深度学习的重镇，至此开启了深度学习在学术界和工业界的浪潮。
2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。2011年以来，微软首次将DL应用在语音识别上，取得了重大突破。微软研究院和Google的语音识别研究人员先后采用DNN技术降低语音识别错误率20％~30％，是语音识别领域十多年来最大的突破性进展。2012年，DNN技术在图像识别领域取得惊人的效果，在ImageNet评测上将错误率从26％降低到15％。在这一年，DNN还被应用于制药公司的DrugeActivity预测问题，并获得世界最好成绩。

**爆发期 2012 - 2017**

2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。

AlexNet的创新点在于:
(1)首次采用ReLU激活函数，极大增大收敛速度且从根本上解决了梯度消失问题。
(2)由于ReLU方法可以很好抑制梯度消失问题，AlexNet抛弃了“预训练+微调”的方法，完全采用有监督训练。也正因为如此，DL的主流学习方法也因此变为了纯粹的有监督学习。
(3)扩展了LeNet5结构，添加Dropout层减小过拟合，LRN层增强泛化能力/减小过拟合。
(4)第一次使用GPU加速模型计算。

2013、2014、2015、2016年，通过ImageNet图像识别比赛，DL的网络结构，训练方法，GPU硬件的不断进步，促使其在其他领域也在不断的征服战场。

2016年3月，由谷歌（Google）旗下DeepMind公司开发的AlphaGo(基于深度学习)与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜；2016年末2017年初，该程序在中国棋类网站上以“大师”（Master）为注册帐号与中日韩数十位围棋高手进行快棋对决，连续60局无一败绩；2017年5月，在中国乌镇围棋峰会上，它与排名世界第一的世界围棋冠军柯洁对战，以3比0的总比分获胜。围棋界公认阿尔法围棋的棋力已经超过人类职业围棋顶尖水平。

![](https://ai-studio-static-online.cdn.bcebos.com/0228ae04d5aa4279b7b33771809a5b591e750a7839494c5296106148706d9e98)


 **2.人工智能、深度学习、机器学习有什么区别与联系**
 
人工智能企图了解智能的本质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术，当下人工智能大爆炸的核心驱动。

![](https://ai-studio-static-online.cdn.bcebos.com/c6d57d1adf894443aa1867da316e26e657c9b7214a8d4980884e1cc2e4594ac1)


**3. 神经元、单层感知机、多层感知机**

**神经元**

受生物神经元细胞的启发，神经科学家Warren McCulloch和逻辑学家Walter Pitts基于神经元细胞的结构特性与传递信息的方式，提出了MCP模型，它是人工神经网络中的最基本结构。

![](https://ai-studio-static-online.cdn.bcebos.com/156c1b110b1f406fb0fec25f5ac7df79299d52872a5a401e9bbf131e6f633f3c)

**单层感知机**

MCP模型就是一个神经元结构，但是没有参数学习的过程，1957年Frank Rosenblatt提出了一种简单的人工神经网络——感知机，单层感知机相较于MCP引入了损失函数，并提出了学习的概念。

![](https://ai-studio-static-online.cdn.bcebos.com/e4d63fe4edb64f59a958a4b9f4344a51235c7e8cd47a49e7b6879828cfcbafdf)

**多层感知机**

由于无法模拟诸如异或以及其他复杂函数的功能，使得单层感知机的应用较为单一，而多层感知机通过增加层数解决了非线性问题。多层感知机由输入层、输出层和至少一层的隐藏层构成。网络中各个隐藏层中神经元可接收相邻前序隐藏层中所有神经元传递而来的信息，经过加工处理后将信息输出给相邻后续隐藏层中所有神经元。

多层感知机可以模拟复杂非线性函数功能，所模拟函数的复杂性取决于网络隐藏层数目和各层中神经元数目，但是需要人为固定一层参数，只能训练其中一层。直到1986年Hinton提出了反向传播算法，使得训练多层网络成为可能。在GPU并行运算能力的大力发展下，网络的层数得以不断增加，新的网络模型也越来越多，感知机也逐渐退出了历史舞台。

![](https://ai-studio-static-online.cdn.bcebos.com/decc07ce23d649ffb93c308f125c13c2c3e93d2567f34fe881251a118c03482b)



**4.什么是前向传播**

前向传播过程，即网络如何根据输入得到输出的。前向传播通过对一层的结点以及对应的连接权值进行加权和运算，结果加上一个偏置项，然后通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，得到的结果就是下一层结点的输出。从第一层（输入层）开始不断的通过这种方法一层层的运算，最后得到输出层结果。

![](https://ai-studio-static-online.cdn.bcebos.com/56f7e24cf712409bbb6c2275bae6f663cea0cc0f542b4af59caf3add1707a27d)




**5.什么是反向传播**

反向传播算法，简称BP算法，适合于多层神经网络的一种学习算法，它建立在梯度下降法的基础上。

BP算法的学习过程由正向传播过程和反向传播过程组成，在正向传播过程中，输入信息通过输入层经隐含层，逐层处理并传向输出层。

如果在输出层得不到期望的输出值，则取输出与期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯量，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。

![](https://ai-studio-static-online.cdn.bcebos.com/a8ca45d7261c4b25a9dd5382bdb951686e3cbc508f844bfe943377500c8685db)


