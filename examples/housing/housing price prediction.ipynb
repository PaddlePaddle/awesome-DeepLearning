{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss is: 2.590484619140625\n",
      "epoch: 1, iter: 0, loss is: 0.06353584676980972\n",
      "epoch: 2, iter: 0, loss is: 0.046668294817209244\n",
      "epoch: 3, iter: 0, loss is: 0.03371404856443405\n",
      "epoch: 4, iter: 0, loss is: 0.04183248430490494\n",
      "epoch: 5, iter: 0, loss is: 0.048998575657606125\n",
      "epoch: 6, iter: 0, loss is: 0.05375518649816513\n",
      "epoch: 7, iter: 0, loss is: 0.03209070861339569\n",
      "epoch: 8, iter: 0, loss is: 0.038903094828128815\n",
      "epoch: 9, iter: 0, loss is: 0.04913901910185814\n",
      "epoch: 10, iter: 0, loss is: 0.04445872828364372\n",
      "epoch: 11, iter: 0, loss is: 0.03459512069821358\n",
      "epoch: 12, iter: 0, loss is: 0.03930223733186722\n",
      "epoch: 13, iter: 0, loss is: 0.028816385194659233\n",
      "epoch: 14, iter: 0, loss is: 0.04804135486483574\n",
      "epoch: 15, iter: 0, loss is: 0.03550158813595772\n",
      "epoch: 16, iter: 0, loss is: 0.03549426794052124\n",
      "epoch: 17, iter: 0, loss is: 0.04055013135075569\n",
      "epoch: 18, iter: 0, loss is: 0.04307300224900246\n",
      "epoch: 19, iter: 0, loss is: 0.04423375427722931\n",
      "epoch: 20, iter: 0, loss is: 0.031134672462940216\n",
      "epoch: 21, iter: 0, loss is: 0.026278242468833923\n",
      "epoch: 22, iter: 0, loss is: 0.04352580010890961\n",
      "epoch: 23, iter: 0, loss is: 0.040716495364904404\n",
      "epoch: 24, iter: 0, loss is: 0.04127642512321472\n",
      "epoch: 25, iter: 0, loss is: 0.031089618802070618\n",
      "epoch: 26, iter: 0, loss is: 0.036018289625644684\n",
      "epoch: 27, iter: 0, loss is: 0.025484977290034294\n",
      "epoch: 28, iter: 0, loss is: 0.03100382722914219\n",
      "epoch: 29, iter: 0, loss is: 0.033902306109666824\n",
      "epoch: 30, iter: 0, loss is: 0.028169620782136917\n",
      "epoch: 31, iter: 0, loss is: 0.06475137174129486\n",
      "epoch: 32, iter: 0, loss is: 0.021623210981488228\n",
      "epoch: 33, iter: 0, loss is: 0.03015873022377491\n",
      "epoch: 34, iter: 0, loss is: 0.03109702095389366\n",
      "epoch: 35, iter: 0, loss is: 0.02384249120950699\n",
      "epoch: 36, iter: 0, loss is: 0.019919371232390404\n",
      "epoch: 37, iter: 0, loss is: 0.028112778440117836\n",
      "epoch: 38, iter: 0, loss is: 0.025789467617869377\n",
      "epoch: 39, iter: 0, loss is: 0.027001235634088516\n",
      "epoch: 40, iter: 0, loss is: 0.03296136483550072\n",
      "epoch: 41, iter: 0, loss is: 0.01914086565375328\n",
      "epoch: 42, iter: 0, loss is: 0.021441960707306862\n",
      "epoch: 43, iter: 0, loss is: 0.01708015613257885\n",
      "epoch: 44, iter: 0, loss is: 0.04253007471561432\n",
      "epoch: 45, iter: 0, loss is: 0.015345875173807144\n",
      "epoch: 46, iter: 0, loss is: 0.03384402394294739\n",
      "epoch: 47, iter: 0, loss is: 0.0354042649269104\n",
      "epoch: 48, iter: 0, loss is: 0.02155259996652603\n",
      "epoch: 49, iter: 0, loss is: 0.026724766939878464\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存成功，模型参数保存在LR_model.pdparams中\n",
      "The test loss is: 0.01243528537452221\n",
      "Inference result is 16.81325912475586, the corresponding label is 19.524263381958008, the loss is 0.0036294071469455957\n"
     ]
    }
   ],
   "source": [
    "import random\r\n",
    "import paddle\r\n",
    "import numpy as np\r\n",
    "from paddle.nn import Linear\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import paddle.nn.functional as F\r\n",
    "\r\n",
    "\r\n",
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    datafile = './data/housing.data'\r\n",
    "    data = np.fromfile(datafile, sep=' ', dtype=np.float32)\r\n",
    "\r\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\r\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\r\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\r\n",
    "    feature_num = len(feature_names)\r\n",
    "\r\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\r\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\r\n",
    "\r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算train数据集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\r\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\r\n",
    "    \r\n",
    "    # 记录数据的归一化参数，在预测时对数据做归一化\r\n",
    "    global max_values\r\n",
    "    global min_values\r\n",
    "    global avg_values\r\n",
    "    max_values = maximums\r\n",
    "    min_values = minimums\r\n",
    "    avg_values = avgs\r\n",
    "\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data\r\n",
    "\r\n",
    "class Regressor(paddle.nn.Layer):\r\n",
    "\r\n",
    "    def __init__(self):\r\n",
    "        # 初始化父类中的一些参数\r\n",
    "        super(Regressor, self).__init__()\r\n",
    "        \r\n",
    "        # 定义两层全连接层\r\n",
    "        self.fc1 = Linear(in_features=13, out_features=10)  # 输入维度是13，输出维度是10\r\n",
    "        self.fc2 = Linear(in_features=10, out_features=1)   # 输入维度是10，输出维度是1\r\n",
    "    \r\n",
    "    # 网络的前向计算\r\n",
    "    def forward(self, inputs):\r\n",
    "        x = F.sigmoid(self.fc1(inputs))\r\n",
    "        x = self.fc2(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "def train(EPOCH_NUM,BATCH_SIZE):\r\n",
    "    # 定义外层循环\r\n",
    "    losses = []\r\n",
    "    for epoch_id in range(EPOCH_NUM):\r\n",
    "        # 在每轮迭代开始之前，将训练数据的顺序随机的打乱\r\n",
    "        np.random.shuffle(training_data)\r\n",
    "        # 将训练数据进行拆分，每个batch包含10条数据\r\n",
    "        mini_batches = [training_data[k:k+BATCH_SIZE] for k in range(0, len(training_data), BATCH_SIZE)]\r\n",
    "        # 定义内层循环\r\n",
    "        for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "            x = np.array(mini_batch[:, :-1]) # 获得当前批次训练数据\r\n",
    "            y = np.array(mini_batch[:, -1:]) # 获得当前批次训练标签（真实房价）\r\n",
    "            # 将numpy数据转为飞桨动态图tensor形式\r\n",
    "            house_features = paddle.to_tensor(x)\r\n",
    "            prices = paddle.to_tensor(y)\r\n",
    "            \r\n",
    "            # 前向计算\r\n",
    "            predicts = model(house_features)\r\n",
    "            \r\n",
    "            # 计算损失\r\n",
    "            loss = F.square_error_cost(predicts, label=prices)\r\n",
    "            avg_loss = paddle.mean(loss)\r\n",
    "            losses.append(np.squeeze(avg_loss.numpy()))\r\n",
    "            if iter_id%20==0:\r\n",
    "                print(\"epoch: {}, iter: {}, loss is: {}\".format(epoch_id, iter_id, np.squeeze(avg_loss.numpy())))\r\n",
    "            \r\n",
    "            # 反向传播\r\n",
    "            avg_loss.backward()\r\n",
    "            # 最小化loss,更新参数\r\n",
    "            opt.step()\r\n",
    "            # 清除梯度\r\n",
    "            opt.clear_grad()\r\n",
    "    return losses\r\n",
    "    \r\n",
    "def testing():\r\n",
    "    x = np.array(test_data[:, :-1]) # 获得当前批次训练数据\r\n",
    "    y = np.array(test_data[:, -1:]) # 获得当前批次训练标签（真实房价）\r\n",
    "    # 将numpy数据转为飞桨动态图tensor形式\r\n",
    "    house_features = paddle.to_tensor(x)\r\n",
    "    prices = paddle.to_tensor(y)\r\n",
    "    \r\n",
    "    # 前向计算\r\n",
    "    predicts = model(house_features)\r\n",
    "    \r\n",
    "    # 计算损失\r\n",
    "    loss = F.square_error_cost(predicts, label=prices)\r\n",
    "    avg_loss = paddle.mean(loss)\r\n",
    "    print(\"The test loss is: {}\".format(np.squeeze(avg_loss.numpy())))\r\n",
    "\r\n",
    "\r\n",
    "def load_one_example():\r\n",
    "    # 从上边已加载的测试集中，随机选择一条作为测试数据\r\n",
    "    # idx = np.random.randint(0, test_data.shape[0])\r\n",
    "    idx = -10\r\n",
    "    one_data, label = test_data[idx, :-1], test_data[idx, -1]\r\n",
    "    # 修改该条数据shape为[1,13]\r\n",
    "    one_data =  one_data.reshape([1,-1])\r\n",
    "\r\n",
    "    return one_data, label\r\n",
    "\r\n",
    "\r\n",
    "def eval():\r\n",
    "\r\n",
    "    # 参数为保存模型参数的文件地址\r\n",
    "    model_dict = paddle.load('LR_model.pdparams')\r\n",
    "    model.load_dict(model_dict)\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    # 参数为数据集的文件地址\r\n",
    "    one_data, label = load_one_example()\r\n",
    "    # 将数据转为动态图的variable格式 \r\n",
    "    one_data = paddle.to_tensor(one_data)\r\n",
    "    label = paddle.to_tensor(label)\r\n",
    "    \r\n",
    "    predict = model(one_data)\r\n",
    "    loss = F.square_error_cost(predict, label=label)\r\n",
    "    avg_loss = paddle.mean(loss)\r\n",
    "\r\n",
    "    # 对结果做反归一化处理\r\n",
    "    predict = predict * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "    predict = np.squeeze(predict)\r\n",
    "    # 对label数据做反归一化处理\r\n",
    "    label = label * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "    label = np.squeeze(label.numpy())\r\n",
    "\r\n",
    "    print(\"Inference result is {}, the corresponding label is {}, the loss is {}\".format(predict.numpy(), label,np.squeeze(avg_loss.numpy()))) \r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "\r\n",
    "    # 声明定义好的线性回归模型\r\n",
    "    model = Regressor()\r\n",
    "    # 开启模型训练模式\r\n",
    "    model.train()\r\n",
    "    # 加载数据\r\n",
    "    training_data, test_data = load_data()\r\n",
    "    # 定义优化算法，使用随机梯度下降SGD\r\n",
    "    # 学习率设置为0.1\r\n",
    "    opt = paddle.optimizer.SGD(learning_rate=0.1, parameters=model.parameters())\r\n",
    "\r\n",
    "\r\n",
    "    EPOCH_NUM = 50   # 设置外层循环次数\r\n",
    "    BATCH_SIZE = 100  # 设置batch大小\r\n",
    "\r\n",
    "    losses = train(EPOCH_NUM, BATCH_SIZE)\r\n",
    "    \r\n",
    "    plot_x = np.arange(len(losses))\r\n",
    "    plot_y = np.array(losses)\r\n",
    "    plt.plot(plot_x, plot_y)\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "    # 保存模型参数，文件名为LR_model.pdparams\r\n",
    "    paddle.save(model.state_dict(), 'LR_model.pdparams')\r\n",
    "    print(\"模型保存成功，模型参数保存在LR_model.pdparams中\")\r\n",
    "\r\n",
    "    testing()             # 测试集上的损失\r\n",
    "    eval()                # 某一条数据的验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / iter   0, loss = 0.6326\n",
      "Epoch   0 / iter   1, loss = 1.1028\n",
      "Epoch   0 / iter   2, loss = 0.1070\n",
      "Epoch   0 / iter   3, loss = 0.0241\n",
      "Epoch   0 / iter   4, loss = 0.0169\n",
      "Epoch   1 / iter   0, loss = 0.0313\n",
      "Epoch   1 / iter   1, loss = 0.0318\n",
      "Epoch   1 / iter   2, loss = 0.0319\n",
      "Epoch   1 / iter   3, loss = 0.0218\n",
      "Epoch   1 / iter   4, loss = 0.0192\n",
      "Epoch   2 / iter   0, loss = 0.0212\n",
      "Epoch   2 / iter   1, loss = 0.0228\n",
      "Epoch   2 / iter   2, loss = 0.0186\n",
      "Epoch   2 / iter   3, loss = 0.0220\n",
      "Epoch   2 / iter   4, loss = 0.0086\n",
      "Epoch   3 / iter   0, loss = 0.0250\n",
      "Epoch   3 / iter   1, loss = 0.0152\n",
      "Epoch   3 / iter   2, loss = 0.0180\n",
      "Epoch   3 / iter   3, loss = 0.0124\n",
      "Epoch   3 / iter   4, loss = 0.0249\n",
      "Epoch   4 / iter   0, loss = 0.0150\n",
      "Epoch   4 / iter   1, loss = 0.0134\n",
      "Epoch   4 / iter   2, loss = 0.0123\n",
      "Epoch   4 / iter   3, loss = 0.0230\n",
      "Epoch   4 / iter   4, loss = 0.0071\n",
      "Epoch   5 / iter   0, loss = 0.0157\n",
      "Epoch   5 / iter   1, loss = 0.0134\n",
      "Epoch   5 / iter   2, loss = 0.0116\n",
      "Epoch   5 / iter   3, loss = 0.0167\n",
      "Epoch   5 / iter   4, loss = 0.0012\n",
      "Epoch   6 / iter   0, loss = 0.0145\n",
      "Epoch   6 / iter   1, loss = 0.0158\n",
      "Epoch   6 / iter   2, loss = 0.0130\n",
      "Epoch   6 / iter   3, loss = 0.0118\n",
      "Epoch   6 / iter   4, loss = 0.0045\n",
      "Epoch   7 / iter   0, loss = 0.0201\n",
      "Epoch   7 / iter   1, loss = 0.0128\n",
      "Epoch   7 / iter   2, loss = 0.0149\n",
      "Epoch   7 / iter   3, loss = 0.0095\n",
      "Epoch   7 / iter   4, loss = 0.0081\n",
      "Epoch   8 / iter   0, loss = 0.0133\n",
      "Epoch   8 / iter   1, loss = 0.0116\n",
      "Epoch   8 / iter   2, loss = 0.0123\n",
      "Epoch   8 / iter   3, loss = 0.0112\n",
      "Epoch   8 / iter   4, loss = 0.0166\n",
      "Epoch   9 / iter   0, loss = 0.0135\n",
      "Epoch   9 / iter   1, loss = 0.0098\n",
      "Epoch   9 / iter   2, loss = 0.0086\n",
      "Epoch   9 / iter   3, loss = 0.0167\n",
      "Epoch   9 / iter   4, loss = 0.0070\n",
      "Epoch  10 / iter   0, loss = 0.0128\n",
      "Epoch  10 / iter   1, loss = 0.0133\n",
      "Epoch  10 / iter   2, loss = 0.0126\n",
      "Epoch  10 / iter   3, loss = 0.0070\n",
      "Epoch  10 / iter   4, loss = 0.0064\n",
      "Epoch  11 / iter   0, loss = 0.0114\n",
      "Epoch  11 / iter   1, loss = 0.0104\n",
      "Epoch  11 / iter   2, loss = 0.0128\n",
      "Epoch  11 / iter   3, loss = 0.0089\n",
      "Epoch  11 / iter   4, loss = 0.0084\n",
      "Epoch  12 / iter   0, loss = 0.0135\n",
      "Epoch  12 / iter   1, loss = 0.0087\n",
      "Epoch  12 / iter   2, loss = 0.0130\n",
      "Epoch  12 / iter   3, loss = 0.0074\n",
      "Epoch  12 / iter   4, loss = 0.0031\n",
      "Epoch  13 / iter   0, loss = 0.0110\n",
      "Epoch  13 / iter   1, loss = 0.0109\n",
      "Epoch  13 / iter   2, loss = 0.0091\n",
      "Epoch  13 / iter   3, loss = 0.0103\n",
      "Epoch  13 / iter   4, loss = 0.0107\n",
      "Epoch  14 / iter   0, loss = 0.0113\n",
      "Epoch  14 / iter   1, loss = 0.0142\n",
      "Epoch  14 / iter   2, loss = 0.0074\n",
      "Epoch  14 / iter   3, loss = 0.0095\n",
      "Epoch  14 / iter   4, loss = 0.0090\n",
      "Epoch  15 / iter   0, loss = 0.0095\n",
      "Epoch  15 / iter   1, loss = 0.0108\n",
      "Epoch  15 / iter   2, loss = 0.0114\n",
      "Epoch  15 / iter   3, loss = 0.0077\n",
      "Epoch  15 / iter   4, loss = 0.0008\n",
      "Epoch  16 / iter   0, loss = 0.0130\n",
      "Epoch  16 / iter   1, loss = 0.0076\n",
      "Epoch  16 / iter   2, loss = 0.0062\n",
      "Epoch  16 / iter   3, loss = 0.0122\n",
      "Epoch  16 / iter   4, loss = 0.0004\n",
      "Epoch  17 / iter   0, loss = 0.0133\n",
      "Epoch  17 / iter   1, loss = 0.0081\n",
      "Epoch  17 / iter   2, loss = 0.0096\n",
      "Epoch  17 / iter   3, loss = 0.0066\n",
      "Epoch  17 / iter   4, loss = 0.0054\n",
      "Epoch  18 / iter   0, loss = 0.0055\n",
      "Epoch  18 / iter   1, loss = 0.0112\n",
      "Epoch  18 / iter   2, loss = 0.0119\n",
      "Epoch  18 / iter   3, loss = 0.0067\n",
      "Epoch  18 / iter   4, loss = 0.0445\n",
      "Epoch  19 / iter   0, loss = 0.0142\n",
      "Epoch  19 / iter   1, loss = 0.0116\n",
      "Epoch  19 / iter   2, loss = 0.0080\n",
      "Epoch  19 / iter   3, loss = 0.0078\n",
      "Epoch  19 / iter   4, loss = 0.0107\n",
      "Epoch  20 / iter   0, loss = 0.0110\n",
      "Epoch  20 / iter   1, loss = 0.0062\n",
      "Epoch  20 / iter   2, loss = 0.0062\n",
      "Epoch  20 / iter   3, loss = 0.0117\n",
      "Epoch  20 / iter   4, loss = 0.0398\n",
      "Epoch  21 / iter   0, loss = 0.0115\n",
      "Epoch  21 / iter   1, loss = 0.0055\n",
      "Epoch  21 / iter   2, loss = 0.0106\n",
      "Epoch  21 / iter   3, loss = 0.0083\n",
      "Epoch  21 / iter   4, loss = 0.0023\n",
      "Epoch  22 / iter   0, loss = 0.0104\n",
      "Epoch  22 / iter   1, loss = 0.0102\n",
      "Epoch  22 / iter   2, loss = 0.0069\n",
      "Epoch  22 / iter   3, loss = 0.0094\n",
      "Epoch  22 / iter   4, loss = 0.0074\n",
      "Epoch  23 / iter   0, loss = 0.0120\n",
      "Epoch  23 / iter   1, loss = 0.0066\n",
      "Epoch  23 / iter   2, loss = 0.0059\n",
      "Epoch  23 / iter   3, loss = 0.0104\n",
      "Epoch  23 / iter   4, loss = 0.0049\n",
      "Epoch  24 / iter   0, loss = 0.0085\n",
      "Epoch  24 / iter   1, loss = 0.0112\n",
      "Epoch  24 / iter   2, loss = 0.0064\n",
      "Epoch  24 / iter   3, loss = 0.0082\n",
      "Epoch  24 / iter   4, loss = 0.0002\n",
      "Epoch  25 / iter   0, loss = 0.0056\n",
      "Epoch  25 / iter   1, loss = 0.0096\n",
      "Epoch  25 / iter   2, loss = 0.0074\n",
      "Epoch  25 / iter   3, loss = 0.0109\n",
      "Epoch  25 / iter   4, loss = 0.0092\n",
      "Epoch  26 / iter   0, loss = 0.0101\n",
      "Epoch  26 / iter   1, loss = 0.0097\n",
      "Epoch  26 / iter   2, loss = 0.0063\n",
      "Epoch  26 / iter   3, loss = 0.0081\n",
      "Epoch  26 / iter   4, loss = 0.0062\n",
      "Epoch  27 / iter   0, loss = 0.0069\n",
      "Epoch  27 / iter   1, loss = 0.0068\n",
      "Epoch  27 / iter   2, loss = 0.0058\n",
      "Epoch  27 / iter   3, loss = 0.0134\n",
      "Epoch  27 / iter   4, loss = 0.0029\n",
      "Epoch  28 / iter   0, loss = 0.0077\n",
      "Epoch  28 / iter   1, loss = 0.0095\n",
      "Epoch  28 / iter   2, loss = 0.0077\n",
      "Epoch  28 / iter   3, loss = 0.0083\n",
      "Epoch  28 / iter   4, loss = 0.0043\n",
      "Epoch  29 / iter   0, loss = 0.0068\n",
      "Epoch  29 / iter   1, loss = 0.0087\n",
      "Epoch  29 / iter   2, loss = 0.0110\n",
      "Epoch  29 / iter   3, loss = 0.0080\n",
      "Epoch  29 / iter   4, loss = 0.0059\n",
      "Epoch  30 / iter   0, loss = 0.0118\n",
      "Epoch  30 / iter   1, loss = 0.0080\n",
      "Epoch  30 / iter   2, loss = 0.0067\n",
      "Epoch  30 / iter   3, loss = 0.0075\n",
      "Epoch  30 / iter   4, loss = 0.0078\n",
      "Epoch  31 / iter   0, loss = 0.0179\n",
      "Epoch  31 / iter   1, loss = 0.0082\n",
      "Epoch  31 / iter   2, loss = 0.0052\n",
      "Epoch  31 / iter   3, loss = 0.0070\n",
      "Epoch  31 / iter   4, loss = 0.0016\n",
      "Epoch  32 / iter   0, loss = 0.0091\n",
      "Epoch  32 / iter   1, loss = 0.0083\n",
      "Epoch  32 / iter   2, loss = 0.0075\n",
      "Epoch  32 / iter   3, loss = 0.0075\n",
      "Epoch  32 / iter   4, loss = 0.0022\n",
      "Epoch  33 / iter   0, loss = 0.0091\n",
      "Epoch  33 / iter   1, loss = 0.0045\n",
      "Epoch  33 / iter   2, loss = 0.0074\n",
      "Epoch  33 / iter   3, loss = 0.0108\n",
      "Epoch  33 / iter   4, loss = 0.0037\n",
      "Epoch  34 / iter   0, loss = 0.0061\n",
      "Epoch  34 / iter   1, loss = 0.0052\n",
      "Epoch  34 / iter   2, loss = 0.0104\n",
      "Epoch  34 / iter   3, loss = 0.0073\n",
      "Epoch  34 / iter   4, loss = 0.0576\n",
      "Epoch  35 / iter   0, loss = 0.0071\n",
      "Epoch  35 / iter   1, loss = 0.0103\n",
      "Epoch  35 / iter   2, loss = 0.0059\n",
      "Epoch  35 / iter   3, loss = 0.0073\n",
      "Epoch  35 / iter   4, loss = 0.0266\n",
      "Epoch  36 / iter   0, loss = 0.0090\n",
      "Epoch  36 / iter   1, loss = 0.0083\n",
      "Epoch  36 / iter   2, loss = 0.0065\n",
      "Epoch  36 / iter   3, loss = 0.0076\n",
      "Epoch  36 / iter   4, loss = 0.0017\n",
      "Epoch  37 / iter   0, loss = 0.0061\n",
      "Epoch  37 / iter   1, loss = 0.0115\n",
      "Epoch  37 / iter   2, loss = 0.0069\n",
      "Epoch  37 / iter   3, loss = 0.0052\n",
      "Epoch  37 / iter   4, loss = 0.0135\n",
      "Epoch  38 / iter   0, loss = 0.0067\n",
      "Epoch  38 / iter   1, loss = 0.0105\n",
      "Epoch  38 / iter   2, loss = 0.0055\n",
      "Epoch  38 / iter   3, loss = 0.0075\n",
      "Epoch  38 / iter   4, loss = 0.0012\n",
      "Epoch  39 / iter   0, loss = 0.0076\n",
      "Epoch  39 / iter   1, loss = 0.0101\n",
      "Epoch  39 / iter   2, loss = 0.0066\n",
      "Epoch  39 / iter   3, loss = 0.0056\n",
      "Epoch  39 / iter   4, loss = 0.0065\n",
      "Epoch  40 / iter   0, loss = 0.0084\n",
      "Epoch  40 / iter   1, loss = 0.0088\n",
      "Epoch  40 / iter   2, loss = 0.0079\n",
      "Epoch  40 / iter   3, loss = 0.0047\n",
      "Epoch  40 / iter   4, loss = 0.0017\n",
      "Epoch  41 / iter   0, loss = 0.0060\n",
      "Epoch  41 / iter   1, loss = 0.0049\n",
      "Epoch  41 / iter   2, loss = 0.0086\n",
      "Epoch  41 / iter   3, loss = 0.0108\n",
      "Epoch  41 / iter   4, loss = 0.0033\n",
      "Epoch  42 / iter   0, loss = 0.0068\n",
      "Epoch  42 / iter   1, loss = 0.0090\n",
      "Epoch  42 / iter   2, loss = 0.0077\n",
      "Epoch  42 / iter   3, loss = 0.0051\n",
      "Epoch  42 / iter   4, loss = 0.0053\n",
      "Epoch  43 / iter   0, loss = 0.0083\n",
      "Epoch  43 / iter   1, loss = 0.0061\n",
      "Epoch  43 / iter   2, loss = 0.0100\n",
      "Epoch  43 / iter   3, loss = 0.0047\n",
      "Epoch  43 / iter   4, loss = 0.0016\n",
      "Epoch  44 / iter   0, loss = 0.0065\n",
      "Epoch  44 / iter   1, loss = 0.0067\n",
      "Epoch  44 / iter   2, loss = 0.0089\n",
      "Epoch  44 / iter   3, loss = 0.0071\n",
      "Epoch  44 / iter   4, loss = 0.0037\n",
      "Epoch  45 / iter   0, loss = 0.0092\n",
      "Epoch  45 / iter   1, loss = 0.0066\n",
      "Epoch  45 / iter   2, loss = 0.0053\n",
      "Epoch  45 / iter   3, loss = 0.0089\n",
      "Epoch  45 / iter   4, loss = 0.0011\n",
      "Epoch  46 / iter   0, loss = 0.0071\n",
      "Epoch  46 / iter   1, loss = 0.0058\n",
      "Epoch  46 / iter   2, loss = 0.0097\n",
      "Epoch  46 / iter   3, loss = 0.0061\n",
      "Epoch  46 / iter   4, loss = 0.0023\n",
      "Epoch  47 / iter   0, loss = 0.0070\n",
      "Epoch  47 / iter   1, loss = 0.0068\n",
      "Epoch  47 / iter   2, loss = 0.0091\n",
      "Epoch  47 / iter   3, loss = 0.0064\n",
      "Epoch  47 / iter   4, loss = 0.0003\n",
      "Epoch  48 / iter   0, loss = 0.0055\n",
      "Epoch  48 / iter   1, loss = 0.0072\n",
      "Epoch  48 / iter   2, loss = 0.0054\n",
      "Epoch  48 / iter   3, loss = 0.0103\n",
      "Epoch  48 / iter   4, loss = 0.0065\n",
      "Epoch  49 / iter   0, loss = 0.0079\n",
      "Epoch  49 / iter   1, loss = 0.0056\n",
      "Epoch  49 / iter   2, loss = 0.0099\n",
      "Epoch  49 / iter   3, loss = 0.0047\n",
      "Epoch  49 / iter   4, loss = 0.0085\n",
      "The testing loss = 0.0069\n",
      "Inference result is 40.41881844108748, the corresponding label is 43.675742574257455, the loss is 0.002619149335611095\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "\r\n",
    "def sigmoid(x):\r\n",
    "    # sigmoid激活函数\r\n",
    "    return 1/(1+np.exp(-x))\r\n",
    "\r\n",
    "def dsigmoid(x):\r\n",
    "    # sigmoid激活函数的导数\r\n",
    "    return x*(1-x)\r\n",
    "\r\n",
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    datafile = './data/housing.data'\r\n",
    "    data = np.fromfile(datafile, sep=' ')\r\n",
    "\r\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\r\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',                       'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\r\n",
    "    feature_num = len(feature_names)\r\n",
    "\r\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\r\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\r\n",
    "\r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算训练集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0),                                  training_data.sum(axis=0) / training_data.shape[0]\r\n",
    "\r\n",
    "    # 记录数据的归一化参数，在预测时对数据做归一化\r\n",
    "    global max_values\r\n",
    "    global min_values\r\n",
    "    global avg_values\r\n",
    "    max_values = maximums\r\n",
    "    min_values = minimums\r\n",
    "    avg_values = avgs\r\n",
    "\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        #print(maximums[i], minimums[i], avgs[i])\r\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data\r\n",
    "\r\n",
    "\r\n",
    "class Network(object):\r\n",
    "    def __init__(self, num_of_weights,hidden_sum):\r\n",
    "        # 随机产生w的初始值\r\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\r\n",
    "        np.random.seed(0)\r\n",
    "        self.w_1 = np.random.randn(num_of_weights, hidden_sum)  # 第一个全连接层的网络参数\r\n",
    "        self.b_1 = np.zeros(hidden_sum)\r\n",
    "        self.w_2 = np.random.randn(hidden_sum,1)                # 第二个全连接层的网络参数\r\n",
    "        self.b_2 = 0.\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        # 前向传播\r\n",
    "        z = np.dot(x, self.w_1) + self.b_1     # 全连接层\r\n",
    "        z = sigmoid(z)                         # sigmoid激活层\r\n",
    "        z = np.dot(z, self.w_2) + self.b_2     # 全连接层\r\n",
    "        return z\r\n",
    "    \r\n",
    "    def loss(self, z, y):\r\n",
    "        # 均方差损失函数计算\r\n",
    "        error = z - y\r\n",
    "        cost = error * error\r\n",
    "        cost = np.mean(cost) / 2\r\n",
    "        return cost\r\n",
    "    \r\n",
    "    def gradient(self, x, y):\r\n",
    "        # 梯度计算\r\n",
    "        o_1 = sigmoid(np.dot(x, self.w_1) + self.b_1)    # 第一个全连接层的输出\r\n",
    "        z = self.forward(x)                              # 第一个全连接层的输出\r\n",
    "\r\n",
    "        gradient_w_1 = x.T.dot((z-y).dot(self.w_2.T) * dsigmoid(o_1))           # 第一个全连接层参数的梯度\r\n",
    "        gradient_b_1 = np.mean((z-y).dot(self.w_2.T) * dsigmoid(o_1), axis=0) \r\n",
    "\r\n",
    "        # gradient_w_2 = o_1.T.dot((z-y))\r\n",
    "        gradient_w_2 = np.mean((z-y)*o_1,axis=0)                                # 第二个全连接层参数的梯度\r\n",
    "        gradient_w_2 = gradient_w_2[:,np.newaxis]\r\n",
    "        gradient_b_2 = np.mean((z-y))\r\n",
    "        return gradient_w_1, gradient_b_1, gradient_w_2, gradient_b_2\r\n",
    "    \r\n",
    "    def update(self, gradient_w_1, gradient_w_2, gradient_b_1, gradient_b_2, eta = 0.01):\r\n",
    "        # 梯度下降，网络参数更新\r\n",
    "        self.w_1 = self.w_1 - eta * gradient_w_1\r\n",
    "        self.b_1 = self.b_1 - eta * gradient_b_1\r\n",
    "        self.w_2 = self.w_2 - eta * gradient_w_2\r\n",
    "        self.b_2 = self.b_2 - eta * gradient_b_2\r\n",
    "            \r\n",
    "                \r\n",
    "    def train(self, training_data, num_epochs, batch_size=10, eta=0.01):\r\n",
    "        n = len(training_data)\r\n",
    "        losses = []\r\n",
    "        for epoch_id in range(num_epochs):\r\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\r\n",
    "            # 然后再按每次取batch_size条数据的方式取出\r\n",
    "            np.random.shuffle(training_data)\r\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\r\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\r\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "                #print(self.w.shape)\r\n",
    "                #print(self.b)\r\n",
    "                x = mini_batch[:, :-1]   # 特征\r\n",
    "                y = mini_batch[:, -1:]   # 标签\r\n",
    "                a = self.forward(x)      # 网络输出\r\n",
    "                loss = self.loss(a, y)   # 网络损失\r\n",
    "                gradient_w_1, gradient_b_1, gradient_w_2, gradient_b_2 = self.gradient(x, y)  # 参数梯度\r\n",
    "                self.update(gradient_w_1, gradient_w_2, gradient_b_1, gradient_b_2, eta)      # 更新参数\r\n",
    "                losses.append(loss)                                                           # 记录损失\r\n",
    "                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\r\n",
    "                                 format(epoch_id, iter_id, loss))\r\n",
    "        \r\n",
    "        return losses\r\n",
    "\r\n",
    "    def test(self,test_data):\r\n",
    "        x = test_data[:, :-1]   # 特征\r\n",
    "        y = test_data[:, -1:]   # 标签\r\n",
    "        a = self.forward(x)      # 网络输出\r\n",
    "        loss = self.loss(a, y)   # 网络损失\r\n",
    "        print('The testing loss = {:.4f}'.format(loss))\r\n",
    "\r\n",
    "    def eval(self,test_data):\r\n",
    "        idx = -12\r\n",
    "        # idx = np.random.randint(0, test_data.shape[0])\r\n",
    "        one_data, label = test_data[idx, :-1], test_data[idx, -1]\r\n",
    "        one_data =  one_data.reshape([1,-1])      # 修改该条数据shape为[1,13]\r\n",
    "        \r\n",
    "        \r\n",
    "        predict = self.forward(one_data)\r\n",
    "        predict = np.squeeze(predict)   # 去除冗余维度\r\n",
    "        loss = self.loss(predict, label)\r\n",
    "\r\n",
    "        # 对结果做反归一化处理\r\n",
    "        predict = predict * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "        # 对label数据做反归一化处理\r\n",
    "        label = label * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "\r\n",
    "        print(\"Inference result is {}, the corresponding label is {}, the loss is {}\".format(predict, label, loss))\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    # 获取数据\r\n",
    "    train_data, test_data = load_data()\r\n",
    "\r\n",
    "    # 创建网络\r\n",
    "    net = Network(13,10)   # 隐层节点数10\r\n",
    "\r\n",
    "    # 启动训练\r\n",
    "    # mini_batch,batch_size=100,learning_rate=0.1\r\n",
    "    losses = net.train(train_data, num_epochs=50, batch_size=100, eta=0.1)  \r\n",
    "    net.test(test_data)  # 测试集的效果\r\n",
    "    net.eval(test_data)  # 单条数据效果验证\r\n",
    "\r\n",
    "    # 画出损失函数的变化趋势\r\n",
    "    plot_x = np.arange(len(losses))\r\n",
    "    plot_y = np.array(losses)\r\n",
    "    plt.plot(plot_x, plot_y)\r\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
