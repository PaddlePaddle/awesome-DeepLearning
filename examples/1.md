问题：1.如何基于层次softmax训练词向量？



答：
     我们就将词典大小设为D，用、、...表示词典中的每个词。
![](https://ai-studio-static-online.cdn.bcebos.com/c0906ba706934a92863d6635ae558f321626c7de4d6648c9a700439fe1a0fd93)
    从这个model中也能看出，模型的输入不仅是输入，而且是其他输入的参数！所以这个model的参数是维度为 Dembeddim 的矩阵（每行就是一个用户定义的embeddim大小的词向量，词典中有D个词，所以一共有D行），而且输入也是从这个矩阵中取出的某一行）。
    假设函数有了，那么根据《一般化机器学习》，我们需要定义损失函数。当然，根据前面所说的词共现信息来定义啦。
    为了好表示，我们将模型输入的词称为中心词(central word)，记为w_c，将这个词两边的词记为目标词(objected word)，记为w_o，假如我们只将中心词附近的m个词认为是它的共现词（也就是中心词左边的m个词以及中心词右边的m个词），那么目标词一共有2m个，分别记为、、...。（下文将会看到，在整个句子的视角下，m被称为窗口大小）,如果我们令m=1，那么对于下面这个长度为T=10句子：今天 我 看见 一只 可爱的 猫 坐 在 桌子 上。那么当我们将“猫”看作中心词时，目标词就是“可爱的”和“坐”，即,今天 我 看见 一只 【可爱的 猫 坐】 在 桌子 上。我们就认为这两个词跟猫的语义是相关的，其他词跟猫是否相关我们不清楚。所以我们要争取让P(可爱的|猫)、 P(坐|猫)尽可能的大。
    讲到这里，最容易想到的就是使用似然函数了。由于这里类别特别多，所以算出来的每个概率都可能非常小，为了避免浮点下溢（值太小，容易在计算机中被当成0，而且容易被存储浮点数的噪声淹没），更明智的选择是使用对数似然函数。所以对于一段长度为T的训练文本，损失函数即：
    ![](https://ai-studio-static-online.cdn.bcebos.com/fb0c06e6d5934737be4d24770c6911d2f549182ce9f941c8b5d29e6e162a03f1)
    当然啦，这里要让长度为m的窗口滑过训练文本中的每个词，滑到每个词时，都要计算2m次后验概率。而每次计算后验概率都要用到softmax函数，而回顾一下softmax函数，它的分母是很恐怖的：
    ![](https://ai-studio-static-online.cdn.bcebos.com/9db6a2092d9f46f1b0c41882c5b00d73331630b2124649eb9571bf7444641daf)
    类别越多，分母越长。而我们这里类别数等于词典大小啊！所以词典有10万个单词的话，分母要计算10万次指数函数？所以直接拿最优化算法去优化这个损失函数的话，肯定会训练到天长地久（好像用词不当）。那怎么办呢？
    一种很巧妙的方法是将原来计算复杂度为D的分母（要计算D次指数函数）通过构造一棵“胡夫曼二叉树(Huffman binary tree)”来将原来扁平的“softmax”给变成树状的softmax，从而将softmax的分母给优化成计算复杂度为log D。这种树形的softmax也叫分层softmax（Hierarchical Softmax）。
    

问题２：除了情感分析，ＬＳＴＭ还可以实现哪些其他类型的ＮＬＰ任务。


答：
NLP领域任务分类
　　通常，NLP问题可以划分为四类任务：序列标注、分类任务、句子关系判断、生成式任务。
１.序列标注：典型的NLP任务，比如分词、词性标注、命名体识别、语义角色标注……，序列标注任务的特点是句子中每个单词都要求模型根据上下文给出一个分类类别。
２.分类任务：比如文本分类、情感计算……，分类任务的特点是不管文章长度，总体上能给出一个分类类别。
３.句子关系判断：比如蕴含（entailment）、QA、语义改写、自然语言推理……，句子关系判断任务的特点是给定两个句子，判断两个句子是否具备某种语义关系。
４.生成式任务：比如机器翻译、文本摘要、写诗造句、看图说话……，生成式任务的特点是输入文本内容，自主生成另外的一段文字。
　　而利用ＬＳＴＭ除了情感分析之外还可以实现文本生成以及分类任务、序列标注问题等。
    人类天生具备的一个能力就是记忆的持久性，可以根据过往经验，从而推断出当前看到的内容的实际含义。如看电影的时候可以通过先前时间去推断后续事件；看一篇文章的时候，同样可以通过过往的知识积累去推断文章中每个词语的含义。而传统的神经网络并没有“持久性”，每一个神经元不能通过前面神经元的结果进行推断，为了解决这一问题科学家提出了递归神经网络（Recurrent Neural Networks，RNN）。RNN是包含循环的神经网络（如图所示），允许信息的持久化。其中A可以看作神经网络的一个缩影，接受某时刻的输入。
    ![](https://ai-studio-static-online.cdn.bcebos.com/cb4612d76e1b4ff88bb225ee0d2498c3f82603a2eb93427c9b03aeec2a14f3d3)
    为了更直观的展示，将回路拆分开来，用一个连续的序列进行表示（如图所示）。一个循环神经网络可以看作是若干个相同的基本单元连接起来，每一个基本单元都可以将信息传递到下一个基本单元。
    ![](https://ai-studio-static-online.cdn.bcebos.com/015cd225083f406093f6dfe89a7b87952427e18090bd4cafb64e49a7b574b5c9)
    常规的RNN存在一个问题就是，无法解决“长期依赖”（long-term dependency）问题，即有用信息和预测点相隔较远。以词语预测为例，“我来自中国，我会讲中文”，这句话里面有用信息与预测点相隔较近，RNN可以很轻易的推断出下一个词语应该是中文，但假如有用信息与预测点相隔较远，如“我来自中国……我会讲中文”，此时RNN便无法推断出接下来的词语。换句话说，RNN的信息持久性不够高，不能保持几十甚至上百步。
  为了弥补传统RNN的这个缺点，人们引入了LSTM（long short-term memory）这个模型。LSTM可以看作是一种特殊的RNN，相较于传统RNN，LSTM天生就对长期依赖有着很好的支持。LSTM模型的核心思想主要有两个，分别为记忆元组（memory cell）和非线性的门单元（nonlinear gating unit），其中记忆元组用于保持系统的状态，非线性的门单元用于在每一个时间点调节流入和流出记忆元组的信息。每个递归的神经网络都可以分解成无数个基本重复单元，传统的RNN是这样，LSTM也是如此。在传统的RNN里面，基本重复单元内部结构十分简单，通常只有一个简单的神经网络层（通常为一个tanh模块，如图所示）；在LSTM中，使用了四个神经网络层并且彼此之间以一种特殊的关系进行交互（如图所示）。
    ![](https://ai-studio-static-online.cdn.bcebos.com/f790b396df904eb2a9721902ecfddf7a237e09974f0d47608971c2376ce589ec)
    ![](https://ai-studio-static-online.cdn.bcebos.com/40df0800e1ae4e28b721895d2cd1131933e2d120e093481f9d33f205adf32931)
    

