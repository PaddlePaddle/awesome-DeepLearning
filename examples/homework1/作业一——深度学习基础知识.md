# <center>作业一——深度学习基础知识</center>

**<p align="right">张森源</p>** 

**<p align="right">18040500018</p>**

---
### <p align="right">请老师看word写的orz赶作业这个没写好</p>


## ①深度学习发展历史

- **1943年**

由神经科学家**麦卡洛克(W.S.McCilloch)** 和**数学家皮兹（W.Pitts）**在《数学生物物理学公告》上发表论文《神经活动中内在思想的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity）。建立了神经网络和数学模型，称为MCP模型。所谓**MCP**模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，也就诞生了所谓的“模拟大脑”，人工神经网络的大门由此开启。

- **1958年**

计算机科学家**罗森布拉特（ Rosenblatt）**提出了两层神经元组成的神经网络，称之为**“感知器”(Perceptrons)**。第一次将MCP用于**机器学习（machine learning）分类(classification)**。“感知器”算法算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年,该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。

- **1969年**

纵观科学发展史，无疑都是充满曲折的，深度学习也毫不例外。 1969年，美国数学家及人工智能先驱 **Marvin Minsky** 在其著作中证明了感知器本质上是一种**线性模型（linear model）**，只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了将近20年的停滞。

- **1986年**

由神经网络之父 **Geoffrey Hinton** 在1986年发明了适用于多层感知器（MLP）的**BP（Backpropagation）**算法，并采用**Sigmoid**进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。

- **90年代时期**

1991年BP算法被指出存在梯度消失问题，也就是说在误差梯度后项传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该问题直接阻碍了深度学习的进一步发展。

此外90年代中期，支持向量机算法诞生（SVM算法）等各种浅层机器学习模型被提出，SVM也是一种有监督的学习模型，应用于模式识别，分类以及回归分析等。支持向量机以统计学为基础，和神经网络有明显的差异，支持向量机等算法的提出再次阻碍了深度学习的发展。

- **发展期 2006年 - 2012年**

2006年，加拿大多伦多大学教授、机器学习领域泰斗、神经网络之父—— Geoffrey Hinton 和他的学生 Ruslan Salakhutdinov 在顶尖学术刊物《科学》上发表了一篇文章，该文章提出了深层网络训练中梯度消失问题的解决方案：**无监督预训练对权值进行初始化+有监督训练微调**。斯坦福大学、纽约大学、加拿大蒙特利尔大学等成为研究深度学习的重镇，至此开启了深度学习在学术界和工业界的浪潮。

2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。2011年以来，微软首次将DL应用在语音识别上，取得了重大突破。微软研究院和Google的语音识别研究人员先后采用DNN技术降低语音识别错误率20％~30％，是语音识别领域十多年来最大的突破性进展。2012年，DNN技术在图像识别领域取得惊人的效果，在ImageNet评测上将错误率从26％降低到15％。在这一年，DNN还被应用于制药公司的DrugeActivity预测问题，并获得世界最好成绩。

- **爆发期 2012 - 2017**

2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。

2013、2014、2015、2016年，通过ImageNet图像识别比赛，DL的网络结构，训练方法，GPU硬件的不断进步，促使其在其他领域也在不断的征服战场。

2016年3月，由谷歌（Google）旗下DeepMind公司开发的AlphaGo(基于深度学习)与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜；2016年末2017年初，该程序在中国棋类网站上以“大师”（Master）为注册帐号与中日韩数十位围棋高手进行快棋对决，连续60局无一败绩；2017年5月，在中国乌镇围棋峰会上，它与排名世界第一的世界围棋冠军柯洁对战，以3比0的总比分获胜。围棋界公认阿尔法围棋的棋力已经超过人类职业围棋顶尖水平。

##②人工智能、机器学习、深度学习有什么区别和联系

一个角度讲，人工智能是一种客观存在，是机器学习和深度学习最后想要实现的目标。机器学习是实现人工智能一大类算法的一个统称，包括了深度学习，深度学习是机器学习领域目前最热门、效果最好的一种算法。
另一个角度讲，人工智能是计算机科学的一个分支，它企图了解智能的本质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。
机器学习属于人工智能的一个分支，也是人工智能的和核心。机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法。而深度学习是机器学习领域目前最热门、效果最好的一种算法。
![图1 人工智能、机器学习与深度学习](/images/relationship.jpg)
<center>图1 人工智能、机器学习与深度学习</center><br></br>

##③神经元、单层感知机和多层感知机
## 神经元

在生物学中，神经元细胞有兴奋与抑制两种状态。大多数神经元细胞在正常情况下处于抑制状态，一旦某个神经元受到刺激并且电位超过一定的阈值后，这个神经元细胞就被激活，处于兴奋状态，并向其他神经元传递信息。基于神经元细胞的结构特性与传递信息方式，神经科学家 Warren McCulloch 和逻辑学家 Walter Pitts 合作提出了“McCulloch–Pitts (MCP) neuron”模型。在人工神经网络中，MCP模型成为人工神经网络中的最基本结构。MCP模型结构如 **图1** 所示。

![图2 MCP 模型结构](/images/neurons.png)

<center>图2 MCP 模型结构</center><br></br>

从 **图1** 可见，给定 n 个二值化（0或1）的输入数据 $x_i$ ($1\le i\le n$)与连接参数 $w_i$ ($1\le i\le n$)，MCP 神经元模型对输入数据线性加权求和，然后使用函数 $\varPhi \left(  \right)$ 将加权累加结果映射为 0 或 1 ，以完成两类分类的任务：
$$
y=\varPhi \left( \sum_{i=1}^n{w_ix_i} \right)
$$

其中 $w_i$ 为预先设定的连接权重值（一般在 0 和 1 中取一个值或者 1 和 -1 中取一个值），用来表示其所对应输入数据对输出结果的影响（即权重）。$\varPhi \left(  \right)$ 将输入端数据与连接权重所得线性加权累加结果与预先设定阈值 $\theta$ 进行比较，根据比较结果输出 1 或 0。

具体而言，如果线性加权累加结果（即 $\sum_{i=1}^m{w_ix_i}
$）大于阈值 $\theta$，则函数 $\varPhi \left(  \right)$ 的输出为1、否则为0。也就是说，如果线性加权累加结果大于阈值 $\theta$，则神经元细胞处于兴奋状态，向后传递 1 的信息，否则该神经元细胞处于抑制状态而不向后传递信息。

从另外一个角度来看，对于任何输入数据 $x_i$ ($1\le i\le n$)，MCP 模型可得到 1 或 0 这样的输出结果，实现了将输入数据分类到 1 或 0 两个类别中，解决了二分类问题。




## 单层感知机

## 1. 单层感知机模型

1957年 Frank Rosenblatt 提出了一种简单的人工神经网络，被称之为感知机。早期的感知机结构和 MCP 模型相似，由一个输入层和一个输出层构成，因此也被称为“单层感知机”。感知机的输入层负责接收实数值的输入向量，输出层则为1或-1两个值。单层感知机可作为一种二分类线性分类模型，结构如 **图1** 所示。

![图3 感知机模型](/images/single_perceptron.png)

<center>图3 感知机模型</center><br></br>

单层感知机的模型可以简单表示为：

$$
f(x) = sign(w*x+b)
$$
对于具有 $n$ 个输入 $x_{i}$ 以及对应连接权重系数为 $w_j$ 的感知机，首先通过线性加权得到输入数据的累加结果 $z$：$z=w_1 x_1+w_2 x_2+ ... +b$。这里 $x_1,x_2,...,x_n$ 为感知机的输入，$w_1,w_2,...,w_n$为网络的权重系数，$b$ 为偏置项（$bias$）。然后将 $z$ 作为激活函数 $\varPhi(\cdot)$ 的输入，这里激活函数 $\varPhi(\cdot)$为 $sign$ 函数，其表达式为：


$$
sign(x) = 
\begin{cases}
+1 \qquad & x \geq 0 \\
-1 \qquad & x \lt 0
\end{cases}
$$
 $\varPhi(\cdot)$会将 $z$ 与某一阈值（此例中，阈值为$0$）进行比较，如果大于等于该阈值则感知器输出为 $1$，否则输出为 $-1$。通过这样的操作，输入数据被分类为 $1$ 或 $-1$ 这两个不同类别。

## 2. 训练过程

给定一个 $n$ 维数据集，如果它可以被一个超平面完全分割，那么我们称这个数据集为线性可分数据集，否则，则为线性不可分的数据集。单层感知机只能处理线性可分数据集，其任务是寻找一个线性可分的超平面将所有的正类和负类划分到超平面两侧。单层感知机与 $MCP$ 模型在连接权重设置上是不同的，即感知机中连接权重参数并不是预先设定好的，而是通过多次迭代训练而得到的。单层感知机通过构建损失函数来计算模型预测值与数据真实值间的误差，通过最小化代价函数来优化模型参数。

其具体的训练过程为：

1. 定义数据集，变量和参数，其中给定一个$m*n$大小的数据集，$x^0, x^1,...,x^m$ 为训练样本, $x_0^m, x_1^m, ..., x_n^m$为第 $m$ 条训练样本，$d^m$ 为期望结果，$y^m$ 为实际结果，$\eta$ 为学习率，$0 \lt \eta \lt 1$；

2. 对权重系数$w_j$进行初始化，初始值为随机值或全零值。同时，设置 $m = 0$，读取第零条训练样本；

3. 将训练样本输入到单层感知机中，根据模型公式，得到实际输出 $y$ ;

4. 根据如下公式更新权重系数;
   
   
   $$
   w^{m+1} = w^m + \eta[d^m - y^m]x^m
   $$
   
5. 当满足收敛条件时，算法结束；若不满足收敛条件则输入下一条样本继续训练，即 $m = m +1$。通常收敛条件可为：

   * 误差小于某个预先设定的较小值 $\epsilon$ ;
   * 迭代的权重系数间权值变化小于某个较小值；
   * 迭代次数超过设定的最大迭代次数。



## 3. 单层感知机存在的问题

![图4 单层感知机模拟不同逻辑函数功能的示意图](/images/xor.png)

<center>图4 单层感知机模拟不同逻辑函数功能的示意图</center><br></br>

单层感知机可被用来区分线性可分数据。在 **图4** 中，逻辑与(AND)、逻辑与非(NAND)和逻辑或(OR)为线性可分函数，所以可利用单层感知机来模拟这些逻辑函数。但是，由于逻辑异或（XOR）是非线性可分的逻辑函数，因此**单层感知机无法模拟逻辑异或函数的功能**。



## 多层感知机

由于无法模拟诸如异或以及其他复杂函数的功能，使得单层感知机的应用较为单一。一个简单的想法是，如果能在感知机模型中增加若干隐藏层，增强神经网络的非线性表达能力，就会让神经网络具有更强拟合能力。因此，由多个隐藏层构成的多层感知机被提出。

如 **图5** 所示，多层感知机由输入层、输出层和至少一层的隐藏层构成。网络中各个隐藏层中神经元可接收相邻前序隐藏层中所有神经元传递而来的信息，经过加工处理后将信息输出给相邻后续隐藏层中所有神经元。

![图5 多层感知机模型](/images/multi_perceptron.png)

<center>图5 多层感知机模型</center><br></br>

在多层感知机中，相邻层所包含的神经元之间通常使用“全连接”方式进行连接。所谓“全连接”是指两个相邻层之间的神经元相互成对连接，但同一层内神经元之间没有连接。多层感知机可以模拟复杂非线性函数功能，所模拟函数的复杂性取决于网络隐藏层数目和各层中神经元数目。





## ④什么是前向传播



假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。
最终不断的通过这种方法一层层的运算，得到输出层结果。对于前向传播来说，不管维度多高，其过程都可以用如下公式表示：

​                                                   α²=σ(z²)=σ(α✳W²+b²)

其中，上标代表层数，星号表示卷积，b表示偏置项bias，σ 表示激活函数。

## ⑤反向传播算法

反向传播算法是多层神经网络的训练中举足轻重的算法。简单的理解，它的确就是复合函数的链式法则。


上面式中的Wij就是相邻两层神经元之间的权值，它们就是深度学习需要学习的参数，也就相当于直线拟合y=k*x+b中的待求参数k和b。

