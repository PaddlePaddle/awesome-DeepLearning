{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, ..., 3.969e+02, 7.880e+00,\n",
       "       1.190e+01])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入需要用到的package\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "# 读入训练数据\n",
    "datafile = './work/housing.data'\n",
    "data = np.fromfile(datafile, sep=' ')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读入之后的数据被转化成1维array，其中array的第0-13项是第一条数据，第14-27项是第二条数据，以此类推.... \n",
    "# 这里对原始数据做reshape，变成N x 14的形式\n",
    "feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS', \n",
    "                 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "feature_num = len(feature_names)\n",
    "data = data.reshape([data.shape[0] // feature_num, feature_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14,)\n",
      "[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00 2.400e+01]\n"
     ]
    }
   ],
   "source": [
    "# 查看数据\r\n",
    "x = data[0]\r\n",
    "print(x.shape)\r\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = 0.8\n",
    "offset = int(data.shape[0] * ratio)\n",
    "training_data = data[:offset]\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 计算train数据集的最大值，最小值，平均值\n",
    "maximums, minimums, avgs = \\\n",
    "                     training_data.max(axis=0), \\\n",
    "                     training_data.min(axis=0), \\\n",
    "     training_data.sum(axis=0) / training_data.shape[0]\n",
    "# 对数据进行归一化处理\n",
    "for i in range(feature_num):\n",
    "    #print(maximums[i], minimums[i], avgs[i])\n",
    "    data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    datafile = './work/housing.data'\r\n",
    "    data = np.fromfile(datafile, sep=' ')\r\n",
    "\r\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\r\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\r\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\r\n",
    "    feature_num = len(feature_names)\r\n",
    "\r\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\r\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\r\n",
    "\r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算训练集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\r\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\r\n",
    "\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        #print(maximums[i], minimums[i], avgs[i])\r\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 获取数据\r\n",
    "training_data, test_data = load_data()\r\n",
    "x = training_data[:, :-1]\r\n",
    "y = training_data[:, -1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / iter   0, loss = 47.6683\n",
      "Epoch   0 / iter   1, loss = 55.5792\n",
      "Epoch   0 / iter   2, loss = 49.0497\n",
      "Epoch   0 / iter   3, loss = 35.5025\n",
      "Epoch   0 / iter   4, loss = 78.2237\n",
      "Epoch   1 / iter   0, loss = 16.4375\n",
      "Epoch   1 / iter   1, loss = 17.6730\n",
      "Epoch   1 / iter   2, loss = 15.2075\n",
      "Epoch   1 / iter   3, loss = 12.3741\n",
      "Epoch   1 / iter   4, loss = 3.5276\n",
      "Epoch   2 / iter   0, loss = 12.5603\n",
      "Epoch   2 / iter   1, loss = 11.9078\n",
      "Epoch   2 / iter   2, loss = 7.1764\n",
      "Epoch   2 / iter   3, loss = 6.3957\n",
      "Epoch   2 / iter   4, loss = 1.2557\n",
      "Epoch   3 / iter   0, loss = 6.8536\n",
      "Epoch   3 / iter   1, loss = 4.1756\n",
      "Epoch   3 / iter   2, loss = 4.5414\n",
      "Epoch   3 / iter   3, loss = 5.3987\n",
      "Epoch   3 / iter   4, loss = 0.1304\n",
      "Epoch   4 / iter   0, loss = 3.4996\n",
      "Epoch   4 / iter   1, loss = 3.2224\n",
      "Epoch   4 / iter   2, loss = 4.3159\n",
      "Epoch   4 / iter   3, loss = 1.2667\n",
      "Epoch   4 / iter   4, loss = 0.0351\n",
      "Epoch   5 / iter   0, loss = 2.2928\n",
      "Epoch   5 / iter   1, loss = 1.0118\n",
      "Epoch   5 / iter   2, loss = 2.8905\n",
      "Epoch   5 / iter   3, loss = 1.8505\n",
      "Epoch   5 / iter   4, loss = 0.1755\n",
      "Epoch   6 / iter   0, loss = 0.7889\n",
      "Epoch   6 / iter   1, loss = 1.5671\n",
      "Epoch   6 / iter   2, loss = 1.8423\n",
      "Epoch   6 / iter   3, loss = 1.5446\n",
      "Epoch   6 / iter   4, loss = 3.0614\n",
      "Epoch   7 / iter   0, loss = 1.1184\n",
      "Epoch   7 / iter   1, loss = 1.0968\n",
      "Epoch   7 / iter   2, loss = 0.6914\n",
      "Epoch   7 / iter   3, loss = 0.7686\n",
      "Epoch   7 / iter   4, loss = 0.1988\n",
      "Epoch   8 / iter   0, loss = 0.8340\n",
      "Epoch   8 / iter   1, loss = 0.7337\n",
      "Epoch   8 / iter   2, loss = 0.8998\n",
      "Epoch   8 / iter   3, loss = 0.5388\n",
      "Epoch   8 / iter   4, loss = 0.2362\n",
      "Epoch   9 / iter   0, loss = 0.4235\n",
      "Epoch   9 / iter   1, loss = 0.4952\n",
      "Epoch   9 / iter   2, loss = 0.7678\n",
      "Epoch   9 / iter   3, loss = 0.8275\n",
      "Epoch   9 / iter   4, loss = 0.2844\n",
      "Epoch  10 / iter   0, loss = 0.2683\n",
      "Epoch  10 / iter   1, loss = 0.8405\n",
      "Epoch  10 / iter   2, loss = 0.4445\n",
      "Epoch  10 / iter   3, loss = 0.4941\n",
      "Epoch  10 / iter   4, loss = 3.1872\n",
      "Epoch  11 / iter   0, loss = 0.4754\n",
      "Epoch  11 / iter   1, loss = 0.3899\n",
      "Epoch  11 / iter   2, loss = 0.3253\n",
      "Epoch  11 / iter   3, loss = 0.5069\n",
      "Epoch  11 / iter   4, loss = 0.1381\n",
      "Epoch  12 / iter   0, loss = 0.4417\n",
      "Epoch  12 / iter   1, loss = 0.4200\n",
      "Epoch  12 / iter   2, loss = 0.3908\n",
      "Epoch  12 / iter   3, loss = 0.2640\n",
      "Epoch  12 / iter   4, loss = 0.1983\n",
      "Epoch  13 / iter   0, loss = 0.2390\n",
      "Epoch  13 / iter   1, loss = 0.4281\n",
      "Epoch  13 / iter   2, loss = 0.4114\n",
      "Epoch  13 / iter   3, loss = 0.2964\n",
      "Epoch  13 / iter   4, loss = 0.1164\n",
      "Epoch  14 / iter   0, loss = 0.2533\n",
      "Epoch  14 / iter   1, loss = 0.3552\n",
      "Epoch  14 / iter   2, loss = 0.2445\n",
      "Epoch  14 / iter   3, loss = 0.2835\n",
      "Epoch  14 / iter   4, loss = 3.6682\n",
      "Epoch  15 / iter   0, loss = 0.3466\n",
      "Epoch  15 / iter   1, loss = 0.2254\n",
      "Epoch  15 / iter   2, loss = 0.2387\n",
      "Epoch  15 / iter   3, loss = 0.3869\n",
      "Epoch  15 / iter   4, loss = 0.2257\n",
      "Epoch  16 / iter   0, loss = 0.2815\n",
      "Epoch  16 / iter   1, loss = 0.2162\n",
      "Epoch  16 / iter   2, loss = 0.3603\n",
      "Epoch  16 / iter   3, loss = 0.2439\n",
      "Epoch  16 / iter   4, loss = 0.1935\n",
      "Epoch  17 / iter   0, loss = 0.2186\n",
      "Epoch  17 / iter   1, loss = 0.2901\n",
      "Epoch  17 / iter   2, loss = 0.2310\n",
      "Epoch  17 / iter   3, loss = 0.2846\n",
      "Epoch  17 / iter   4, loss = 0.1619\n",
      "Epoch  18 / iter   0, loss = 0.1951\n",
      "Epoch  18 / iter   1, loss = 0.2332\n",
      "Epoch  18 / iter   2, loss = 0.1397\n",
      "Epoch  18 / iter   3, loss = 0.4061\n",
      "Epoch  18 / iter   4, loss = 0.1249\n",
      "Epoch  19 / iter   0, loss = 0.3587\n",
      "Epoch  19 / iter   1, loss = 0.1475\n",
      "Epoch  19 / iter   2, loss = 0.2125\n",
      "Epoch  19 / iter   3, loss = 0.2124\n",
      "Epoch  19 / iter   4, loss = 0.0915\n",
      "Epoch  20 / iter   0, loss = 0.1202\n",
      "Epoch  20 / iter   1, loss = 0.2557\n",
      "Epoch  20 / iter   2, loss = 0.3254\n",
      "Epoch  20 / iter   3, loss = 0.1986\n",
      "Epoch  20 / iter   4, loss = 0.0227\n",
      "Epoch  21 / iter   0, loss = 0.1944\n",
      "Epoch  21 / iter   1, loss = 0.2652\n",
      "Epoch  21 / iter   2, loss = 0.2475\n",
      "Epoch  21 / iter   3, loss = 0.1576\n",
      "Epoch  21 / iter   4, loss = 0.1489\n",
      "Epoch  22 / iter   0, loss = 0.1146\n",
      "Epoch  22 / iter   1, loss = 0.1671\n",
      "Epoch  22 / iter   2, loss = 0.2846\n",
      "Epoch  22 / iter   3, loss = 0.2012\n",
      "Epoch  22 / iter   4, loss = 1.8733\n",
      "Epoch  23 / iter   0, loss = 0.1478\n",
      "Epoch  23 / iter   1, loss = 0.3328\n",
      "Epoch  23 / iter   2, loss = 0.1861\n",
      "Epoch  23 / iter   3, loss = 0.1656\n",
      "Epoch  23 / iter   4, loss = 0.0921\n",
      "Epoch  24 / iter   0, loss = 0.1922\n",
      "Epoch  24 / iter   1, loss = 0.2351\n",
      "Epoch  24 / iter   2, loss = 0.1761\n",
      "Epoch  24 / iter   3, loss = 0.2092\n",
      "Epoch  24 / iter   4, loss = 0.0321\n",
      "Epoch  25 / iter   0, loss = 0.1543\n",
      "Epoch  25 / iter   1, loss = 0.2852\n",
      "Epoch  25 / iter   2, loss = 0.1732\n",
      "Epoch  25 / iter   3, loss = 0.1756\n",
      "Epoch  25 / iter   4, loss = 0.0103\n",
      "Epoch  26 / iter   0, loss = 0.2238\n",
      "Epoch  26 / iter   1, loss = 0.1888\n",
      "Epoch  26 / iter   2, loss = 0.1930\n",
      "Epoch  26 / iter   3, loss = 0.1635\n",
      "Epoch  26 / iter   4, loss = 0.0514\n",
      "Epoch  27 / iter   0, loss = 0.1163\n",
      "Epoch  27 / iter   1, loss = 0.1102\n",
      "Epoch  27 / iter   2, loss = 0.2392\n",
      "Epoch  27 / iter   3, loss = 0.2804\n",
      "Epoch  27 / iter   4, loss = 0.0912\n",
      "Epoch  28 / iter   0, loss = 0.1933\n",
      "Epoch  28 / iter   1, loss = 0.2075\n",
      "Epoch  28 / iter   2, loss = 0.2015\n",
      "Epoch  28 / iter   3, loss = 0.1218\n",
      "Epoch  28 / iter   4, loss = 0.1018\n",
      "Epoch  29 / iter   0, loss = 0.1198\n",
      "Epoch  29 / iter   1, loss = 0.2093\n",
      "Epoch  29 / iter   2, loss = 0.2216\n",
      "Epoch  29 / iter   3, loss = 0.1506\n",
      "Epoch  29 / iter   4, loss = 0.2077\n",
      "Epoch  30 / iter   0, loss = 0.3410\n",
      "Epoch  30 / iter   1, loss = 0.1515\n",
      "Epoch  30 / iter   2, loss = 0.1127\n",
      "Epoch  30 / iter   3, loss = 0.1019\n",
      "Epoch  30 / iter   4, loss = 0.0697\n",
      "Epoch  31 / iter   0, loss = 0.2222\n",
      "Epoch  31 / iter   1, loss = 0.1894\n",
      "Epoch  31 / iter   2, loss = 0.1226\n",
      "Epoch  31 / iter   3, loss = 0.1625\n",
      "Epoch  31 / iter   4, loss = 0.0663\n",
      "Epoch  32 / iter   0, loss = 0.2536\n",
      "Epoch  32 / iter   1, loss = 0.1329\n",
      "Epoch  32 / iter   2, loss = 0.1424\n",
      "Epoch  32 / iter   3, loss = 0.1609\n",
      "Epoch  32 / iter   4, loss = 0.0041\n",
      "Epoch  33 / iter   0, loss = 0.1299\n",
      "Epoch  33 / iter   1, loss = 0.2388\n",
      "Epoch  33 / iter   2, loss = 0.1484\n",
      "Epoch  33 / iter   3, loss = 0.1620\n",
      "Epoch  33 / iter   4, loss = 0.0707\n",
      "Epoch  34 / iter   0, loss = 0.2517\n",
      "Epoch  34 / iter   1, loss = 0.0833\n",
      "Epoch  34 / iter   2, loss = 0.1238\n",
      "Epoch  34 / iter   3, loss = 0.2154\n",
      "Epoch  34 / iter   4, loss = 0.0669\n",
      "Epoch  35 / iter   0, loss = 0.1390\n",
      "Epoch  35 / iter   1, loss = 0.1797\n",
      "Epoch  35 / iter   2, loss = 0.1948\n",
      "Epoch  35 / iter   3, loss = 0.1535\n",
      "Epoch  35 / iter   4, loss = 0.0082\n",
      "Epoch  36 / iter   0, loss = 0.0984\n",
      "Epoch  36 / iter   1, loss = 0.2213\n",
      "Epoch  36 / iter   2, loss = 0.2141\n",
      "Epoch  36 / iter   3, loss = 0.1267\n",
      "Epoch  36 / iter   4, loss = 0.0520\n",
      "Epoch  37 / iter   0, loss = 0.1076\n",
      "Epoch  37 / iter   1, loss = 0.1267\n",
      "Epoch  37 / iter   2, loss = 0.1351\n",
      "Epoch  37 / iter   3, loss = 0.2863\n",
      "Epoch  37 / iter   4, loss = 0.0151\n",
      "Epoch  38 / iter   0, loss = 0.1844\n",
      "Epoch  38 / iter   1, loss = 0.0686\n",
      "Epoch  38 / iter   2, loss = 0.1764\n",
      "Epoch  38 / iter   3, loss = 0.2178\n",
      "Epoch  38 / iter   4, loss = 0.0672\n",
      "Epoch  39 / iter   0, loss = 0.1190\n",
      "Epoch  39 / iter   1, loss = 0.2272\n",
      "Epoch  39 / iter   2, loss = 0.1917\n",
      "Epoch  39 / iter   3, loss = 0.1031\n",
      "Epoch  39 / iter   4, loss = 0.0615\n",
      "Epoch  40 / iter   0, loss = 0.1437\n",
      "Epoch  40 / iter   1, loss = 0.1300\n",
      "Epoch  40 / iter   2, loss = 0.1689\n",
      "Epoch  40 / iter   3, loss = 0.1973\n",
      "Epoch  40 / iter   4, loss = 0.0616\n",
      "Epoch  41 / iter   0, loss = 0.1680\n",
      "Epoch  41 / iter   1, loss = 0.1799\n",
      "Epoch  41 / iter   2, loss = 0.1657\n",
      "Epoch  41 / iter   3, loss = 0.1284\n",
      "Epoch  41 / iter   4, loss = 0.0212\n",
      "Epoch  42 / iter   0, loss = 0.1505\n",
      "Epoch  42 / iter   1, loss = 0.2403\n",
      "Epoch  42 / iter   2, loss = 0.0800\n",
      "Epoch  42 / iter   3, loss = 0.1609\n",
      "Epoch  42 / iter   4, loss = 0.1097\n",
      "Epoch  43 / iter   0, loss = 0.2773\n",
      "Epoch  43 / iter   1, loss = 0.1911\n",
      "Epoch  43 / iter   2, loss = 0.0760\n",
      "Epoch  43 / iter   3, loss = 0.0969\n",
      "Epoch  43 / iter   4, loss = 0.0168\n",
      "Epoch  44 / iter   0, loss = 0.1319\n",
      "Epoch  44 / iter   1, loss = 0.1148\n",
      "Epoch  44 / iter   2, loss = 0.1165\n",
      "Epoch  44 / iter   3, loss = 0.2723\n",
      "Epoch  44 / iter   4, loss = 0.0770\n",
      "Epoch  45 / iter   0, loss = 0.0828\n",
      "Epoch  45 / iter   1, loss = 0.2241\n",
      "Epoch  45 / iter   2, loss = 0.1333\n",
      "Epoch  45 / iter   3, loss = 0.1884\n",
      "Epoch  45 / iter   4, loss = 0.0353\n",
      "Epoch  46 / iter   0, loss = 0.1227\n",
      "Epoch  46 / iter   1, loss = 0.2124\n",
      "Epoch  46 / iter   2, loss = 0.0634\n",
      "Epoch  46 / iter   3, loss = 0.2035\n",
      "Epoch  46 / iter   4, loss = 0.5820\n",
      "Epoch  47 / iter   0, loss = 0.1069\n",
      "Epoch  47 / iter   1, loss = 0.0888\n",
      "Epoch  47 / iter   2, loss = 0.1837\n",
      "Epoch  47 / iter   3, loss = 0.2247\n",
      "Epoch  47 / iter   4, loss = 0.0567\n",
      "Epoch  48 / iter   0, loss = 0.1720\n",
      "Epoch  48 / iter   1, loss = 0.0952\n",
      "Epoch  48 / iter   2, loss = 0.0936\n",
      "Epoch  48 / iter   3, loss = 0.2344\n",
      "Epoch  48 / iter   4, loss = 0.1387\n",
      "Epoch  49 / iter   0, loss = 0.1527\n",
      "Epoch  49 / iter   1, loss = 0.1647\n",
      "Epoch  49 / iter   2, loss = 0.1799\n",
      "Epoch  49 / iter   3, loss = 0.0959\n",
      "Epoch  49 / iter   4, loss = 0.3047\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\r\n",
    "\r\n",
    "class Network(object):\r\n",
    "    #def __init__(self, num_of_weights):\r\n",
    "    def __init__(self, num_of_weights):\r\n",
    "        # 随机产生w的初始值\r\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\r\n",
    "       #np.random.seed(0)\r\n",
    "       self.w0 = np.random.randn(num_of_weights, num_of_weights)\r\n",
    "       self.w1 = np.random.randn(num_of_weights, 1)\r\n",
    "       self.b0 = 0.\r\n",
    "       self.b1 = 0.\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        z1 = np.dot(x, self.w0) + self.b0\r\n",
    "        z2=np.maximum(z1,0)\r\n",
    "        z3 =np.dot(z2,self.w1) + self.b1\r\n",
    "        return z1,z2,z3\r\n",
    "    \r\n",
    "    def loss(self, z, y):\r\n",
    "        error = z - y\r\n",
    "        num_samples = error.shape[0]\r\n",
    "        cost = error * error\r\n",
    "        cost = np.sum(cost) / num_samples\r\n",
    "        return cost\r\n",
    "    \r\n",
    "    #梯度更新参数\r\n",
    "    def gradient(self, x, y):\r\n",
    "        z1,z2,z3 = self.forward(x)\r\n",
    "        N = x.shape[0]\r\n",
    "        gradient_w0 = 1. / N * np.sum((z3 - y) * z2 * x, axis=0)\r\n",
    "        gradient_w0 = gradient_w0[:, np.newaxis]\r\n",
    "        gradient_b0 = 1. / N * np.sum((z3 - y)*(y-z2))\r\n",
    "        gradient_w1 = 1. / N * np.sum((z3 - y) * z2, axis=0)\r\n",
    "        gradient_w1 = gradient_w1[:, np.newaxis]\r\n",
    "        gradient_b1 = 1. / N * np.sum(z3 - y)\r\n",
    "        return gradient_w0, gradient_b0,gradient_w1,gradient_b1\r\n",
    "\r\n",
    "\r\n",
    "    def update(self, gradient_w0, gradient_b0,gradient_w1,gradient_b1, eta = 0.01):\r\n",
    "        self.w0 = self.w0 - eta * gradient_w0\r\n",
    "        self.b0 = self.b0 - eta * gradient_b0\r\n",
    "        self.w1 = self.w1 - eta * gradient_w1\r\n",
    "        self.b1 = self.b1 - eta * gradient_b1    \r\n",
    "                \r\n",
    "    def train(self, training_data, num_epochs, batch_size=10, eta=0.01):\r\n",
    "        n = len(training_data)\r\n",
    "        losses = []\r\n",
    "        for epoch_id in range(num_epochs):\r\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\r\n",
    "            # 然后再按每次取batch_size条数据的方式取出\r\n",
    "            np.random.shuffle(training_data)\r\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\r\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\r\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "                #print(self.w.shape)\r\n",
    "                #print(self.b)\r\n",
    "                x = mini_batch[:, :-1]\r\n",
    "                y = mini_batch[:, -1:]\r\n",
    "                z1,z2,z3= self.forward(x)\r\n",
    "                loss = self.loss(z3, y)\r\n",
    "                gradient_w0,gradient_b0,gradient_w1,gradient_b1 = self.gradient(x, y)\r\n",
    "                self.update(gradient_w0,gradient_b0,gradient_w1,gradient_b1, eta)\r\n",
    "                losses.append(loss)\r\n",
    "                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\r\n",
    "                                 format(epoch_id, iter_id, loss))\r\n",
    "        \r\n",
    "        return losses\r\n",
    "\r\n",
    "# 获取数据\r\n",
    "train_data, test_data = load_data()\r\n",
    "\r\n",
    "# 创建网络\r\n",
    "net = Network(13)\r\n",
    "# 启动训练\r\n",
    "losses = net.train(train_data, num_epochs=50, batch_size=100, eta=0.01)\r\n",
    "\r\n",
    "# 画出损失函数的变化趋势\r\n",
    "plot_x = np.arange(len(losses))\r\n",
    "plot_y = np.array(losses)\r\n",
    "plt.plot(plot_x, plot_y)\r\n",
    "plt.show()\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
