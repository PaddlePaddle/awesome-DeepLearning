# 7.7作业（石瑜恺）

## 1.损失函数补充

（1）对数损失函数：就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大 。
$$
L(Y,P(Y|X))=−log P(Y|X)
$$
由此可以引出交叉熵损失：
$$
Loss=-\frac1n\sum_x[ylna+(1-y)ln(1-a)]
$$
（2）平方损失函数：最小二乘法是线性回归的一种，OLS将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理，可以参考【central limit theorem】），最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。
$$
L(Y,f(X))=(Y−f(X))^2
$$
由此，可以引出我们常用的MSE均方差损失：
$$
MSE=\frac1n\sum_{i=1}^n(Yi~−Yi)^2
$$
（3）指数损失函数：对离群点、噪声非常敏感。经常用在AdaBoost算法中。
$$
L(Y|f(X))=\frac1n\sum_{i=1}^nexp[-y_if(x_i)]
$$
（4）PSNR：基于MSE可以得到信号重建的损失函数：
$$
PSNR=10*log^{10}(\frac{MAX_I^2}{MSE})
$$
其中MAX表示图像点颜色的最大数值。

## 2.损失函数python实现

（1）交叉熵损失函数

```
def CrossEntropy_loss(self, y, y_pred, eps):
	loss = -np.sum(y * np.log(y_pred + eps), axis=-1)
	return loss
```

（2）MSE损失函数

```
def MSE_loss(self, y, y_pred):
        return 0.5 * np.sum((y_pred - y) ** 2, axis=-1)
```

（3）指数损失函数

```
def Index_loss(self, y, y_pred):
	loss = -np.sum(y * np.exp(-y_pred * eps), axis=-1)
	return loss
```

（4）PSNR

```
def PSNR(img1, img2):
   mse = np.mean((img1 - img2) ** 2 )
   if mse < 1.0e-10:
      return 100
   return 10 * math.log10(255.0**2/mse)
```

## 3.池化方法

平均池化：计算图像区域的平均值作为该区域池化后的值。

最大池化：选图像区域的最大值作为该区域池化后的值。

随机池化：只需对feature map中的元素按照其概率值大小随机选择，即元素值大的被选中的概率也大。

## 4.数据增强方法

（1）翻转（Flip）
可以对图片进行水平和垂直翻转。一些框架不提供垂直翻转功能。但是，一个垂直反转的图片等同于图片的180度旋转，然后再执行水平翻转。

（2）旋转（Rotation）
一个关键性的问题是当旋转之后图像的维数可能并不能保持跟原来一样。如果你的图片是正方形的，那么以直角旋转将会保持图像大小。如果它是长方形，那么180度的旋转将会保持原来的大小。以更精细的角度旋转图像也会改变最终的图像尺寸。

（3）缩放比例（Scale）
图像可以向外或向内缩放。向外缩放时，最终图像尺寸将大于原始图像尺寸。大多数图像框架从新图像中剪切出一个部分，其大小等于原始图像。我们将在下一节中处理向内缩放，因为它会缩小图像大小，迫使我们对超出边界的内容做出假设。

（4）裁剪（Crop）
与缩放不同，只是从原始图像中随机抽样一个部分。然后将此部分的大小调整为原始图像大小。这种方法通常称为随机裁剪。

（5）高斯噪声（Gaussian Noise）
当神经网络试图学习可能无用的高频特征（大量出现的模式）时，通常会发生过度拟合。具有零均值的高斯噪声基本上在所有频率中具有数据点，从而有效地扭曲高频特征。这也意味着较低频率的组件（通常是您的预期数据）也会失真，但神经网络可以学会超越它。添加适量的噪音可以增强学习能力。

## 5.图像分类方法

图像分类方法有传统的特征提取加检测、KNN聚类、SVM、深度学习、迁移学习等等。具体方案有AlexNet、VGGNet、GoogleNet等。