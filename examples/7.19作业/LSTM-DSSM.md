# LSTM-DSSM

## 概念

LSTM是一种时间循环神经网络，是为了解决一般的RNN存在的长期依赖问题而专门设计出来的，所有的RNN都具有一种重复神经网络模块的链式形式。在标准RNN中，这个重复的结构模块只有一个非常简单的结构，例如一个tanh层。而LSTM-DSSM 是加入了peephole的 LSTM， 即是LSTM 的一个变种。

## 模型

LSTM的重要模块如下：

<img src="https://blog-10039692.file.myqcloud.com/1501555993000_6630_1501555993959.png" alt="img" style="zoom:80%;" />

LSTM-DSSM的模块如下：

<img src="https://blog-10039692.file.myqcloud.com/1501556197309_9865_1501556198338.png" alt="img" style="zoom: 80%;" />

为了可以看的更清晰，转化为下图：

![img](https://blog-10039692.file.myqcloud.com/1501556209287_3423_1501556210288.png)

这里三条黑线就是所谓的 peephole，传统的 LSTM 中遗忘门、输入门和输出门只用了 h(t-1) 和 xt 来控制门缝的大小，peephole 的意思是说不但要考虑 h(t-1) 和 xt，也要考虑 Ct-1 和 Ct，其中遗忘门和输入门考虑了 Ct-1，而输出门考虑了 Ct。总体来说需要考虑的信息更丰富了。

 LSTM-DSSM 整体的网络结构如下：

<img src="https://blog-10039692.file.myqcloud.com/1501556241446_432_1501556242436.png" alt="img" style="zoom:80%;" />

从红色的部分可以清晰的看到残差传递的方向。

## 作用

应用于各种类型的自然语言处理任务中，例如信息检索、搜索引擎、问答系统、信息流推荐、复述问题、知识检索、机器翻译等。

## 场景

应用于苏宁语义召回系统：

**1. 数据准备**

利用苏宁完善的数据仓库环境，在每日的固定时间执行HIVE脚本，获取用户搜索词、对应的商品Title、编码、品牌品类，质量分等字段的信息。语料按搜索词搜索次数，商品质量排序，选取质量高的作为语料，并且从友商平台上抓取商品信息作为补充。同时，在spark平台上进行初始语料的处理，包括分词、去除无意义词、中英文分开处理等预处理步骤，最后要将处理完毕的语料转化成一个正DOC，四个负DOC, 即可以直接应用于模型训练的形式。另外还需要针对召回，对语料进行去重处理，避免召回同一件商品的情况出现。最后，给召回系统提供待召回的商品title集以及准确对应的商品ID和商品质量分集。

**2.在线匹配**

从上文可以看出，模型将准备好的待召回商品title集处理为npy文件，存在磁盘之中。另外输出了tf-serving服务框架所需要的pb格式模型，可以将用户搜索词实时转化为语义向量。接下来要做的事情就是计算query向量与所有商品语义向量之间的两两余弦距离，返回TopN, 找到对应的商品ID，提供给前台展示。

需要注意的是，这里找TopN的过程需要‘快准狠’，我们使用的Facebook开源的Faiss框架，为了保证准确度，没有使用任何自带的高级索引功能，只使用最简单的暴力计算两两之间的距离，这样肯定能找到正确结果。通过测试，在百万级数据的规模，维度为256，使用暴力检索，耗时也不到1ms，完全可以接受。另外出于业务目标考虑，提高高质量分物品的权重，这简单通过余弦距离乘以质量分来实现，人为提高高质量分物品与搜索词的余弦距离。

**3.系统环境**

整个语义召回系统的系统环境组成较为清晰，包括在Spark平台上的大规模数据处理，jupyter深度学习平台上的模型训练和语义向量生成，Linux主机上的Faiss匹配找出TopN的过程，tf-serving线上服务器的布置，以及方便结果调试的前端匹配结果展示。



## 优缺点：

优点：可以解决CNN-DSSM 无法捕获较远距离上下文特征的缺点

缺点：LSTM-DSSM既是端到端模型也是弱监督模型，需要海量样本数据且会有效果不可控的隐患。
