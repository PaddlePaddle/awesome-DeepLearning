模型拓展题——FastText:一个词向量计算和文本分类模型

FastText简介
    预备知识
    对句子或单词的所有长度为N的子句或子字符串进行操作，如2-gram中，对“girl”的字符串为“gi”,“ir”,"rl"进行操作，通常有操作如下：
    ![](https://ai-studio-static-online.cdn.bcebos.com/cd48793c00bc4be2ab422072b11701f6598ce9d57a4f445dbeb71f3f96f31596)
    
    
    
    FastText模型
1.FastText模型，类似CBOW模型，使用n-gram特征代替单个词的特征，提取序列信息，效果与深度学习分类器持平，但速度快得多。其模型架构如下：
    ![](https://ai-studio-static-online.cdn.bcebos.com/5bf3ba90159945c58ab0d700c463aa8216e42c95ed1a4064b03bf086078095ac)
    对整篇文档的n-gram特征，计算词向量，取平均得到文档的向量表示
将上述文档向量作为线性分类器的输入，并使用层次softmax计算文档属于每个类别的概率
将所有类别按照频率构建哈夫曼二叉树，每次对两个类别进行二分类（如使用LR），决定走左子树或右子树，将复杂度由线性降低为对数
损失函数为对数似然函数取负，即 ![](https://ai-studio-static-online.cdn.bcebos.com/94dcd3116a504d808b4cc0ac779103f3511389189ae14d46bd1d880609addd26)
    在词向量的生成过程中，用的loss函数是NCE或negative sampling，而不是常规的softmax。在《learning tensorflow》这本书中，作者这样说道：but it is sufficient to think of it （NCE） as a sort of efficient approximation to the ordinary softmax function used in classification tasks。由此看来，NCE是softmax的一种近似，但是为什么要做这种近似，而不直接用softmax呢？
    当类别数很大时（CBOW中是单词数），softmax复杂度很高，为了更高效地进行，将softmax计算过程转化为二分类（LR）。具体地，将单词与真实类别的true pair、单词与随机类别的randomly corrupted pair送入分类器，待优化的分类器只需判断输入的pair是真或假即可。（主要思想与负采样和层次softmax相同）。
     ![](https://ai-studio-static-online.cdn.bcebos.com/ca08ea61c8af4a40bab0a11a391edcf3c91410534a97451a93698d7e5cfb3e38)
     
