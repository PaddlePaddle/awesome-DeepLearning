# 层归一化详解

### 概念

在介绍层归一化之前，我们先认识一个问题：为什么要进行归一化的处理？

神经网络的学习过程本质上是在学习数据的分布，如果没有进行归一化的处理，那么每一批次的训练数据的分布是不一样的，从大的方向上来看，神经网络则需要在这多个分布当中找到平衡点，从小的方向上来看 ，由于每层的网络输入数据分布在不断地变化 ，那么会导致每层网络都在找平衡点，显然网络就变得难以收敛 。当然我们可以对输入数据进行归一化处理（例如对输入图像除以255），但这也仅能保证输入层的 数据分布是一样的，并不能保证每层网络输入数据分布是一样的，所以在网络的中间我们也是需要加入归一化的处理。此次我们所介绍的是层归一化(Layer Normalization)

归一化**定义**：数据标准化(Normalization)，也称为归一化，归一化就是将你需要处理的数据在通过某种算法经过处理后，限制将其限定在你需要的一定的范围内。

**层归一化**：通过计算在一个训练样本上某一层所有的神经元的均值和方差来对输入进行归一化。

提出原因：批归一化（batch Normalization，BN）算法对mini-batch数据集过分依赖，无法应用到在线学习任务中（此时mini-batch数据集包含的样例个数为1），在递归神经网络（Recurrent neural network，RNN）中BN的效果也不明显 ，RNN多用于自然语言处理任务，网络在不同训练周期内输入的句子，句子长度往往不同，在RNN中应用BN时，在不同时间周期使用mini-batch数据集的大小都需要不同，计算复杂，而且如果一个测试句子比训练集中的任何一个句子都长，在测试阶段RNN神经网络预测性能会出现严重偏差。

![picture7](.\images\picture7.png)

### 算法流程



1. 计算出均值
   $$
   \mu_B\leftarrow\frac{1}{m}\sum_{i=1}^{m}x_i
   $$
   
2. 计算出方差 
   $$
   \sigma_B^2\leftarrow\frac{1}{m}\sum_{i=1}^{m}(x_i-\mu_B)^2
   $$
   
3. 归一化处理到均值为0，方差为1
   $$
   \hat x_i \leftarrow \frac{x_i-\mu_B}{\sqrt{\sigma_B^2+\epsilon}}
   $$
   
4. 变换重构，恢复出这一层网络所要学到的分布
   $$
   y_i \leftarrow \gamma \hat x_i + \beta  \equiv LN_{\gamma,\beta}(x_i)
   $$

**paddle中的API**： `paddle.nn.functional.layer_norm((x, normalized_shape, weight=None, bias=None, epsilon=1e-05, name=None);`

其中各个参数的含义：

- `x (int)` - 输入，数据类型为float32, float64。
- `normalized_shape (int|list|tuple)` - 期望的输入是 `[ ∗ , normalizedshape[0] , normalizedshape[1] , ... , normalizedshape[−1] ]` ,如果是一个整数，会作用在最后一个维度。
- `weight` (Tensor) - 权重的Tensor, 默认为None。
- `bias` (Tensor) - 偏置的Tensor, 默认为None。
- `epsilon` (float, 可选) - 为了数值稳定加在分母上的值。默认值：1e-05。
- `name` (string, 可选) – LayerNorm的名称, 默认值为None。

### 算法作用

1）改善流经网络的梯度。

2）允许更大的学习率，大幅提高训练速度。应用算法，可以采用初始较大的学习率，学习率的衰减速度也较大，因为算法收敛会比较快。当然即使选择了较小的学习率，因为它具有快速训练收敛的特性，所以收敛速度也会较之前更快。

3）减少对初始化的强烈依赖。

4）改善正则化策略：作为正则化的一种形式，轻微减少了对dropout的需求。

6）可以把训练数据彻底打乱（防止每批训练的时候，某一个样本都经常被挑选到）。

### 应用场景

层归一化在递归神经网络RNN中的效果是受益最大的，它的表现批归优于一化（batch normalization，BN），特别是在长序列和小批量的任务当中 。例如在以下所罗列的任务当中：

1. 图像与语言的顺序嵌入（Order embedding of images and language）
2. 教机器阅读和理解（Teaching machines to read and comprehend）
3. Skip-thought向量（Skip-thought vectors）
4. 使用DRAW对二值化的MNIST进行建模（Modeling binarized MNIST using DRAW）
5. 手写序列生成（Handwriting sequence generation）
6. 排列不变MNIST（Permutation invariant MNIST）

但是，研究表明，在卷积神经网络中，它的表现并没有BN的表现要好。

### 参考文献

> [1]https://blog.csdn.net/u013289254/article/details/99690730
>
> [2]刘建伟,赵会丹,罗雄麟,许鋆.深度学习批归一化及其相关算法研究进展[J].自动化学报,2020,46(06):1090-1120.
>
> [3] Ba J L ,  Kiros J R ,  Hinton G E . Layer Normalization[J].  2016.
>
> [4]https://blog.csdn.net/pipisorry/article/details/95906888
>
> [5]王岩. 深度神经网络的归一化技术研究[D].南京邮电大学,2019.
