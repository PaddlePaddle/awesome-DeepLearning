YOLOV1 
将一幅图像分成SxS个网格(grid cell)，如果某个object的中心 落在这个网格中，则这个网格就负责预测这个object。每个网络需要预测B个BBox的位置信息和confidence（置信度）信息，一个BBox对应着四个位置信息和一个confidence信息。confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息：


YOLOV2
YOLOV2相对V1版本，在继续保持处理速度的基础上，从预测更准确（Better），速度更快（Faster），识别对象更多（Stronger）这三个方面进行了改进。其中识别更多对象也就是扩展到能够检测9000种不同对象，YOLOV2也可称之为YOLO9000。
文章提出了一种新的训练方法–联合训练算法，这种算法可以把这两种的数据集混合到一起。使用一种分层的观点对物体进行分类，用巨量的分类数据集数据来扩充检测数据集，从而把两种不同的数据集混合起来。
联合训练算法的基本思路就是：同时在检测数据集和分类数据集上训练物体检测器（Object Detectors ），用检测数据集的数据学习物体的准确位置，用分类数据集的数据来增加分类的类别量、提升健壮性。

YOLOV3
YOLOV3的模型比之前的模型复杂了不少，可以通过改变模型结构的大小来权衡速度与精度。
多尺度预测 （引入FPN）。
更好的基础分类网络（darknet-53, 类似于ResNet引入残差结构）。
分类器不在使用Softmax，分类损失采用binary cross-entropy loss（二分类交叉损失熵）
YOLOV3不使用Softmax对每个框进行分类，主要考虑因素有两个：
Softmax使得每个框分配一个类别（score最大的一个），而对于Open Images这种数据集，目标可能有重叠的类别标签，因此Softmax不适用于多标签分类。
Softmax可被独立的多个logistic分类器替代，且准确率不会下降。
分类损失采用binary cross-entropy loss。
多尺度预测
每种尺度预测3个box, anchor的设计方式仍然使用聚类,得到9个聚类中心,将其按照大小均分给3个尺度.
尺度1: 在基础网络之后添加一些卷积层再输出box信息.
尺度2: 从尺度1中的倒数第二层的卷积层上采样(x2)再与最后一个16x16大小的特征图相加,再次通过多个卷积后输出box信息.相比尺度1变大两倍.
尺度3: 与尺度2类似,使用了32x32大小的特征图.

YOLOV4
1. 提出了一种高效而强大的目标检测模型。它使每个人都可以使用1080 Ti或2080 Ti GPU 训练超快速和准确的目标检测器。
2. 在检测器训练期间，验证了SOTA的Bag-of Freebies 和Bag-of-Specials方法的影响。
3. 改进了SOTA的方法，使它们更有效，更适合单GPU训练，包括CBN，PAN，SAM等。文章将目前主流的目标检测器框架进行拆分：input、backbone、neck 和 head.

YOLOV5：

(1) CSPDarknet53，CSP就是CSPNet论文里面跨阶段局部融合网络，仿照的是Densenet密集跨层挑层连接思想，但是考虑到内存消耗过大，故修改为部分局部跨层融合做法，图示如上所示
(2) neck模块采用的是PANet和增强模块SPP。SPP结构非常容易理解，就是不同kernel size的pool操作进行融合，在YOLOV3的改进版中也有应用，对整个运行速度影响很小，但是效果提升明显。而PANet是FPN结构的改进版本，目的是加快信息之间的流通，具体细节可以参考想读懂YOLOV4，你需要先了解下列技术(二)
yolov5的模型构建仿照了darknet中采用的cfg模式，即通过配置文件来构建网络，但是考虑到darknet中的cfg文件细粒度过高，对于重新构建网络来说是很累人的，可读性比较差，本文作者借鉴了cfg思想，但是进行了适当改进即不再细分到conv+bn+act层，而最细粒度是模块，为后续模型构建、结构理解有很大好处，但是这种写法缺点是不再能直接采用第三方工具例如netron进行网络模型可视化了。

