# 损失函数方法补充
## 0-1损失函数
当预测错误时，损失函数为1，当预测正确时，损失函数值为0。该损失函数不考虑预测值和真实值的误差程度。只要错误，就是1。
感知机就是用的这种损失函数。但是由于相等这个条件太过严格，因此我们可以放宽条件，即满足 |Y−f(X)|<T 时认为相等。
## Dice Loss
是用来度量集合相似度的度量函数，通常用于计算两个样本之间的像素相似度，公式如下：

![](https://ai-studio-static-online.cdn.bcebos.com/7903be6c803a4de7a1ed69c8edef067dd2d85b33a2cf4b6691de50f59549fb1b)


Dice是公式后面部分,是两个样本A和B的相似度度量。分子是矩阵A和B逐个元素相乘（点乘），再求和，即相交。分母是矩阵分别求和（矩阵内所有元素加起来），再相加。对于二分类问题，Target分割图是只有两个值，因此 可以有效忽视背景像素，只关注要检测的目标部分，预测结果Prediction和真实标签Target越相似，Dice 系数越高，Dice Loss越小。
Dice Loss适用于目标样本极度不均衡的情况，但目标很小时，使用Dice Loss会因为分割不好导致Loss很大，对反向传播有不利的影响，使得训练不稳定。


```python
import paddle.nn as nn
import paddle.nn.functional as F

class SoftDiceLoss(nn.Layer):
    def __init__(self, weight=None, size_average=True):
        super(SoftDiceLoss, self).__init__()
    def forward(self, logits, targets):
        num = targets.size(0)
        smooth = 1
        probs = F.sigmoid(logits)
        m1 = probs.flatten(num, -1)
        m2 = targets.flatten(num, -1)
        intersection = (m1 * m2)
        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)
        score = 1 - score.sum() / num
        return score
```

# 池化方法补充
## 随机池化方法
  stochastic pooling方法非常简单，只需对feature map中的元素按照其概率值大小随机选择，即元素值大的被选中的概率也大。而不像max-pooling那样，永远只取那个最大值元素。

计算过程

  1）先将方格中的元素同时除以它们的和sum，得到概率矩阵；
  
  2）按照概率随机选中方格；
  
  3）pooling得到的值就是方格位置的值。
  
  使用stochastic pooling时(即test过程)，其推理过程也很简单，对矩阵区域求加权平均即可。
  
  在反向传播求导时，只需保留前向传播已经记录被选中节点的位置的值，其它值都为0,这和max-pooling的反向传播非常类似。

　　假设feature map中的pooling区域元素值如下：
 
![](https://ai-studio-static-online.cdn.bcebos.com/b086d741ff0d4df1a884e42fe5946cf932c7df2bcd0a4cfcabfa13a1e842f3e6)

3*3大小的，元素值和sum=0+1.1+2.5+0.9+2.0+1.0+0+1.5+1.0=10
   
方格中的元素同时除以sum后得到的矩阵元素为：

![](https://ai-studio-static-online.cdn.bcebos.com/fa6ff6f8304f4c0683714f8a8e11eacd5f04fd6a49e74d61b79f036879d15200)

每个元素值表示对应位置处值的概率，现在只需要按照该概率来随机选一个，方法是：将其看作是9个变量的多项式分布，然后对该多项式分布采样即可，theano中有直接的multinomial()来函数完成。当然也可以自己用01均匀分布来采样，将单位长度1按照那9个概率值分成9个区间（概率越大，覆盖的区域越长，每个区间对应一个位置），然后随机生成一个数后看它落在哪个区间。
比如如果随机采样后的矩阵为：

![](https://ai-studio-static-online.cdn.bcebos.com/97d02c86db984f1da0bd1ea16970d5e5510dedb848ba4ceabc124ad9c2c7f4be)

则这时候的poolng值为1.5

使用stochastic pooling时(即test过程)，其推理过程也很简单，对矩阵区域求加权平均即可。比如对上面的例子求值过程为： 

0*0+1.1*0.11+2.5*0.25+0.9*0.09+2.0*0.2+1.0*0.1+0*0+1.5*0.15+1.0*0.1=1.625 说明此时对小矩形pooling后的结果为1.625.

在反向传播求导时，只需保留前向传播已经记录被选中节点的位置的值，其它值都为0,这和max-pooling的反向传播非常类似。

Stochastic pooling优点：

1.方法简单;

2.泛化能力更强;

3.可用于卷积层（文章中是与Dropout和DropConnect对比的，说是Dropout和DropConnect不太适合于卷积层. 不过个人感觉这没什么可比性，因为它们在网络中所处理的结构不同）。




# 数据增强方法修改及补充
## 颜色扰动
颜色变换的另一个重要变换是颜色扰动，就是在某一个颜色空间通过增加或减少某些颜色分量，或者更改颜色通道的顺序。

## 添加Coarse Dropout噪声
基于噪声的数据增强就是在原来的图片的基础上，随机叠加一些噪声，最常见的做法就是高斯噪声。更复杂一点的就是在面积大小可选定、位置随机的矩形区域上丢弃像素产生黑色矩形块，从而产生一些彩色噪声，以Coarse Dropout方法为代表，甚至还可以对图片上随机选取一块区域并擦除图像信息。

# 图像分类方法综述
## KNN
那么KNN算法如何应用到图像分类问题中，其实问题也就是如何评价一张待分类的图像A与P个训练样本图像中间的距离呢？
其中关键的问题就是图像的特征选择成什么，把问题往更大的方面考虑下，对于图像而言，（传统）机器学习与深度学习的一个很大区别是后者的自动特征抽取，所以深度学习的问世在一定程度上改变了人们对图像处理问题的侧重点，从特征描述到网络结构。所以在下面我们可以不严格的分为两类考虑，直接使用图像与使用一种图像特征提取方法。

1.直接分类

所谓的直接分类本质上是将图像的每个像素点的像素值作为特征，那么此时两种图像的距离（假设使用L1）就是每个对应位置的像素点的像素值差值的绝对值的和。

2.对特征分类

然后很多时候我们不会直接使用像素值作为图像的特征来使用，因为它并不能从本质上反映了人对图像的认知，比如我们将一张图稍稍向一个方向平移一段距离，在人眼看来他们应该是一类，甚至就是同一张，但是如果用像素值计算距离的话，距离确很大。
所以在更多的时候，要计算距离的对象是一些描述子生成的特征，举个例子，HOG+SVM的方法在行人检测中有很好的效果，而SVM的作用也是个分类器，如果换成KNN的话也是可行的（可行指的是原理上可行，效果如何并未考证），所以此时KNN计算的对象其实是HOG生成的描述子，而不再是图像的像素。

但是很不幸的是，KNN在图像问题中几乎不会使用，这个观点来源于斯坦福CS231n，它的原话是 K-Nearest Neighbor on images never used.
原因有两个：
1.很差的测试效率；
2.整个图像水平的距离度量可能非常不直观。
如说第二个原因可以靠着一些特征描述子来解决的话，那么第一个问题就是KNN算法的硬伤，在机器学习中其实我们对测试阶段的时间容忍要远远高于训练阶段，因为最终使用模型解决问题时足够快就可以了，CNN普遍是这样。但是这个问题在KNN中就会无限的暴露出来，“在线”学习的方式决定了样本量越大，分类过程就会越慢。

总结

1.对于样本不平均问题，KNN相比于其他监督学习算法容忍度更差。

2.KNN的计算量和数据存储量都很大。

3.但是KNN的思想简单，在某些方便可以带来很高的准确率，比如在经典的手写数字识别问题上，KNN的准确率可以排在第二位。

4.KNN是一种在线的学习方式，效率低，而且样本量越大效率就越低。
## SVM

SVM(Support Vector Machine，支持向量机)，是一种二类分类模型，其基本模型定义为特征空间上的即那个最大的线性分类器，器学习策略是间隔最大化，最终可转化为一个凸二次规划问题的解决。(线性支持向量机、非线性支持向量机)。

一.线性SVM

SVM的主要思想是建立一个超平面作为决策曲面，是的正例和反例之间的隔离边缘被最大化。对于二维线性可分情况，令H为把两类训练样本没有错误地分开的分类县，H1、H2分别为过各类中离分类线最近的样本且平行于分类线的直线，它们之间的距离讲座分类间隔。所谓最优分类线就是要求分类线不但能将两类正确分开，而且使分类间隔最大。在高维空间，最优分类线就成为最优分类线。

1.线性分类器。
即用一个超平面将正负样本分离开。

2.最优分类面
对于SVM，存在一个分类面，两点集到此平面的最小距离最大，两个点集中的边缘点到此平面的距离最大

二.非线性SVM与核函数

在学习样本是线性不可分，但却是非线性可分的情况下，可以通过非线性变换吧学习样本变换到高维空间，使其咋高维空间里是线性可分的。用K(x,y)代替原来的点积(x,y)，Mercer定理(任何半正定的函数都可以作为核函数)指出，核函数K(x,y)通过与其想联系的非线性变换φ隐含地把特征向量映射到高维特征空间，使得系学习样本成为线性可分的。常用核函数有：

(1).多项式核函数；

(2).径向基函数；

(3).Sigmoid函数；

## CNN
