### CBOW实验原理

#### 1、概念介绍

​	2013年，Mikolov提出了经典的word2vec算法，该算法通过上下文来学习语义信息。word2vec算法有两种训练模式，Skip Gram和CBOW（constinuous bags of words），其中Skip Gram根据目标单词预测上下文，CBOW根据上下文预测目标单词，最后使用模型的部分参数作为词向量。

![](https://ai-studio-static-online.cdn.bcebos.com/fde96072fef244389681cb324a503869256eee636d5440f392b1a71ea1b26270)

+ 第一层为输入层：包含context(w)中上下文的２×win（窗口）个词向量。即对应目标单词w，选取其上下文各win个单词的词向量作为输入。
+ 第二层为投影层：将输入层的２×win个向量做累加求和。
+ 第三层为输出层：对应一颗二叉树，叶子节点共Ｎ个，对应词典里的每个词。 我们是通过哈弗曼树来求得某个词的条件概率的。假设某个词ｗ，从根节点出发到ｗ这个叶子节点，中间会经过４词分支，每一次分支都可以视为一次二分类。从二分类来说，word2ecv定义分到左边为负类（编码为１），分到右边为正类（编码label为０）。在逻辑回归中，一个节点被分为正类的概率为ｐ，分为负类的概率为１－ｐ。将每次分类的结果累乘则得到p(w|Context(w))

#### 2、训练过程

+ Step1:准备好语料，将训练数据保存为txt文件中。
  另取一些数据作为测试数据。
+ Step2:设置一个类class，保存词以及它的哈夫曼树路径、哈弗曼编码、词频。
+ Step3:初始化各类参数，扫描语料库，统计词频，并依据每个词的词频生成生成哈弗曼树。生成哈弗曼树后生成每个词的哈弗曼编码以及路径。初始化输入层词向量syn0以及哈弗曼树上非叶子结点的向量syn1。
+ Step4:训练，迭代优化。 训练过程中就是通过不断的输入，用随机梯度上升的方法，去更新词向量的值（syn0），非叶子结点处向量的值(syn1)。实质上就是让词向量在词向量空间中找到正确的位置。

