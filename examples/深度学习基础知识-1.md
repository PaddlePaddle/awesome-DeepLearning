# 深度学习基础知识-1

## **深度学习发展历史**

<center>
<img style="border-radius: 0.3125em;
box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
src="figure/history_of_DL.jpg">
<br>
<div style="color:orange;
display: inline-block;
color: #999;
padding: 2px;">深度学习发展历史</div>
</center>

​                                                                                                                                                                                     图片来源于网络

### **$\bullet$**1943——MCP模型的建立

由神经科学家麦卡洛克(W.S.McCilloch) 和数学家皮兹（W.Pitts）在《数学生物物理学公告》上发表论文《神经活动中内在思想的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity）。建立了神经网络和数学模型，称为MCP模型。所谓MCP模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，也就诞生了所谓的“模拟大脑”，人工神经网络的大门由此开启。

### $\bullet$1958——感知机模型

计算机科学家罗森布拉特（ Rosenblatt）提出了两层神经元组成的神经网络，称之为“感知器”(Perceptrons)。第一次将MCP用于机器学习（machine learning）分类(classification)。“感知器”算法算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年,该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。

### $\bullet$1969——第一次低谷

美国数学家及人工智能先驱 马文·明斯基（Marvin Minsky） 在其著作中证明了感知器本质上是一种线性模型（linear model），只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了将近20年的停滞。

### $\bullet$1986——多层感知机

由神经网络之父 Geoffrey Hinton 在1986年发明了适用于多层感知器（MLP）的BP（Backpropagation）算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。

### $\bullet$20世纪90年代——第二次低谷

1991年BP算法被指出存在梯度消失问题，也就是说在误差梯度后项传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该问题直接阻碍了深度学习的进一步发展。

此外90年代中期，支持向量机算法诞生（SVM算法）等各种浅层机器学习模型被提出，SVM也是一种有监督的学习模型，应用于模式识别，分类以及回归分析等。支持向量机以统计学为基础，和神经网络有明显的差异，支持向量机等算法的提出再次阻碍了深度学习的发展。

### $\bullet$现代——爆发期

2006年，杰弗里·辛顿（Geoffrey Hinton） 和他的学生 Ruslan Salakhutdinov 在顶尖学术刊物《科学》上提出了深层网络训练中梯度消失问题的解决方案：**无监督预训练对权值进行初始化+有监督训练微调**。斯坦福大学、纽约大学、加拿大蒙特利尔大学等成为研究深度学习的重镇，至此开启了深度学习在学术界和工业界的浪潮。

2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。2011年以来，微软首次将DL应用在语音识别上，取得了重大突破。微软研究院和Google的语音识别研究人员先后采用DNN技术降低语音识别错误率20％~30％，是语音识别领域十多年来最大的突破性进展。2012年，DNN技术在图像识别领域取得惊人的效果，在ImageNet评测上将错误率从26％降低到15％。在这一年，DNN还被应用于制药公司的DrugeActivity预测问题，并获得世界最好成绩。

2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。

## 人工智能、机器学习、深度学习的区别与联系

人工智能企图了解智能的本质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术，当下人工智能大爆炸的核心驱动。

<center>
<img style="border-radius: 0.3125em;
box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
src="figure/AI_ML_DL.png">
<br>
<div style="color:orange;
display: inline-block;
color: #999;
padding: 2px;">AI、ML、DL的关系</div>
</center>

## 神经元、单层感知机、多层感知机

### $\bullet$MCP模型

受生物神经元细胞的启发，神经科学家Warren McCulloch和逻辑学家Walter Pitts基于神经元细胞的结构特性与传递信息的方式，提出了MCP模型，它是人工神经网络中的最基本结构，MCP模型结构如图所示：

<center>
<img style="border-radius: 0.3125em;
box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
src="figure/MCP.webp">
<br>
<div style="color:orange;
display: inline-block;
color: #999;
padding: 2px;">MCP模型结构</div>
</center>



从上图可见，给定$n$个二值化（0或1）的输入数据 $x_i$ (1≤$i$≤$n$)与连接参数 $w_i$ (1≤$i$≤$n$)，MCP 神经元模型对输入数据线性加权求和，然后使用函数 $\Phi ()$ 将加权累加结果映射为 0 或 1 ，以完成两类分类的任务： $y=\Phi(\sum_{i=1}^n w_ix_i)$

其中 $w_i$ 为预先设定的连接权重值（一般在 0 和 1 中取一个值或者 1 和 -1 中取一个值），用来表示其所对应输入数据对输出结果的影响（即权重）。$\Phi()$ 将输入端数据与连接权重所得线性加权累加结果与预先设定阈值 $θ$ 进行比较，根据比较结果输出 1 或 0。

具体而言，如果线性加权累加结果（即 $\sum_{i=1}^n w_ix_i$）大于阈值 $θ$，则函数 $Φ()$ 的输出为1、否则为0。也就是说，如果线性加权累加结果大于阈值 $θ$，则神经元细胞处于兴奋状态，向后传递 1 的信息，否则该神经元细胞处于抑制状态而不向后传递信息。

从另外一个角度来看，对于任何输入数据 $x_i$ (1≤$i$≤$n$)，MCP 模型可得到 1 或 0 这样的输出结果，实现了将输入数据分类到 1 或 0 两个类别中，解决了二分类问题。

### $\bullet$单层感知机

上述MCP模型是一个神经元结构，但是没有参数学习的过程，1957年Frank Rosenblatt提出了一种简单的人工神经网络——感知机，单层感知机相较于MCP引入了损失函数，并提出了学习的感念，结构如图所示：

<center>
<img style="border-radius: 0.3125em;
box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
src="figure/感知机模型.jfif">
<br>
<div style="color:orange;
display: inline-block;
color: #999;
padding: 2px;">单层感知机模型</div>
</center>

感知机是最基础的神经网络结构，属于前馈型网络，单层感知机是二分类的线性分类模型，输入是被感知数据集的特征向量，输出时数据集的类别{+1,-1}。单层感知机的函数近似非常有限，其决策边界必须是一个超平面，严格要求数据是线性可分的。支持向量机，用核函数修正了感知器的不足，将特征向量有效的映射到更高维的空间使得样本成为线性可分的数据集。

具体单层感知机的模型可以简单表示为：

​                                                                                         $$f(x)=sign(w∗x+b)$$

我们主要考虑两种算法：感知器法则和delta法则。delta法则与感知器法则相比，它可以在线性不可分的训练样本上收敛到目标的最佳近似。

感知器法则的实现步骤：

$\bullet$ Step1 采用随机的方法初始化权值、偏置；

$\bullet$ Step2 反复将训练样本输入感知器模型，若误分类（实际输出与标签不同），修改感知器的权重和偏置；

$\bullet$ Step3 重复这个过程，直至感知器能够正确分类所有的训练样本。

步骤2中权重和偏置的更新公式：
$$
\omega_i=\omega_i+\eta(t-o)x_i
$$
参数$\eta$是学习率。

### $\bullet$多层感知机

由于无法模拟诸如异或以及其他复杂函数的功能，使得单层感知机的应用较为单一，而多层感知机通过增加层数解决了非线性问题。

<center>
<img style="border-radius: 0.3125em;
box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
src="figure/多层感知机模型.jfif">
<br>
<div style="color:orange;
display: inline-block;
color: #999;
padding: 2px;">多层感知机模型</div>
</center>


如图所示，多层感知机由输入层、输出层和至少一层的隐藏层构成。网络中各个隐藏层中神经元可接收相邻前序隐藏层中所有神经元传递而来的信息，经过加工处理后将信息输出给相邻后续隐藏层中所有神经元。

多层感知机可以模拟复杂非线性函数功能，所模拟函数的复杂性取决于网络隐藏层数目和各层中神经元数目，但是需要人为固定一层参数，只能训练其中一层。直到1986年Hinton提出了反向传播算法，使得训练多层网络成为可能。在GPU并行运算能力的大力发展下，网络的层数得以不断增加，新的网络模型也越来越多，感知机也逐渐退出了历史舞台。

## 前向传播

前向传播是指从输入X得到输出Y的过程。

<img src="https://img-blog.csdnimg.cn/img_convert/07e9efff73ad4e8cf528305497236cdf.png" style="zoom: 67%;" >

&emsp;
        举个例子，假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。最终不断的通过这种方法一层层的运算，得到输出层结果。

对于前向传播来说，不管维度多高，其过程都可以用如下公式表示：
$$a^n = \sigma(z^n) = \sigma(a^{n-1}*W^n+b^n)$$ $\quad$ 其中，上标代表层数，星号表示卷积，b表示偏置项bias ，$\sigma$表示激活函数。

使用公式一层一层迭代，最终由输入得到输出。



## 反向传播

反向传播算法，简称BP算法，适合于多层神经元网络的一种学习算法，它建立在梯度下降法的基础上。BP网络的输入输出关系实质上是一种映射关系：一个n输入m输出的BP神经网络所完成的功能是从n维欧氏空间向m维欧氏空间中一有限域的连续映射，这一映射具有高度非线性。它的信息处理能力来源于简单非线性函数的多次复合，因此具有很强的函数复现能力。这是BP算法得以应用的基础。

<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE1LmNuYmxvZ3MuY29tL2Jsb2cvODUzNDY3LzIwMTYwNi84NTM0NjctMjAxNjA2MzAxNTIwMTg5MDYtMTUyNDMyNTgxMi5wbmc?x-oss-process=image/format,png" style="zoom:50%;" >

反向传播算法主要由两个环节(激励传播、权重更新)反复循环迭代，直到网络的对输入的响应达到预定的目标范围为止。

BP算法的学习过程由正向传播过程和反向传播过程组成。在正向传播过程中，输入信息通过输入层经隐含层，逐层处理并传向输出层。如果在输出层得不到期望的输出值，则取输出与期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯量，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。
