## 深度学习基础知识

### ①CNN-DSSM

针对 DSSM 词袋模型丢失上下文信息的缺点，CLSM[2]（convolutional latent semantic model）应运而生，又叫 CNN-DSSM。CNN-DSSM 与 DSSM 的区别主要在于输入层和表示层。

![](\images\3.png)



![](\images\4.png)

（1）卷积层——Convolutional layer

卷积层的作用是提取滑动窗口下的上下文特征。以下图为例，假设输入层是一个 302`*`90000（302 行，9 万列）的矩阵，代表 302 个字向量（query 的和 Doc 的长度一般小于 300，这里少了就补全，多了就截断），每个字向量有 9 万维。而卷积核是一个 3`*`90000 的权值矩阵，卷积核以步长为 1 向下移动，得到的 feature map 是一个 300`*`1 的矩阵，feature map 的计算公式是(输入层维数 302-卷积核大小 3 步长 1)/步长 1=300。而这样的卷积核有 300 个，所以形成了 300 个 300`*`1 的 feature map 矩阵。

![](\images\5.png)

（2）池化层——Max pooling layer

池化层的作用是为句子找到全局的上下文特征。池化层以 Max-over-time pooling 的方式，每个 feature map 都取最大值，得到一个 300 维的向量。Max-over-pooling 可以解决可变长度的句子输入问题（因为不管 Feature Map 中有多少个值，只需要提取其中的最大值）。不过我们在上一步已经做了句子的定长处理（固定句子长度为 302），所以就没有可变长度句子的问题。最终池化层的输出为各个 Feature Map 的最大值，即一个 300`*`1 的向量。这里多提一句，之所以 Max pooling 层要保持固定的输出维度，是因为下一层全链接层要求有固定的输入层数，才能进行训练。

（3）全连接层——Semantic layer

最后通过全连接层把一个 300 维的向量转化为一个 128 维的低维语义向量。全连接层采用 tanh 函数：

![](\images\6.png)

优点：CNN-DSSM 通过卷积层提取了滑动窗口下的上下文信息，又通过池化层提取了全局的上下文信息，上下文信息得到较为有效的保留。

缺点：对于间隔较远的上下文信息，难以有效保留。举个例子，I grew up in France... I speak fluent French，显然 France 和 French 是具有上下文依赖关系的，但是由于 CNN-DSSM 滑动窗口（卷积核）大小的限制，导致无法捕获该上下文信息。

![](\images\1.png)

### ②LSTM-DSSM

LSTM-DSSM 其实用的是 LSTM 的一个变种——加入了peephole[6]的 LSTM。

![](\images\7.png)



![](\images\2.png)

DSSM 的 2 个缺点：

1.DSSM 是端到端的模型，虽然省去了人工特征转化、特征工程和特征组合，但端到端的模型有个问题就是效果不可控。对于一些要保证较高的准确率的场景，用有监督人工标注的 query 分类作为打底，再结合无监督的 word2vec、LDA 等进行语义特征的向量化，显然比较可控（至少 query 分类的准确率可以达到 95%以上）。

2.DSSM 是弱监督模型，因为引擎的点击曝光日志里 Query 和 Title 的语义信息比较弱。举个例子，搜索引擎第一页的信息往往都是 Query 的包含匹配，笔者统计过，完全的语义匹配只有不到 2%。这就意味着几乎所有的标题里都包含用户 Query 里的关键词，而仅用点击和曝光就能作为正负样例的判断？显然不太靠谱，因为大部分的用户进行点击时越靠前的点击的概率越大，而引擎的排序又是由 pCTR、CVR、CPC 等多种因素决定的。从这种非常弱的信号里提取出语义的相似性或者差别，那就需要有海量的训练样本。DSSM 论文中提到，实验的训练样本超过 1 亿。笔者和同事也亲测过，用传统 CTR 预估模型千万级的样本量来训练，模型无法收敛。可是这样海量的训练样本，恐怕只有搜索引擎才有吧？普通的搜索业务 query 有上千万，可资源顶多只有几百万，像论文中说需要挑出点击和曝光置信度比较高且资源热度也比较高的作为训练样本，这样就过滤了 80%的长尾 query 和 Title 结果对，所以也只有搜索引擎才有这样的训练语料了吧。另一方面，超过 1 亿的训练样本作为输入，用深度学习模型做训练，需要大型的 GPU 集群，这个对于很多业务来说也是不具备的条件。

### ③MMoE多任务学习

![](\images\8.png)

它的脑洞大开之处在于跳出了Shared Bottom那种将整个隐藏层一股脑的共享的思维定式，而是将共享层有意识的（按照数据领域之类的）划分成了多个**Expert**，并引入了gate机制，得以个性化组合使用共享层。

观察一下上面Shared Bottom的模型结构图和MMoE的图，不难发现，MMoE实际上就是把Shared Bottom层替换成了一个双Gate的MoE层：

![](\images\9.png)

![](\images\10.png)

![](\images\11.png)

- Gate
   把输入通过一个线性变换映射到![nums_expert](https://math.jianshu.com/math?formula=nums_expert)维，再算个softmax得到每个Expert的权重
- Expert
   简单的基层全连接网络，relu激活，每个Expert独立权重

至于最后到底要怎么连接，要不要跟论文里一样，这些都是个性化自己调整的东西，随自己整就好了。至于损失怎么算，就是几个任务加权求和就可以了。（暂时没看到有什么更加新颖的方法）

最后我们看一下作者拿MMoE和其他多任务模型做的一个对比：

![](\images\12.png)

### ④ShareBottom多任务学习

将id特征embedding，和dense特征concat一起，作为share bottom网络输入，id特征embedding可以使用end2end和预训练两种方式。预训练可以使用word2vec，GraphSAGE等工业界落地的算法，训练全站id embedding特征，在训练dnn或则multi task的过程中fine-tune。end2end训练简单，可以很快就将模型train起来，直接输入id特征，模型从头开始学习id的embedding向量。这种训练方式最致命的缺陷就是训练不充分，某些id在训练集中出现次数较少甚至没有出现过，在inference阶段遇到训练过程中没有遇到的id，就直接走冷启了。
![](\images\13.png)

lacel weight

  调整正样本的比例，对于正负样本分布不均衡的时候，该参数可以使得模型提高对正样本的关注，提高模型的召回,auc指标。

​       当正负样本比例相差悬殊时(我们点击正负样本比例是1：10，订单正负样本比例是千分之一的比例)，提高正样本权重，对AUC提升会比较明显，订单AUC得到明显的提升。每个子任务正负样本比都为1:1可能导致各个子任务之间学习相互抑制。对于正负样本比例比较悬殊的，可以调整正样本为负样本的一半，效果比较好。

loss weight
  调整损失的权重， 让共享层更关注某一个任务，也能解决一部分样本分布不均衡带来的过拟合。通常，MTL训练的过程中，是对多个子任务的损失线性加权:
![](\images\14.png)

这样有个明显的缺点，就是这个w i 需要很强的先验知识，人工预设，在训练的过程中保持不变。大家都知道，损失值在深度学习中的地位，直接决定了梯度的大小和传播，调参给出的权值很难保证我们给出的就是最优解，使得每个子任务都达到最优。这也是多任务学习一个主流的研究方向—pareto 最优解。这是一个经济学的概念，感兴趣的同学可以去了解一下，我会在下面附上相关的参考文献。言归正传，loss weight不合适，会导致正负样本严重不均衡的子任务会快出现过拟合的情况，如下图所示：

![](\images\15.png)

深度学习的表达能力很强，模型很快就学习到了正样本的特征，继续训练导致模型出现过拟合的情况。而对于正样本较多的子任务，模型还没有学习充分。因此，我们需要给损失一个合适的权重，某种程度output weight
  调整模型输出的权重组合。不同的权重值，影响排序时对应的子任务的重要性。对于输出的权重组合，可以利用grid search的思想，选出我们关心的离线评估指标最高的一个组合作为线上排序依据，在离线空间的组合逼近连续空间的最优值。最直接的做法可以将三个子任务的输出，在0到1区间，每隔0.1采样一个权值，在这1000组参数中，选择离线指标最高的一个组合作为最终的输出加权。

### ⑤YouTube深度学习视频

与几乎所有推荐系统一样，Youtube的推荐系统也分为“召回”与“排序”两个子系统。

![](\images\16.jpg)

- 召回：从百万级别的候选视频中选出几百个用户喜欢的视频。
- 对召回系统的要求是，“低延时”与“高精度（precision）”，“个性化”方面可以适当放宽以保证性能要求

- 排序：将召回的视频打分，将用户最有可能点击的视频排在前面。
- 结果的“个性化”是排序的重要任务

Youtube在这两个子系统中，都使用了深度神经网络，结构上并没有太大的不同，主要是体现在所使用的特征上。

- “召回”使用的特征，相对较少和简单，以满足性能要求。

- “排序”使用了更多的特征，以保证精准的个性化。

  召回网络

  ![](\images\17.jpg)

  Youtube将“召回”过程建模成一个拥有海量类别的多分类问题，类别总数就是视频库中的视频总数，数量在百万级别。

  ![](\images\18.png)

  - 上式是一个标准的softmax公式，表示用户U在上下文C的情况下，观看第i个视频的概率
  - V是视频库中所有视频的集合，数量在百万级别
  - ![[公式]](https://www.zhihu.com/equation?tex=v_i) 代表第i个视频的向量，是需要优化的变量，通过训练才得到
  - u是由“用户特征”与“上下文特征”拼接得到的特征向量，后续会讲到，用户历史上看过的视频的向量也是u的组成部分

  接下来，需要解决的问题：

  - 如何构建 ![[公式]](https://www.zhihu.com/equation?tex=v_i) ？如何构建u？
  - 每次计算，上边softmax的分母，都需要遍历V中所有百万级别的所有视频，从而，如何高效训练，成为一个问题
  - 线上召回时，同样需要遍历百万视频才能计算出分母，这样一来，能否满足“低延时”的需求？

