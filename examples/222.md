一/二.损失函数的补充及代码实现

​	损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分

​	机器学习任务中的损失函数可以大体分为两种类型：回归损失和分类损失。在此基础上，在深度学习任务中又发展了很多不同的损失函数，由于在网络训练过程中损失函数指导着网络的学习，因此选择合适的损失函数也很重要。常见的有下面几种：

​	回归损失：平均绝对误差(MAE/L1损失)，平均平方误差(MSE/L2损失)，smooth L1 loss，Huber损失，log cosh loss，quantile loss；

​	分类损失：0-1损失，logistic loss(对数损失)，hinge loss(铰链损失)，exponential loss(指数损失)，KL散度；

​	识别、检测和分割常用的损失：softmax cross-entropy loss，weighted cross-entropy loss，focal loss，OHEM，center loss，triplet loss，contrastive loss，L-softmax，LMCL，IOU loss，GIOU loss，DIOU loss，CIOU loss，dice loss。

**1.回归损失**

1.1 **MAE / L1 loss**：

平均绝对误差(Mean Absolute Error，MAE)是对估计值和真实值之差取绝对值的平均值。

```
import numpy as np
#定义L1损失函数
def L1_loss(y_true,y_pre): 
    return np.sum(np.abs(y_true-y_pre))
print('L1 loss is {}'.format(L1_loss(y_true,y_pre)))
```



1.2  **MSE / L2 Loss**:

均方误差(Mean Square Error，MSE)是对估计值和真实值之差取平方和的平均值。

```
import numpy as np
def L2_loss(y_true,y_pre):
    return np.sum(np.square(y_true-y_pre))
print('L2 loss is {}'.format(L2_loss(y_true,y_pre)))
```



1.3 **Huber loss**：

Huber loss对于离群点非常的有效，它与smooth L1一样同时结合了L1和L2的优点，不过多出来了一个 δ \deltaδ 参数需要进行训练。δ \deltaδ 趋于0时，Huber loss会趋向于MAE；δ \deltaδ 趋于无穷时，Huber loss会趋向于MSE。

```
def Huber(true, pred, delta):
    loss = np.where(np.abs(true-pred) < delta , 0.5*((true-pred)**2), delta*np.abs(true - pred) - 0.5*(delta**2))
    return np.sum(loss)
```



**2.分类损失**

2.1 **0-1 loss**：

以二分类问题为例，错误率=1-正确率，也就是0-1损失函数，由于对每个错分类点都施以相同的惩罚，这样对分类误差更大的点不会得到更多的关注；另外，该函数不连续且非凸，优化困难。

2.2**交叉熵损失函数**

Logistic回归主要用于二分类问题，包含激活函数和损失函数两部分。激活函数是logistic函数（又叫sigmoid函数），损失函数是交叉熵函数。
logistic函数作用在分类算法的最后，它使得对任意的输入值，对应的输出都在区间(0,1)内，可以将输入从实数域映射到(0,1)区间，刚好满足概率的范围。

```
import numpy as np
def cross_entropy(a, y):
    return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))
```



三.池化方法的补充

池化操作是卷积神经网络中的一个特殊的操作，主要就是在一定的区域内提出该区域的关键信息(一个亚采样过程)。其操作往往出现在卷积层之后，它能起到减少卷积层输出的特征量数目的作用，从而能减少模型参数同时能改善过拟合现象。池化操作通过池化模板和步长两个关键变量构成。模板描述了提取信息区域的大小(size_PL)，一般是一个方形窗口；步长(stride)描述了窗口在卷积层输出特征图上的移动步长，一般和模板边长相等(即模板移动前后不重叠)。

1.**平均池化**（average pooling)

平均池化是对池化模板进行均值化操作，这能保留模板内的数据的整体特征从而背景信息。



2.**最大池化**(max pooling)

最大池化是保留模板内信息的最大值，这是在提取纹理特征，保留更多的局部细节。



3.**随机池化**(stochastic pooling)

模板内元素值大的被选中的概率也大，这种方法既不会一直选择max值。但这种池化效果并不稳定即不能保证池化的结果一定是好的，可能产生更坏的结果。随机池化伴随着概率矩阵，每个元素对应一个被选取的概率，模板内概率和为1。



4.**重叠池化**(overlapping pooling)

前三种池化方法一般设置stride和size_PL相等，可以称之为一般方法。如果步长和池化模板尺寸不相等且两个池化区域存在重叠，这种池化方法称之为重叠池化。



5.**金字塔池化**(spatial pyramid pooling)
一般CNN对输入的图像尺寸有着特定的要求，因为这是全卷积层的神经元个数对输入的特征维度是固定的。但采用金字塔池化，则可以将任意图像的卷积特征图像转化为所指定维度的特征向量输入给全卷积层。这就解决了CNN输入图像可以是任意尺寸的问题。
空间金字塔池化是将池化层转化为多尺度的池化，即利用多个不同大小尺度的池化模板来进行池化操作。

四。数据增强

数据增强也叫数据扩增，意思是在不实质性的增加数据的情况下，让有限的数据产生等价于更多数据的价值。数据增强可以分为，**有监督的数据增强和无监督的数据增强方法。**其中有监督的数据增强又可以分为**单样本数据增强和多样本数据增强方法，无监督的数据增强分为生成新的数据和学习增强策略两个方向。**

1.有监督的数据增强

有监督数据增强，即**采用预设的数据变换规则，在已有数据的基础上**进行数据的扩增，包含单样本数据增强和多样本数据增强，其中单样本又包括几何操作类，颜色变换类。

1.1 单样本数据增强

（1）几何变换类

几何变换类即对图像进行几何变换，包括**翻转，旋转，裁剪，变形，缩放**等各类操作，下面展示其中的若干个操作。

- *水平翻转和垂直翻转*
- *随机旋转*
- *随机裁剪*
- *变形缩放*

翻转操作和旋转操作，对于那些对方向不敏感的任务，比如图像分类，都是很常见的操作，在caffe等框架中翻转对应的就是mirror操作。

翻转和旋转不改变图像的大小，而裁剪会改变图像的大小。通常在训练的时候会采用随机裁剪的方法，在测试的时候选择裁剪中间部分或者不裁剪。值得注意的是，在一些竞赛中进行模型测试时，一般都是裁剪输入的多个版本然后将结果进行融合，对预测的改进效果非常明显。

以上操作都不会产生失真，而缩放变形则是失真的。

很多的时候，网络的训练输入大小是固定的，但是数据集中的图像却大小不一，此时就可以选择上面的裁剪成固定大小输入或者缩放到网络的输入大小的方案，后者就会产生失真，通常效果比前者差。

（2）颜色变换类

上面的几何变换类操作，没有改变图像本身的内容，它可能是选择了图像的一部分或者对像素进行了重分布。如果要改变图像本身的内容，就属于颜色变换类的数据增强了，常见的包括**噪声、模糊、颜色变换、擦除、填充**等等。

基于噪声的数据增强就是在原来的图片的基础上，随机叠加一些噪声，最常见的做法就是高斯噪声。更复杂一点的就是在面积大小可选定、位置随机的矩形区域上丢弃像素产生黑色矩形块，从而产生一些彩色噪声，以Coarse Dropout方法为代表，甚至还可以对图片上随机选取一块区域并擦除图像信息。

- *添加Coarse Dropout噪声*
- *颜色扰动*



2.2多样本数据增强

（1）SMOTE

SMOTE即Synthetic Minority Over-sampling Technique方法，它是通过人工合成新样本来处理样本不平衡问题，从而提升分类器性能。

类不平衡现象是很常见的，它指的是数据集中各类别数量不近似相等。如果样本类别之间相差很大，会影响分类器的分类效果。假设小样本数据数量极少，如仅占总体的1%，则即使小样本被错误地全部识别为大样本，在经验风险最小化策略下的分类器识别准确率仍能达到99%，但由于没有学习到小样本的特征，实际分类效果就会很差。

SMOTE方法是基于插值的方法，它可以为小样本类合成新的样本，主要流程为：

第一步，定义好特征空间，将每个样本对应到特征空间中的某一点，根据样本不平衡比例确定好一个采样倍率N；

第二步，对每一个小样本类样本(x,y)，按欧氏距离找出K个最近邻样本，从中随机选取一个样本点，假设选择的近邻点为(xn,yn)。在特征空间中样本点与最近邻样本点的连线段上随机选取一点作为新样本点，满足以下公式：

第三步，重复以上的步骤，直到大、小样本数量平衡。

(2)SamplePairing

SamplePairing方法的原理非常简单，从训练集中随机抽取两张图片分别经过基础数据增强操作(如随机翻转等)处理后经像素以取平均值的形式叠加合成一个新的样本，标签为原样本标签中的一种。这两张图片甚至不限制为同一类别，这种方法对于医学图像比较有效。

经SamplePairing处理后可使训练集的规模从N扩增到N×N。实验结果表明，因SamplePairing数据增强操作可能引入不同标签的训练样本，导致在各数据集上使用SamplePairing训练的误差明显增加，而在验证集上误差则有较大幅度降低。

尽管SamplePairing思路简单，性能上提升效果可观，符合奥卡姆剃刀原理，但遗憾的是可解释性不强。

（3）mixup

mixup是Facebook人工智能研究院和MIT在“Beyond Empirical Risk Minimization”中提出的基于邻域风险最小化原则的数据增强方法，它使用线性插值得到新样本数据。

令(xn,yn)是插值生成的新数据，(xi,yi)和(xj,yj)是训练集随机选取的两个数据，则数据生成方式如下

λ的取指范围介于0到1。提出mixup方法的作者们做了丰富的实验，实验结果表明可以改进深度学习模型在ImageNet数据集、CIFAR数据集、语音数据集和表格数据集中的泛化误差，降低模型对已损坏标签的记忆，增强模型对对抗样本的鲁棒性和训练生成对抗网络的稳定性。



**SMOTE，SamplePairing，mixup三者思路上有相同之处，都是试图将离散样本点连续化来拟合真实样本分布**，不过所增加的样本点在特征空间中仍位于已知小样本点所围成的区域内。如果能够在给定范围之外适当插值，也许能实现更好的数据增强效果。



2.无监督的数据增强

无监督的数据增强方法包括两类：

(1) 通过模型学习数据的分布，随机生成与训练数据集分布一致的图片，代表方法GAN。

(2) 通过模型，学习出适合当前任务的数据增强方法，代表方法AutoAugment。