{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "使用paddle实现房价预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss is: [0.07831255]\n",
      "epoch: 1, iter: 0, loss is: [0.01426903]\n",
      "epoch: 2, iter: 0, loss is: [0.00989218]\n",
      "epoch: 3, iter: 0, loss is: [0.03912247]\n",
      "epoch: 4, iter: 0, loss is: [0.03260785]\n",
      "epoch: 5, iter: 0, loss is: [0.05925754]\n",
      "epoch: 6, iter: 0, loss is: [0.01505163]\n",
      "epoch: 7, iter: 0, loss is: [0.01314226]\n",
      "epoch: 8, iter: 0, loss is: [0.01887406]\n",
      "epoch: 9, iter: 0, loss is: [0.00588439]\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "from paddle.nn import Linear\r\n",
    "import paddle.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import random\r\n",
    "from sklearn.metrics import r2_score\r\n",
    "\r\n",
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    datafile = 'data/data49224/housing.data'\r\n",
    "    data = np.fromfile(datafile, sep=' ', dtype=np.float32)\r\n",
    "\r\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\r\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\r\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\r\n",
    "    feature_num = len(feature_names)\r\n",
    "\r\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\r\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\r\n",
    "\r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算train数据集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\r\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\r\n",
    "    \r\n",
    "    # 记录数据的归一化参数，在预测时对数据做归一化\r\n",
    "    global max_values\r\n",
    "    global min_values\r\n",
    "    global avg_values\r\n",
    "    max_values = maximums\r\n",
    "    min_values = minimums\r\n",
    "    avg_values = avgs\r\n",
    "\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data\r\n",
    "\r\n",
    "\r\n",
    "class Regressor(paddle.nn.Layer):\r\n",
    "    # self代表类的实例自身\r\n",
    "    def __init__(self):\r\n",
    "        # 初始化父类中的一些参数\r\n",
    "        super(Regressor, self).__init__()\r\n",
    "        \r\n",
    "        # 定义一层全连接层，输入维度是13，输出维度是1\r\n",
    "        self.fc1 = Linear(in_features=13, out_features=100)\r\n",
    "        self.fc2 = Linear(in_features=100, out_features=1)\r\n",
    "\r\n",
    "    \r\n",
    "    # 网络的前向计算\r\n",
    "    def forward(self, inputs):\r\n",
    "        x = self.fc1(inputs)\r\n",
    "        x = self.fc2(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "\r\n",
    "# 声明定义好的线性回归模型\r\n",
    "model = Regressor()\r\n",
    "# 开启模型训练模式\r\n",
    "model.train()\r\n",
    "# 加载数据\r\n",
    "training_data, test_data = load_data()\r\n",
    "# 定义优化算法，使用随机梯度下降SGD\r\n",
    "# 学习率设置为0.01\r\n",
    "opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())\r\n",
    "\r\n",
    "\r\n",
    "EPOCH_NUM = 10   # 设置外层循环次数\r\n",
    "BATCH_SIZE = 10  # 设置batch大小\r\n",
    "\r\n",
    "# 定义外层循环\r\n",
    "for epoch_id in range(EPOCH_NUM):\r\n",
    "    # 在每轮迭代开始之前，将训练数据的顺序随机的打乱\r\n",
    "    np.random.shuffle(training_data)\r\n",
    "    # 将训练数据进行拆分，每个batch包含10条数据\r\n",
    "    mini_batches = [training_data[k:k+BATCH_SIZE] for k in range(0, len(training_data), BATCH_SIZE)]\r\n",
    "    # 定义内层循环\r\n",
    "    for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "        x = np.array(mini_batch[:, :-1]) # 获得当前批次训练数据\r\n",
    "        y = np.array(mini_batch[:, -1:]) # 获得当前批次训练标签（真实房价）\r\n",
    "        # 将numpy数据转为飞桨动态图tensor形式\r\n",
    "        house_features = paddle.to_tensor(x)\r\n",
    "        prices = paddle.to_tensor(y)\r\n",
    "        \r\n",
    "        # 前向计算\r\n",
    "        predicts = model(house_features)\r\n",
    "        \r\n",
    "        # 计算损失\r\n",
    "        loss = F.square_error_cost(predicts, label=prices)\r\n",
    "        avg_loss = paddle.mean(loss)\r\n",
    "        if iter_id%50==0:\r\n",
    "            print(\"epoch: {}, iter: {}, loss is: {}\".format(epoch_id, iter_id, avg_loss.numpy()))\r\n",
    "        \r\n",
    "        # 反向传播\r\n",
    "        avg_loss.backward()\r\n",
    "        # 最小化loss,更新参数\r\n",
    "        opt.step()\r\n",
    "        # 清除梯度\r\n",
    "        opt.clear_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1299184\n",
      "-0.2711648291194919\n"
     ]
    }
   ],
   "source": [
    "xx = test_data[:, 0:13]\r\n",
    "yy = test_data[:, [13]]\r\n",
    "\r\n",
    "house_features = paddle.to_tensor(xx)\r\n",
    "prices = paddle.to_tensor(yy)\r\n",
    "\r\n",
    "yy_pred = model(house_features)\r\n",
    "\r\n",
    "a_loss = np.sqrt(((yy_pred.numpy() - yy) ** 2).mean())\r\n",
    "print(a_loss)\r\n",
    "r2=r2_score(yy, yy_pred, multioutput='variance_weighted')\r\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "使用python与numpy实现房价预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 13.41863193557366\n",
      "epoch 10 loss: 12.624472337774511\n",
      "epoch 20 loss: 11.92628098937508\n",
      "epoch 30 loss: 11.306850711064122\n",
      "epoch 40 loss: 10.752403551407639\n",
      "epoch 50 loss: 10.251627061951122\n",
      "epoch 60 loss: 9.797434808853678\n",
      "epoch 70 loss: 9.382661674785723\n",
      "epoch 80 loss: 9.003071057436543\n",
      "epoch 90 loss: 8.654534675280427\n",
      "epoch 100 loss: 8.333669645535478\n",
      "epoch 110 loss: 8.037455735074571\n",
      "epoch 120 loss: 7.763082172783301\n",
      "epoch 130 loss: 7.50812632305749\n",
      "epoch 140 loss: 7.270446094345604\n",
      "epoch 150 loss: 7.048713071484074\n",
      "epoch 160 loss: 6.841312342792019\n",
      "epoch 170 loss: 6.6471111468690225\n",
      "epoch 180 loss: 6.464974672812303\n",
      "epoch 190 loss: 6.293824275287203\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.metrics import r2_score\r\n",
    "import math\r\n",
    "#导入numpy，pandas\r\n",
    "\r\n",
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    datafile = 'data/data49224/housing.data'\r\n",
    "    data = np.fromfile(datafile, sep=' ', dtype=np.float32)\r\n",
    "\r\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\r\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\r\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\r\n",
    "    feature_num = len(feature_names)\r\n",
    "\r\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\r\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\r\n",
    "\r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算train数据集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\r\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\r\n",
    "    \r\n",
    "    # 记录数据的归一化参数，在预测时对数据做归一化\r\n",
    "    global max_values\r\n",
    "    global min_values\r\n",
    "    global avg_values\r\n",
    "    max_values = maximums\r\n",
    "    min_values = minimums\r\n",
    "    avg_values = avgs\r\n",
    "\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data\r\n",
    "\r\n",
    "# 获取数据\r\n",
    "training_data, test_data = load_data()\r\n",
    "x = training_data[:, :-1]\r\n",
    "y = training_data[:, -1:]\r\n",
    "\r\n",
    "hidden_dim = 100\r\n",
    "\r\n",
    "w1 = np.random.randn(x.shape[1],hidden_dim)\r\n",
    "w2 = np.random.randn(hidden_dim,y.shape[1])\r\n",
    "\r\n",
    "lr = 1e-06\r\n",
    "\r\n",
    "losses = []\r\n",
    "for i in range(100):\r\n",
    "    #迭代500次\r\n",
    "    #前向传播\r\n",
    "    h = x.dot(w1) #隐藏层\r\n",
    "    h_relu = np.maximum(h,0) #relu激活函数\r\n",
    "    y_hat = h_relu.dot(w2)\r\n",
    "    \r\n",
    "    #计算损失\r\n",
    "    loss = np.square(y_hat - y).sum()/len(y_hat)\r\n",
    "    losses.append(loss)\r\n",
    "    \r\n",
    "    #计算梯度\r\n",
    "    y_hat_grad = 2.0*(y_hat-y)\r\n",
    "    w2_grad = h_relu.T.dot(y_hat_grad)\r\n",
    "    h_relu_grad = y_hat_grad.dot(w2.T)\r\n",
    "    h_grad = h_relu_grad.copy()\r\n",
    "    h_grad[h < 0] = 0\r\n",
    "    w1_grad = x.T.dot(h_grad)\r\n",
    "    \r\n",
    "    #更新参数\r\n",
    "    w1 = w1 - lr*w1_grad\r\n",
    "    w2 = w2 - lr*w2_grad\r\n",
    "    \r\n",
    "    if i%10 == 0:\r\n",
    "        print(\"epoch \"+str(i)+\" loss:\",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7406329550353123\n",
      "-1052.7818992702794\n"
     ]
    }
   ],
   "source": [
    "plot_x = np.arange(len(losses))\r\n",
    "plot_y = np.array(losses)\r\n",
    "plt.plot(plot_x, plot_y)\r\n",
    "plt.show()\r\n",
    "\r\n",
    "#测试集进行验证\r\n",
    "test = test_data[:, 0:13]\r\n",
    "test_label = test_data[:, [13]]\r\n",
    "\r\n",
    "h = test.dot(w1) #隐藏层\r\n",
    "h_relu = np.maximum(h,0) #relu激活函数\r\n",
    "test_label_pred = h_relu.dot(w2)\r\n",
    "\r\n",
    "a_loss = np.sqrt(((test_label_pred - test_label) ** 2).mean())\r\n",
    "print(a_loss)\r\n",
    "r2=r2_score(test_label, test_label_pred, multioutput='variance_weighted')\r\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "深度学习的发展历史 \n",
    "作为机器学习最重要的一个分支，深度学习近年来发展迅猛，在国内外都引起了广泛的关注。然而深度学习的火热也不是一时兴起的，而是经历了一段漫长的发展史。接下来我们了解一下深度学习的发展历程。 1.深度学习的起源阶段 1943年，心里学家麦卡洛克和数学逻辑学家皮兹发表论文《神经活动中内在思想的逻辑演算》，提出了MP模型。MP模型是模仿神经元的结构和工作原理，构成出的一个基于神经网络的数学模型，本质上是一种“模拟人类大脑”的神经元模型。MP模型作为人工神经网络的起源，开创了人工神经网络的新时代，也奠定了神经网络模型的基础。 1949年，加拿大著名心理学家唐纳德·赫布在《行为的组织》中提出了一种基于无监督学习的规则——海布学习规则(Hebb Rule)。海布规则模仿人类认知世界的过程建立一种“网络模型”，该网络模型针对训练集进行大量的训练并提取训练集的统计特征，然后按照样本的相似程度进行分类，把相互之间联系密切的样本分为一类，这样就把样本分成了若干类。海布学习规则与“条件反射”机理一致，为以后的神经网络学习算法奠定了基础，具有重大的历史意义。 20世纪50年代末，在MP模型和海布学习规则的研究基础上，美国科学家罗森布拉特发现了一种类似于人类学习过程的学习算法——感知机学习。并于1958年，正式提出了由两层神经元组成的神经网络，称之为“感知器”。感知器本质上是一种线性模型，可以对输入的训练集数据进行二分类，且能够在训练集中自动更新权值。感知器的提出吸引了大量科学家对人工神经网络研究的兴趣，对神经网络的发展具有里程碑式的意义。 但随着研究的深入，在1969年，“AI之父”马文·明斯基和LOGO语言的创始人西蒙·派珀特共同编写了一本书籍《感知器》，在书中他们证明了单层感知器无法解决线性不可分问题（例如：异或问题）。由于这个致命的缺陷以及没有及时推广感知器到多层神经网络中，在20世纪70年代，人工神经网络进入了第一个寒冬期，人们对神经网络的研究也停滞了将近20年。 2.深度学习的发展阶段 1982年，著名物理学家约翰·霍普菲尔德发明了Hopfield神经网络。Hopfield神经网络是一种结合存储系统和二元系统的循环神经网络。Hopfield网络也可以模拟人类的记忆，根据激活函数的选取不同，有连续型和离散型两种类型，分别用于优化计算和联想记忆。但由于容易陷入局部最小值的缺陷，该算法并未在当时引起很大的轰动。 直到1986年，深度学习之父杰弗里·辛顿提出了一种适用于多层感知器的反向传播算法——BP算法。BP算法在传统神经网络正向传播的基础上，增加了误差的反向传播过程。反向传播过程不断地调整神经元之间的权值和阈值，直到输出的误差达到减小到允许的范围之内，或达到预先设定的训练次数为止。BP算法完美的解决了非线性分类问题，让人工神经网络再次的引起了人们广泛的关注。\n",
    "图2 深度学习之父杰弗里·辛顿 但是由于八十年代计算机的硬件水平有限，如：运算能力跟不上，这就导致当神经网络的规模增大时，再使用BP算法会出现“梯度消失”的问题。这使得BP算法的发展受到了很大的限制。再加上90年代中期，以SVM为代表的其它浅层机器学习算法被提出，并在分类、回归问题上均取得了很好的效果，其原理又明显不同于神经网络模型，所以人工神经网络的发展再次进入了瓶颈期。 3.深度学习的爆发阶段 2006年，杰弗里·辛顿以及他的学生鲁斯兰·萨拉赫丁诺夫正式提出了深度学习的概念。他们在世界顶级学术期刊《科学》发表的一篇文章中详细的给出了“梯度消失”问题的解决方案——通过无监督的学习方法逐层训练算法，再使用有监督的反向传播算法进行调优。该深度学习方法的提出，立即在学术圈引起了巨大的反响，以斯坦福大学、多伦多大学为代表的众多世界知名高校纷纷投入巨大的人力、财力进行深度学习领域的相关研究。而后又在迅速蔓延到工业界中。 2012年，在著名的ImageNet图像识别大赛中，杰弗里·辛顿领导的小组采用深度学习模型AlexNet一举夺冠。AlexNet采用ReLU激活函数，从根本上解决了梯度消失问题，并采用GPU极大的提高了模型的运算速度。同年，由斯坦福大学著名的吴恩达教授和世界顶尖计算机专家Jeff Dean共同主导的深度神经网络——DNN技术在图像识别领域取得了惊人的成绩，在ImageNet评测中成功的把错误率从26％降低到了15％。深度学习算法在世界大赛的脱颖而出，也再一次吸引了学术界和工业界对于深度学习领域的关注。 随着深度学习技术的不断进步以及数据处理能力的不断提升，2014年，Facebook基于深度学习技术的DeepFace项目，在人脸识别方面的准确率已经能达到97%以上，跟人类识别的准确率几乎没有差别。这样的结果也再一次证明了深度学习算法在图像识别方面的一骑绝尘。 2016年，随着谷歌公司基于深度学习开发的AlphaGo以4:1的比分战胜了国际顶尖围棋高手李世石，深度学习的热度一时无两。后来，AlphaGo又接连和众多世界级围棋高手过招，均取得了完胜。这也证明了在围棋界，基于深度学习技术的机器人已经超越了人类。 2017年，基于强化学习算法的AlphaGo升级版AlphaGo Zero横空出世。其采用“从零开始”、“无师自通”的学习模式，以100:0的比分轻而易举打败了之前的AlphaGo。除了围棋，它还精通国际象棋等其它棋类游戏，可以说是真正的棋类“天才”。此外在这一年，深度学习的相关算法在医疗、金融、艺术、无人驾驶等多个领域均取得了显著的成果。所以，也有专家把2017年看作是深度学习甚至是人工智能发展最为突飞猛进的一年。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "人工智能、机器学习与深度学习之间的关系与区别 \n",
    "人工智能：从概念提出到走向繁荣 1956年，几个计算机科学家相聚在达特茅斯会议，提出了“人工智能”的概念，梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。其后，人工智能就一直萦绕于人们的脑海之中，并在科研实验室中慢慢孵化。之后的几十年，人工智能一直在两极反转，或被称作人类文明耀眼未来的预言，或被当成技术疯子的狂想扔到垃圾堆里。直到2012年之前，这两种声音还在同时存在。 2012年以后，得益于数据量的上涨、运算力的提升和机器学习新算法（深度学习）的出现，人工智能开始大爆发。据领英近日发布的《全球AI领域人才报告》显示，截至2017年一季度，基于领英平台的全球AI（人工智能）领域技术人才数量超过190万，仅国内人工智能人才缺口达到500多万。 人工智能的研究领域也在不断扩大，图二展示了人工智能研究的各个分支，包括专家系统、机器学习、进化计算、模糊逻辑、计算机视觉、自然语言处理、推荐系统等。\n",
    "图二 人工智能研究分支 但目前的科研工作都集中在弱人工智能这部分，并很有希望在近期取得重大突破，电影里的人工智能多半都是在描绘强人工智能，而这部分在目前的现实世界里难以真正实现（通常将人工智能分为弱人工智能和强人工智能，前者让机器具备观察和感知的能力，可以做到一定程度的理解和推理，而强人工智能让机器获得自适应能力，解决一些之前没有遇到过的问题）。 弱人工智能有希望取得突破，是如何实现的，“智能”又从何而来呢？这主要归功于一种实现人工智能的方法——机器学习。 \n",
    "机器学习：一种实现人工智能的方法 机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。 举个简单的例子，当我们浏览网上商城时，经常会出现商品推荐的信息。这是商城根据你往期的购物记录和冗长的收藏清单，识别出这其中哪些是你真正感兴趣，并且愿意购买的产品。这样的决策模型，可以帮助商城为客户提供建议并鼓励产品消费。 机器学习直接来源于早期的人工智能领域，传统的算法包括决策树、聚类、贝叶斯分类、支持向量机、EM、Adaboost等等。从学习方法上来分，机器学习算法可以分为监督学习（如分类问题）、无监督学习（如聚类问题）、半监督学习、集成学习、深度学习和强化学习。 传统的机器学习算法在指纹识别、基于Haar的人脸检测、基于HoG特征的物体检测等领域的应用基本达到了商业化的要求或者特定场景的商业化水平，但每前进一步都异常艰难，直到深度学习算法的出现。 \n",
    "深度学习：一种实现机器学习的技术 深度学习本来并不是一种独立的学习方法，其本身也会用到有监督和无监督的学习方法来训练深度神经网络。但由于近几年该领域发展迅猛，一些特有的学习手段相继被提出（如残差网络），因此越来越多的人将其单独看作一种学习的方法。 最初的深度学习是利用深度神经网络来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。其实有不少想法早年间也曾有过，但由于当时训练数据量不足、计算能力落后，因此最终的效果不尽如人意。 深度学习摧枯拉朽般地实现了各种任务，使得似乎所有的机器辅助功能都变为可能。无人驾驶汽车，预防性医疗保健，甚至是更好的电影推荐，都近在眼前，或者即将实现。 三者的区别和联系 机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术。我们就用最简单的方法——同心圆，可视化地展现出它们三者的关系。 图三 三者关系示意图 目前，业界有一种错误的较为普遍的意识，即“深度学习最终可能会淘汰掉其他所有机器学习算法”。这种意识的产生主要是因为，当下深度学习在计算机视觉、自然语言处理领域的应用远超过传统的机器学习方法，并且媒体对深度学习进行了大肆夸大的报道。 深度学习，作为目前最热的机器学习方法，但并不意味着是机器学习的终点。起码目前存在以下问题：\n",
    "深度学习模型需要大量的训练数据，才能展现出神奇的效果，但现实生活中往往会遇到小样本问题，此时深度学习方法无法入手，传统的机器学习方法就可以处理；\n",
    "有些领域，采用传统的简单的机器学习方法，可以很好地解决了，没必要非得用复杂的深度学习方法；\n",
    "深度学习的思想，来源于人脑的启发，但绝不是人脑的模拟，举个例子，给一个三四岁的小孩看一辆自行车之后，再见到哪怕外观完全不同的自行车，小孩也十有八九能做出那是一辆自行车的判断，也就是说，人类的学习过程往往不需要大规模的训练数据，而现在的深度学习方法显然不是对人脑的模拟。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "神经元、单层感知机与多层感知机 \n",
    "神经元 　　\n",
    "对于神经元的研究由来已久，1904年生物学家就已经知晓了神经元的组成结构。 　　\n",
    "一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。 　　\n",
    "人脑中的神经元形状可以用下图做简单的说明：\n",
    "1943年，心理学家McCulloch和数学家Pitts参考了生物神经元的结构，发表了抽象的神经元模型MP。在下文中，我们会具体介绍神经元模型。\n",
    "神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。 　　\n",
    "连接是神经元中最重要的东西。每一个连接上都有一个权重。 　　\n",
    "一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好。\n",
    "我们使用a来表示输入，用w来表示权值。一个表示连接的有向箭头可以这样理解：在初端，传递的信号大小仍然是a，端中间有加权参数w，经过这个加权后的信号会变成aw，因此在连接的末端，信号的大小就变成了aw。 　　在其他绘图模型里，有向箭头可能表示的是值的不变传递。而在神经元模型里，每个有向箭头表示的是值的加权传递。\n",
    "可见z是在输入和权值的线性加权和叠加了一个函数g的值。在MP模型里，函数g是sgn函数，也就是取符号函数。这个函数当输入大于0时，输出1，否则输出0。\n",
    "下面对神经元模型的图进行一些扩展。首先将sum函数与sgn函数合并到一个圆圈里，代表神经元的内部计算。其次，把输入a与输出z写到连接线的左上方，便于后面画复杂的网络。最后说明，一个神经元可以引出多个代表输出的有向箭头，但值都是一样的。 \n",
    "神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。\n",
    "当我们用“神经元”组成网络以后，描述网络中的某个“神经元”时，我们更多地会用“单元”（unit）来指代。同时由于神经网络的表现形式是一个有向图，有时也会用“节点”（node）来表达同样的意思。 单层感知机 1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字--“感知器”（Perceptron）（有的文献翻译成“感知机”，下文统一用“感知器”来指代）。 \n",
    "感知器是当时首个可以学习的人工神经网络。Rosenblatt现场演示了其学习识别简单图像的过程，在当时的社会引起了轰动。 　\n",
    "人们认为已经发现了智能的奥秘，许多学者和科研机构纷纷投入到神经网络的研究中。美国军方大力资助了神经网络的研究，并认为神经网络比“原子弹工程”更重要。这段时间直到1969年才结束，这个时期可以看作神经网络的第一次高潮。\n",
    "下面来说明感知器模型。 　　\n",
    "在原来MP模型的“输入”位置添加神经元节点，标志其为“输入单元”。其余不变，于是我们就有了下图：从本图开始，我们将权值w1, w2, w3写到“连接线”的中间。\n",
    "在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。 　\n",
    "我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。有一些文献会按照网络拥有的层数来命名，例如把“感知器”称为两层神经网络。但在本文里，我们根据计算层的数量来命名。 　\n",
    "假如我们要预测的目标不再是一个值，而是一个向量，例如[2,3]。那么可以在输出层再增加一个“输出单元”。\n",
    "可以看到，z2的计算中除了三个新的权值：w4，w5，w6以外，其他与z1是一样的。\n",
    "目前的表达公式有一点不让人满意的就是：w4，w5，w6是后来加的，很难表现出跟原先的w1，w2，w3的关系。 \n",
    "因此我们改用二维的下标，用wx,y来表达一个权值。下标中的x代表后一层神经元的序号，而y代表前一层神经元的序号（序号的顺序从上到下）。 　\n",
    "例如，w1,2代表后一层的第1个神经元与前一层的第2个神经元的连接的权值（这种标记方式参照了Andrew Ng的课件）。\n",
    "如果我们仔细看输出的计算公式，会发现这两个公式就是线性代数方程组。因此可以用矩阵乘法来表达这两个公式。 \n",
    "例如，输入的变量是[a1，a2，a3]T（代表由a1，a2，a3组成的列向量），用向量a来表示。方程的左边是[z1，z2]T，用向量z来表示。 \n",
    "系数则是矩阵W（2行3列的矩阵，排列形式与公式中的一样）。\n",
    "于是，输出公式可以改写成： g(W * a) = z 　\n",
    "这个公式就是神经网络中从前一层计算后一层的矩阵运算。 　\n",
    "与神经元模型不同，感知器中的权值是通过训练得到的。因此，根据以前的知识我们知道，感知器类似一个逻辑回归模型，可以做线性分类任务。\n",
    "我们可以用决策分界来形象的表达分类的效果。决策分界就是在二维的数据平面中划出一条直线，当数据的维度是3维的时候，就是划出一个平面，当数据的维度是n维时，就是划出一个n-1维的超平面。 　\n",
    "感知器只能做简单的线性分类任务。 多层感知机 两层神经网络是本文的重点，因为正是在这时候，神经网络开始了大范围的推广与使用。\n",
    "Minsky说过单层神经网络无法解决异或问题。但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法。 \n",
    "1986年，Rumelhar和Hinton等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。目前，大量的教授神经网络的教材，都是重点介绍两层（带一个隐藏层）神经网络的内容。 　\n",
    "这时候的Hinton还很年轻，30年以后，正是他重新定义了神经网络，带来了神经网络复苏的又一春。\n",
    "两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。我们扩展上节的单层神经网络，在右边新加一个层次（只含有一个节点）。 　　现在，我们的权值矩阵增加到了两个，我们用上标来区分不同层次之间的变量。 \n",
    "例如ax(y)代表第y层的第x个节点。z1，z2变成了a1(2)，a2(2)。\n",
    "计算最终输出z的方式是利用了中间层的a1(2)，a2(2)和第二个权值矩阵计算得到的.\n",
    "假设我们的预测目标是一个向量，那么与前面类似，只需要在“输出层”再增加节点即可。\n",
    "我们使用向量和矩阵来表示层次中的变量。a(1)，a(2)，z是网络中传输的向量数据。W(1)和W(2)是网络的矩阵参数。\n",
    "使用矩阵运算来表达整个计算公式的话如下： g(W(1) * a(1)) = a(2); g(W(2) * a(2)) = z;\n",
    "由此可见，使用矩阵运算来表达是很简洁的，而且也不会受到节点数增多的影响（无论有多少节点参与运算，乘法两端都只有一个变量）。因此神经网络的教程中大量使用矩阵运算来描述。 　　需要说明的是，至今为止，我们对神经网络的结构图的讨论中都没有提到偏置节点（bias unit）。事实上，这些节点是默认存在的。它本质上是一个只含有存储功能，且存储值永远为1的单元。在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。正如线性回归模型与逻辑回归模型中的一样。 　\n",
    "偏置单元与后一层的所有节点都有连接，我们设这些参数值为向量b，称之为偏置。\n",
    "可以看出，偏置节点很好认，因为其没有输入（前一层中没有箭头指向它）。有些神经网络的结构图中会把偏置节点明显画出来，有些不会。一般情况下，我们都不会明确画出偏置节点。 　　在考虑了偏置以后的一个神经网络的矩阵运算如下： g(W(1) * a(1) + b(1)) = a(2); g(W(2) * a(2) + b(2)) = z;\n",
    "需要说明的是，在两层神经网络中，我们不再使用sgn函数作为函数g，而是使用平滑函数sigmoid作为函数g。我们把函数g也称作激活函数（active function）。 　\n",
    "事实上，神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。初学者可能认为画神经网络的结构图是为了在程序中实现这些圆圈与线，但在一个神经网络的程序中，既没有“线”这个对象，也没有“单元”这个对象。实现一个神经网络最需要的是线性代数库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "前向传播\n",
    "\n",
    "如图所示，这里讲得已经很清楚了，前向传播的思想比较简单。 举个例子，假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。 最终不断的通过这种方法一层层的运算，得到输出层结果。 对于前向传播来说，不管维度多高，其过程都可以用如下公式表示： a 2 = σ( z 2 ) = σ ( a 1 ∗ W 2 + b 2 ) a^2 = \\sigma(z^2) = \\sigma(a^1 * W^2 + b^2)a2=σ(z2)=σ(a1∗W2+b2) 其中，上标代表层数，星号表示卷积，b表示偏置项bias，σ \\sigmaσ表示激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "反向传播\n",
    "BackPropagation算法是多层神经网络的训练中举足轻重的算法。反向传播算法，简称BP算法，适合于多层神经元网络的一种学习算法，它建立在梯度下降法的基础上。BP网络的输入输出关系实质上是一种映射关系：一个n输入m输出的BP神经网络所完成的功能是从n维欧氏空间向m维欧氏空间中一有限域的连续映射，这一映射具有高度非线性。它的信息处理能力来源于简单非线性函数的多次复合，因此具有很强的函数复现能力。这是BP算法得以应用的基础。简单的理解，它的确就是复合函数的链式法则，但其在实际运算中的意义比链式法则要大的多。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
