[toc]

## 群组归一化(Group Normalization)
### 概念
**GN（Group Normalization）是一种深度学习的归一化方法，可以用来代替BN（Batch Normalization）**

![error](https://amaarora.github.io/images/BN_batch_size.png)

BN是深度学习中常使用的归一化方法，它以batch的维度做归一化。当batch_size过小时，对导致其性能下降。一般来说，每个GPU上的batch设为32最合适，但是对于一些深度学习任务来说，他们的batch_size往往只有1～2，比如目标检测、图像分割、视频分类等。输入的图像数据很大时，较大的batch_size显存会吃不消。且BN在batch维度上进行归一化，但batch维度并不是固定不变的，会导致训练、验证、测试这三个阶段存在不一致。

GN计算的是channel方向每个group的均值和方差，和batch_size没关系，所以不受batch_size大小的约束，随着batch_size的减小，GN的表现基本不受影响。

### 算法流程
![normalization](https://amaarora.github.io/images/GN_BN_LN_IN.png)

深度学习的数据维度一般为[N, C, H, W]格式，其中N为batch_size，C为feature的channel，H/W为feature的高/宽。

GN首先将channel分为许多组（group），即现将feature的维度由[N, C, H, W] $reshape$ 为[N, G, C//G, H, W]，然后再对每一组进行归一化，归一化的维度为[C//G, H, W]。

##### 实现代码
```python
def GroupNorm(x, gamma, beta, G, eps=1e-5):
    """x: input features with shape [N, C, H, W]"""
    """gamma,bate: scale and offset, with shape [1, C, 1, 1]"""
    """G: number of groups for GN"""
    N, C, H, W = x.shape
    x = tf.reshape(x, [N, G, C//G, H, W])
    mean, var = tf.nn.moments(x, [2, 3, 4], keep dims=True)
    x = (x-mean) / tf.sqrt(var + eps)
    x = tf.reshape(x, [N, C, H, W])
    return x*gamma + bate
```

### 作用
对数据进行归一化，将有量纲转化为无量纲，同时数据归一化至同一量级，解决数据间的可比性问题；提高求解最优解的速度。

### 应用场景
GN适用于占用显存比较大的任务以及要求batch_size较小的场景，例如图像分割、目标检测、视频分类等。

***
## 可变形卷积（Deformable Convolution）
### DCN v1
#### 背景
在计算机视觉领域，同一物体在不同场景，角度中未知的几何变换是检测/识别的一大挑战，通常来说我们有两种做法:

1. 通过充足的数据增强，扩充足够多的样本去增强模型适应尺度变换的能力。
2. 设置一些针对几何变换不变的特征或者算法，比如SIFT和sliding windows。

两种方法都有缺陷，第一种方法因为样本的局限性显然模型的泛化能力比较低，无法泛化到一般场景中，第二种方法则因为手工设计的不变特征和算法对于过于复杂的变换是很难的而无法设计。所以作者提出了Deformable Conv（可变形卷积）来解决这个问题。

#### 可变形卷积
可变形卷积顾名思义就是卷积的位置是可变形的，并非在传统的N × N的网格上做卷积，这样能更准确地提取到我们想要的特征（传统的卷积仅仅只能提取到矩形框的特征）。

![Deformable Convolution](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTcxMzExNS00Y2YzYWFmNDdjYjhhNDFkLnBuZw?x-oss-process=image/format,png)

在上面这张图里面，左边传统的卷积显然没有提取到完整绵羊的特征，而右边的可变形卷积则提取到了完整的不规则绵羊的特征。

可变卷积实际上就是在每一个卷积采样点加上了一个偏移量，如下图所示：

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTcxMzExNS1lYjQ2MWRjZDk2MWE3MzllLnBuZw?x-oss-process=image/format,png)
<font size=2> (a) 所示的正常卷积规律的采样 9 个点（绿点），(b)(c)(d) 为可变形卷积，在正常的采样坐标上加上一个位移量（蓝色箭头），其中 (c)(d) 作为 (b) 的特殊情况，展示了可变形卷积可以作为尺度变换，比例变换和旋转变换等特殊情况。</font>

#### 偏移量
可变形卷积就是在传统的卷积操作上加入了一个偏移量，正是这个偏移量才让卷积变形为不规则的卷积，这里要注意这个偏移量可以是小数。那么这个偏移量如何算呢？

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTcxMzExNS1kN2FmYTZkYzA2NjU4NzkzLnBuZw?x-oss-process=image/format,png)

对于输入的一张feature map，假设原来的卷积操作是3×3的，那么为了学习偏移量offset，我们定义另外一个3×3的卷积层（图中上面的那层），输出的维度其实就是原来feature map大小，channel数等于2N（分别表示x,y方向的偏移）。下面的可变形卷积可以看作先基于上面那部分生成的offset做了一个插值操作，然后再执行普通的卷积。

### DCN v2
#### 背景
DCN v1有可能引入了无用的上下文（区域）来干扰我们的特征提取，这显然会降低算法的表现。为了解决这一问题，作者提出了三个解决方法：
1. More Deformable Conv Layers（使用更多的可变形卷积）。
2. Modulated Deformable Modules（在DCNv1基础（添加offset）上添加每个采样点的权重）
3. R-CNN Feature Mimicking（模拟R-CNN的feature）。

##### More Deformable Conv Layers
在DCN v1中只在conv 5中使用了三个可变形卷积，在DCN v2中把conv3到conv5都换成了可变形卷积，提高算法对几何形变的建模能力。

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTcxMzExNS01NTFiNWNiYTZjNjQwYjU4LnBuZw?x-oss-process=image/format,png)

#### Modulated Deformable Modules
为了进一步增强变形卷积网络在空间支持区域的操纵能力，引入了一种调制机制。可变形ConvNets模块不仅可以在感知输入特征时调整偏移量，还可以从不同的空间位置/桶中调节输入特征幅值。在极端情况下，模块可以通过将特征振幅设置为零来决定不接收来自特定位置的信号。所以来自相应空间位置的图像内容会大大减少或不影响模块输出。因此，调制机制为网络模块提供了另一个自由度来调整其空间支持区域。

我们知道在DCN v1中的卷积是添加了一个$offset \Delta{P_n}$：

$$y(P_0)=\sum_{P_n\in R}W(P_n) \times x(P+P_k+\Delta{P_n})$$

为了解决引入了一些无关区域的问题，在DCN v2中我们不只添加每一个采样点的偏移，还添加了一个权重系数$\Delta{m_k}$，来区分我们引入的区域是否为我们感兴趣的区域，假如这个采样点的区域我们不感兴趣，则把权重学习为0即可：

$$y(p)=\sum_{k=1}^{K}w_k \times x(p+p_k+\Delta{p_k})\times \Delta{m_k}$$

总的来说，DCN v1中引入的offset是要寻找有效信息的区域位置，DCN v2中引入权重系数是要给找到的这个位置赋予权重，这两方面保证了有效信息的准确提取。

#### R-CNN Feature Mimicking
把R-CNN当做teacher network，让DCN V2的ROIPooling之后的feature去模拟R-CNN的feature，类似知识蒸馏的做法.

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xNTcxMzExNS0zYWRjMzNmMmVjOTU4YzdjLnBuZw?x-oss-process=image/format,png)

左边的网络为主网络（Faster RCNN），右边的网络为子网络（RCNN）。实现上大致是用主网络训练过程中得到的RoI去裁剪原图，然后将裁剪到的图resize到224×224大小作为子网络的输入，这部分最后提取的特征和主网络输出的1024维特征作为feature mimicking loss的输入，用来约束这2个特征的差异（通过一个余弦相似度计算，如下图所示），同时子网络通过一个分类损失进行监督学习，因为并不需要回归坐标，所以没有回归损失。在inference阶段仅有主网络部分，因此这个操作不会在inference阶段增加计算成本。

**总的loss由三部分组成：mimic loss + R-CNN classification loss + Faster-RCNN loss.**