{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mnist dataset from ./datasets/mnist.json.gz ......\n",
      "mnist dataset load done\n",
      "训练数据集数量:  50000\n",
      "loading mnist dataset from ./datasets/mnist.json.gz ......\n",
      "mnist dataset load done\n",
      "训练数据集数量:  10000\n",
      "epoch_id: 0, batch_id: 0, loss is: [4.125336]\n",
      "epoch_id: 0, batch_id: 100, loss is: [0.37126273]\n",
      "epoch_id: 0, batch_id: 200, loss is: [0.41942185]\n",
      "epoch_id: 0, batch_id: 300, loss is: [0.15893297]\n",
      "epoch_id: 0, batch_id: 400, loss is: [0.1465611]\n",
      "epoch_id: 0, train acc is: 0.10933499783277512, val acc is 0.10603799670934677\n",
      "epoch_id: 1, batch_id: 0, loss is: [0.28063676]\n",
      "epoch_id: 1, batch_id: 100, loss is: [0.10084137]\n",
      "epoch_id: 1, batch_id: 200, loss is: [0.04126925]\n",
      "epoch_id: 1, batch_id: 300, loss is: [0.14755137]\n",
      "epoch_id: 1, batch_id: 400, loss is: [0.07274561]\n",
      "epoch_id: 1, train acc is: 0.10920819640159607, val acc is 0.10621000081300735\n",
      "epoch_id: 2, batch_id: 0, loss is: [0.04018362]\n",
      "epoch_id: 2, batch_id: 100, loss is: [0.05628991]\n",
      "epoch_id: 2, batch_id: 200, loss is: [0.05002473]\n",
      "epoch_id: 2, batch_id: 300, loss is: [0.06819806]\n",
      "epoch_id: 2, batch_id: 400, loss is: [0.04537084]\n",
      "epoch_id: 2, train acc is: 0.1093221977353096, val acc is 0.10605599731206894\n",
      "epoch_id: 3, batch_id: 0, loss is: [0.01335844]\n",
      "epoch_id: 3, batch_id: 100, loss is: [0.06710545]\n",
      "epoch_id: 3, batch_id: 200, loss is: [0.07221249]\n",
      "epoch_id: 3, batch_id: 300, loss is: [0.01276855]\n",
      "epoch_id: 3, batch_id: 400, loss is: [0.01791067]\n",
      "epoch_id: 3, train acc is: 0.10908939689397812, val acc is 0.10615500062704086\n",
      "epoch_id: 4, batch_id: 0, loss is: [0.04317577]\n",
      "epoch_id: 4, batch_id: 100, loss is: [0.00182154]\n",
      "epoch_id: 4, batch_id: 200, loss is: [0.00229842]\n",
      "epoch_id: 4, batch_id: 300, loss is: [0.0247922]\n",
      "epoch_id: 4, batch_id: 400, loss is: [0.02249678]\n",
      "epoch_id: 4, train acc is: 0.10933300107717514, val acc is 0.10616999864578247\n",
      "epoch_id: 5, batch_id: 0, loss is: [0.00131645]\n",
      "epoch_id: 5, batch_id: 100, loss is: [0.0138014]\n",
      "epoch_id: 5, batch_id: 200, loss is: [0.0578677]\n",
      "epoch_id: 5, batch_id: 300, loss is: [0.00821569]\n",
      "epoch_id: 5, batch_id: 400, loss is: [0.01130528]\n",
      "epoch_id: 5, train acc is: 0.10916540026664734, val acc is 0.1061599999666214\n",
      "epoch_id: 6, batch_id: 0, loss is: [0.00160216]\n",
      "epoch_id: 6, batch_id: 100, loss is: [0.00399531]\n",
      "epoch_id: 6, batch_id: 200, loss is: [0.00380849]\n",
      "epoch_id: 6, batch_id: 300, loss is: [0.00153845]\n",
      "epoch_id: 6, batch_id: 400, loss is: [0.01099092]\n",
      "epoch_id: 6, train acc is: 0.10942859947681427, val acc is 0.106283999979496\n",
      "epoch_id: 7, batch_id: 0, loss is: [0.00101606]\n",
      "epoch_id: 7, batch_id: 100, loss is: [0.00988846]\n",
      "epoch_id: 7, batch_id: 200, loss is: [0.00178979]\n",
      "epoch_id: 7, batch_id: 300, loss is: [0.00083396]\n",
      "epoch_id: 7, batch_id: 400, loss is: [0.00291473]\n",
      "epoch_id: 7, train acc is: 0.10952919721603394, val acc is 0.10623300075531006\n",
      "epoch_id: 8, batch_id: 0, loss is: [0.00310417]\n",
      "epoch_id: 8, batch_id: 100, loss is: [0.0165395]\n",
      "epoch_id: 8, batch_id: 200, loss is: [0.00034422]\n",
      "epoch_id: 8, batch_id: 300, loss is: [0.00671941]\n",
      "epoch_id: 8, batch_id: 400, loss is: [0.0008107]\n",
      "epoch_id: 8, train acc is: 0.10922800004482269, val acc is 0.10617300122976303\n",
      "epoch_id: 9, batch_id: 0, loss is: [0.0022109]\n",
      "epoch_id: 9, batch_id: 100, loss is: [0.00105568]\n",
      "epoch_id: 9, batch_id: 200, loss is: [0.00470483]\n",
      "epoch_id: 9, batch_id: 300, loss is: [0.00318833]\n",
      "epoch_id: 9, batch_id: 400, loss is: [0.00129185]\n",
      "epoch_id: 9, train acc is: 0.10970140248537064, val acc is 0.10622700303792953\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "\n",
    "# 数据准备\n",
    "def load_data(mode='train'):\n",
    "    datafile = './datasets/mnist.json.gz'\n",
    "    print('loading mnist dataset from {} ......'.format(datafile))\n",
    "    # 加载json数据文件\n",
    "    data = json.load(gzip.open(datafile))\n",
    "    print('mnist dataset load done')\n",
    "\n",
    "    # 读取到的数据区分训练集，验证集，测试集\n",
    "    train_set, val_set, eval_set = data\n",
    "    if mode=='train':\n",
    "        # 获得训练数据集\n",
    "        imgs, labels = train_set[0], train_set[1]\n",
    "    elif mode=='valid':\n",
    "        # 获得验证数据集\n",
    "        imgs, labels = val_set[0], val_set[1]\n",
    "    elif mode=='eval':\n",
    "        # 获得测试数据集\n",
    "        imgs, labels = eval_set[0], eval_set[1]\n",
    "    else:\n",
    "        raise Exception(\"mode can only be one of ['train', 'valid', 'eval']\")\n",
    "    print(\"训练数据集数量: \", len(imgs))\n",
    "\n",
    "    # 获得数据集长度\n",
    "    imgs_length = len(imgs)\n",
    "\n",
    "    # 定义数据集每个数据的序号，根据序号读取数据\n",
    "    index_list = list(range(imgs_length))\n",
    "    # 读入数据时用到的批次大小\n",
    "    BATCHSIZE = 100\n",
    "\n",
    "    # 定义数据生成器\n",
    "    def data_generator():\n",
    "        if mode == 'train':\n",
    "            # 训练模式下打乱数据\n",
    "            random.shuffle(index_list)\n",
    "        imgs_list = []\n",
    "        labels_list = []\n",
    "        for i in index_list:\n",
    "            # 将数据处理成希望的类型\n",
    "            img = np.reshape(imgs[i], [1, 28, 28]).astype('float32')\n",
    "            label = np.reshape(labels[i], [1]).astype('int64')\n",
    "            imgs_list.append(img) \n",
    "            labels_list.append(label)\n",
    "            if len(imgs_list) == BATCHSIZE:\n",
    "                # 获得一个batchsize的数据，并返回\n",
    "                yield np.array(imgs_list), np.array(labels_list)\n",
    "                # 清空数据读取列表\n",
    "                imgs_list = []\n",
    "                labels_list = []\n",
    "\n",
    "        # 如果剩余数据的数目小于BATCHSIZE，\n",
    "        # 则剩余数据一起构成一个大小为len(imgs_list)的mini-batch\n",
    "        if len(imgs_list) > 0:\n",
    "            yield np.array(imgs_list), np.array(labels_list)\n",
    "    return data_generator\n",
    "\n",
    "\n",
    "# 使用ResNet50网络结构\n",
    "resnet50 = paddle.vision.models.resnet50(num_classes = 10)\n",
    "resnet50.conv1 = paddle.nn.Conv2D(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "\n",
    "\n",
    "# 模型训练、评估与保存\n",
    "class Trainer(object):\n",
    "    def __init__(self, model_path, model, optimizer):\n",
    "        self.model_path = model_path   # 模型存放路径 \n",
    "        self.model = model             # 定义的模型\n",
    "        self.optimizer = optimizer     # 优化器\n",
    "\n",
    "    def save(self):\n",
    "        # 保存模型\n",
    "        paddle.save(self.model.state_dict(), self.model_path)\n",
    "\n",
    "    def val_epoch(self, datasets):\n",
    "        self.model.eval()  # 将模型设置为评估状态\n",
    "        acc = list()\n",
    "        for batch_id, data in enumerate(datasets()):\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "            pred = self.model(images)   # 获取预测值\n",
    "            # 取 pred 中得分最高的索引作为分类结果\n",
    "            pred = paddle.argmax(pred, axis=-1)  \n",
    "            res = paddle.equal(pred, labels)\n",
    "            res = paddle.cast(res, dtype='float32')\n",
    "            acc.extend(res.numpy())  # 追加\n",
    "        acc = np.array(acc).mean()\n",
    "        return acc\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images, labels = data\n",
    "        images = paddle.to_tensor(images)\n",
    "        labels = paddle.to_tensor(labels)\n",
    "        # 前向计算的过程\n",
    "        predicts = self.model(images)\n",
    "        # 计算损失\n",
    "        loss = F.cross_entropy(predicts, labels)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "        # 后向传播，更新参数的过程\n",
    "        avg_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.clear_grad()\n",
    "        return avg_loss\n",
    "\n",
    "    def train_epoch(self, datasets, epoch):\n",
    "        self.model.train()\n",
    "        for batch_id, data in enumerate(datasets()):\n",
    "            loss = self.train_step(data)\n",
    "            # 每训练了100批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"epoch_id: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, loss.numpy()))\n",
    "\n",
    "    def train(self, train_datasets, val_datasets, epochs):\n",
    "        for i in range(epochs):\n",
    "            self.train_epoch(train_datasets, i)\n",
    "            train_acc = self.val_epoch(train_datasets)\n",
    "            val_acc = self.val_epoch(val_datasets)\n",
    "            print(\"epoch_id: {}, train acc is: {}, val acc is {}\".format(i, train_acc, val_acc))\n",
    "            self.save() # 每个epoch保存一次模型\n",
    "\n",
    "\n",
    "# 训练配置\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "model_path = './mnist.pdparams'\n",
    "\n",
    "train_loader = load_data(mode='train')\n",
    "val_loader = load_data(mode='eval')\n",
    "\n",
    "model = resnet50\n",
    "opt = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters())\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_path=model_path,\n",
    "    model=model,\n",
    "    optimizer=opt\n",
    ")\n",
    "\n",
    "trainer.train(train_datasets=train_loader, val_datasets=val_loader, epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
