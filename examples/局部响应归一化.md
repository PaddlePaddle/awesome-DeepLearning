## 局部响应归一化

###  一、概念

​        LRN首次是在2012的AlexNet当中使用，其中使用的激活函数是ReLU，而不是当时更常见的tanh和sigmoid。其中的意图是对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。

### 二、算法流程

​         定义的邻域在整个通道上，对于每个(x,y)位置，在深度维度上进行归一化，其公式如下：

![img](file:///C:\Users\asus\AppData\Local\Temp\ksohtml14668\wps1.jpg) 

​        其中i表示第i个核在位置（x,y）运用激活函数ReLU后的输出，a(x,y)、 b(x,y)分别为归一化前后(x,y)处的像素值，n是同一位置上临近的核的数目，N是通道的总数。参数K, n, alpha, belta 都是超参数，一般设置k=2, n=5, alpha=1*e-4, beta=0.75。

​        从公式来看，特别注意一下∑叠加的方向是沿着通道方向的，即每个点值的平方和是沿着a中的第3维channel方向的，也就是一个点同方向的前面n/2个通道（最小为第0个通道）和后n/2个通道（最大为第d-1个通道）的点的平方和(共n+1个点)。即当前层的像素值会和前后各n/2层相关联，做归一化处理。

### 三、应用

​        这个技术主要是深度学习训练时的一种提高准确度的技术方法。其中caffe、tensorflow等里面是很常见的方法，其跟激活函数是有区别的，LRN一般是在激活、池化后进行的一中处理方法。

### 四、后期争议

​       在2015年 Very Deep Convolutional Networks for Large-Scale Image Recognition.提到LRN基本没什么用。因而在后面的Googlenet，以及之后的一些CNN架构模型，LRN已经不再使用，因为出现了更加有说服能力的块归一化，也称之为批量归一化，即BN。
