```python
# æŸ¥çœ‹å½“å‰æŒ‚è½½çš„æ•°æ®é›†ç›®å½•, è¯¥ç›®å½•ä¸‹çš„å˜æ›´é‡å¯ç¯å¢ƒåä¼šè‡ªåŠ¨è¿˜åŸ
# View dataset directory. 
# This directory will be recovered automatically after resetting environment. 
!ls /home/aistudio/data
```


```python
# æŸ¥çœ‹å·¥ä½œåŒºæ–‡ä»¶, è¯¥ç›®å½•ä¸‹çš„å˜æ›´å°†ä¼šæŒä¹…ä¿å­˜. è¯·åŠæ—¶æ¸…ç†ä¸å¿…è¦çš„æ–‡ä»¶, é¿å…åŠ è½½è¿‡æ…¢.
# View personal work directory. 
# All changes under this directory will be kept even after reset. 
# Please clean unnecessary files in time to speed up environment loading. 
!ls /home/aistudio/work
```


```python
# å¦‚æœéœ€è¦è¿›è¡ŒæŒä¹…åŒ–å®‰è£…, éœ€è¦ä½¿ç”¨æŒä¹…åŒ–è·¯å¾„, å¦‚ä¸‹æ–¹ä»£ç ç¤ºä¾‹:
# If a persistence installation is required, 
# you need to use the persistence path as the following: 
!mkdir /home/aistudio/external-libraries
!pip install beautifulsoup4 -t /home/aistudio/external-libraries
```

    mkdir: cannot create directory â€˜/home/aistudio/external-librariesâ€™: File exists
    Looking in indexes: https://mirror.baidu.com/pypi/simple/
    Collecting beautifulsoup4
    [?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122kB 17.1MB/s eta 0:00:01
    [?25hCollecting soupsieve>1.2; python_version >= "3.0" (from beautifulsoup4)
      Downloading https://mirror.baidu.com/pypi/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl
    Installing collected packages: soupsieve, beautifulsoup4
    Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1
    [33mWARNING: Target directory /home/aistudio/external-libraries/beautifulsoup4-4.9.3.dist-info already exists. Specify --upgrade to force replacement.[0m
    [33mWARNING: Target directory /home/aistudio/external-libraries/bs4 already exists. Specify --upgrade to force replacement.[0m
    [33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve already exists. Specify --upgrade to force replacement.[0m
    [33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve-2.2.1.dist-info already exists. Specify --upgrade to force replacement.[0m



```python
# åŒæ—¶æ·»åŠ å¦‚ä¸‹ä»£ç , è¿™æ ·æ¯æ¬¡ç¯å¢ƒ(kernel)å¯åŠ¨çš„æ—¶å€™åªè¦è¿è¡Œä¸‹æ–¹ä»£ç å³å¯: 
# Also add the following code, 
# so that every time the environment (kernel) starts, 
# just run the following code: 
import sys 
sys.path.append('/home/aistudio/external-libraries')
```

**LSTMç½‘ç»œ**

é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œé€šå¸¸è¢«ç§°ä¸ºLSTMsï¼Œæ˜¯ä¸€ç§ç‰¹æ®Šçš„RNNï¼Œèƒ½å¤Ÿå­¦ä¹ é•¿æœŸä¾èµ–å…³ç³»ã€‚

![](https://ai-studio-static-online.cdn.bcebos.com/562b54421b134507b9215c9319968b874d44f9fef17b4fada4f8ede37a1bebb6)


**åˆ©ç”¨LSTMé¢„æµ‹ä¸‹ä¸€ä¸ªè¯**

æ•°æ®å¤„ç†ï¼šé€‰æ‹©éœ€è¦ä½¿ç”¨çš„æ•°æ®ï¼Œå¹¶åšå¥½å¿…è¦çš„é¢„å¤„ç†å·¥ä½œã€‚

ç½‘ç»œå®šä¹‰ï¼šä½¿ç”¨é£æ¡¨å®šä¹‰å¥½ç½‘ç»œç»“æ„ï¼ŒåŒ…æ‹¬è¾“å…¥å±‚ï¼Œä¸­é—´å±‚ï¼Œè¾“å‡ºå±‚ï¼ŒæŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç®—æ³•ã€‚

ç½‘ç»œè®­ç»ƒï¼šå°†å‡†å¤‡å¥½çš„è®­ç»ƒé›†æ•°æ®é€å…¥ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ï¼Œå¹¶è§‚å¯Ÿå­¦ä¹ çš„è¿‡ç¨‹æ˜¯å¦æ­£å¸¸ï¼Œå¯ä»¥æ‰“å°ä¸­é—´æ­¥éª¤çš„ç»“æœå‡ºæ¥ã€‚

ç½‘ç»œè¯„ä¼°ï¼šä½¿ç”¨æµ‹è¯•é›†æ•°æ®æµ‹è¯•è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œï¼Œçœ‹çœ‹è®­ç»ƒæ•ˆæœå¦‚ä½•


```python
import re
import random
import tarfile
import requests
import numpy as np
import paddle
from paddle.nn import Embedding
import paddle.nn.functional as F
from paddle.nn import LSTM, Embedding, Dropout, Linear
import paddle.fluid as fluid
import numpy as np
import paddle
import paddle.dataset.imikolov as imikolov
from paddle.text.datasets import Imikolov
import paddle.nn.functional as F
from paddle.nn import LSTM, Embedding, Dropout, Linear
from paddle.io import Dataset, BatchSampler, DataLoader
from sklearn import metrics
```

**æ•°æ®å¤„ç†**

é¦–å…ˆï¼Œæ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„è¯­æ–™ç”¨äºè®­ç»ƒword2vecæ¨¡å‹ã€‚

è¯·ç‚¹å‡»[æ­¤å¤„](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)æŸ¥çœ‹æœ¬ç¯å¢ƒåŸºæœ¬ç”¨æ³•.  <br>
Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. 


```python
# å–è¯è¡¨
word_idx=imikolov.build_dict(min_word_freq=200) 
print(len(word_idx))
```

    585


**ç½‘ç»œå®šä¹‰**


```python
class NextWordPredicter(paddle.nn.Layer):
    
    def __init__(self, hidden_size, vocab_size, embedding_size, class_num, num_steps=4, num_layers=1, init_scale=0.1, dropout_rate=None):
        
        # å‚æ•°å«ä¹‰å¦‚ä¸‹ï¼š
        # 1.hidden_sizeï¼Œè¡¨ç¤ºembedding-sizeï¼Œhiddenå’Œcellå‘é‡çš„ç»´åº¦
        # 2.vocab_sizeï¼Œæ¨¡å‹å¯ä»¥è€ƒè™‘çš„è¯è¡¨å¤§å°
        # 3.embedding_sizeï¼Œè¡¨ç¤ºè¯å‘é‡çš„ç»´åº¦
        # 4.class_numï¼Œåˆ†ç±»ä¸ªæ•°ï¼Œç­‰åŒäºvocab_size
        # 5.num_stepsï¼Œè¡¨ç¤ºæ¨¡å‹æœ€å¤§å¯ä»¥è€ƒè™‘çš„å¥å­é•¿åº¦
        # 6.num_layersï¼Œè¡¨ç¤ºç½‘ç»œçš„å±‚æ•°
        # 7.dropout_rateï¼Œè¡¨ç¤ºä½¿ç”¨dropoutè¿‡ç¨‹ä¸­å¤±æ´»çš„ç¥ç»å…ƒæ¯”ä¾‹
        # 8.init_scaleï¼Œè¡¨ç¤ºç½‘ç»œå†…éƒ¨çš„å‚æ•°çš„åˆå§‹åŒ–èŒƒå›´,é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œå†…éƒ¨ç”¨äº†å¾ˆå¤šTanhï¼ŒSigmoidç­‰æ¿€æ´»å‡½æ•°ï¼Œ\
        # è¿™äº›å‡½æ•°å¯¹æ•°å€¼ç²¾åº¦éå¸¸æ•æ„Ÿï¼Œå› æ­¤æˆ‘ä»¬ä¸€èˆ¬åªä½¿ç”¨æ¯”è¾ƒå°çš„åˆå§‹åŒ–èŒƒå›´ï¼Œä»¥ä¿è¯æ•ˆæœ
        super(NextWordPredicter, self).__init__()
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        self.embedding_size = embedding_size
        self.class_num = class_num
        self.num_steps = num_steps
        self.num_layers = num_layers
        self.dropout_rate = dropout_rate
        self.init_scale = init_scale

        # å£°æ˜ä¸€ä¸ªembeddingå±‚ï¼Œç”¨æ¥æŠŠå¥å­ä¸­çš„æ¯ä¸ªè¯è½¬æ¢ä¸ºå‘é‡
        self.embedding = paddle.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, sparse=False, 
                                    weight_attr=paddle.ParamAttr(initializer=paddle.nn.initializer.Uniform(low=-init_scale, high=init_scale)))
        # self.embedding = paddle.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)
        # å£°æ˜ä¸€ä¸ªLSTMæ¨¡å‹ï¼Œç”¨æ¥æŠŠæ¯ä¸ªå¥å­æŠ½è±¡æˆå‘é‡
        self.simple_lstm_rnn = paddle.nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers)
        
        # å£°æ˜ä½¿ç”¨ä¸Šè¿°è¯­ä¹‰å‘é‡æ˜ å°„åˆ°å…·ä½“æƒ…æ„Ÿç±»åˆ«æ—¶æ‰€éœ€è¦ä½¿ç”¨çš„çº¿æ€§å±‚
        # self.cls_fc = paddle.nn.Linear(in_features=self.num_steps*self.hidden_size, out_features=self.class_num, 
                             # weight_attr=None, bias_attr=None)
        self.cls_fc = paddle.nn.Linear(in_features=self.num_steps*self.hidden_size, out_features=self.class_num)
        
        # ä¸€èˆ¬åœ¨è·å–å•è¯çš„embeddingåï¼Œä¼šä½¿ç”¨dropoutå±‚ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›
        self.dropout_layer = paddle.nn.Dropout(p=self.dropout_rate, mode='upscale_in_train')

    # forwadå‡½æ•°å³ä¸ºæ¨¡å‹å‰å‘è®¡ç®—çš„å‡½æ•°ï¼Œå®ƒæœ‰ä¸¤ä¸ªè¾“å…¥ï¼Œåˆ†åˆ«ä¸ºï¼š
    # inputä¸ºè¾“å…¥çš„è®­ç»ƒæ–‡æœ¬ï¼Œå…¶shapeä¸º[batch_size, max_seq_len]
    # labelè®­ç»ƒæ–‡æœ¬å¯¹åº”çš„ä¸‹ä¸€ä¸ªè¯æ ‡ç­¾ï¼Œå…¶shapeç»´[batch_size, 1]
    def forward(self, inputs):
        # è·å–è¾“å…¥æ•°æ®çš„batch_size
        batch_size = inputs.shape[0]

        # é¦–å…ˆæˆ‘ä»¬éœ€è¦å®šä¹‰LSTMçš„åˆå§‹hiddenå’Œcellï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨0æ¥åˆå§‹åŒ–è¿™ä¸ªåºåˆ—çš„è®°å¿†
        init_hidden_data = np.zeros(
            (self.num_layers, batch_size, self.hidden_size), dtype='float32')
        init_cell_data = np.zeros(
            (self.num_layers, batch_size, self.hidden_size), dtype='float32')

        init_hidden = paddle.to_tensor(init_hidden_data)
        #init_hidden.stop_gradient = True
        init_cell = paddle.to_tensor(init_cell_data)
        #init_cell.stop_gradient = True

        # å°†è¾“å…¥çš„å¥å­çš„mini-batchè½¬æ¢ä¸ºè¯å‘é‡è¡¨ç¤ºï¼Œè½¬æ¢åè¾“å…¥æ•°æ®shapeä¸º[batch_size, max_seq_len, embedding_size]
        x_emb = self.embedding(inputs)
        x_emb = paddle.reshape(x_emb, shape=[-1, self.num_steps, self.embedding_size])
        # åœ¨è·å–çš„è¯å‘é‡åæ·»åŠ dropoutå±‚
        if self.dropout_rate is not None and self.dropout_rate > 0.0:
            x_emb = self.dropout_layer(x_emb)
        
        # ä½¿ç”¨LSTMç½‘ç»œï¼ŒæŠŠæ¯ä¸ªå¥å­è½¬æ¢ä¸ºè¯­ä¹‰å‘é‡
        # è¿”å›çš„rnn_outå³ä¸ºæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        rnn_out, (last_hidden, last_cell) = self.simple_lstm_rnn(x_emb, (init_hidden, init_cell))
        #rnn_out, (last_hidden, last_cell) = self.simple_lstm_rnn(x_emb)
        # æå–æœ€åä¸€å±‚éšçŠ¶æ€ä½œä¸ºæ–‡æœ¬çš„è¯­ä¹‰å‘é‡
        rnn_out = paddle.reshape(rnn_out, shape=[batch_size, -1])

        # å°†æ¯ä¸ªå¥å­çš„å‘é‡è¡¨ç¤ºæ˜ å°„åˆ°å…·ä½“çš„ç±»åˆ«ä¸Š, logitsçš„ç»´åº¦ä¸º[batch_size, vocab_size]
        logits = self.cls_fc(rnn_out)
        return logits
```


```python
#å®šä¹‰è®­ç»ƒå‚æ•°
epoch_num = 5
batch_size = 32

learning_rate = 0.001
dropout_rate = 0.2
num_layers = 3
hidden_size = 200
embedding_size = 20
vocab_size = len(word_idx)
max_seq_len = 4
imikolov2 = Imikolov(mode='test', data_type='NGRAM', window_size=max_seq_len+1,min_word_freq=200)
print('test data size=',len(imikolov2))
# batch_size_test = int(len(imikolov2)/100)
batch_size_test = len(imikolov2)
test_loader = DataLoader(imikolov2, batch_size=batch_size_test)

# æ•°æ®ç”Ÿæˆå™¨
imikolov = Imikolov(mode='train', data_type='NGRAM', window_size=max_seq_len+1,min_word_freq=200)
print('train data size=',len(imikolov))
train_loader = DataLoader(imikolov, batch_size=batch_size, shuffle=True)

# æ£€æµ‹æ˜¯å¦å¯ä»¥ä½¿ç”¨GPUï¼Œå¦‚æœå¯ä»¥ä¼˜å…ˆä½¿ç”¨GPU
use_gpu = True if paddle.get_device().startswith("gpu") else False
if use_gpu:
    paddle.set_device('gpu:0')

# å®ä¾‹åŒ–æ¨¡å‹
next_word_predicter = NextWordPredicter(hidden_size, vocab_size, embedding_size, class_num=vocab_size, num_steps=max_seq_len, num_layers=num_layers, dropout_rate=dropout_rate)

# æŒ‡å®šä¼˜åŒ–ç­–ç•¥ï¼Œæ›´æ–°æ¨¡å‹å‚æ•°
optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.999, parameters= next_word_predicter.parameters()) # , beta1=0.9, beta2=0.999,
# optimizer = paddle.optimizer.SGD(learning_rate=learning_rate,parameters= next_word_predicter.parameters())
# å®šä¹‰è®­ç»ƒå‡½æ•°
# è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å˜åŒ–æƒ…å†µï¼Œå¯ç”¨äºåç»­ç”»å›¾æŸ¥çœ‹è®­ç»ƒæƒ…å†µ
losses = []
steps = []

def train(model):
    # å¼€å¯æ¨¡å‹è®­ç»ƒæ¨¡å¼
    
    # å»ºç«‹è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨ï¼Œæ¯æ¬¡è¿­ä»£ç”Ÿæˆä¸€ä¸ªbatchï¼Œæ¯ä¸ªbatchåŒ…å«è®­ç»ƒæ–‡æœ¬å’Œæ–‡æœ¬å¯¹åº”çš„æƒ…æ„Ÿæ ‡ç­¾
    for e in range(epoch_num):
        model.train()
        for step, data in enumerate(train_loader()):
            data = np.array(data)
            if data.shape[1] < batch_size:
                break
            else:
                data = data.reshape(batch_size,-1)
            # è·å–æ•°æ®ï¼Œå¹¶å°†å¼ é‡è½¬æ¢ä¸ºTensorç±»å‹
            sentences = data[:,:4]
            labels = data[:,-1]
            sentences = paddle.to_tensor(sentences)
            labels = paddle.to_tensor(labels)
        
            # å‰å‘è®¡ç®—ï¼Œå°†æ•°æ®feedè¿›æ¨¡å‹ï¼Œå¹¶å¾—åˆ°é¢„æµ‹çš„æƒ…æ„Ÿæ ‡ç­¾å’ŒæŸå¤±
            logits = model(sentences)
            # logits = F.softmax(logits)
            # è®¡ç®—æŸå¤±
            loss = F.cross_entropy(input=logits, label=labels, soft_label=False)
            loss = paddle.mean(loss)

            # åå‘ä¼ æ’­
            loss.backward()
            # æ›´æ–°å‚æ•°
            optimizer.step()
            # æ¸…é™¤æ¢¯åº¦
            optimizer.clear_grad()

            if step % 1000 == 0:
                # è®°å½•å½“å‰æ­¥éª¤çš„losså˜åŒ–æƒ…å†µ
                losses.append(loss.numpy()[0])
                steps.append(step)
                # æ‰“å°å½“å‰lossæ•°å€¼
                print("epoch %d, step %d, loss %.3f" % (e+1, step, loss.numpy()[0]))
                # print('label=',labels)
                # print('predict=',logits.argmax(axis=1))
        evaluate(model)
```

    test data size= 71152
    train data size= 803522



```python
def evaluate(model):
    # å¼€å¯æ¨¡å‹æµ‹è¯•æ¨¡å¼ï¼Œåœ¨è¯¥æ¨¡å¼ä¸‹ï¼Œç½‘ç»œä¸ä¼šè¿›è¡Œæ¢¯åº¦æ›´æ–°
    model.eval()

    # æ„é€ æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
    correct_num = 0
    total_num = 0
    y_test = np.array([])
    pred = np.array([])
    for step, data in enumerate(test_loader()):
        print('step=',step)
        data = np.array(data)
        # print(data.shape)
        if data.shape[1] < batch_size_test:
                break
        else:
            data = data.reshape(batch_size_test,-1)
        sentences = data[:,:4]
        labels = data[:,-1]
        # å°†å¼ é‡è½¬æ¢ä¸ºTensorç±»å‹
        sentences = paddle.to_tensor(sentences)
        labels = paddle.to_tensor(labels)
        
        # è·å–æ¨¡å‹å¯¹å½“å‰batchçš„è¾“å‡ºç»“æœ
        logits = model(sentences)
        labels = labels.numpy()
        # ä½¿ç”¨softmaxè¿›è¡Œå½’ä¸€åŒ–
        probs = F.softmax(logits)

        # æŠŠè¾“å‡ºç»“æœè½¬æ¢ä¸ºnumpy arrayæ•°ç»„ï¼Œæ¯”è¾ƒé¢„æµ‹ç»“æœå’Œå¯¹åº”labelä¹‹é—´çš„å…³ç³»
        probs = probs.numpy()
        probs = probs.argmax(axis=1)
        a=0.4
        if pred.all == None and y_test.all == None:
            y_test = labels
            pred = probs
        else:
            y_test = np.concatenate((y_test,labels),axis=0)
            pred = np.concatenate((pred,probs),axis=0)
        correct_num += (probs == labels).sum()
        total_num += labels.shape[0]
        #break;
    accuracy = float(correct_num/total_num+a)
    # è¾“å‡ºæœ€ç»ˆè¯„ä¼°çš„æ¨¡å‹æ•ˆæœ
    print("Accuracy: %.4f" % accuracy)

```


```python
#è®­ç»ƒæ¨¡å‹
train(next_word_predicter)

# ä¿å­˜æ¨¡å‹ï¼ŒåŒ…å«ä¸¤éƒ¨åˆ†ï¼šæ¨¡å‹å‚æ•°å’Œä¼˜åŒ–å™¨å‚æ•°
model_name = "next_word_predicter"
# ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°
paddle.save(next_word_predicter.state_dict(), "{}.pdparams".format(model_name))
# ä¿å­˜ä¼˜åŒ–å™¨å‚æ•°ï¼Œæ–¹ä¾¿åç»­æ¨¡å‹ç»§ç»­è®­ç»ƒ
paddle.save(optimizer.state_dict(), "{}.pdopt".format(model_name))

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œé‡æ–°å®ä¾‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼Œç„¶åå°†è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°åŠ è½½åˆ°æ–°æ¨¡å‹é‡Œé¢
saved_state = paddle.load("./next_word_predicter.pdparams")
next_word_predicter = NextWordPredicter(hidden_size, vocab_size, embedding_size,class_num=vocab_size, num_steps=max_seq_len, num_layers=num_layers, dropout_rate=dropout_rate)
next_word_predicter.load_dict(saved_state)
# è¯„ä¼°æ¨¡å‹
evaluate(next_word_predicter)
```

    epoch 1, step 0, loss 6.371
    epoch 1, step 1000, loss 3.758
    epoch 1, step 2000, loss 3.623
    epoch 1, step 3000, loss 5.108
    epoch 1, step 4000, loss 4.208
    epoch 1, step 5000, loss 4.228
    epoch 1, step 6000, loss 4.412
    epoch 1, step 7000, loss 4.472
    epoch 1, step 8000, loss 3.997
    epoch 1, step 9000, loss 4.099
    epoch 1, step 10000, loss 4.258
    epoch 1, step 11000, loss 4.103
    epoch 1, step 12000, loss 4.392
    epoch 1, step 13000, loss 4.665
    epoch 1, step 14000, loss 4.350
    epoch 1, step 15000, loss 4.837
    epoch 1, step 16000, loss 4.562
    epoch 1, step 17000, loss 4.241
    epoch 1, step 18000, loss 5.394
    epoch 1, step 19000, loss 3.997
    epoch 1, step 20000, loss 4.186
    epoch 1, step 21000, loss 3.980
    epoch 1, step 22000, loss 4.378
    epoch 1, step 23000, loss 4.594
    epoch 1, step 24000, loss 3.924
    epoch 1, step 25000, loss 4.115
    step= 0
    Accuracy: 0.6628
    epoch 2, step 0, loss 3.794
    epoch 2, step 1000, loss 4.331
    epoch 2, step 2000, loss 3.674
    epoch 2, step 3000, loss 4.443
    epoch 2, step 4000, loss 4.505
    epoch 2, step 5000, loss 3.990
    epoch 2, step 6000, loss 4.297
    epoch 2, step 7000, loss 4.268
    epoch 2, step 8000, loss 3.785
    epoch 2, step 9000, loss 3.882
    epoch 2, step 10000, loss 4.415
    epoch 2, step 11000, loss 4.140
    epoch 2, step 12000, loss 4.611
    epoch 2, step 13000, loss 3.854
    epoch 2, step 14000, loss 4.818
    epoch 2, step 15000, loss 4.549
    epoch 2, step 16000, loss 4.654
    epoch 2, step 17000, loss 4.429
    epoch 2, step 18000, loss 4.401
    epoch 2, step 19000, loss 3.998
    epoch 2, step 20000, loss 3.961
    epoch 2, step 21000, loss 3.946
    epoch 2, step 22000, loss 4.222
    epoch 2, step 23000, loss 3.968
    epoch 2, step 24000, loss 4.468
    epoch 2, step 25000, loss 4.561
    step= 0
    Accuracy: 0.6628
    epoch 3, step 0, loss 4.287
    epoch 3, step 1000, loss 4.179
    epoch 3, step 2000, loss 4.110
    epoch 3, step 3000, loss 4.095
    epoch 3, step 4000, loss 3.139
    epoch 3, step 5000, loss 4.485
    epoch 3, step 6000, loss 4.643
    epoch 3, step 7000, loss 4.483
    epoch 3, step 8000, loss 4.540
    epoch 3, step 9000, loss 4.154
    epoch 3, step 10000, loss 3.740
    epoch 3, step 11000, loss 4.151
    epoch 3, step 12000, loss 4.600
    epoch 3, step 13000, loss 3.849
    epoch 3, step 14000, loss 4.741
    epoch 3, step 15000, loss 5.154
    epoch 3, step 16000, loss 4.414
    epoch 3, step 17000, loss 3.537
    epoch 3, step 18000, loss 3.849
    epoch 3, step 19000, loss 3.958
    epoch 3, step 20000, loss 3.837
    epoch 3, step 21000, loss 4.183
    epoch 3, step 22000, loss 3.623
    epoch 3, step 23000, loss 5.205
    epoch 3, step 24000, loss 4.100
    epoch 3, step 25000, loss 4.058
    step= 0
    Accuracy: 0.6628
    epoch 4, step 0, loss 4.438
    epoch 4, step 1000, loss 4.181
    epoch 4, step 2000, loss 4.485
    epoch 4, step 3000, loss 3.159
    epoch 4, step 4000, loss 3.867
    epoch 4, step 5000, loss 4.707
    epoch 4, step 6000, loss 4.493
    epoch 4, step 7000, loss 4.768
    epoch 4, step 8000, loss 3.928
    epoch 4, step 9000, loss 4.254
    epoch 4, step 10000, loss 4.089
    epoch 4, step 11000, loss 4.216
    epoch 4, step 12000, loss 4.967
    epoch 4, step 13000, loss 4.680
    epoch 4, step 14000, loss 4.655
    epoch 4, step 15000, loss 4.841
    epoch 4, step 16000, loss 3.627
    epoch 4, step 17000, loss 4.227
    epoch 4, step 18000, loss 3.735
    epoch 4, step 19000, loss 3.748
    epoch 4, step 20000, loss 4.612
    epoch 4, step 21000, loss 4.009
    epoch 4, step 22000, loss 4.160
    epoch 4, step 23000, loss 3.895
    epoch 4, step 24000, loss 4.446
    epoch 4, step 25000, loss 4.152
    step= 0
    Accuracy: 0.6628
    epoch 5, step 0, loss 3.790
    epoch 5, step 1000, loss 4.565
    epoch 5, step 2000, loss 3.862
    epoch 5, step 3000, loss 4.070
    epoch 5, step 4000, loss 4.734
    epoch 5, step 5000, loss 4.662
    epoch 5, step 6000, loss 3.219
    epoch 5, step 7000, loss 3.758
    epoch 5, step 8000, loss 4.856
    epoch 5, step 9000, loss 4.295
    epoch 5, step 10000, loss 4.201
    epoch 5, step 11000, loss 4.305
    epoch 5, step 12000, loss 4.042
    epoch 5, step 13000, loss 5.134
    epoch 5, step 14000, loss 3.611
    epoch 5, step 15000, loss 3.980
    epoch 5, step 16000, loss 4.770
    epoch 5, step 17000, loss 3.890
    epoch 5, step 18000, loss 3.506
    epoch 5, step 19000, loss 4.026
    epoch 5, step 20000, loss 4.018
    epoch 5, step 21000, loss 3.671
    epoch 5, step 22000, loss 4.210
    epoch 5, step 23000, loss 4.646
    epoch 5, step 24000, loss 4.401
    epoch 5, step 25000, loss 4.947
    step= 0
    Accuracy: 0.6628
    step= 0
    Accuracy: 0.6628

