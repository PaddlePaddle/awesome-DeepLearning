{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from collections import OrderedDict, namedtuple, defaultdict\n",
    "import random\n",
    "\n",
    "\n",
    "# 添加 dropout 和 L2正则项 添加后与deepctr还是有区别 L2正则项需要继续改进\n",
    "# init()代码太冗杂了，封装几个函数\n",
    "\n",
    "class Deepfm(nn.Module):\n",
    "\n",
    "    def __init__(self, feat_sizes, sparse_feature_columns, dense_feature_columns,dnn_hidden_units=[400, 400,400], dnn_dropout=0.0, ebedding_size=4,\n",
    "                 l2_reg_linear=0.00001, l2_reg_embedding=0.00001, l2_reg_dnn=0, init_std=0.0001, seed=1024,\n",
    "                 device='cpu'):\n",
    "\n",
    "        super(Deepfm, self).__init__()\n",
    "        self.feat_sizes = feat_sizes\n",
    "        self.device = device\n",
    "        self.sparse_feature_columns = sparse_feature_columns\n",
    "        self.dense_feature_columns = dense_feature_columns\n",
    "        self.embedding_size = ebedding_size\n",
    "        self.l2_reg_linear = l2_reg_linear\n",
    "\n",
    "        # self.feature_index 建立feature到列名到输入数据X的相对位置的映射\n",
    "        self.feature_index = self.build_input_features(self.feat_sizes)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros((1,)))\n",
    "        # self.weight\n",
    "        self.weight = nn.Parameter(torch.Tensor(len(self.dense_feature_columns), 1)).to(device)\n",
    "        torch.nn.init.normal_(self.weight, mean=0, std=0.0001)\n",
    "\n",
    "        self.embedding_dict1 = self.create_embedding_matrix(self.sparse_feature_columns , feat_sizes , 1 ,\n",
    "                                                       sparse=False, device=self.device)\n",
    "\n",
    "        self.embedding_dict2 = self.create_embedding_matrix(self.sparse_feature_columns , feat_sizes , self.embedding_size ,\n",
    "                                                       sparse=False, device=self.device)\n",
    "        # dnn\n",
    "        self.dropout = nn.Dropout(dnn_dropout)\n",
    "        self.dnn_input_size = self.embedding_size * len(self.sparse_feature_columns) + len(self.dense_feature_columns)\n",
    "        hidden_units = [self.dnn_input_size] + dnn_hidden_units\n",
    "        self.linears = nn.ModuleList(\n",
    "            [nn.Linear(hidden_units[i], hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "        self.relus = nn.ModuleList(\n",
    "            [nn.ReLU() for i in range(len(hidden_units) - 1)])\n",
    "        for name, tensor in self.linears.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=init_std)\n",
    "        # self.linears =self.linears.to(device)\n",
    "        self.dnn_linear = nn.Linear(\n",
    "            dnn_hidden_units[-1], 1, bias=False).to(device)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        :param X: pd.DtateFrame\n",
    "        :return:  y_pre\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "            FM liner\n",
    "        '''\n",
    "        sparse_embedding_list1 = [self.embedding_dict1[feat](\n",
    "            X[:, self.feature_index[feat][0]:self.feature_index[feat][1]].long())\n",
    "            for feat in self.sparse_feature_columns]\n",
    "\n",
    "        dense_value_list2 = [X[:, self.feature_index[feat][0]:self.feature_index[feat][1]]\n",
    "                             for feat in self.dense_feature_columns]\n",
    "        linear_sparse_logit = torch.sum(\n",
    "            torch.cat(sparse_embedding_list1, dim=-1), dim=-1, keepdim=False)\n",
    "        linear_dense_logit = torch.cat(\n",
    "            dense_value_list2, dim=-1).matmul(self.weight)\n",
    "        logit = linear_sparse_logit + linear_dense_logit\n",
    "\n",
    "        sparse_embedding_list = [self.embedding_dict2[feat](\n",
    "            X[:, self.feature_index[feat][0]:self.feature_index[feat][1]].long())\n",
    "            for feat in self.sparse_feature_columns]\n",
    "        '''\n",
    "            FM second\n",
    "        '''\n",
    "        fm_input = torch.cat(sparse_embedding_list, dim=1)  # shape: (batch_size,field_size,embedding_size)\n",
    "        square_of_sum = torch.pow(torch.sum(fm_input, dim=1, keepdim=True), 2)  # shape: (batch_size,1,embedding_size)\n",
    "        sum_of_square = torch.sum(torch.pow(fm_input, 2), dim=1, keepdim=True)  # shape: (batch_size,1,embedding_size)\n",
    "        cross_term = square_of_sum - sum_of_square\n",
    "        cross_term = 0.5 * torch.sum(cross_term, dim=2, keepdim=False)  # shape: (batch_size,1)\n",
    "        logit += cross_term\n",
    "\n",
    "        '''\n",
    "            DNN\n",
    "        '''\n",
    "        #  sparse_embedding_list、 dense_value_list2\n",
    "        dnn_sparse_input = torch.cat(sparse_embedding_list, dim=1)\n",
    "        batch_size = dnn_sparse_input.shape[0]\n",
    "        # print(dnn_sparse_input.shape)\n",
    "        dnn_sparse_input=dnn_sparse_input.reshape(batch_size,-1)\n",
    "        # dnn_sparse_input shape: [ batch_size, len(sparse_feat)*embedding_size ]\n",
    "        dnn_dense_input = torch.cat(dense_value_list2, dim=-1)\n",
    "        # print(dnn_sparse_input.shape)\n",
    "        # dnn_dense_input shape: [ batch_size, len(dense_feat) ]\n",
    "        dnn_total_input = torch.cat([dnn_sparse_input, dnn_dense_input], dim=-1)\n",
    "        deep_input = dnn_total_input\n",
    "\n",
    "        for i in range(len(self.linears)):\n",
    "            fc = self.linears[i](deep_input)\n",
    "            fc = self.relus[i](fc)\n",
    "            fc = self.dropout(fc)\n",
    "            deep_input = fc\n",
    "        dnn_output = self.dnn_linear(deep_input)\n",
    "\n",
    "        logit += dnn_output\n",
    "        '''\n",
    "            output\n",
    "        '''\n",
    "        y_pred = torch.sigmoid(logit+self.bias)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, train_input, y_label, val_input, y_val, batch_size=5000, epochs=15, verbose=5):\n",
    "        x = [train_input[feature] for feature in self.feature_index]\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            if len(x[i].shape) == 1:\n",
    "                x[i] = np.expand_dims(x[i], axis=1)  # 扩展成2维，以便后续cat\n",
    "\n",
    "        train_tensor_data = Data.TensorDataset(torch.from_numpy(np.concatenate(x, axis=-1)), torch.from_numpy(y_label))\n",
    "        train_loader = DataLoader(dataset=train_tensor_data,shuffle=True ,batch_size=batch_size)\n",
    "\n",
    "        print(self.device, end=\"\\n\")\n",
    "        model = self.train()\n",
    "        loss_func = F.binary_cross_entropy\n",
    "        # loss_func = F.binary_cross_entropy_with_logits\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.0)\n",
    "        # optimizer = optim.Adagrad(model.parameters(),lr=0.01)\n",
    "        # 显示 一次epoch需要几个step\n",
    "        sample_num = len(train_tensor_data)\n",
    "        steps_per_epoch = (sample_num - 1) // batch_size + 1\n",
    "\n",
    "        print(\"Train on {0} samples,  {1} steps per epoch\".format(\n",
    "            len(train_tensor_data), steps_per_epoch))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            loss_epoch = 0\n",
    "            total_loss_epoch = 0.0\n",
    "            train_result = {}\n",
    "            pred_ans = []\n",
    "            true_ans = []\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                for index, (x_train, y_train) in enumerate(train_loader):\n",
    "                    x = x_train.to(self.device).float()\n",
    "                    y = y_train.to(self.device).float()\n",
    "\n",
    "                    y_pred = model(x).squeeze()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = loss_func(y_pred, y.squeeze(),reduction='mean')\n",
    "                    #L2 norm\n",
    "                    loss = loss + self.l2_reg_linear * self.get_L2_Norm()\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    optimizer.step()\n",
    "                    total_loss_epoch = total_loss_epoch + loss.item()\n",
    "                    y_pred = y_pred.cpu().data.numpy()  # .squeeze()\n",
    "                    pred_ans.append(y_pred)\n",
    "                    true_ans.append(y.squeeze().cpu().data.numpy())\n",
    "\n",
    "            if (epoch % verbose == 0):\n",
    "                print('epoch %d train loss is %.4f train AUC is %.4f' %\n",
    "                      (epoch,total_loss_epoch / steps_per_epoch,roc_auc_score(np.concatenate(true_ans), np.concatenate(pred_ans))))\n",
    "                self.val_auc_logloss(val_input, y_val, batch_size=50000)\n",
    "                print(\" \")\n",
    "\n",
    "    def predict(self, test_input, batch_size = 256, use_double=False):\n",
    "        \"\"\"\n",
    "        :param x: The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs).\n",
    "        :param batch_size: Integer. If unspecified, it will default to 256.\n",
    "        :return: Numpy array(s) of predictions.\n",
    "        \"\"\"\n",
    "        model = self.eval()\n",
    "        x = [test_input[feature] for feature in self.feature_index]\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            if len(x[i].shape) == 1:\n",
    "                x[i] = np.expand_dims(x[i], axis=1)  # 扩展成2维，以便后续cat\n",
    "\n",
    "        tensor_data = Data.TensorDataset(\n",
    "            torch.from_numpy(np.concatenate(x, axis=-1)))\n",
    "        test_loader = DataLoader(\n",
    "            dataset=tensor_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        pred_ans = []\n",
    "        with torch.no_grad():\n",
    "            for index, x_test in enumerate(test_loader):\n",
    "                x = x_test[0].to(self.device).float()\n",
    "                # y = y_test.to(self.device).float()\n",
    "\n",
    "                y_pred = model(x).cpu().data.numpy()  # .squeeze()\n",
    "                pred_ans.append(y_pred)\n",
    "\n",
    "        if use_double:\n",
    "            return np.concatenate(pred_ans).astype(\"float64\")\n",
    "        else:\n",
    "            return np.concatenate(pred_ans)\n",
    "\n",
    "    def val_auc_logloss(self, val_input, y_val, batch_size=50000, use_double=False):\n",
    "        pred_ans = self.predict(val_input, batch_size)\n",
    "        print(\"test LogLoss is %.4f test AUC is %.4f\"%(log_loss(y_val, pred_ans),roc_auc_score(y_val, pred_ans)) )\n",
    "\n",
    "    def get_L2_Norm(self ):\n",
    "\n",
    "        loss = torch.zeros((1,), device=self.device)\n",
    "        loss = loss + torch.norm(self.weight)\n",
    "        for t in self.embedding_dict1.parameters():\n",
    "            loss = loss+ torch.norm(t)\n",
    "        for t in self.embedding_dict2.parameters():\n",
    "            loss = loss+ torch.norm(t)\n",
    "        return  loss\n",
    "\n",
    "    def build_input_features(self, feat_sizes):\n",
    "        # Return OrderedDict: {feature_name:(start, start+dimension)}\n",
    "        features = OrderedDict()\n",
    "        start = 0\n",
    "        for feat in feat_sizes:\n",
    "            feat_name = feat\n",
    "            if feat_name in features:\n",
    "                continue\n",
    "            features[feat_name] = (start, start + 1)\n",
    "            start += 1\n",
    "        return  features\n",
    "\n",
    "    def create_embedding_matrix(self ,sparse_feature_columns, feat_sizes,embedding_size,init_std=0.0001, sparse=False, device='cpu'):\n",
    "        embedding_dict = nn.ModuleDict(\n",
    "            {feat: nn.Embedding(feat_sizes[feat], embedding_size, sparse=False)\n",
    "             for feat in sparse_feature_columns}\n",
    "        )\n",
    "        for tensor in embedding_dict.values():\n",
    "            nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "        return embedding_dict.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from collections import OrderedDict, namedtuple, defaultdict\n",
    "import random\n",
    "#from deepctrmodels.deepfm import Deepfm\n",
    "from deepctr.models import DeepFM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    seed = 1024\n",
    "    torch.manual_seed(seed)  # 为CPU设置随机种子\n",
    "    torch.cuda.manual_seed(seed)  # 为当前GPU设置随机种子\n",
    "    torch.cuda.manual_seed_all(seed)  # 为所有GPU设置随机种子\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    sparse_features = ['C' + str(i) for i in range(1, 27)]   #C代表类别特征 class\n",
    "    dense_features =  ['I' + str(i) for i in range(1, 14)]   #I代表数值特征 int\n",
    "    col_names = ['label'] + dense_features + sparse_features\n",
    "    data = pd.read_csv('dac/train.txt', names=col_names, sep='\\t')\n",
    "    # data = pd.read_csv('criteo_train_1m.txt', names=col_names, sep='\\t')\n",
    "    # data = pd.read_csv('total.txt')\n",
    "    feature_names = sparse_features + dense_features         #全体特征名\n",
    "    data[sparse_features] = data[sparse_features].fillna('-1', )   # 类别特征缺失 ，使用-1代替\n",
    "    data[dense_features] = data[dense_features].fillna(0, )        # 数值特征缺失，使用0代替\n",
    "    target = ['label']                                             # label\n",
    "\n",
    "    # 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "    # 使用LabelEncoder()，为类别特征的每一个item编号\n",
    "    for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "    # 数值特征 max-min 0-1归化\n",
    "    mms = MinMaxScaler(feature_range=(0, 1))\n",
    "    data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "\n",
    "    # 2.count #unique features for each sparse field,and record dense feature field name\n",
    "    feat_sizes1={ feat:1 for feat in dense_features}\n",
    "    feat_sizes2 = {feat: len(data[feat].unique()) for feat in sparse_features}\n",
    "    feat_sizes={}\n",
    "    feat_sizes.update(feat_sizes1)\n",
    "    feat_sizes.update(feat_sizes2)\n",
    "    # print(feat_sizes)\n",
    "\n",
    "    # 3.generate input data for model\n",
    "    train, test = train_test_split(data, test_size=0.2,random_state=2020)\n",
    "    # print(train.head(5))\n",
    "    # print(test.head(5))\n",
    "    train_model_input = {name: train[name] for name in feature_names}\n",
    "    test_model_input =  {name: test[name]  for name in feature_names}\n",
    "\n",
    "    # 4.Define Model,train,predict and evaluate\n",
    "    device = 'cpu'\n",
    "    use_cuda = True\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        print('cuda ready...')\n",
    "        device = 'cuda:0'\n",
    "\n",
    "    model = DeepFM(feat_sizes ,sparse_feature_columns = sparse_features,dense_feature_columns = dense_features,\n",
    "                   dnn_hidden_units=[400,400,400] , dnn_dropout=0.9 , ebedding_size = 8 ,\n",
    "                   l2_reg_linear=1e-3, device=device)\n",
    "\n",
    "\n",
    "\n",
    "    model.fit(train_model_input, train[target].values , test_model_input , test[target].values ,batch_size=50000, epochs=150, verbose=1)\n",
    "\n",
    "    pred_ans = model.predict(test_model_input, 50000)\n",
    "\n",
    "    print(\"final test\")\n",
    "    print(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\n",
    "    print(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
