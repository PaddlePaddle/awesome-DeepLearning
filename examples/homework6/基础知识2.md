1、DSSM原理
通过query和document的点击曝光日志，用DNN把query和document表达为低维语义向量，并通过余弦相识度来计算两个语义向量的距离，最终训练出语义相识度模型。

该模型既可以用来预测两个句子的语义相似度，又可以获得某句子的低维语义Embedding向量。

（总而言之，核心就是将文本信息转为embbeding后，两个embedding求相似度）
![](https://ai-studio-static-online.cdn.bcebos.com/a3ae852f9bcb4a6db87ed4f1facf88fffd0b2f3865d844c5b14dc74f04614b02)



It uses a DNN to map high-dimensional sparse text features into low-dimensional dense features in a semantic space. The first hidden layer, with 30k units, accomplishes word hashing. The word-hashed features are then projected through multiple layers of non-linear projections. The final layer’s neural activities in this DNN form the feature in the semantic space.

在原始的paper中，dssm主要用来建模每一个query与多个documents的相似度，通过原始的text，word hashing之后，经过多层线性投影，获得128维的隐语义向量，进行相似度计算。

Q代表Query信息，D表示Document信息。

（1）Term Vector：表示文本的Embedding向量；

（2）Word Hashing技术：为解决Term Vector太大问题，对bag-of-word向量降维；（在搜索引擎中，给定一个query，会返回一些按照相关性分数排序的文档。通常情况下，输入的term向量使用最原始的bag of words特征，通过one-hot进行编码。但是在实际场景中，词典的大小将会非常大，如果直接将该数据输入给DNN，神经网络是无法进行训练和预测的。因此，在DSSM中引入了word hashing的方法，并且作为DNN中的第一层。）

（3）Multi-layer nonlinear projection：表示深度学习网络的隐层；

（4）Semantic feature ：表示Query和Document 最终的Embedding向量；

（5）Relevance measured by cosine similarity：表示计算Query与Document之间的余弦相似度；

（6）Posterior probability computed by softmax：表示通过Softmax 函数把Query 与正样本Document的语义相似性转化为一个后验概率；(具体公式可以看参考1)

我们拆分一下，这样的塔式结构，单独的结构：
![](https://ai-studio-static-online.cdn.bcebos.com/098067b4803e4119b3856f36eecd928d524f5de070904a2797c72dc29a958fd7)



DSSM module主要包括两个部分：

特征的抽象提取，主要是为了提取出对任务有效的模型特征；
loss函数，主要是为了定义metric标注，建模问题相似度描述；
DSSM module中，特征的抽取部分f(x), g(x) 可以是包括CNN、RNN、FC在内的任意网络类型，或者组合，loss 函数针对建模问题不同，解决分类应用时，例如交叉熵问题时，使用交叉熵；当处理回归问题时，使用cosine距离；解决排序问题时，则使用pairwise loss函数。

1.1、wordhash
word hashing方法是用来减少输入向量的维度，该方法基于字母的 n-gram。给定一个单词（good），我们首先增加词的开始和结束部分（#good#），然后将该词转换为字母  n-gram的形式（假设为trigrams：#go，goo，ood，od#）。最后该词使用字母 n-gram的向量来表示。

这种方法的问题在于有可能造成冲突，因为两个不同的词可能有相同的 n-gram向量来表示。word hashing在2个词典中的统计。与原始的ont-hot向量表示的词典大小相比，word hashing明显降低了向量表示的维度。




2、DSSM模型训练


点击日志里通常包含了用户搜索的query和用户点击的doc，可以假定如果用户在当前query下对doc进行了点击，则该query与doc是相关的。通过该规则，可以通过点击日志构造训练集与测试集。

首先，通过softmax 函数可以把query 与样本 doc 的语义相似性转化为一个后验概率：![](https://ai-studio-static-online.cdn.bcebos.com/8d904b742592474f9d343a7eaa78be159f78bfa8ab794ad3aa352d8a83d167ef)




其中  是一个softmax函数的平滑因子，D 表示被排序的候选文档集合，在实际中，对于正样本，每一个（query， 点击doc）对，使用  表示；对于负样本，随机选择4个曝光但未点击的doc，用  ![](https://ai-studio-static-online.cdn.bcebos.com/5f647c4ac6344537beee2bfddcda291d280dcc639056432ab0140da2a1a18fa1) 来表示。

在训练阶段，通过极大似然估计来最小化损失函数：![](https://ai-studio-static-online.cdn.bcebos.com/44f474b1da824557af44b476aa53093284de364145f8438a8d5e60763629a4f7)

其中  表示神经网络的参数。模型通过随机梯度下降（SGD）来进行优化，最终可以得到各网络层的参数  。

3、DSSM总结

优点：

解决了LSA、LDA、Autoencoder等方法存在的一个最大的问题：字典爆炸（导致计算复杂度非常高），因为在英文单词中，词的数量可能是没有限制的，但是字母 n -gram的数量通常是有限的
基于词的特征表示比较难处理新词，字母的 n -gram可以有效表示，鲁棒性较强
使用有监督方法，优化语义embedding的映射问题
省去了人工的特征工程

缺点：

word hashing可能造成冲突
DSSM采用了词袋模型，损失了上下文信息
在排序中，搜索引擎的排序由多种因素决定，由于用户点击时doc的排名越靠前，点击的概率就越大，如果仅仅用点击来判断是否为正负样本，噪声比较大，难以收敛
对于中文而言，处理方式与英文有很多不一样的地方。中文往往需要进行分词，但是我们可以仿照英文的处理方式，将中文的最小粒度看作是单字（在某些文献里看到过用偏旁部首，笔画，拼音等方法）。因此，通过这种word hashing方式，可以将向量空间大大降低。


1. 推荐系统的应用场景
作为全球最大的视频分享网站，YouTube 平台中几乎所有的视频都来自 UGC（User Generated Content，用户原创内容），这样的内容产生模式有两个特点：
![](https://ai-studio-static-online.cdn.bcebos.com/57cdc7b592ee45e18933eb9790d87d5d85cb6510cf594150b75dd0a3834c4505)

一是其商业模式不同于 Netflix，以及国内的腾讯视频、爱奇艺这样的流媒体，这些流媒体的大部分内容都是采购或自制的电影、剧集等头部内容，而 YouTube 的内容都是用户上传的自制视频，种类风格繁多，头部效应没那么明显；
二是由于 YouTube 的视频基数巨大，用户难以发现喜欢的内容。
2.YouTube 推荐系统架构
为了对海量的视频进行快速、准确的排序，YouTube 也采用了经典的召回层 + 排序层的推荐系统架构。



其推荐过程可以分成二级。第一级是用候选集生成模型（Candidate Generation Model）完成候选视频的快速筛选，在这一步，候选视频集合由百万降低到几百量级，这就相当于经典推荐系统架构中的召回层。第二级是用排序模型（Ranking Model）完成几百个候选视频的精排，这相当于经典推荐系统架构中的排序层。

无论是候选集生成模型还是排序模型，YouTube 都采用了深度学习的解决方案。

3.候选集生成模型
用于视频召回的候选集生成模型，架构如下图所示。
![](https://ai-studio-static-online.cdn.bcebos.com/7684fe5d475d4a6cab309e6790bbda96efb0f4aef54d46d6bf137bb7da66bef7)


最底层是它的输入层，输入的特征包括用户历史观看视频的 Embedding 向量，以及搜索词的 Embedding 向量。对于这些 Embedding 特征，YouTube 是利用用户的观看序列和搜索序列，采用了类似 Item2vec 的预训练方式生成的。

除了视频和搜索词 Embedding 向量，特征向量中还包括用户的地理位置 Embedding、年龄、性别等特征。这里我们需要注意的是，对于样本年龄这个特征，YouTube 不仅使用了原始特征值，还把经过平方处理的特征值也作为一个新的特征输入模型。
这个操作其实是为了挖掘特征非线性的特性。

确定好了特征，这些特征会在 concat 层中连接起来，输入到上层的 ReLU 神经网络进行训练。

三层 ReLU 神经网络过后，YouTube 又使用了 softmax 函数作为输出层。值得一提的是，这里的输出层不是要预测用户会不会点击这个视频，而是要预测用户会点击哪个视频，这就跟一般深度推荐模型不一样。

总的来讲，YouTube 推荐系统的候选集生成模型，是一个标准的利用了 Embedding 预训练特征的深度推荐模型，它遵循Embedding MLP 模型的架构，只是在最后的输出层有所区别。

4. 候选集生成模型独特的线上服务方法
5. 排序模型
![](https://ai-studio-static-online.cdn.bcebos.com/9b8cb363149d4072ae2c2593cbc4076ce67690467de9410cafde7ffafc50ef77)

输入层，相比于候选集生成模型需要对几百万候选集进行粗筛，排序模型只需对几百个候选视频进行排序，因此可以引入更多特征进行精排。具体来说，YouTube 的输入层从左至右引入的特征依次是：

impression video ID embedding：当前候选视频的 Embedding；
watched video IDs average embedding：用户观看过的最后 N 个视频 Embedding 的平均值；
language embedding：用户语言的 Embedding 和当前候选视频语言的 Embedding；
time since last watch：表示用户上次观看同频道视频距今的时间；
#previous impressions：该视频已经被曝光给该用户的次数；
这 5 类特征连接起来之后，需要再经过三层 ReLU 网络进行充分的特征交叉，然后就到了输出层。这里重点注意，排序模型的输出层与候选集生成模型又有所不同。不同主要有两点：一是候选集生成模型选择了 softmax 作为其输出层，而排序模型选择了 weighted logistic regression（加权逻辑回归）作为模型输出层；二是候选集生成模型预测的是用户会点击“哪个视频”，排序模型预测的是用户“要不要点击当前视频”。

其实，排序模型采用不同输出层的根本原因就在于，YouTube 想要更精确地预测 用户的观看时长，因为观看时长才是 YouTube 最看中的商业指标，而使用 Weighted LR 作为输出层，就可以实现这样的目标。

在 Weighted LR 的训练中，我们需要为每个样本设置一个权重，权重的大小，代表了这个样本的重要程度。为了能够预估观看时长，YouTube 将正样本的权重设置为用户观看这个视频的时长，然后再用 Weighted LR 进行训练，就可以让模型学到用户观看时长的信息。

对于排序模型，必须使用 TensorFlow Serving 等模型服务平台，来进行模型的线上推断。

6. 训练和测试样本的处理
为了能够提高模型的训练效率和预测准确率，Youtube采取了诸多处理训练样本的工程措施，主要有3点：

候选集生成模型把推荐模型转换成 多分类问题，在预测下一次观看的场景中，每一个备选视频都会是一个分类，而如果采用softmax对其训练是很低效的。
Youtube采用word2vec中常用的 负采样训练方法减少每次预测的分类数量，从而加快整个模型的收敛速度。
在对训练集的预处理过程中，Youtube没有采用原始的用户日志，而是 对每个用户提取等数量的训练样本。
YouTube这样做的目的是减少高度活跃用户对模型损失的过度影响，使模型过于偏向活跃用户的行为模式，忽略数量更广大的长尾用户体验。
在处理测试集时，Youtube没有采用经典的随机留一法，而是一定要以用户最近一次观看的行为作为测试集。
只留最后一次观看行为做测试集主要是为了避免引入未来信息(future information)，产生于事实不符的数据穿越问题。
7. 处理用户对新视频的爱好
8. 总结
YouTube 推荐系统的架构是一个典型的召回层加排序层的架构，其中候选集生成模型负责从百万候选集中召回几百个候选视频，排序模型负责几百个候选视频的精排，最终选出几十个推荐给用户。

候选集生成模型是一个典型的 Embedding MLP 的架构，要注意的是它的输出层一个多分类的输出层，预测的是用户点击了“哪个”视频。在候选集生成模型的 serving 过程中，需要从输出层提取出视频 Embedding，从最后一层 ReLU 层得到用户 Embedding，然后利用 最近邻搜索快速 得到候选集。

排序模型同样是一个 Embedding MLP 的架构，不同的是，它的输入层包含了更多的用户和视频的特征，输出层采用了 Weighted LR 作为输出层，并且使用观看时长作为正样本权重，让模型能够预测出观看时长，这更接近 YouTube 要达成的商业目标。


```python
# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:
# If a persistence installation is required, 
# you need to use the persistence path as the following: 
!mkdir /home/aistudio/external-libraries
!pip install beautifulsoup4 -t /home/aistudio/external-libraries
```


```python
# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: 
# Also add the following code, 
# so that every time the environment (kernel) starts, 
# just run the following code: 
import sys 
sys.path.append('/home/aistudio/external-libraries')
```

请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>
Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. 
