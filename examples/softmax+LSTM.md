## 1ã€åŸºäºå±‚æ¬¡softmaxçš„ä¼˜åŒ–ç­–ç•¥

2013å¹´ï¼ŒMikolovæå‡ºçš„ç»å…¸word2vecç®—æ³•å°±æ˜¯é€šè¿‡ä¸Šä¸‹æ–‡æ¥å­¦ä¹ è¯­ä¹‰ä¿¡æ¯ã€‚word2vecåŒ…å«ä¸¤ä¸ªç»å…¸æ¨¡å‹ï¼šCBOWï¼ˆContinuous Bag-of-Wordsï¼‰å’ŒSkip-gramã€‚CBOWé€šè¿‡ä¸Šä¸‹æ–‡çš„è¯å‘é‡æ¨ç†ä¸­å¿ƒè¯ã€‚è€ŒSkip-gramåˆ™æ ¹æ®ä¸­å¿ƒè¯æ¨ç†ä¸Šä¸‹æ–‡ã€‚

è¾“å‡ºæ—¶éœ€è¦ç»“æœsoftmaxå‡½æ•°å½’ä¸€åŒ–ï¼Œå¾—åˆ°å¯¹ä¸­å¿ƒè¯çš„æ¨ç†æ¦‚ç‡
$$
ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥({O_i})= \frac{exp({O_i})}{\sum_jexp({O_j})}
$$
ä»ä¸Šé¢çš„å…¬å¼å¯ä»¥çœ‹å‡ºï¼Œsoftmaxåˆ†æ¯é‚£é¡¹å½’ä¸€åŒ–ï¼Œæ¯æ¬¡éœ€è¦è®¡ç®—æ‰€æœ‰çš„Oçš„è¾“å‡ºå€¼ï¼Œæ‰å¯ä»¥å¾—åˆ°å½“å‰jèŠ‚ç‚¹çš„è¾“å‡ºï¼Œå½“Vå¾ˆå¤§çš„æ—¶å€™ï¼ŒO(V)çš„è®¡ç®—ä»£ä»·ä¼šéå¸¸é«˜ã€‚æ‰€ä»¥åœ¨è®­ç»ƒword2vecæ¨¡å‹çš„æ—¶å€™ï¼Œç”¨åˆ°äº†ä¸¤ä¸ªtricksï¼Œä¸€ä¸ªæ˜¯negative samplingï¼Œæ¯æ¬¡åªé‡‡å°‘é‡çš„è´Ÿæ ·æœ¬ï¼Œä¸éœ€è¦è®¡ç®—å…¨éƒ¨çš„Vï¼›

å¦å¤–ä¸€ä¸ªæ˜¯hierarchical softmaxï¼Œé€šè¿‡æ„å»ºèµ«å¤«æ›¼treeæ¥åšå±‚çº§softmaxï¼Œä»å¤æ‚åº¦O(V)é™ä½åˆ°O(log_2V)

### **hierachical softmax**

#### 1ã€å“ˆå¤«æ›¼æ ‘

å“ˆå¤«æ›¼æ ‘( Huffman tree) åˆç§°æœ€ä¼˜äºŒå‰æ ‘ï¼Œæ˜¯ç§å¸¦æƒè·¯å¾„æœ€çŸ­çš„æ ‘ï¼Œå…¶ç‰¹ç‚¹æ˜¯**æƒé‡è¶Šå¤§ï¼Œå¶å­èŠ‚ç‚¹å°±è¶Šé è¿‘æ ¹èŠ‚ç‚¹ï¼Œå³æƒé‡è¶Šå¤§ï¼Œå¶å­èŠ‚ç‚¹æœç´¢è·¯å¾„è¶ŠçŸ­**

æ ¹æ®è¿™ä¸ªç‰¹æ€§æ„é€ å‡ºçš„å±‚æ¬¡Softmaxèƒ½å¤Ÿç¼©çŸ­ç›®æ ‡ç±»åˆ«çš„æœç´¢è·¯å¾„ã€‚

![img](https://img-blog.csdnimg.cn/20200517121425833.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JHb29kSGFiaXQ=,size_16,color_FFFFFF,t_70)



é¦–å…ˆå¯¹æ‰€æœ‰åœ¨Vè¯è¡¨çš„è¯ï¼Œæ ¹æ®è¯é¢‘æ¥æ„å»ºå“ˆå¤«æ›¼treeï¼Œè¯é¢‘è¶Šå¤§ï¼Œè·¯å¾„è¶ŠçŸ­ï¼Œç¼–ç ä¿¡æ¯æ›´å°‘ã€‚treeä¸­çš„æ‰€æœ‰çš„å¶å­èŠ‚ç‚¹æ„æˆäº†è¯Vï¼Œä¸­é—´èŠ‚ç‚¹åˆ™å…±æœ‰V-1ä¸ªï¼Œä¸Šé¢çš„æ¯ä¸ªå¶å­èŠ‚ç‚¹å­˜åœ¨å”¯ä¸€çš„ä»æ ¹åˆ°è¯¥èŠ‚ç‚¹çš„pathï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œè¯**w_2**çš„path n(**w_2**,1 ) ,n(**w_2**,2) , n(**w_3**,3) å…¶ä¸­n(w,j)è¡¨ç¤ºè¯wçš„pathçš„ç¬¬jä¸ªèŠ‚ç‚¹ã€‚

#### 2ã€å¶å­èŠ‚ç‚¹è¯çš„æ¦‚ç‡è¡¨ç¤º

ä¸Šå›¾å‡è®¾æˆ‘ä»¬éœ€è¦è®¡ç®—**w_2**çš„è¾“å‡ºæ¦‚ç‡ï¼Œæˆ‘ä»¬å®šä¹‰ä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œæ¯æ¬¡ç»è¿‡ä¸­é—´èŠ‚ç‚¹ï¼Œåšä¸€ä¸ªäºŒåˆ†ç±»ä»»åŠ¡ï¼ˆå·¦è¾¹æˆ–è€…å³è¾¹ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬å®šä¹‰ä¸­é—´èŠ‚ç‚¹çš„nå·¦è¾¹æ¦‚ç‡ä¸ºï¼š
$$
p(n,left) = Ïƒ({v_n^{'}}^T.h)
$$
å…¶ä¸­v_n'æ˜¯ä¸­é—´èŠ‚ç‚¹çš„å‘é‡ï¼Œå³è¾¹æ¦‚ç‡ï¼š
$$
p(n, right)=1-Ïƒ({v_n^{'}}^T.h)= Ïƒ(-{v_n^{'}}^T.h)
$$
ä»æ ¹èŠ‚ç‚¹åˆ°**w_2**ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¦‚ç‡å€¼ä¸ºï¼š
$$
p(w_2=w_O)=
p(n(w_2,1),left).p(n(w_2,2),left).p(n(w_3,3),right)=
Ïƒ({v_{n(w_2,1)}^{'}}^T.h). Ïƒ({v_{n(w_2,2)}^{'}}^T.h). Ïƒ(-{v_{n(w_3,3)}^{'}}^T.h)
$$
å…¶ä¸­ Ïƒä¸ºsigmoidå‡½æ•°

#### 3ã€ å„å¶å­èŠ‚ç‚¹æ¦‚ç‡å€¼ç›¸åŠ ä¸º1

å¯ä»¥å¾—å‡º
$$
\sum_{i=1}^{V}p(w_i=w_O) = 1
$$

### è®­ç»ƒ

 **1ã€é¢„å¤„ç†ï¼šæ„å»ºhaffmanæ ‘**
æ ¹æ®è¯­æ–™ä¸­çš„æ¯ä¸ªwordçš„è¯é¢‘æ„å»ºèµ«å¤«æ›¼treeï¼Œè¯é¢‘è¶Šé«˜ï¼Œåˆ™ç¦»æ ‘æ ¹è¶Šè¿‘ï¼Œè·¯å¾„è¶ŠçŸ­ã€‚å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œè¯å…¸V VVä¸­çš„æ¯ä¸ªwordéƒ½åœ¨å¶å­èŠ‚ç‚¹ä¸Šï¼Œæ¯ä¸ªwordéœ€è¦è®¡ç®—ä¸¤ä¸ªä¿¡æ¯ï¼šè·¯å¾„ï¼ˆç»è¿‡çš„æ¯ä¸ªä¸­é—´èŠ‚ç‚¹ï¼‰ä»¥åŠèµ«å¤«æ›¼ç¼–ç ï¼Œæ„å»ºå®Œèµ«å¤«æ›¼treeåï¼Œæ¯ä¸ªå¶å­èŠ‚ç‚¹éƒ½æœ‰å”¯ä¸€çš„è·¯å¾„å’Œç¼–ç ï¼Œhierarchical softmaxä¸softmaxä¸åŒçš„æ˜¯ï¼Œåœ¨hierarchical softmaxä¸­ï¼Œä¸å¯¹Vä¸­çš„wordè¯è¿›è¡Œå‘é‡å­¦ä¹ ï¼Œè€Œæ˜¯å¯¹ä¸­é—´èŠ‚ç‚¹è¿›è¡Œå‘é‡å­¦ä¹ ï¼Œè€Œæ¯ä¸ªå¶å­ä¸Šçš„èŠ‚ç‚¹å¯ä»¥é€šè¿‡è·¯å¾„ä¸­ç»è¿‡çš„ä¸­é—´èŠ‚ç‚¹å»è¡¨ç¤ºã€‚

**2 æ¨¡å‹çš„è¾“å…¥**
è¾“å…¥éƒ¨åˆ†ï¼Œåœ¨cbowæˆ–è€…skip-gramæ¨¡å‹ï¼Œè¦ä¹ˆæ˜¯ä¸Šä¸‹æ–‡wordå¯¹åº”çš„idè¯å‘é‡å¹³å‡ï¼Œè¦ä¹ˆæ˜¯ä¸­å¿ƒè¯å¯¹åº”çš„idå‘é‡ï¼Œä½œä¸ºhiddenå±‚çš„è¾“å‡ºå‘é‡

**3 æ ·æœ¬label**
ä¸åŒsoftmaxçš„æ˜¯ï¼Œæ¯ä¸ªè¯wordå¯¹åº”çš„æ˜¯ä¸€ä¸ªVå¤§å°çš„one-hot labelï¼Œhierarchical softmaxä¸­æ¯ä¸ªå¶å­èŠ‚ç‚¹wordï¼Œå¯¹åº”çš„labelæ˜¯èµ«å¤«æ›¼ç¼–ç ï¼Œä¸€èˆ¬é•¿åº¦ä¸è¶…è¿‡ log_2Vï¼Œåœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œæ¯ä¸ªå¶å­èŠ‚ç‚¹çš„labelç»Ÿä¸€ç¼–ç åˆ°ä¸€ä¸ªå›ºå®šçš„é•¿åº¦ï¼Œä¸è¶³çš„å¯ä»¥è¿›è¡Œpad

## 2ã€LSTM

### å‰å‘ä¼ æ’­ç®—æ³•

ä¸€ä¸ªæ—¶åˆ»çš„å‰å‘ä¼ æ’­è¿‡ç¨‹å¦‚ä¸‹ï¼Œå¯¹æ¯”RNNå¤šäº†12ä¸ªå‚æ•°

![img](https://img-blog.csdnimg.cn/20190429152237896.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2doajc4NjExMA==,size_16,color_FFFFFF,t_70)

- è¾“å…¥é—¨ï¼šæ§åˆ¶æœ‰å¤šå°‘è¾“å…¥ä¿¡å·ä¼šè¢«èåˆã€‚

$$
i_{t}=sigmoid(W_{i}X_{t}+V_{i}H_{t-1}+b_i) 
$$

- é—å¿˜é—¨ï¼šæ§åˆ¶æœ‰å¤šå°‘è¿‡å»çš„è®°å¿†ä¼šè¢«é—å¿˜ã€‚
  $$
  f_{t}=sigmoid(W_{f}X_{t}+V_{f}H_{t-1}+b_f)
  $$
  
- è¾“å‡ºé—¨ï¼šæ§åˆ¶æœ€ç»ˆè¾“å‡ºå¤šå°‘èåˆäº†è®°å¿†çš„ä¿¡æ¯

$$
o_{t}=sigmoid(W_{o}X_{t}+V_{o}H_{t-1}+b_o)
$$

- å•å…ƒçŠ¶æ€ï¼šè¾“å…¥ä¿¡å·å’Œè¿‡å»çš„è¾“å…¥ä¿¡å·åšä¸€ä¸ªä¿¡æ¯èåˆã€‚

$$
g_{t}=tanh(W_{g}X_{t}+V_{g}H_{t-1}+b_g)
$$

é€šè¿‡å­¦ä¹ è¿™äº›é—¨çš„æƒé‡è®¾ç½®ï¼Œé•¿çŸ­æ—¶è®°å¿†ç½‘ç»œå¯ä»¥æ ¹æ®å½“å‰çš„è¾“å…¥ä¿¡å·å’Œè®°å¿†ä¿¡æ¯ï¼Œæœ‰é€‰æ‹©æ€§åœ°å¿½ç•¥æˆ–è€…å¼ºåŒ–å½“å‰çš„è®°å¿†æˆ–æ˜¯è¾“å…¥ä¿¡å·ï¼Œå¸®åŠ©ç½‘ç»œæ›´å¥½åœ°å­¦ä¹ é•¿å¥å­çš„è¯­ä¹‰ä¿¡æ¯ï¼š

- è®°å¿†ä¿¡å·ï¼š
  $$
  c_{t} = f_{t} \cdot c_{t-1} + i_{t} \cdot g_{t}
  $$
  
- è¾“å‡ºä¿¡å·ï¼š
  $$
  h_{t} = o_{t} \cdot tanh(c_{t})
  $$

### åå‘ä¼ æ’­

åå‘ä¼ æ’­é€šè¿‡æ¢¯åº¦ä¸‹é™æ³•è¿­ä»£æ›´æ–°æ‰€æœ‰çš„å‚æ•°            

é•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œçš„è®­ç»ƒç®—æ³•åŒæ ·é‡‡ç”¨åå‘ä¼ æ’­ç®—æ³•ï¼Œä¸»è¦æœ‰ä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š                                                                       

1.  å‰å‘è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„è¾“å‡ºå€¼,å¯¹äºé•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œæ¥è¯´ï¼Œå³ f_t ã€ i_t ã€c_t, O_t, h_t

    äº”ä¸ªå‘é‡çš„å€¼ï¼›

2.  åå‘è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„è¯¯å·®é¡¹ delta å€¼ã€‚ä¸å¾ªç¯ç¥ç»ç½‘ç»œä¸€æ ·ï¼Œé•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œè¯¯å·®é¡¹çš„åå‘ä¼ æ’­ä¹Ÿæ˜¯åŒ…æ‹¬ä¸¤ä¸ªæ–¹å‘: ä¸€ä¸ªæ˜¯æ²¿æ—¶é—´è½´çš„åå‘ä¼ æ’­ï¼Œå³ä»å½“å‰ boldsymbol_tæ—¶åˆ»å¼€å§‹ï¼Œè®¡ç®—æ¯ä¸ªæ—¶åˆ»çš„è¯¯å·®é¡¹; ä¸€ä¸ªæ˜¯å»¶ç½‘ç»œå±‚çš„åå‘ä¼ æ’­ï¼Œè¯¯å·®é¡¹å‘ä¸Šä¸€å±‚ä¼ æ’­ï¼Œè®¡ç®—æ¯ä¸€å±‚çš„è¯¯å·®é¡¹ï¼›

3. æ ¹æ®æ¯ä¸ªæ—¶åˆ»çš„è¯¯å·®é¡¹ï¼Œè®¡ç®—æ¯ä¸ªæƒé‡å‚æ•°çš„è¯¯å·®æ¢¯åº¦, æ›´æ–°æƒé‡å‚æ•°ã€‚



### litmè¯æ€§åˆ†æ

```
import torch
from torch import nn
from torch.autograd import Variable

# ç»™å‡ºä¸¤å¥è¯ä½œä¸ºè®­ç»ƒé›†ï¼Œæ¯ä¸ªå•è¯ç»™å‡ºè¯æ€§
train_data = [
    ("The dog ate the apple".split(), ["DET", "NN", "V", "DET", "NN"]),  # DET:é™å®šè¯ï¼ŒNN:åè¯ï¼ŒVï¼šåŠ¨è¯
    ("Everybody read that book".split(), ["NN", "V", "DET", "NN"])
]
print('======train_data=====')
print(train_data)

# å¯¹å•è¯ç»™å‡ºæ•°å­—ç¼–ç ï¼Œä»¥ä¾¿ä¼ å…¥Embeddingä¸­è½¬åŒ–æˆè¯å‘é‡
word_to_idx = {} # å¯¹å•è¯ç¼–ç 
tag_to_idx = {}# å¯¹è¯æ€§ç¼–ç 

for context, tag in train_data:  # contextæ˜¯å¥å­ï¼Œtagæ˜¯åé¢çš„è¯æ€§
    for word in context:  # éå†æ¯ä¸ªå•è¯ï¼Œç»™æ¯ä¸ªå•è¯ç¼–å·
        if word.lower() not in word_to_idx:        # å¦‚æœè¯¥å•è¯æ²¡æœ‰å‡ºç°è¿‡
            word_to_idx[word.lower()] = len(word_to_idx)
            # lower()å‡½æ•°ï¼Œå°†å­—ç¬¦ä¸²ä¸­çš„æ‰€æœ‰å¤§å†™å­—æ¯è½¬æ¢ä¸ºå°å†™å­—æ¯
            # å¯¹è¯¥å•è¯è¿›è¡Œç¼–å·ï¼Œä»0å¼€å§‹
    for label in tag:  # ç»™æ¯ä¸ªè¯æ€§æ ‡labelï¼Œä»¥åŠç¼–å·
        if label.lower() not in tag_to_idx:  # å¦‚æœè¯¥è¯æ€§æ²¡æœ‰å‡ºç°è¿‡
            tag_to_idx[label.lower()] = len(tag_to_idx)  # å¯¹è¯¥è¯æ€§è¿›è¡Œç¼–å·ï¼Œä»0å¼€å§‹

#  å®šä¹‰ç¼–å·å’Œtagçš„å­—å…¸ï¼Œæ–¹ä¾¿æµ‹è¯•æ—¶ä½¿ç”¨ï¼Œèƒ½å¤Ÿé€šè¿‡æŸ¥æ‰¾ç¼–å·æ‰¾åˆ°tag
idx_to_tag = {tag_to_idx[tag.lower()]: tag for tag in tag_to_idx}

# å¯¹a-zçš„å­—ç¬¦è¿›è¡Œæ•°å­—ç¼–ç 
alphabet = 'abcdefghijklmnopqrstuvwxyz'
character_to_idx = {}
for i in range(len(alphabet)):
    character_to_idx[alphabet[i]] = i

# è¿™ä¸‰ä¸ªç¼–ç ä¹‹åï¼Œç”¨å­—å…¸è¿™ç§å®¹å™¨å­˜å‚¨ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªæ•°å­—ç¼–å·
print('=====å­—ç¬¦å¯¹åº”æ•°å­—=====')
print(tag_to_idx)  # len=3
print(idx_to_tag)  # len=3
print(word_to_idx)  # len=8
print(character_to_idx)  # len=26


# å­—ç¬¦ç¼–ç ï¼Œå°†ä¼ å…¥å­—ç¬¦xä¸­å¯¹åº”çš„ç¼–ç ï¼Œè½¬åŒ–æˆLongTensorç±»å‹
def make_sequence(x, dic):
    idx = [dic[i.lower()] for i in x]
    idx = torch.LongTensor(idx)
    return idx


print('=====''make_sequence()å‡½æ•°è¾“å‡ºç»“æœæŸ¥çœ‹''=====')
print(make_sequence('abcdef', character_to_idx).type())  #å¾—åˆ°è¯¥å­—ç¬¦ä¸²çš„æ•°æ®ç±»å‹
print(make_sequence('abcdef', character_to_idx).size())   #å¾—åˆ°è¯¥å­—ç¬¦ä¸²çš„å¤§å°
print(make_sequence('abcdef', character_to_idx))        #å¾—åˆ°è¯¥å­—ç¬¦ä¸²æ¯ä¸ªå­—ç¬¦å¯¹åº”çš„ç¼–å·
print(make_sequence(train_data[0][0], word_to_idx))     #å¾—åˆ°The dog ate the appleè¿™å¥è¯æ¯ä¸ªå•è¯å¯¹åº”çš„ç¼–å·


# å®šä¹‰å­—æ¯å­—ç¬¦çš„LSTM
class char_lstm(nn.Module):
    def __init__(self, n_char, char_dim, char_hidden):
        # n_charï¼š26ä¸ªå­—æ¯ï¼Œchar_dimï¼šå•è¯å­—æ¯å‘é‡ç»´åº¦ï¼Œchar_hiddenï¼šå­—æ¯LSTMçš„è¾“å‡ºç»´åº¦ï¼Œ
        super(char_lstm, self).__init__()
        self.char_embedding = nn.Embedding(n_char, char_dim)  #26ä¸ªå­—æ¯ç¼–å·æ˜ å°„åˆ°ä½ç»´ç©ºé—´ï¼ŒåŠ é€Ÿè¿ç®—
        self.char_lstm = nn.LSTM(char_dim, char_hidden)      #è¾“å…¥char_dimç»´ï¼Œè¾“å‡ºchar_hiddenç»´

    def forward(self, x):
        x = self.char_embedding(x)
        out, _ = self.char_lstm(x)  #å¾—åˆ°è¾“å‡ºå’Œéšè—çŠ¶æ€
        return out[-1]  # (batch, hidden_size)  out[-1]å¯ä»¥è¡¨ç¤ºæˆ‘ä»¬éœ€è¦çš„çŠ¶æ€


# å®šä¹‰è¯æ€§åˆ†æçš„LSTM
class lstm_tagger(nn.Module):
    # n_wordï¼šå•è¯çš„æ•°ç›®ï¼Œn_dimï¼šå•è¯å‘é‡ç»´åº¦ï¼Œn_charå’Œchar_dimåŒç†ï¼Œchar_hiddenï¼šå­—æ¯LSTMçš„è¾“å‡ºç»´åº¦ï¼Œ
    # n_hiddenï¼šå•è¯è¯æ€§é¢„æµ‹LSTMçš„è¾“å‡ºç»´åº¦ï¼Œn_tagï¼šè¾“å‡ºçš„è¯æ€§åˆ†ç±»
    def __init__(self, n_word, n_char, char_dim, n_dim, char_hidden, n_hidden, n_tag):
                    # 8,26,10, 100, 50, 128,3
        super(lstm_tagger, self).__init__()
        self.word_embedding = nn.Embedding(n_word, n_dim)
        self.char_lstm = char_lstm(n_char, char_dim, char_hidden)
        self.lstm = nn.LSTM(n_dim + char_hidden, n_hidden)  # è¯æ€§åˆ†æLSTMè¾“å…¥ï¼šè¯å‘é‡ç»´åº¦æ•°+å­—ç¬¦LSTMè¾“å‡ºç»´åº¦æ•°
        self.classify =nn.Linear(n_hidden, n_tag)

    # å­—ç¬¦å¢å¼ºï¼Œä¼ å…¥å¥å­çš„åŒæ—¶ä½œä¸ºåºåˆ—çš„åŒæ—¶ï¼Œè¿˜è¦ä¼ å…¥å¥å­ä¸­çš„å•è¯ï¼Œç”¨wordè¡¨ç¤º
    def forward(self, x, word):
        char = []
        for w in word:  # å¯¹äºæ¯ä¸ªå•è¯ï¼Œéå†å­—æ¯ï¼Œåšå­—æ¯å­—ç¬¦çš„lstm
            char_list = make_sequence(w, character_to_idx)
            char_list = char_list.unsqueeze(1)  # (seq, batch, feature) æ»¡è¶³ lstm è¾“å…¥æ¡ä»¶
            # unsqueeze(1)åœ¨ç¬¬äºŒç»´åº¦ä¸Šå¢åŠ ä¸€ä¸ªç»´åº¦
            char_infor = self.char_lstm(Variable(char_list))  # (batch, char_hidden)
            char.append(char_infor)  #æ¯ä¸ªå•è¯çš„ç‰¹å¾ç©ºé—´
        char = torch.stack(char, dim=0)  # (seq, batch, feature)
        x = self.word_embedding(x)  # (batch, seq, word_dim)
        # print(x.shape)
        x = x.permute(1, 0, 2)  # æ”¹å˜é¡ºåºï¼Œå˜æˆ(seq, batch, word_dim)
        x = torch.cat((x, char), dim=2)  # æ²¿ç€ç‰¹å¾é€šé“å°†æ¯ä¸ªè¯çš„è¯åµŒå…¥å’Œå­—ç¬¦ lstm è¾“å‡ºçš„ç»“æœæ‹¼æ¥åœ¨ä¸€èµ·
        x, _ = self.lstm(x)
        seq, batch, h = x.shape
        x = x.view(-1, h)  # é‡æ–° reshape è¿›è¡Œåˆ†ç±»çº¿æ€§å±‚
        out = self.classify(x) #size(len, n_tag)
        return out


net = lstm_tagger(len(word_to_idx), len(character_to_idx), 10, 100, 50, 128, len(tag_to_idx))
# (n_word, n_char, char_dim, n_dim, char_hidden, n_hidden, n_tag)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(net.parameters(), lr=1e-2)

print('=====å¼€å§‹è®­ç»ƒ=====')
# å¼€å§‹è®­ç»ƒ
for e in range(500):# è®­ç»ƒ500è½®
    train_loss = 0
    for word, tag in train_data: # éå†æ•°æ®é›†ä¸­çš„å­—ç¬¦ä¸²å’Œè¯æ€§ä¸²
        # word ['The', 'dog', 'ate', 'the', 'apple']
        # tag ['DET', 'NN', 'V', 'DET', 'NN']
        word_list = make_sequence(word, word_to_idx).unsqueeze(0)  # åœ¨ç¬¬ä¸€ç»´åº¦ä¸Šï¼Œæ·»åŠ ç¬¬ä¸€ç»´ batch
        tag = make_sequence(tag, tag_to_idx)
        word_list = Variable(word_list)
        tag = Variable(tag)
        # å‰å‘ä¼ æ’­
        out = net(word_list, word)
        loss = criterion(out, tag)
        train_loss += loss.item()
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    if (e + 1) % 50 == 0:
        print('Epoch: {}, Loss: {:.5f}'.format(e + 1, train_loss / len(train_data)))

# çœ‹çœ‹é¢„æµ‹çš„ç»“æœ
print('=====æµ‹è¯•é˜¶æ®µ=====')
net = net.eval() # è®¾ç½®æˆæµ‹è¯•æ¨¡å¼

test_sent = 'Everybody ate the apple read the book' #æµ‹è¯•çš„å¥å­
test = make_sequence(test_sent.split(), word_to_idx).unsqueeze(0) # å¾—åˆ°å•è¯ç¼–å·

test_set = test_sent.split()
out = net(Variable(test), test_set)
print('=====è¾“å‡ºout======')
print(out)
print('outçš„sizeæ˜¯ï¼š ', out.size())  # è¾“å…¥çš„test_sentæ˜¯7ä¸ªå•è¯ï¼Œå•è¯è¯æœ‰ä¸‰ç§ï¼šdet,nn,vï¼Œæ‰€ä»¥ç»“æœæ˜¯torch.Size([7, 3])
print('æ¯ä¸€è¡Œtensorçš„ä¸‰ä¸ªå€¼ä»£è¡¨ç€ï¼š', tag_to_idx)
# æœ€åå¯ä»¥å¾—åˆ°ä¸€ä¸ª7x3çš„tensorï¼Œå› ä¸ºæœ€åä¸€å±‚çš„çº¿æ€§å±‚æ²¡æœ‰ä½¿ç”¨ softmaxï¼Œæ‰€ä»¥æ•°å€¼ä¸å¤ªåƒä¸€ä¸ªæ¦‚ç‡ï¼Œ
# ä½†æ˜¯æ¯ä¸€è¡Œæ•°å€¼æœ€å¤§çš„å°±è¡¨ç¤ºå±äºè¯¥ç±»ï¼Œå¯ä»¥çœ‹åˆ°ç¬¬ä¸€ä¸ªå•è¯ 'Everybody' å±äº nnï¼Œ
# ç¬¬äºŒä¸ªå•è¯ 'ate' å±äº vï¼Œç¬¬ä¸‰ä¸ªå•è¯ 'the' å±äºdetï¼Œç¬¬å››ä¸ªå•è¯ 'apple' å±äº nnï¼Œ
# æ‰€ä»¥å¾—åˆ°çš„è¿™ä¸ªé¢„æµ‹ç»“æœæ˜¯æ­£ç¡®çš„

print('=====æµ‹è¯•ç»“æœ=====')
for i in range(len(test_set)):
    pred_tag_idx = out[i].argmax().item()
    # out[i]è¡¨ç¤ºoutè¿™ä¸ªtensorçš„ç¬¬iè¡Œï¼Œ
    # argmax()æ‰¾å‡ºè¿™ä¸€è¡Œæœ€å¤§å€¼æ‰€åœ¨çš„ä½ç½®ï¼Œ
    # .item()æ–¹æ³•å°†tensorç±»å‹çš„pred_tag_idxå˜ä¸ºintç±»å‹ï¼Œæ‰å¯ä»¥ç”¨äºå­—å…¸æŸ¥è¯¢ç´¢å¼•
    pred_word = idx_to_tag[pred_tag_idx]
    print('è¿™ä¸ªå•è¯æ˜¯: ', test_set[i], '. å®ƒçš„è¯æ€§æ˜¯: ', idx_to_tag[pred_tag_idx])
```

### äºŒã€ä½¿ç”¨keraså®ç°LSTM æƒ…æ„Ÿåˆ†æ

kerasæä¾›ä¸€ä¸ªLSTMå±‚ï¼Œç”¨å®ƒæ¥æ„é€ å’Œè®­ç»ƒä¸€ä¸ªå¤šå¯¹ä¸€çš„RNNã€‚æˆ‘ä»¬çš„ç½‘ç»œå¸æ”¶ä¸€ä¸ªåºåˆ—ï¼ˆè¯åºåˆ—ï¼‰å¹¶è¾“å‡ºä¸€ä¸ªæƒ…æ„Ÿåˆ†æå€¼ï¼ˆæ­£æˆ–è´Ÿï¼‰ã€‚
è®­ç»ƒé›†æºè‡ªäºkaggleä¸Šæƒ…æ„Ÿåˆ†ç±»ç«èµ›ï¼ŒåŒ…å«7000ä¸ªçŸ­å¥ UMICH SI650
æ¯ä¸ªå¥å­æœ‰ä¸€ä¸ªå€¼ä¸º1æˆ–0çš„åˆ†åˆ«ç”¨æ¥ä»£æ›¿æ­£è´Ÿæƒ…æ„Ÿçš„æ ‡ç­¾ï¼Œè¿™ä¸ªæ ‡ç­¾å°±æ˜¯æˆ‘ä»¬å°†è¦å­¦ä¹ é¢„æµ‹çš„ã€‚

**å¯¼å…¥æ‰€éœ€åº“**

```
from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM
from keras.models import Sequential
from keras.preprocessing import sequence
from sklearn.model_selection import train_test_split
import collections
import matplotlib.pyplot as plt
import nltk
import numpy as np
import os
```

**æ¢ç´¢æ€§åˆ†æ**
ç‰¹åˆ«åœ°æƒ³çŸ¥é“è¯­æ–™ä¸­æœ‰å¤šå°‘ä¸ªç‹¬ç«‹çš„è¯ä»¥åŠæ¯ä¸ªå¥å­åŒ…å«å¤šå°‘ä¸ªè¯ï¼š

```p
#Read training data and generate vocabulary

maxlen = 0
word_freqs = collections.Counter()
num_recs = 0
ftrain = open(os.path.join(DATA_DIR, "umich-sentiment-train.txt"), 'rb')
for line in ftrain:
    label, sentence = line.strip().split("\t")
    words = nltk.word_tokenize(sentence.decode("ascii", "ignore").lower())
    if len(words) > maxlen:
        maxlen = len(words)
    for word in words:
        word_freqs[word] += 1
    num_recs += 1
ftrain.close()
```

é€šè¿‡ä¸Šè¿°ä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°è¯­æ–™çš„å€¼
maxlen: 42
len(word_freqs): 2313
æˆ‘ä»¬å°†å•è¯æ€»æ•°é‡è®¾ä¸ºå›ºå®šå€¼ï¼Œå¹¶æŠŠæ‰€æœ‰å…¶ä»–è¯çœ‹ä½œå­—å…¸å¤–çš„è¯ï¼Œè¿™äº›è¯å…¨éƒ¨ç”¨ä¼ªè¯unkï¼ˆunknownï¼‰æ›¿æ¢ï¼Œé¢„æµ‹æ—¶å€™å°†æœªè§çš„è¯è¿›è¡Œæ›¿æ¢
å¥å­åŒ…å«çš„å•è¯æ•°ï¼ˆmaxlenï¼‰è®©æˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€ä¸ªå›ºå®šçš„åºåˆ—é•¿åº¦ï¼Œå¹¶ä¸”ç”¨0è¿›è¡Œè¡¥è¶³çŸ­å¥ï¼ŒæŠŠæ›´é•¿çš„å¥å­æˆªçŸ­è‡³åˆé€‚çš„é•¿åº¦ã€‚
æŠŠVOCABULARY_SIZEè®¾ç½®ä¸º2002ï¼Œå³æºäºå­—å…¸çš„2000ä¸ªè¯ï¼ŒåŠ ä¸Šä¼ªè¯UNKå’Œå¡«å……è¯PADï¼ˆç”¨æ¥è¡¥è¶³å¥å­åˆ°å›ºå®šé•¿åº¦çš„è¯ï¼‰
è¿™é‡ŒæŠŠå¥å­æœ€å¤§é•¿åº¦MAX_SENTENCE_LENGTHå®šä¸º40

```
MAX_FEATURES = 2000
MAX_SENTENCE_LENGTH = 40
```

ä¸‹ä¸€æ­¥æˆ‘ä»¬éœ€è¦ä¸¤ä¸ªæŸ¥è¯¢è¡¨ï¼ŒRNNçš„æ¯ä¸€ä¸ªè¾“å…¥è¡Œéƒ½æ˜¯ä¸€ä¸ªè¯åºåˆ—ç´¢å¼•ï¼Œç´¢å¼•æŒ‰è®­ç»ƒé›†ä¸­è¯çš„ä½¿ç”¨é¢‘åº¦ä»é«˜åˆ°ä½æ’åºã€‚è¿™ä¸¤å¼ æŸ¥è¯¢è¡¨å…è®¸æˆ‘ä»¬é€šè¿‡ç»™å®šçš„è¯æ¥æŸ¥æ‰¾ç´¢å¼•ä»¥åŠé€šè¿‡ç»™å®šçš„ç´¢å¼•æ¥æŸ¥æ‰¾è¯ã€‚

```
#1 is UNK, 0 is PAD
#We take MAX_FEATURES-1 featurs to accound for PAD
vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2
word2index = {x[0]: i+2 for i, x in 
                enumerate(word_freqs.most_common(MAX_FEATURES))}
word2index["PAD"] = 0
word2index["UNK"] = 1
index2word = {v:k for k, v in word2index.items()}
```

**æ¥ç€æˆ‘ä»¬å°†åºåˆ—è½¬æ¢æˆè¯ç´¢å¼•åºåˆ—**
**è¡¥è¶³MAX_SENTENCE_LENGTHå®šä¹‰çš„è¯çš„é•¿åº¦**
**å› ä¸ºæˆ‘ä»¬çš„è¾“å‡ºæ ‡ç­¾æ˜¯äºŒåˆ†ç±»ï¼ˆæ­£è´Ÿæƒ…æ„Ÿï¼‰**

```
#convert sentences to sequences
X = np.empty((num_recs, ), dtype=list)
y = np.zeros((num_recs, ))
i = 0
ftrain = open(os.path.join(DATA_DIR, "umich-sentiment-train.txt"), 'rb')
for line in ftrain:
    label, sentence = line.strip().split("\t")
    words = nltk.word_tokenize(sentence.decode("ascii", "ignore").lower())
    seqs = []
    for word in words:
        if word2index.has_key(word):
            seqs.append(word2index[word])
        else:
            seqs.append(word2index["UNK"])
    X[i] = seqs
    y[i] = int(label)
    i += 1
ftrain.close()

# Pad the sequences (left padded with zeros)
X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)
```

**åˆ’åˆ†æµ‹è¯•é›†ä¸è®­ç»ƒé›†**

```
# Split input into training and test
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, 
                                                random_state=42)
print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)
```

**è®­ç»ƒæ¨¡å‹**

```
EMBEDDING_SIZE = 128
HIDDEN_LAYER_SIZE = 64
# ç¾ä¼¦æ‰¹å¤§å°32
BATCH_SIZE = 32
# ç½‘ç»œè®­ç»ƒ10è½®
NUM_EPOCHS = 10
# Build model
model = Sequential()
model.add(Embedding(vocab_size, EMBEDDING_SIZE, 
                    input_length=MAX_SENTENCE_LENGTH))
model.add(SpatialDropout1D(Dropout(0.2)))
model.add(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1))
model.add(Activation("sigmoid"))

model.compile(loss="binary_crossentropy", optimizer="adam", 
              metrics=["accuracy"])

history = model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, 
                    epochs=NUM_EPOCHS,
                    validation_data=(Xtest, ytest))
```


**æœ€åæˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹å¹¶æ‰“å°å‡ºè¯„åˆ†å’Œå‡†ç¡®ç‡**

```
# evaluate

score, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)
print("Test score: %.3f, accuracy: %.3f" % (score, acc))

for i in range(5):
    idx = np.random.randint(len(Xtest))
    xtest = Xtest[idx].reshape(1,40)
    ylabel = ytest[idx]
    ypred = model.predict(xtest)[0][0]
    sent = " ".join([index2word[x] for x in xtest[0].tolist() if x != 0])
    print("%.0f\t%d\t%s" % (ypred, ylabel, sent))
```


è‡³æ­¤æˆ‘ä»¬ä½¿ç”¨keraså®ç°lstmçš„æƒ…æ„Ÿåˆ†æå®ä¾‹
åœ¨æ­¤å®ä¾‹ä¸­å¯å­¦ä¹ åˆ°kerasæ¡†æ¶çš„ä½¿ç”¨ã€lstmæ¨¡å‹æ­å»ºã€çŸ­è¯­å¥å¤„ç†æ–¹å¼
