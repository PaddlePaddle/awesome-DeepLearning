?		在深度学习中，随着网络的层数逐渐加深，Internal Covariate Shift现象会变得愈发严重，为了有效缓解这一现象，很多归一化操作被先后提出。如，最常用的BN（Batch Normalization），LN（Layer Normalization），IN（Instance Normalization），GN（Group Normalization）等。

?		简单来说，不同的归一化操作之间的区别是归一化的维度不同，导致不同的归一化操作应用于某些特定的领域时效果较好。

![img](https://img-blog.csdnimg.cn/20191218210414670.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzMjU4OTUz,size_16,color_FFFFFF,t_70)														BN，LN，IN，GN示意图

?		下文中feature的尺寸都为 ![[公式]](https://www.zhihu.com/equation?tex=%5BN%2CC%2CH%2CW%5D) 。其中N为批数目（batch size)，C为feature map的通道数（channels)，H、W为单通道特征图的长（Height)和宽（Width)。

?		总的来说：

**BN（Batch Normalization）：以N张图片的一个通道为单位进行Normalization操作。**

**LN（Layer Normalization）：以一张图片的所有通道为单位进行Normalization操作。**

**IN（Instance Normalization）：以一张图片的一个通道为单位进行Normalization操作。**

**GN（Group Normalization）：以一张图片的部分通道为单位进行Normalization操作。**

## 1. Batch Normalization

?		于2015年由 Google 提出，BN 独立地规范化每一个层不同批次的 ，但规范化的参数是一个 mini-batch 的一阶统计量和二阶统计量。这就要求 每一个 mini-batch 的统计量是整体统计量的近似估计，或者说每一个 mini-batch 彼此之间，以及和整体数据，都应该是近似同分布的。分布差距较小的 mini-batch 可以看做是为规范化操作和模型训练引入了噪声，可以增加模型的鲁棒性；但如果每个 mini-batch的原始分布差别很大，那么不同 mini-batch 的数据将会进行不一样的数据变换，这就增加了模型训练的难度。训练时，网络会记录每一个batch滑动平均的均值和方差，训练结束的时候这四个参数就固定了供测试时直接加载使用。

?		BN应该是我们最熟悉的归一化操作了，其作用在整个mini-batch上，沿着C维度对N,H,W三个维度进行归一化，换句话说，是对mini-batch中相应的feature进行归一化。

?		假设一个mini--batch中相应的feature可表示为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D%3D%5C%7Bx_1%2Cx_2%2C%5Cldots%2Cx_C%5C%7D) ， ![[公式]](https://www.zhihu.com/equation?tex=x_i+) 的尺寸为 ![[公式]](https://www.zhihu.com/equation?tex=N%5Ctimes+H%5Ctimes+W) ，BN可以定义为：
> for i=1, ..., C

> ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_i%5Cleftarrow%5Cfrac%7B1%7D%7BNHW%7D%5Csum+x_i)

> ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma_i%5E2%5Cleftarrow+%5Cfrac%7B1%7D%7BNHW%7D%5Csum%28x_i-%5Cmu_i%29%5E2)

> ![[公式]](https://www.zhihu.com/equation?tex=%5Cwidehat%7Bx%7D_i%5Cleftarrow%5Cfrac%7Bx_i-%5Cmu_i%7D%7B%5Csqrt%7B%5Csigma_i%5E2%2B%5Cepsilon%7D%7D)

> ![[公式]](https://www.zhihu.com/equation?tex=y_i%5Cleftarrow%5Cgamma%5Cwidehat%7Bx%7D_i%2B%5Cbeta%3DBN_%7B%5Cgamma%2C%5Cbeta%7D%28x_i%29)

?		可见，BN对于均值和方差的计算与mini-batch的大小有关。当batch size足够大时，每个mini-batch可以较好地反映真实数据的分布；但是，若batch size较小，每个mini-batch就无法反映所有数据的真实分布。因此，BN操作对batch size的选择有较高的要求。

?		BN 比较适用的场景是：每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle，否则效果会差很多。

?		另外，由于 BN 需要在运行过程中统计每个 mini-batch 的一阶统计量和二阶统计量，因此不适用于动态的网络结构 和 RNN 网络。不过，也有研究者专门提出了适用于 RNN 的 BN 使用方法，这里先不展开了。

## 2. Layer Normalization

?		LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小 mini-batch 场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。
?		但是，BN 的转换是针对单个神经元可训练的――不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换――所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。

?		LN不再对mini-batch中的所有特征计算均值和方差，而是沿着batch维度对CHW三个维度进行归一化操作，从而克服了BN操作对于batch size大小比较敏感的缺点，非常适用于RNN网络的加速训练。

?		对于mini-batch中的每组特征，令 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D%3D%5C%7Bx_1%2Cx_2%2C%5Cldots%2Cx_B%5C%7D) , ![[公式]](https://www.zhihu.com/equation?tex=x_i+) 的尺寸为 ![[公式]](https://www.zhihu.com/equation?tex=C%5Ctimes+H%5Ctimes+W) ,LN操作可以定义为：

> for i = 1, ..., N

> ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_i%5Cleftarrow%5Cfrac%7B1%7D%7BCHW%7D%5Csum+x_i)

> ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma_i%5E2%5Cleftarrow+%5Cfrac%7B1%7D%7BCHW%7D%5Csum%28x_i-%5Cmu_i%29%5E2)

> ![[公式]](https://www.zhihu.com/equation?tex=%5Cwidehat%7Bx%7D_i%5Cleftarrow%5Cfrac%7Bx_i-%5Cmu_i%7D%7B%5Csqrt%7B%5Csigma_i%5E2%2B%5Cepsilon%7D%7D)

> ![[公式]](https://www.zhihu.com/equation?tex=y_i%5Cleftarrow%5Cgamma%5Cwidehat%7Bx%7D_i%2B%5Cbeta%3DLN_%7B%5Cgamma%2C%5Cbeta%7D%28x_i%29)


## 3. Instance Normalization

?		在GAN和style transfer的任务中，目前的IN norm要好于BN，IN主要用于对单张图像的数据做处理，而BN主要是对Bacth的数据做处理。由于BN在训练时每个batch的均值和方差会由于shuffle都会改变，所以可以理解为一种数据增强，而IN可以理解为对数据做一个归一化的操作。
换句话说，BN的计算是要受其他样本影响的，由于每个batch的均值和标准差不稳定，对于单个数据而言，相对于是引入了噪声，但在分类这种问题上，结果和数据的整体分布有关系，因此需要通过BN获得数据的整体分布。而instance norm的信息都是来自于自身的图片，相当于对全局信息做了一次整合和调整，在图像转换这种问题上，BN获得的整体信息不会带来任何收益，带来的噪声反而会弱化实例之间的独立性：这类生成式方法，每张图片自己的风格比较独立不应该与batch中其他的样本产生太大联系。

?		IN操作是将归一化作用于具体的某个图像实例，在HW维度进行归一化，用于图像风格转换是非常有效的。

?		令 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D%3D%5C%7Bx_%7Bij%7D%5C%7D%2Ci%3D1%2C%5Cldots+N%2Cj%3D1%2C%5Cldots+C) , ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bij%7D) 的尺寸为 ![[公式]](https://www.zhihu.com/equation?tex=H%5Ctimes+W) ，是一个具体的图像实例，此时，IN操作可以定义为：

> for i = 1, ..., N

> for j = 1, ..., C

> ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_%7Bij%7D%5Cleftarrow%5Cfrac%7B1%7D%7BHW%7D%5Csum+x_%7Bij%7D)

> ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma_%7Bij%7D%5E2%5Cleftarrow+%5Cfrac%7B1%7D%7BHW%7D%5Csum%28x_%7Bij%7D-%5Cmu_%7Bij%7D%29%5E2)

> ![[公式]](https://www.zhihu.com/equation?tex=%5Cwidehat%7Bx%7D_%7Bij%7D%5Cleftarrow%5Cfrac%7Bx_%7Bij%7D-%5Cmu_%7Bij%7D%7D%7B%5Csqrt%7B%5Csigma_%7Bij%7D%5E2%2B%5Cepsilon%7D%7D)

> ![[公式]](https://www.zhihu.com/equation?tex=y_%7Bij%7D%5Cleftarrow%5Cgamma%5Cwidehat%7Bx%7D_%7Bij%7D%2B%5Cbeta%3DIN_%7B%5Cgamma%2C%5Cbeta%7D%28x_%7Bij%7D%29)


## 4. Group Normalization

?		Group normalization是2018年3月份何恺明大神的又一力作，优化了BN在比较小的mini-batch情况下表现不太好的劣势。批量维度进行归一化会带来一些问题――批量统计估算不准确导致批量变小时，BN 的误差会迅速增加。在训练大型网络和将特征转移到计算机视觉任务中（包括检测、分割和视频），内存消耗限制了只能使用小批量的BN。
> for i = 1, ..., N

> ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_i%5Cleftarrow%5Cfrac%7B1%7D%7BGHW%7D%5Csum+x_i)

> ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma_i%5E2%5Cleftarrow+%5Cfrac%7B1%7D%7BGHW%7D%5Csum%28x_i-%5Cmu_i%29%5E2)

> ![[公式]](https://www.zhihu.com/equation?tex=%5Cwidehat%7Bx%7D_i%5Cleftarrow%5Cfrac%7Bx_i-%5Cmu_i%7D%7B%5Csqrt%7B%5Csigma_i%5E2%2B%5Cepsilon%7D%7D)

> ![[公式]](https://www.zhihu.com/equation?tex=y_i%5Cleftarrow%5Cgamma%5Cwidehat%7Bx%7D_i%2B%5Cbeta%3DGN_%7B%5Cgamma%2C%5Cbeta%7D%28x_i%29)


?		事实上，GN的极端情况就是LN和IN，分别对应G = C和G = 1。

