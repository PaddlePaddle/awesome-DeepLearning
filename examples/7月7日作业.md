# 7月7日作业

## 一、损失函数补充

#### 1.对数损失函数

**log对数损失函数**的标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L%28Y%2C+P%28Y%7CX%29%29+%3D+-logP%28Y%7CX%29++%5C%5C)

**python代码：**

```python
def logloss(y_true, y_pred, eps=1e-15):
    import numpy as np

    # Prepare numpy array data
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    assert (len(y_true) and len(y_true) == len(y_pred))

    # Clip y_pred between eps and 1-eps
    p = np.clip(y_pred, eps, 1-eps)
    loss = np.sum(- y_true * np.log(p) - (1 - y_true) * np.log(1-p))

    return loss / len(y_true)
```

#### 2.绝对值损失

每个训练样本的绝对误差是预测值和实际值之间的距离，与符号无关。绝对误差也称为**L1 Loss**：

![img](https://pics5.baidu.com/feed/d788d43f8794a4c279739e32282040d0ac6e39ab.jpeg?token=c7efd91f844fa949eec52e96c4fe621a&s=5BAC3862CDB06C114CFDF0CE0000A0B1)

正如我之前提到的，成本是这些绝对误差的平均值(MAE)。

**与MSE相比，MAE成本对异常值更加健壮**。但是，在数学方程中处理绝对或模数运算符并不容易。我们可以认为这是MAE的缺点

**python代码：**

```python
def calculateMAE(imageA, imageB):
    """
    Calculate MAE between 2 images
    np: numpy

    """
    mae = np.sum(imageB.astype("float") - imageA.astype("float"))
    mae /= float(imageA.shape[0] * imageA.shape[1] * 255) 

    if (mae < 0):
        return mae * -1
    else:
        return mae
```

#### 3.Hinge损失

**Hinge损失主要用于带有类标签-1和1的支持向量机(SVM)**。因此，请确保将数据集中"恶性"类的标签从0更改为-1。

Hinge损失不仅会惩罚错误的预测，还会惩罚不自信的正确预测。

数据对(x，y)的Hinge损失如图：

![img](https://pics7.baidu.com/feed/d1160924ab18972b0a7abe9ffa19208c9f510a78.jpeg?token=7b52a5f814966ddbdfffb81a793cb080&s=6BAC3862CD30CC114CD461DE0000A0B1)

**python代码：**

```python
def update_weights_Hinge(m1, m2, b, X1, X2, Y, learning_rate):

    m1_deriv = 0
    m2_deriv = 0
    b_deriv = 0
    N = len(X1)
    for i in range(N):
        # 计算偏导数
        if Y[i]*(m1*X1[i] + m2*X2[i] + b) <= 1:
            m1_deriv += -X1[i] * Y[i]
            m2_deriv += -X2[i] * Y[i]
            b_deriv += -Y[i]
        # 否则偏导数为0
    # 我们减去它，因为导数指向最陡的上升方向
    m1 -= (m1_deriv / float(N)) * learning_rate
    m2 -= (m2_deriv / float(N)) * learning_rate
    b -= (b_deriv / float(N)) * learning_rate
return m1, m2, b
```

#### 3.Huber损失

Huber损失结合了MSE和MAE的最佳特性。对于较小的误差，它是二次的，否则是线性的(对于其梯度也是如此)。Huber损失需要确定*δ*参数：

![img](https://pic2.zhimg.com/80/v2-333ad2d90605b5d0de9ee4ec509ee49d_720w.jpg)

**python代码：**

```python
def update_weights_Huber(m, b, X, Y, delta, learning_rate):
    m_deriv = 0
    b_deriv = 0
    N = len(X)
    for i in range(N):
        # 小值的二次导数，大值的线性导数
        if abs(Y[i] - m*X[i] - b) <= delta:
            m_deriv += -X[i] * (Y[i] - (m*X[i] + b))
            b_deriv += - (Y[i] - (m*X[i] + b))
        else:
            m_deriv += delta * X[i] * ((m*X[i] + b) - Y[i]) / abs((m*X[i] + b) - Y[i])
            b_deriv += delta * ((m*X[i] + b) - Y[i]) / abs((m*X[i] + b) - Y[i])
    #我们减去它，因为导数指向最陡的上升方向
    m -= (m_deriv / float(N)) * learning_rate
    b -= (b_deriv / float(N)) * learning_rate
    return m, b
```

## 二、池化方法

##### 1.随机池化

​		Stochastic pooling是**一种简单有效的正则化CNN的方法，能够降低max pooling的过拟合现象，提高泛化能力**。对于pooling层的输入，根据输入的多项式分布随机选择一个值作为输出。训练阶段和测试阶段的操作略有不同。

**训练阶段：**

1）前向传播：先将池化窗口中的元素全部除以它们的和，得到概率矩阵；再按照概率随机选中的方格的值，作为该区域池化后的值。

2）反向传播：求导时，只需保留前向传播中已经被选中节点的位置的值，其它值都为0，类似max-pooling的反向传播。

**测试阶段：**

在测试时也使用Stochastic Pooling会对预测值引入噪音，降低性能。取而代之的是使用概率矩阵加权平均。比使用Average Pooling表现要好一些。在平均意义上，与Average Pooling近似，在局部意义上，服从Max Pooling准则。

##### 2.重叠池化

​		重叠池化，即相邻池化窗口之间会有重叠区域。如果定义池化窗口的大小为sizeX，定义两个相邻池化窗口的水平位移 / 竖直位移为stride，此时**sizeX>stride**。

##### 3.全局池化

​		Global Pooling就是**池化窗口的大小 = 整张特征图的大小**。这样，每个 W×H×C 的特征图输入就会被转化为 1×1×C 的输出，也等同于每个位置权重都为 1/(W×H) 的全连接层操作。

## 三、数据增强方法补充

##### 1.有监督数据增强

**1.1. 单样本数据增强**

所谓单样本数据增强，即增强一个样本的时候，全部围绕着该样本本身进行操作，包括**几何变换类，颜色变换类**等。

**(1) 几何变换类**

几何变换类即对图像进行几何变换，包括**翻转，旋转，裁剪，变形，缩放**等各类操作，下面展示其中的若干个操作。

![img](https://pic1.zhimg.com/50/v2-32e1c1c73829766419c66be671e58291_hd.jpg?source=1940ef5c)![img](https://pic1.zhimg.com/80/v2-32e1c1c73829766419c66be671e58291_720w.jpg?source=1940ef5c)

水平翻转和垂直翻转

![img](https://pic2.zhimg.com/50/v2-e447be5d3ee9ad088180a8716d5bcbcf_hd.jpg?source=1940ef5c)![img](https://pic2.zhimg.com/80/v2-e447be5d3ee9ad088180a8716d5bcbcf_720w.jpg?source=1940ef5c)

随机旋转

![img](https://pic3.zhimg.com/50/v2-62aac6ef7f005550bd4f96f50b2c8f3c_hd.jpg?source=1940ef5c)![img](https://pic3.zhimg.com/80/v2-62aac6ef7f005550bd4f96f50b2c8f3c_720w.jpg?source=1940ef5c)

随机裁剪

![img](https://pic1.zhimg.com/50/v2-103ef69ae45ee9d2b8da4b514b5e6f9e_hd.jpg?source=1940ef5c)![img](https://pic1.zhimg.com/80/v2-103ef69ae45ee9d2b8da4b514b5e6f9e_720w.jpg?source=1940ef5c)

变形缩放

翻转操作和旋转操作，对于那些对方向不敏感的任务，比如图像分类，都是很常见的操作，在caffe等框架中翻转对应的就是mirror操作。

翻转和旋转不改变图像的大小，而裁剪会改变图像的大小。通常在训练的时候会采用随机裁剪的方法，在测试的时候选择裁剪中间部分或者不裁剪。值得注意的是，在一些竞赛中进行模型测试时，一般都是裁剪输入的多个版本然后将结果进行融合，对预测的改进效果非常明显。

以上操作都不会产生失真，而缩放变形则是失真的。

很多的时候，网络的训练输入大小是固定的，但是数据集中的图像却大小不一，此时就可以选择上面的裁剪成固定大小输入或者缩放到网络的输入大小的方案，后者就会产生失真，通常效果比前者差。

**(2) 颜色变换类**

上面的几何变换类操作，没有改变图像本身的内容，它可能是选择了图像的一部分或者对像素进行了重分布。如果要改变图像本身的内容，就属于颜色变换类的数据增强了，常见的包括**噪声、模糊、颜色变换、擦除、填充**等等。

基于噪声的数据增强就是在原来的图片的基础上，随机叠加一些噪声，最常见的做法就是高斯噪声。更复杂一点的就是在面积大小可选定、位置随机的矩形区域上丢弃像素产生黑色矩形块，从而产生一些彩色噪声，以Coarse Dropout方法为代表，甚至还可以对图片上随机选取一块区域并擦除图像信息。

![img](https://pic1.zhimg.com/50/v2-392f615a959b6b924b63937c9fec45cf_hd.jpg?source=1940ef5c)![img](https://pic1.zhimg.com/80/v2-392f615a959b6b924b63937c9fec45cf_720w.jpg?source=1940ef5c)

添加Coarse Dropout噪声

颜色变换的另一个重要变换是颜色扰动，就是在某一个颜色空间通过增加或减少某些颜色分量，或者更改颜色通道的顺序。

![img](https://pic1.zhimg.com/50/v2-e29ec3453559e288015b5a5391fd3e50_hd.jpg?source=1940ef5c)![img](https://pic1.zhimg.com/80/v2-e29ec3453559e288015b5a5391fd3e50_720w.jpg?source=1940ef5c)

颜色扰动

**1.2. 多样本数据增强**

不同于单样本数据增强，多样本数据增强方法利用多个样本来产生新的样本，下面介绍几种方法。

**(1) SMOTE[1]**

SMOTE即Synthetic Minority Over-sampling Technique方法，它是通过人工合成新样本来处理样本不平衡问题，从而提升分类器性能。

类不平衡现象是很常见的，它指的是数据集中各类别数量不近似相等。如果样本类别之间相差很大，会影响分类器的分类效果。假设小样本数据数量极少，如仅占总体的1%，则即使小样本被错误地全部识别为大样本，在经验风险最小化策略下的分类器识别准确率仍能达到99%，但由于没有学习到小样本的特征，实际分类效果就会很差。

SMOTE方法是基于插值的方法，它可以为小样本类合成新的样本，主要流程为：

第一步，定义好特征空间，将每个样本对应到特征空间中的某一点，根据样本不平衡比例确定好一个采样倍率N；

第二步，对每一个小样本类样本(x,y)，按欧氏距离找出K个最近邻样本，从中随机选取一个样本点，假设选择的近邻点为(xn,yn)。在特征空间中样本点与最近邻样本点的连线段上随机选取一点作为新样本点，满足以下公式：

![img](https://pic1.zhimg.com/50/v2-e49771f0cf504363a2a25f70320c2102_hd.jpg?source=1940ef5c)![img](https://pic1.zhimg.com/80/v2-e49771f0cf504363a2a25f70320c2102_720w.jpg?source=1940ef5c)

第三步，重复以上的步骤，直到大、小样本数量平衡。

该方法的示意图如下。

![img](https://pic3.zhimg.com/50/v2-da756c8413f440e7f9fed62ffd926541_hd.jpg?source=1940ef5c)![img](https://pic3.zhimg.com/80/v2-da756c8413f440e7f9fed62ffd926541_720w.jpg?source=1940ef5c)

在python中，SMOTE算法已经封装到了imbalanced-learn库中，如下图为算法实现的数据增强的实例，左图为原始数据特征空间图，右图为SMOTE算法处理后的特征空间图。

![img](https://pic1.zhimg.com/50/v2-c4e208eb36591cd9c82101e9d0d54b90_hd.jpg?source=1940ef5c)![img](https://pic1.zhimg.com/80/v2-c4e208eb36591cd9c82101e9d0d54b90_720w.jpg?source=1940ef5c)

**(2) SamplePairing[2]**

SamplePairing方法的原理非常简单，从训练集中随机抽取两张图片分别经过基础数据增强操作(如随机翻转等)处理后经像素以取平均值的形式叠加合成一个新的样本，标签为原样本标签中的一种。这两张图片甚至不限制为同一类别，这种方法对于医学图像比较有效。

![img](https://pic1.zhimg.com/50/v2-ff1e13e1dd0b3cb6e4d7329b4d2aabf1_hd.jpg?source=1940ef5c)![img](https://pic1.zhimg.com/80/v2-ff1e13e1dd0b3cb6e4d7329b4d2aabf1_720w.jpg?source=1940ef5c)

经SamplePairing处理后可使训练集的规模从N扩增到N×N。实验结果表明，因SamplePairing数据增强操作可能引入不同标签的训练样本，导致在各数据集上使用SamplePairing训练的误差明显增加，而在验证集上误差则有较大幅度降低。

尽管SamplePairing思路简单，性能上提升效果可观，符合奥卡姆剃刀原理，但遗憾的是可解释性不强。

**(3) mixup[3]**

mixup是Facebook人工智能研究院和MIT在“Beyond Empirical Risk Minimization”中提出的基于邻域风险最小化原则的数据增强方法，它使用线性插值得到新样本数据。

令(xn,yn)是插值生成的新数据，(xi,yi)和(xj,yj)是训练集随机选取的两个数据，则数据生成方式如下

![img](https://pic2.zhimg.com/50/v2-1227a3b22b5d242f9ea3706e46b65dd1_hd.jpg?source=1940ef5c)![img](https://pic2.zhimg.com/80/v2-1227a3b22b5d242f9ea3706e46b65dd1_720w.jpg?source=1940ef5c)

λ的取指范围介于0到1。提出mixup方法的作者们做了丰富的实验，实验结果表明可以改进深度学习模型在ImageNet数据集、CIFAR数据集、语音数据集和表格数据集中的泛化误差，降低模型对已损坏标签的记忆，增强模型对对抗样本的鲁棒性和训练生成对抗网络的稳定性。

**SMOTE，SamplePairing，mixup三者思路上有相同之处，都是试图将离散样本点连续化来拟合真实样本分布**，不过所增加的样本点在特征空间中仍位于已知小样本点所围成的区域内。如果能够在给定范围之外适当插值，也许能实现更好的数据增强效果。

##### 2.无监督数据增强

**2.1 GAN**

关于GAN(generative adversarial networks)，我们已经说的太多了。它包含两个网络，一个是生成网络，一个是对抗网络，基本原理如下：

(1) G是一个生成图片的网络，它接收随机的噪声z，通过噪声生成图片，记做G(z) 。

(2) D是一个判别网络，判别一张图片是不是“真实的”，即是真实的图片，还是由G生成的图片。

![img](https://pic2.zhimg.com/50/v2-4934b0fc932ea0b3fc14e7365c771344_hd.jpg?source=1940ef5c)![img](https://pic2.zhimg.com/80/v2-4934b0fc932ea0b3fc14e7365c771344_720w.jpg?source=1940ef5c)

GAN的以假乱真能力就不多说了。

**2.2 Autoaugmentation**

AutoAugment是Google提出的自动选择最优数据增强方案的研究，这是无监督数据增强的重要研究方向。它的基本思路是使用增强学习从数据本身寻找最佳图像变换策略，对于不同的任务学习不同的增强方法，流程如下：

(1) 准备16个常用的数据增强操作。

(2) 从16个中选择5个操作，随机产生使用该操作的概率和相应的幅度，将其称为一个sub-policy，一共产生5个sub-polices。

(3) 对训练过程中每一个batch的图片，随机采用5个sub-polices操作中的一种。

(4) 通过模型在验证集上的泛化能力来反馈，使用的优化方法是增强学习方法。

(5) 经过80~100个epoch后网络开始学习到有效的sub-policies。

(6) 之后串接这5个sub-policies，然后再进行最后的训练。

总的来说，就是学习已有数据增强的组合策略，对于门牌数字识别等任务，研究表明剪切和平移等几何变换能够获得最佳效果。

![img](https://pic1.zhimg.com/50/v2-abecc2d60b3533f3063cb5f5e1b54e25_hd.jpg?source=1940ef5c)![img](https://pic1.zhimg.com/80/v2-abecc2d60b3533f3063cb5f5e1b54e25_720w.jpg?source=1940ef5c)

而对于ImageNet中的图像分类任务，AutoAugment学习到了不使用剪切，也不完全反转颜色，因为这些变换会导致图像失真。AutoAugment学习到的是侧重于微调颜色和色相分布。

![img](https://pic3.zhimg.com/50/v2-f580c886bb0d61a3996394a34e80e15d_hd.jpg?source=1940ef5c)![img](https://pic3.zhimg.com/80/v2-f580c886bb0d61a3996394a34e80e15d_720w.jpg?source=1940ef5c)

## 四、图像分类方法综述

### 1、图像分类介绍

　　什么是图像分类，核心是从给定的分类集合中给图像分配一个标签的任务。实际上，这意味着我们的任务是分析一个输入图像并返回一个将图像分类的标签。标签来自预定义的可能类别集。

　　示例：我们假定一个可能的类别集categories = {dog, cat, eagle}，之后我们提供一张图1给分类系统：

　　　![img](https://img2020.cnblogs.com/i-beta/1126989/202003/1126989-20200311150933618-1507264443.png)

　　这里的目标是根据输入图像，从类别集中分配一个类别，这里为dog,我们的分类系统也可以根据概率给图像分配多个标签，如dog:95%，cat:4%，eagle:1%。

　　图像分类的任务就是给定一个图像，正确给出该图像所属的类别。对于超级强大的人类视觉系统来说，判别出一个图像的类别是件很容易的事，但是对于计算机来说，并不能像人眼那样一下获得图像的语义信息。

　　计算机能看到的只是一个个像素的数值，对于一个RGB图像来说，假设图像的尺寸是32*32，那么机器看到的就是一个形状为3*32*32的矩阵，或者更正式地称其为“张量”（“张量”简单来说就是高维的矩阵），那么机器的任务其实也就是寻找一个函数关系，这个函数关系能够将这些像素的数值映射到一个具体的类别（类别可以用某个数值表示）。

## 2、应用场景

　　图像分类更适用于图像中待分类的物体是单一的，如上图1中待分类物体是单一的，如果图像中包含多个目标物，如下图3，可以使用多标签分类或者目标检测算法。

　　　![img](https://img2020.cnblogs.com/i-beta/1126989/202003/1126989-20200311151251399-1963260468.png)

## 3、传统图像分类算法

　　通常完整建立图像识别模型一般包括底层特征学习、特征编码、空间约束、分类器设计、模型融合等几个阶段，如图4所示。

　　　![img](https://img2020.cnblogs.com/i-beta/1126989/202003/1126989-20200311151335303-1506267832.png)

　　**1).** **底层特征提取**: 通常从图像中按照固定步长、尺度提取大量局部特征描述。常用的局部特征包括SIFT(Scale-Invariant Feature Transform, 尺度不变特征转换) 、HOG(Histogram of Oriented Gradient, 方向梯度直方图) 、LBP(Local Bianray Pattern, 局部二值模式)等，一般也采用多种特征描述，防止丢失过多的有用信息。

　　**2).** **特征编码**: 底层特征中包含了大量冗余与噪声，为了提高特征表达的鲁棒性，需要使用一种特征变换算法对底层特征进行编码，称作特征编码。常用的特征编码方法包括向量量化编码、稀疏编码、局部线性约束编码、Fisher向量编码等。

　　**3).** **空间特征约束**: 特征编码之后一般会经过空间特征约束，也称作特征汇聚。特征汇聚是指在一个空间范围内，对每一维特征取最大值或者平均值，可以获得一定特征不变形的特征表达。金字塔特征匹配是一种常用的特征汇聚方法，这种方法提出将图像均匀分块，在分块内做特征汇聚。

　　**4).** **通过分类器分类**: 经过前面步骤之后一张图像可以用一个固定维度的向量进行描述，接下来就是经过分类器对图像进行分类。通常使用的分类器包括SVM(Support Vector Machine, 支持向量机)、随机森林等。而使用核方法的SVM是最为广泛的分类器，在传统图像分类任务上性能很好。

　　这种传统的图像分类方法在PASCAL VOC竞赛中的图像分类算法中被广泛使用 。

## 4、深度学习算法

　　Alex Krizhevsky在2012年ILSVRC提出的CNN模型取得了历史性的突破，效果大幅度超越传统方法，获得了ILSVRC2012冠军，该模型被称作AlexNet。这也是首次将深度学习用于大规模图像分类中。

　　从AlexNet之后，涌现了一系列CNN模型，不断地在ImageNet上刷新成绩，如图5展示。随着模型变得越来越深以及精妙的结构设计，Top-5的错误率也越来越低，降到了3.5%附近。而在同样的ImageNet数据集上，人眼的辨识错误率大概在5.1%，也就是目前的深度学习模型的识别能力已经超过了人眼。

　　　![img](https://img2020.cnblogs.com/i-beta/1126989/202003/1126989-20200311151412233-1493714597.png)

##### 4.1 CNN

　　传统CNN包含卷积层、全连接层等组件，并采用softmax多类别分类器和多类交叉熵损失函数，一个典型的卷积神经网络如图6所示，我们先介绍用来构造CNN的常见组件。

　　　![img](https://img2020.cnblogs.com/i-beta/1126989/202003/1126989-20200311151457903-1971501453.png)

　　l  卷积层(convolution layer): 执行卷积操作提取底层到高层的特征，发掘出图片局部关联性质和空间不变性质。

　　l  池化层(pooling layer): 执行降采样操作。通过取卷积输出特征图中局部区块的最大值(max-pooling)或者均值(avg-pooling)。降采样也是图像处理中常见的一种操作，可以过滤掉一些不重要的高频信息。

　　l  全连接层(fully-connected layer，或者fc layer): 输入层到隐藏层的神经元是全部连接的。

　　l  非线性变化: 卷积层、全连接层后面一般都会接非线性变化函数，例如Sigmoid、Tanh、ReLu等来增强网络的表达能力，在CNN里最常使用的为ReLu激活函数。

　　l  Dropout: 在模型训练阶段随机让一些隐层节点权重不工作，提高网络的泛化能力，一定程度上防止过拟合。

　　另外，在训练过程中由于每层参数不断更新，会导致下一次输入分布发生变化，这样导致训练过程需要精心设计超参数。如2015年Sergey Ioffe和Christian Szegedy提出了Batch Normalization (BN)算法  中，每个batch对网络中的每一层特征都做归一化，使得每层分布相对稳定。BN算法不仅起到一定的正则作用，而且弱化了一些超参数的设计。

　　经过实验证明，BN算法加速了模型收敛过程，在后来较深的模型中被广泛使用。

##### 4.2 VGG

　　牛津大学VGG(Visual Geometry Group)组在2014年ILSVRC提出的模型被称作VGG模型。该模型相比以往模型进一步加宽和加深了网络结构，它的核心是五组卷积操作，每两组之间做Max-Pooling空间降维。同一组内采用多次连续的3X3卷积，卷积核的数目由较浅组的64增多到最深组的512，同一组内的卷积核数目是一样的。卷积之后接两层全连接层，之后是分类层。

　　由于每组内卷积层的不同，有11、13、16、19层这几种模型，下图展示一个16层的网络结构。VGG模型结构相对简洁，提出之后也有很多文章基于此模型进行研究，如在ImageNet上首次公开超过人眼识别的模型就是借鉴VGG模型的结构

　　　![img](https://img2020.cnblogs.com/i-beta/1126989/202003/1126989-20200311151555311-1576945570.png)

##### 4.3 GoogLeNet

　　GoogLeNet 在2014年ILSVRC的获得了冠军，在介绍该模型之前我们先来了解NIN(Network in Network)模型和Inception模块，因为GoogLeNet模型由多组Inception模块组成，模型设计借鉴了NIN的一些思想。

　　NIN模型主要有两个特点：

　　1.引入了多层感知卷积网络(Multi-Layer Perceptron Convolution, MLPconv)代替一层线性卷积网络。MLPconv是一个微小的多层卷积网络，即在线性卷积后面增加若干层1x1的卷积，这样可以提取出高度非线性特征。

　　2.传统的CNN最后几层一般都是全连接层，参数较多。而NIN模型设计最后一层卷积层包含类别维度大小的特征图，然后采用全局均值池化(Avg-Pooling)替代全连接层，得到类别维度大小的向量，再进行分类。这种替代全连接层的方式有利于减少参数。

　　Inception模块如下图8所示，下图左是最简单的设计，输出是3个卷积层和一个池化层的特征拼接。这种设计的缺点是池化层不会改变特征通道数，拼接后会导致特征的通道数较大，经过几层这样的模块堆积后，通道数会越来越大，导致参数和计算量也随之增大。

　　为了改善这个缺点，下图右引入3个1x1卷积层进行降维，所谓的降维就是减少通道数，同时如NIN模型中提到的1x1卷积也可以修正线性特征。

　　　![img](https://img2020.cnblogs.com/i-beta/1126989/202003/1126989-20200311151630755-2077556332.png)

　　GoogLeNet由多组Inception模块堆积而成。另外，在网络最后也没有采用传统的多层全连接层，而是像NIN网络一样采用了均值池化层；但与NIN不同的是，GoogLeNet在池化层后加了一个全连接层来映射类别数。

　　除了这两个特点之外，由于网络中间层特征也很有判别性，GoogLeNet在中间层添加了两个辅助分类器，在后向传播中增强梯度并且增强正则化，而整个网络的损失函数是这个三个分类器的损失加权求和。

　　GoogLeNet整体网络结构如图9所示，总共22层网络：开始由3层普通的卷积组成；接下来由三组子网络组成，第一组子网络包含2个Inception模块，第二组包含5个Inception模块，第三组包含2个Inception模块；然后接均值池化层、全连接层。

　　　![img](https://img2020.cnblogs.com/i-beta/1126989/202003/1126989-20200311151800645-1447161874.png)

　　上面介绍的是GoogLeNet第一版模型(称作GoogLeNet-v1)。GoogLeNet-v2引入BN层；GoogLeNet-v3 对一些卷积层做了分解，进一步提高网络非线性能力和加深网络；GoogLeNet-v4引入下面要讲的ResNet设计思路。从v1到v4每一版的改进都会带来准确度的提升，介于篇幅，这里不再详细介绍v2到v4的结构。

##### 4.4 ResNet

　　ResNet(Residual Network) 是2015年ImageNet图像分类、图像物体定位和图像物体检测比赛的冠军。针对随着网络训练加深导致准确度下降的问题，ResNet提出了残差学习方法来减轻训练深层网络的困难。

　　在已有设计思路(BN, 小卷积核，全卷积网络)的基础上，引入了残差模块。每个残差模块包含两条路径，其中一条路径是输入特征的直连通路，另一条路径对该特征做两到三次卷积操作得到该特征的残差，最后再将两条路径上的特征相加。

　　残差模块如图10所示，左边是基本模块连接方式，由两个输出通道数相同的3x3卷积组成。右边是瓶颈模块(Bottleneck)连接方式，之所以称为瓶颈，是因为上面的1x1卷积用来降维(图示例即256->64)，下面的1x1卷积用来升维(图示例即64->256)，这样中间3x3卷积的输入和输出通道数都较小(图示例即64->64)。

　　　![img](https://img2020.cnblogs.com/i-beta/1126989/202003/1126989-20200311151831376-1190864035.png)

　　图11展示了50、101、152层网络连接示意图，使用的是瓶颈模块。这三个模型的区别在于每组中残差模块的重复次数不同(见图右上角)。ResNet训练收敛较快，成功的训练了上百乃至近千层的卷积神经网络。

　　　![img](https://img2020.cnblogs.com/i-beta/1126989/202003/1126989-20200311151937565-832752578.png)