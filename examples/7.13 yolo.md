```python

```

# 目标检测知识
### 解决目标检测正负样本不均衡的手段
<font face="黑体" size = 2>
1.OHEM算法（online hard example miniing，发表于2016年的CVPR）主要是针对训练过程中的困难样本自动选择，其核心思想是根据输入样本的损失进行筛选，筛选出困难样本（即对分类和检测影响较大的样本），然后将筛选得到的这些样本应用在随机梯度下降中训练。


传统的Fast RCNN系列算法在正负样本选择的时候采用当前RoI与真实物体的IoU阈值比较的方法，这样容易忽略一些较为重要的难负样本，并且固定了正、负样本的比例与最大数量，显然不是最优的选择。以此为出发点，OHEM将交替训练与SGD优化方法进行了结合，在每张图片的RoI中选择了较难的样本，实现了在线的难样本挖掘。


OHEM实现在线难样本挖掘的网络如上图所示。图中包含了两个相同的RCNN网络，上半部的a部分是只可读的网络，只进行前向运算；下半部的b网络即可读也可写，需要完成前向计算与反向传播。


在一个batch的训练中，基于Fast RCNN的OHEM算法可以分为以下5步：

（1）按照原始Fast RCNN算法，经过卷积提取网络与RoI Pooling得到了每一张图像的RoI。

（2）上半部的a网络对所有的RoI进行前向计算，得到每一个RoI的损失。

（3）对RoI的损失进行排序，进行一步NMS操作，以去除掉重叠严重的RoI，并在筛选后的RoI中选择出固定数量损失较大的部分，作为难样本。

（4）将筛选出的难样本输入到可读写的b网络中，进行前向计算，得到损失。

（5）利用b网络得到的反向传播更新网络，并将更新后的参数与上半部的a网络同步，完成一次迭代。

当然，为了实现方便，OHEM的简单实现可以是：在原有的Fast-RCNN里的loss layer里面对所有的props计算其loss，根据loss对其进行排序，选出K个hard examples，反向传播时，只对这K个props的梯度/残差回传，而其他的props的梯度/残差设为0。

但是，由于其特殊的损失计算方式，把简单的样本都舍弃了，导致模型无法提升对于简单样本的检测精度，这也是OHEM方法的一个弊端。

优点：

1) 对于数据的类别不平衡问题不需要采用设置正负样本比例的方式来解决，这种在线选择方式针对性更强；

２) 随着数据集的增大，算法的提升更加明显；

缺点：

只保留loss较高的样本，完全忽略简单的样本，这本质上是改变了训练时的输入分布（仅包含困难样本），这会导致模型在学习的时候失去对简单样本的判别能力。

### yolo各代模型的对比
YOLOv2相对v1版本，在继续保持处理速度的基础上，从预测更准确（Better），速度更快（Faster），识别对象更多（Stronger）这三个方面进行了改进。其中识别更多对象也就是扩展到能够检测9000种不同对象，称之为YOLO9000。

文章提出了一种新的训练方法–联合训练算法，这种算法可以把这两种的数据集混合到一起。使用一种分层的观点对物体进行分类，用巨量的分类数据集数据来扩充检测数据集，从而把两种不同的数据集混合起来。

联合训练算法的基本思路就是：同时在检测数据集和分类数据集上训练物体检测器（Object Detectors ），用检测数据集的数据学习物体的准确位置，用分类数据集的数据来增加分类的类别量、提升健壮性。

YOLOv3 的先验检测（Prior detection）系统将分类器或定位器重新用于执行检测任务。他们将模型应用于图像的多个位置和尺度。而那些评分较高的区域就可以视为检测结果。此外，相对于其它目标检测方法，我们使用了完全不同的方法。我们将一个单神经网络应用于整张图像，该网络将图像划分为不同的区域，因而预测每一块区域的边界框和概率，这些边界框会通过预测的概率加权。我们的模型相比于基于分类器的系统有一些优势。它在测试时会查看整个图像，所以它的预测利用了图像中的全局信息。与需要数千张单一目标图像的 R-CNN 不同，它通过单一网络评估进行预测。这令 YOLOv3 非常快，一般它比 R-CNN 快 1000 倍、比 Fast R-CNN 快 100 倍。

改进之处

多尺度预测 （引入FPN）。

更好的基础分类网络（darknet-53, 类似于ResNet引入残差结构）。

分类器不在使用Softmax，分类损失采用binary cross-entropy loss（二分类交叉损失熵）

YOLOv3不使用Softmax对每个框进行分类，主要考虑因素有两个：

Softmax使得每个框分配一个类别（score最大的一个），而对于Open Images这种数据集，目标可能有重叠的类别标签，因此Softmax不适用于多标签分类。

Softmax可被独立的多个logistic分类器替代，且准确率不会下降。

分类损失采用binary cross-entropy loss。

yolov4
1. 提出了一种高效而强大的目标检测模型。它使每个人都可以使用1080 Ti或2080 Ti GPU 训练超快速和准确的目标检测器

2. 在检测器训练期间，验证了SOTA的Bag-of Freebies 和Bag-of-Specials方法的影响。

3. 改进了SOTA的方法，使它们更有效，更适合单GPU训练，包括CBN ，PAN ，SAM 等。文章将目前主流的目标检测器框架进行拆分：input、backbone、neck 和 head.

YOLOv5 的表现要优于谷歌开源的目标检测框架 EfficientDet，尽管 YOLOv5 的开发者没有明确地将其与 YOLOv4 进行比较，但他们却声称 YOLOv5 能在 Tesla P100 上实现 140 FPS 的快速检测；相较而言，YOLOv4 的基准结果是在 50 FPS 速度下得到的

### Faster-RCNN
)输入测试图像；
(2)将整张图片输入CNN，进行特征提取；
(3)用RPN先生成一堆Anchor box，对其进行裁剪过滤后通过softmax判断anchors属于前景(foreground)或者后景(background)，即是物体or不是物体，所以这是一个二分类；同时，另一分支bounding box regression修正anchor box，形成较精确的proposal（注：这里的较精确是相对于后面全连接层的再一次box regression而言）
(4)把建议窗口映射到CNN的最后一层卷积feature map上；
(5)通过RoI pooling层使每个RoI生成固定尺寸的feature map；
(6)利用Softmax Loss(探测分类概率) 和Smooth L1 Loss(探测边框回归)对分类概率和边框回归(Bounding box regression)联合训练.

相比FASTER-RCNN，主要两处不同:
(1)使用RPN(Region Proposal Network)代替原来的Selective Search方法产生建议窗口；
(2)产生建议窗口的CNN和目标检测的CNN共享

### Yolov5
YOLOv5是一种单阶段目标检测算法，该算法在YOLOv4的基础上添加了一些新的改进思路，使其速度与精度都得到了极大的性能提升。主要的改进思路如下所示：

输入端：在模型训练阶段，提出了一些改进思路，主要包括Mosaic数据增强、自适应锚框计算、自适应图片缩放；
基准网络：融合其它检测算法中的一些新思路，主要包括：Focus结构与CSP结构；
Neck网络：目标检测网络在BackBone与最后的Head输出层之间往往会插入一些层，Yolov5中添加了FPN+PAN结构；
Head输出层：输出层的锚框机制与YOLOv4相同，主要改进的是训练时的损失函数GIOU_Loss，以及预测框筛选的DIOU_nms。
—————
