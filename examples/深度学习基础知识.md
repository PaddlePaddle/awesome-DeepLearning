**CNN-DSSM**

**概念：**

CNN-DSSM在DSSM的基础上改进了数据的预处理和深度，针对 DSSM 词袋模型丢失上下文信息的缺点。CNN-DSSM 与 DSSM 的区别主要在于输入层和表示层。

**模型：**

![](https://ai-studio-static-online.cdn.bcebos.com/514dcb88b0604792b82468168f9401881e4bf33f3d074c80a49df3f83b3adfe4)

输入：Query是代表用户输入，document是数据库中的文档。
**输入层**
**word-n-gram层**：是对输入做了一个获取上下文信息的窗口，图中是word-trigram，取连续的3个单词。

word-trigram包含了上下文信息的滑动窗口。举个例子：
把 online auto body ... 这句话提取出前三个词online auto，之后再分别对这三个词进行letter-trigram映射到一个 3 万维的向量空间里，然后把三个向量 concat 起来，最终映射到一个 9 万维的向量空间里

Letter-trigram：是把上层的三个单词通过3个字母的形式映射到3w维，然后把3个单词连接起来成9w维的空间
    英文的处理方式
 
**表示层**
  
**Convolutional layer：**是通过Letter-trigram层乘上卷积矩阵获得，是普通的卷积操作。
  
  卷积层的作用是提取滑动窗口下的上下文特征。以下图为例，假设输入层是一个 302*90000（302 行，9 万列）的矩阵，代表 302 个字向量（query 的和 Doc 的长度一般小于 300，这里少了就补全，多了就截断），每个字向量有 9 万维。而卷积核是一个 3*90000 的权值矩阵，卷积核以步长为 1 向下移动，得到的 feature map 是一个 300*1 的矩阵，feature map 的计算公式是(输入层维数 302-卷积核大小 3 步长 1)/步长 1=300。而这样的卷积核有 300 个，所以形成了 300 个 300*1 的 feature map 矩阵。

**Max-pooling：**是把卷积结果经过池化操作。
  
  池化层的作用是为句子找到全局的上下文特征。池化层以 Max-over-time pooling 的方式，每个 feature map 都取最大值，得到一个 300 维的向量。Max-over-pooling 可以解决可变长度的句子输入问题（因为不管 Feature Map 中有多少个值，只需要提取其中的最大值）。不过我们在上一步已经做了句子的定长处理（固定句子长度为 302），所以就没有可变长度句子的问题。最终池化层的输出为各个 Feature Map 的最大值，即一个 300*1 的向量。这里多提一句，之所以 Max pooling 层要保持固定的输出维度，是因为下一层全链接层要求有固定的输入层数，才能进行训练。
  
**Semantic layer：**是语义层，是池化层经过全连接得到的。
  
  最后通过全连接层把一个 300 维的向量转化为一个 128 维的低维语义向量。
  
  **匹配层**
Query 和 Doc 的语义相似性可以用这两个语义向量(128 维) 的 cosine 距离(即余弦相似度) 来表示：

  ![](https://ai-studio-static-online.cdn.bcebos.com/57b547a0d52d428da34d2bbe256860f3125583a1861d4a6a9901928e18bf3b50)
  

通过 softmax 函数可以把 Query 与正样本 Doc 的语义相似性转化为一个后验概率：
  
  ![](https://ai-studio-static-online.cdn.bcebos.com/03c76267e50243a09c9351f4f64659a3daccba145a2d44c584eccde8c9c4b0d4)


其中 r 为 softmax 的平滑因子，D 为 Query 下的正样本，D-为 Query 下的负样本（采取随机负采样），D 为 Query 下的整个样本空间。

在训练阶段，通过极大似然估计，我们最小化损失函数：

  ![](https://ai-studio-static-online.cdn.bcebos.com/820e00ade7284502a3fe909eec569397f23a1c2d6b194d4cb4c8813e1a568cbb)

残差会在表示层的 DNN 中反向传播，最终通过随机梯度下降（SGD）使模型收敛，得到各网络层的参数{Wi,bi}。
作用
  
  CNN-DSSM 通过卷积层提取了滑动窗口下的上下文信息，又通过池化层提取了全局的上下文信息，上下文信息得到较为有效的保留。
  
**场景**
  
  文本相似度计算
  
**优缺点**
  
  优点：CNN-DSSM 通过卷积层提取了滑动窗口下的上下文信息，又通过池化层提取了全局的上下文信息，上下文信息得到较为有效的保留。

  缺点：对于间隔较远的上下文信息，难以有效保留。举个例子，I grew up in France... I speak fluent French，显然 France 和 French 是具有上下文依赖关系的，但是由于 CNN-DSSM 滑动窗口（卷积核）大小的限制，导致无法捕获该上下文信息。

**LSTM-DSSM**

**概念**

针对 CNN-DSSM 无法捕获较远距离上下文特征的缺点，有人提出了用LSTM-DSSM[3]（Long-Short-Term Memory）来解决该问题

**模型：**

**LSTM**

LSTM是一种 RNN 特殊的类型，可以学习长期依赖信息。

![](https://ai-studio-static-online.cdn.bcebos.com/919db413c6784e87a67756a39742f302dc96ef1aa13e4133bbde4942cf6e6d0c)

**LSTM-DSSM**

LSTM-DSSM 其实用的是 LSTM 的一个变种——加入了peep hole的 LSTM。如下图所示：

![](https://ai-studio-static-online.cdn.bcebos.com/d265d1ac1c3d483788814bad1556dc967ec8697253f643c1b596145f31ad628e)

**LSTM-DSSM 整体的网络结构**

![](https://ai-studio-static-online.cdn.bcebos.com/d62e79fa5292462dbb4350214aa2daf4c3e85246f24f472ca7de85bfa07f2f0c)

**作用：**

捕获较远距离上下文特征

**场景**

文本匹配

**优缺点**
****************
优点：捕获较远距离上下文特征

**多目标模型的场景和作用：**

在推荐系统中想要达成某些目标的时候，需要采取多目标的方式处理。以给用户推荐视频为例，我们既希望提高用户的点击率，同时也希望提高视频的播放时长，视频点赞、转发等。
这些目标的达成并非是简单的相辅相成，更多的可能是相互竞争的关系。要是我们只让模型学习点击率，那么经过训练的模型推荐结果很可能导致标题党和封面党大行其道，真正的好的视频却被雪藏了，这显然不是我们希望看到的。而如果一味的追求高点赞，也可能就忽略了一些相对冷门的或新的佳作。以及很多视频由于各种可能的原因并不适合大多数群体等...因此，我们无法追求某个单一目标的达成，而需要同时优化这些有利于产品良性循环的任务目标，让它们相互平衡，从而提升用户体验，带来和留住更多的用户。

**Share bottom:**

**概念：**

多任务学习，最为常规的思路就是共享底部最抽象的表示层，然后在上层分化出不同的任务。Shared-Bottom的思路就是多个目标底层共用一套共享layer，在这之上基于不同的目标构建不同的Tower。这样的好处就是底层的layer复用，减少计算量，同时也可以防止过拟合的情况出现。
缺陷：这种架构极大地限制了模型表达的能力，为什么这么说？因为我们在共享特征的上层直接接入了多个目标的输出，而由于多个任务各自有不同的数据分布，也就是说我们对不同任务的输出具有一定的差异性，而相同的特征输入会极大地削弱模型的多任务输出表达而在某种程度上降低了多目标模型的泛化能力。模型在面对不相关的目标时，效果将会大大降低。

![](https://ai-studio-static-online.cdn.bcebos.com/dd5c46a1c153425a8b8021288a73d233d8207c99700b42208818a00c27523f97)


**MMoE：**

为了进行不相关任务的多任务学习，很多人做了很多工作都见效甚微，然后后来就有了Google的这个相当新颖的模型MMoE。

**优势：**

它的优势在于跳出了Shared Bottom那种将整个隐藏层一股脑的共享的思维定式，而是将共享层有意识的（按照数据领域之类的）划分成了多个Expert，并引入了gate机制，得以个性化组合使用共享层。对比MMoE和Share Bottom层，MMoE实际上是把Shared Bottom层替换成了一个双Gate的MoE层。Gate network也叫门控网络，可以控制在不同应用场景下每个expert的参与权重，也可以认为这是对每次训练让网络有意识的去针对某一项进行练习。从某种角度上来讲，MMoE的厉害之处在于它的expert可以定义成任意一种单独的模型。这就使得我们在expert上可以灵活自由地实现自己想要的模型设计。

**模型结构：**

由图可以看出，我们在定义两个任务优化模型时，在特征输入阶段，进行特征转换，分别产生若干expert，作为我们模型的基学习器，然后在每个任务对应的输入，分别使用"gate network"来表征最后的结果输出。 

![](https://ai-studio-static-online.cdn.bcebos.com/8ce3b5c4a0614b029af00dc64796bbd41c90a8231621485283b3a5b8cf343c07)

两种模型的loss对比

![](https://ai-studio-static-online.cdn.bcebos.com/cea80f70550740f3a77cc91206a0dea723a0a071e8154ba993f8120cbf683695)

可以看出来，在多数情况下MoE模型都要优于Share Bottom模型。

**缺陷：**

需要进行大量经验化的调参以及实现起来较为复杂。







请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>
Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. 
