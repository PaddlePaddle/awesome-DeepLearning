# 一，深度学习发展历史  
![](https://ai-studio-static-online.cdn.bcebos.com/91d3fb07214d4aa1b27c53c2f5fff3d8de72ba938341414e9719b0482970bb55)


图源来自微信公众号（深度学习大讲堂）


深度学习从06年崛起之前共经历过两次低谷，根据这两次低谷可以大致将深度学习的发展史划为三个阶段：
  
第一个阶段为1958年~1969年。

这个阶段的代表成果当属1958年Rosenblatt发明的感知器模型，它的指导思想起源于1943年的MCP人工神经元模型，当时是希望能够用计算机来模拟人的神经元反应的过程，
  
MCP人工神经元模型将神经元简化为了三个过程：输入信号线性加权，求和，非线性激活（阈值法）。
  
感知器算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。
  
1962年，该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。
  
1969年，美国数学家及人工智能先驱Minsky在其著作中证明了感知器本质上是一种线性模型，只能处理线性分类问题，却最简单的XOR（异或）问题都无法正确分类。神经网络的研究因此也陷入了近20年的停滞。

第二个阶段为1986年~1998年

1986年，Hinton发明了适用于多层感知器（MLP）的BP算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。

1989年，Robert Hecht-Nielsen证明了MLP的万能逼近定理，即对于任何闭区间内的一个连续函数f，都可以用含有一个隐含层的BP网络来逼近该定理的发现极大的鼓舞了神经网络的研究人员。

同年，LeCun发明了卷积神经网络-LeNet，并将其用于数字识别，且取得了较好的成绩。

1991年，BP算法被指出存在梯度消失问题，即在误差梯度后向传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该发现对此时的NN发展雪上加霜。

1997年，LSTM模型被发明，尽管该模型在序列建模上的特性非常突出，但由于正处于NN的下坡期，也没有引起足够的重视。

第三个阶段为2006年-至今，该阶段又分为两个时期：快速发展期（2006~2012）与爆发期（2012~至今）

快速发展期（2006~2012）

2006年，Hinton提出了深层网络训练中梯度消失问题的解决方案：无监督预训练对权值进行初始化+有监督训练微调。其主要思想是先通过自学习的方法学习到训练数据的结构（自动编码器），然后在该结构上进行有监督训练微调。但是由于没有特别有效的实验验证，该论文并没有引起重视。

2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。

2011年，微软首次将DL应用在语音识别上，取得了重大突破。

爆发期（2012~至今）

2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。 

2013,2014,2015年，通过ImageNet图像识别比赛，DL的网络结构，训练方法，GPU硬件的不断进步，促使其在其他领域也在不断的征服战场

2015年，Hinton，LeCun，Bengio论证了局部极值问题对于DL的影响，结果是Loss的局部极值问题对于深层网络来说影响可以忽略。该论断也消除了笼罩在神经网络上的局部极值问题的阴霾。
2015，DeepResidualNet发明。

# 二，人工智能，机器学习。深度学习有什么区别与联系？
![](https://ai-studio-static-online.cdn.bcebos.com/f54708dd567548c1a0cf9f86a9e66b86faf7c37f583d46958aae11eb259c2075)
  
五十年代，人工智能曾一度被极为看好。之后，人工智能的一些较小的子集发展了起来。先是机器学习，然后是深度学习。深度学习又是机器学习的子集。深度学习造成了前所未有的巨大的影响。
  
1956年，几个计算机科学家相聚在达特茅斯会议（Dartmouth Conferences），提出了“人工智能”的概念。
  
人工智能——为机器赋予人的智能
  
人工智能的终极目标是用计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。这就是我们现在所说的“强人工智能”（General AI）。这个无所不能的机器，它有着我们所有的感知（甚至比人更多），我们所有的理性，可以像我们一样思考。
而我们目前能实现的，一般被称为“弱人工智能”（Narrow AI）。弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。

机器学习——一种实现人工智能的方法

机器学习最基本的做法是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。
机器学习直接来源于早期的人工智能领域。传统算法包括决策树学习、推导逻辑规划、聚类、分类、回归、强化学习和贝叶斯网络等等，机器学习最成功的应用领域是计算机视觉。

深度学习——一种实现机器学习的技术

  人工神经网络（Artificial Neural Networks）是早期机器学习中的一个重要的算法，神经网络的原理是受神经元启发，但与大脑个神经元不同，人工神经网络具有离散的层、连接和数据传播的方向。神经网络通过训练更新网络参数，具有自适应的特点，理论上说，只要数据的样本量足够大，神经网络可以达到非常高的准确率。
  
# 三，神经元，单层感知机，多层感知机

## 神经元

人工神经元是对生物神经元的仿生模拟，模拟的是生物神经元组成的神经网络在接收刺激及信号传递等行为。

![](https://ai-studio-static-online.cdn.bcebos.com/bc339701393c40c485e78cdf4d8d417826ab8b56d6bf40c6a09d47374de6e028)

生物神经元按照功能可以分为：

感觉神经元（传入神经元）：其树突的末端分布于身体的外周部，接收来自体内外的刺激，将兴奋传至脊髓和脑。

运动神经元（传出神经元）：其轴突大于肌肉和腺体。

联络神经元（中间神经元）：结余上述2种神经元之间，起着神经元之间技能联系的作用，多存在于脑和脊髓里。

![](https://ai-studio-static-online.cdn.bcebos.com/76bbd79a294a4588b16c403af1ac7492dbc08e54b48e4756a05b233066c4370d)

人工神经元的输入模拟的是生物神经元的树突，用于接收外界的刺激，输入既可以是外部的，也可以是上一个神经元的输出。w代表不同刺激的刺激强度，将所有刺激求和处理就代表神经元的内部强度，传递函数用于将神经元的强度格式化为理想结果。

## 单层感知机

定义：网络接收若干输入，并通过输入函数、传输函数给出一个网络的输出，我们称之为感知机，即单一的最简单的神经网络。

过程：通过修改权重和偏置达到简单的分类效果。

缺点是无法解决非线性问题（异或问题）

## 多层感知机（多层神经网络：也称为前馈神经网络）

特点：前馈网络的各神经元接受前一级输入，并输出到下一级，无反馈。

节点：输入节点，输出节点

计算单元：可有任意一个输入，但只有一个输出，输出可耦合到任意多个其他节点输入。

层：可见层-输入和输出节点；隐层-中间层。

我们把由输入的信号源、隐层及输出组成的层叫做多层神经网络。

# 四，什么是前向传播？

前向传播通过对一层的结点以及对应的连接权值进行加权和运算，结果加上一个偏置项，然后通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，得到的结果就是下一层结点的输出。从第一层（输入层）开始不断的通过这种方法一层层的运算，最后得到输出层结果。
 ![](https://ai-studio-static-online.cdn.bcebos.com/933966a3708b4360931e9a7a6ad7bcda6988f42bc48c4e278ce08be0f580d382)
 
 # 五，什么是反向传播？
 
反向传播就是梯度下降使用reverse-mode autodiff。
前向传播，就是make predictions，计算输出误差，然后计算每个神经元节点对误差的贡献，
求贡献就是反向传播，根据前向传播的误差来求梯度，然后根据贡献调整原来的权重。



```python

```
