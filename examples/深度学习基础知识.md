# 一、深度学习发展历史

## 1.1 深度学习的起源阶段
- 1943年，心理学家麦卡洛克和数学逻辑学家皮兹发表论文《神经活动中内在思想的逻辑演算》，提出了MP模型。MP模型是模仿神经元的结构和工作原理，构成出的一个基于神经网络的数学模型，本质上是一种“模拟人类大脑”的神经元模型。MP模型作为人工神经网络的起源，开创了人工神经网络的新时代，也奠定了神经网络模型的基础。
- 1949年，加拿大著名心理学家唐纳德·赫布在《行为的组织》中提出了一种基于无监督学习的规则——海布学习规则(Hebb Rule)。海布规则模仿人类认知世界的过程建立一种“网络模型”，该网络模型针对训练集进行大量的训练并提取训练集的统计特征，然后按照样本的相似程度进行分类，把相互之间联系密切的样本分为一类，这样就把样本分成了若干类。海布学习规则与“条件反射”机理一致，为以后的神经网络学习算法奠定了基础，具有重大的历史意义。
- 20世纪50年代末，在MP模型和海布学习规则的研究基础上，美国科学家罗森布拉特发现了一种类似于人类学习过程的学习算法——感知机学习。并于1958年，正式提出了由两层神经元组成的神经网络，称之为“感知器”。感知器本质上是一种线性模型，可以对输入的训练集数据进行二分类，且能够在训练集中自动更新权值。感知器的提出吸引了大量科学家对人工神经网络研究的兴趣，对神经网络的发展具有里程碑式的意义。
- 单层感知器模型：顾名思义就是模仿人类的神经元学习，通过输入测试样本的数据，对建立的神经元模型不断修正，使得模型更加精确。
- 但随着研究的深入，在1969年，“AI之父”马文·明斯基和LOGO语言的创始人西蒙·派珀特共同编写了一本书籍《感知器》，在书中他们证明了单层感知器无法解决线性不可分问题（例如：异或问题）。由于这个致命的缺陷以及没有及时推广感知器到多层神经网络中，在20世纪70年代，人工神经网络进入了第一个寒冬期，人们对神经网络的研究也停滞了将近20年。

## 1.2 深度学习发展阶段
- 1982年，著名物理学家约翰·霍普菲尔德发明了Hopfield神经网络。Hopfield神经网络是一种结合存储系统和二元系统的循环神经网络。Hopfield网络也可以模拟人类的记忆，根据激活函数的选取不同，有连续型和离散型两种类型，分别用于优化计算和联想记忆。“The hippocampal system may be used to guide sequential decision-making by co-representing environment states with the returns achieved from the various possible actions。”但由于容易陷入局部最小值的缺陷，该算法并未在当时引起很大的轰动。
- 直到1986年，深度学习之父杰弗里·辛顿提出了一种适用于多层感知器的反向传播算法——BP算法。BP算法在传统神经网络正向传播的基础上，增加了误差的反向传播过程。反向传播过程不断地调整神经元之间的权值和阈值，直到输出的误差达到减小到允许的范围之内，或达到预先设定的训练次数为止。“Back-propagation is very popular neural network learning algorithm because it is conceptually simple, computationally efficient, and because it often works.”BP算法完美的解决了非线性分类问题，让人工神经网络再次的引起了人们广泛的关注。
- 但是由于八十年代计算机的硬件水平有限，如：运算能力跟不上，这就导致当神经网络的规模增大时，再使用BP算法会出现“梯度消失”的问题。这使得BP算法的发展受到了很大的限制。再加上90年代中期，以SVM为代表的其它浅层机器学习算法被提出，并在分类、回归问题上均取得了很好的效果，其原理又明显不同于神经网络模型，所以人工神经网络的发展再次进入了瓶颈期。


## 1.3 深度学习的爆发阶段：
- 2006年，杰弗里·辛顿以及他的学生鲁斯兰·萨拉赫丁诺夫正式提出了深度学习的概念。他们在世界顶级学术期刊《科学》发表的一篇文章中详细的给出了“梯度消失”问题的解决方案——通过无监督的学习方法逐层训练算法，再使用有监督的反向传播算法进行调优。该深度学习方法的提出，立即在学术圈引起了巨大的反响，以斯坦福大学、多伦多大学为代表的众多世界知名高校纷纷投入巨大的人力、财力进行深度学习领域的相关研究。而后又在迅速蔓延到工业界中。

## 1.4 图灵测试
把一台计算机和一个人分别放在两个房间中，房间外的一个人同时询问人和计算机问题，房间外的人无法分别哪一个是计算机，就说明计算机具有人工智能。

## 1.5 大脑
1981年，David Hubel和Torsten Wiesel，以及Roger Sperry。他们发现了人的视觉系统处理信息是分级的。
![image.png](attachment:image.png)
高层的特征是底层特征的组合，从底层到高层的特征表达越来越抽象和概念化，也即越来越能表现语义或者意图。
边缘特征–>基本形状和目标的局部特征->整个目标 这个过程其实和我们的常识是相吻合的，因为复杂的图形，往往就是由一些基本结构组合而成的。同时我们还可以看出：大脑是一个深度架构，认知过程也是深度的。
![image-2.png](attachment:image-2.png)

## 1.6 深度学习
低层次特征---组合---抽象的高层特征。深度学习，恰恰就是通过组合底层特征形成更加抽象的高层特征（或属性类别）。例如：在计算机视觉领域，深度学习算法从原始图像去学习得到一个低层次表达，例如边缘检测器，小波滤波器，然后在这些低层次表达的基础上，通过线性或者非线性组合，来获得一个高层次的表达。此外，不仅图像存在这个规律，声音也是类似的。比如，研究人员从某个声音库中通过算法自动发现20种基本的声音结构，其余的声音都可以由这20种基本结构来合成。

# 二、人工智能，机器学习 ，深度学习的区别和联系

## 2.1人工智能（Artificial Intelligence）
人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门技术科学。“人工智能”是“一门技术科学”，它研究与开发的对象是“理论、技术及应用系统”，研究的目的是为了“模拟、延伸和扩展人的智能”。人工智能在50年代就提出了。

## 2.2 机器学习   
随着人对计算机科学的期望越来越高，要求它解决的问题越来越复杂，已经远远不能满足人们的诉求了。于是有人提出了一个新的思路——能否不为难研究者，让机器自己去学习呢？机器学习就是用算法解析数据，不断学习，对世界中发生的事做出判断和预测的一项技术。研究人员不会亲手编写软件、确定特殊指令集、然后让程序完成特殊任务；相反，研究人员会用大量数据和算法“训练”机器，让机器学会如何执行任务。这里有三个重要的信息：
- 1、“机器学习”是“模拟、延伸和扩展人的智能”的一条路径，所以是人工智能的一个子集；
- 2、“机器学习”是要基于大量数据的，也就是说它的“智能”是用大量数据喂出来的；
- 3、正是因为要处理海量数据，所以大数据技术尤为重要；“机器学习”只是大数据技术上的一个应用。常用的10大机器学习算法有：决策树、随机森林、逻辑回归、SVM、朴素贝叶斯、K最近邻算法、K均值算法、Adaboost算法、神经网络、马尔科夫。

## 2.3 深度学习
相较而言，深度学习是一个比较新的概念，严格地说是2006年提出的。深度学习是用于建立、模拟人脑进行分析学习的神经网络，并模仿人脑的机制来解释数据的一种机器学习技术。它的基本特点，是试图模仿大脑的神经元之间传递，处理信息的模式。最显著的应用是计算机视觉和自然语言处理(NLP)领域。显然，“深度学习”是与机器学习中的“神经网络”是强相关，“神经网络”也是其主要的算法和手段；或者我们可以将“深度学习”称之为“改良版的神经网络”算法。深度学习又分为卷积神经网络（Convolutional neural networks，简称CNN）和深度置信网（Deep Belief Nets，简称DBN）。其主要的思想就是模拟人的神经元，每个神经元接受到信息，处理完后传递给与之相邻的所有神经元即可。

# 三、神经元、单层感知机、多层感知机

##  3.1 神经元
即神经元细胞，是神经系统最基本的结构和功能单位。分为细胞体和突起两部分。细胞体由细胞核、细胞膜、细胞质组成，具有联络和整合输入信息并传出信息的作用。突起有树突和轴突两种。树突短而分枝多，直接由细胞体扩张突出，形成树枝状，其作用是接受其他神经元轴突传来的冲动并传给细胞体。轴突长而分枝少，为粗细均匀的细长突起，常起于轴丘，其作用是接受外来刺激，再由细胞体传出。

![](https://ai-studio-static-online.cdn.bcebos.com/5b0f092a75844097912d819ddf024f5b8bbbde62f7094c1c89250e34c4081628)

## 3.2感知器

感知器是人工神经网络中的一种典型结构， 它的主要的特点是结构简单，对所能解决的问题 存在着收敛算法，并能从数学上严格证明，从而对神经网络研究起了重要的推动作用。
感知器，也可翻译为感知机，是Frank Rosenblatt在1957年就职于Cornell航空实验室(Cornell Aeronautical Laboratory)时所发明的一种人工神经网络。它可以被视为一种最简单形式的前馈式人工神经网络，是一种二元线性分类器。

Frank Rosenblatt给出了相应的感知器学习算法，常用的有感知机学习、最小二乘法和梯度下降法。譬如，感知机利用梯度下降法对损失函数进行极小化，求出可将训练数据进行线性划分的分离超平面，从而求得感知器模型。

感知器是生物神经细胞的简单抽象，神经细胞结构大致可分为：树突、突触、细胞体及轴突。单个神经细胞可被视为一种只有两种状态的机器——激动时为‘是’，而未激动时为‘否’。神经细胞的状态取决于从其它的神经细胞收到的输入信号量，及突触的强度（抑制或加强）。当信号量总和超过了某个阈值时，细胞体就会激动，产生电脉冲。电脉冲沿着轴突并通过突触传递到其它神经元。为了模拟神经细胞行为，与之对应的感知机基础概念被提出，如权量（突触）、偏置（阈值）及激活函数（细胞体）。
在人工神经网络领域中，感知器也被指为单层的人工神经网络，以区别于较复杂的多层感知器（Multilayer Perceptron）。 作为一种线性分类器，（单层）感知器可说是最简单的前向人工神经网络形式。尽管结构简单，感知器能够学习并解决相当复杂的问题。感知器主要的本质缺陷是它不能处理线性不可分问题。

感知器使用特征向量来表示的前馈式人工神经网络，它是一种二元分类器，把矩阵上的输入（实数值向量）映射到输出值上（一个二元的值）。
 W是实数的表式权重的向量， Wx是点积,b是偏置，一个不依赖于任何输入值的常数。偏置可以认为是激励函数的偏移量，或者神经元一个基础活跃等级
 f（x）(0 或 1)用于对输入数据进行分类，看它是肯定的还是否定的，这属于二元分类问题。如果是偏置负的，那么加权后的输入必须产生一个肯定的值并且大于，这样才能令分类神经元大于阈值0。从空间上看，偏置改变了决策边界的位置。
由于输入直接经过权重关系转换为输出，所以感知机可以被视为最简单形式的前馈式人工神经网络。

![](https://ai-studio-static-online.cdn.bcebos.com/d887fc2dddf2420d8191b95e7801c35ed44f3c8aede74acd85552393eced32d4)


 多层感知机（MLP，Multilayer Perceptron）也叫人工神经网络（ANN，Artificial Neural Network），除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构，如下图：
 
![](https://ai-studio-static-online.cdn.bcebos.com/21c8cde915ee4a8da866a065c6e67bf4e8a04c99b617426ea1006ae3aa0bb15c)
 

# 四、前向传播

各层神经元节点值和参数值的乘积和经过激励函数赋给下一层神经元节点的过程。
![](https://ai-studio-static-online.cdn.bcebos.com/08666e9ab28146588a02a721999f3ef57bd3204aadd04eb49e982f8d3a3afa5d)

# 五、反向传播

预测值和真实值之间的误差，基于连续偏导的规则（链式法则）从后层神经元向前层神经元递进并更改参数值的过程。
![](https://ai-studio-static-online.cdn.bcebos.com/c6b9c602eae24f108a22c064355be396162ab1f772e444f89fa8a8ebce91b434)
