**深度学习基础知识**
**1. 损失函数方法补充：**
在机器学习中，所有的机器学习算法都或多或少的依赖于对目标函数最大化或者最小化的过程，我们常常把最小化的函数称为损失函数，它主要用于衡量机器学习模型的预测能力。
**1.1 回归问题**
回归问题中有众多损失函数，而目前机器学习主流的损失函数还是均方误差函数，平均绝对误差也使用较多，在此基础上发展出了很多其他函数，Huber损失函数就是其中一种。
（1）平均绝对误差——L1损失函数
平均绝对误差（MAE）是另一种常用的回归损失函数，它是目标值与预测值之差绝对值的和，表示了预测值的平均误差幅度，而不需要考虑误差的方向，范围是0到∞，其公式如下所示：
![](https://ai-studio-static-online.cdn.bcebos.com/5890992d2a744b6889663cda2b656b9eed4715d09bab45e296b698df648cbe22)
**python**
import numpy as np
#定义L1损失函数
def L1_loss(y_true,y_pre): 
    return np.sum(np.abs(y_true-y_pre))
print('L1 loss is {}'.format(L1_loss(y_true,y_pre)))

（2）均方误差——L2损失函数
均方误差（MSE）是回归损失函数中最常用的误差，它是预测值与目标值之间差值的平方和，其公式如下所示：
![](https://ai-studio-static-online.cdn.bcebos.com/1ce4b99335114e55844d714b350303e9ae607380227744d8a62ea5983ccce3dc)
**python**
import numpy as np
def L2_loss(y_true,y_pre):
    return np.sum(np.square(y_true-y_pre))
print('L2 loss is {}'.format(L2_loss(y_true,y_pre)))

（3）Huber损失——平滑平均绝对误差
Huber损失相比于平方损失来说对于异常值不敏感，但它同样保持了可微的特性。它基于绝对误差，但在误差很小的时候变成了平方误差。我们可以使用超参数δ来调节这一误差的阈值。当δ趋向于0时它就退化成了MAE，而当δ趋向于无穷时则退化为了MSE，其表达式如下，是一个连续可微的分段函数：
![](https://ai-studio-static-online.cdn.bcebos.com/b3b532c55f424babb8bb9f9010b894d49f0327d0899d43588ecd1a53266acbf9)
**python**
def Huber(true, pred, delta):
    loss = np.where(np.abs(true-pred) < delta , 0.5*((true-pred)**2), delta*np.abs(true - pred) - 0.5*(delta**2))
    return np.sum(loss)
     
**1.2 分类问题**
分类问题相对回归问题更为具体，目标量只存在于一个有限集合，并且是离散的。分类问题往往比回归问题多出了一步，用于判断类别。回归问题的损失函数就是性能度量函数，而分类问题的损失函数不能直接用于性能度量，其最终评估的标准不是离目标的距离，而是类别判断的准确率。为了最大地提升类别判断准确率，我们需要为分类问题定义不同的损失函数。
（1）0-1损失函数
以二分类问题为例，错误率=1-正确率，也就是0-1损失函数，可以定义为：
![](https://ai-studio-static-online.cdn.bcebos.com/b05c2495c5bd4563808b27f8985e7d3a55a7084f7f804dabbb61df2b7042de33)

（2）交叉熵损失函数（Logistic回归）
Logistic回归主要用于二分类问题，包含激活函数和损失函数两部分。激活函数是logistic函数（又叫sigmoid函数），损失函数是交叉熵函数。
logistic函数作用在分类算法的最后，它使得对任意的输入值，对应的输出都在区间(0,1)内，可以将输入从实数域映射到(0,1)区间，刚好满足概率的范围。函数公式为：
![](https://ai-studio-static-online.cdn.bcebos.com/f9773b379dad4a609d377b286144ab47141fcdb689c7429098f8f0eceb162b71)
结合交叉熵函数，通过推导可以得到logistic的损失函数表达式：
![](https://ai-studio-static-online.cdn.bcebos.com/c32445d264db49f385858d79e6807566f64db6415d2e44c48e8465a3a8bc0f69)
**python**
import numpy as np
def cross_entropy(a, y):
    return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))
 
（3）交叉熵损失函数（Softmax激活）
交叉熵用到了log函数的特性，它不改变凸函数的凹凸性，同时能简化计算，是常用的技巧。这里主要讲多分类问题，激活函数是softmax，作用在神经网络的最后一层，损失函数是对数似然函数，也可以看成是交叉熵损失函数。
softmax激活函数表达式为：
![](https://ai-studio-static-online.cdn.bcebos.com/9757ecc51dde4a8d927cf4e6672a2ec66f9b8d41e21d4b3eb82e5b8e0a34e3f1)
当类别数C=2时，softmax函数就等于sigmoid函数，可见两者只是分量个数的差异。激活后的是长度为C的向量，每个分量都在（0，1）区间内，并且和为1，而最大分量所对应的类别就是我们所预测的结果。
**python**
def softmax(x):
    shift_x = x - np.max(x)    # 防止输入增大时输出为nan
    exp_x = np.exp(shift_x)
    return exp_x / np.sum(exp_x)
    
**2. 池化方法补充： **

   池化（Pooling）是卷积神经网络中的一个重要的概念，它实际上是一种形式的降采样。在图像处理中，由于图像中存在较多冗余信息，可用某一区域子块的统计信息（如最大值或均值等）来刻画该区域中所有像素点呈现的空间分布模式，以替代区域子块中所有像素点取值，这就是卷积神经网络中池化(pooling)操作。
（1）一般池化（General Pooling）
池化作用于图像中不重合的区域（这与卷积操作不同），过程如下图。
![](https://ai-studio-static-online.cdn.bcebos.com/1c61737a647943348d40c69869933753ac60176efe2045e9a22417a3f61af493)
我们定义池化窗口的大小为sizeX，即下图中红色正方形的边长，定义两个相邻池化窗口的水平位移/竖直位移为stride。一般池化由于每一池化窗口都是不重复的，所以sizeX=stride。

最常见的池化操作为平均池化mean pooling和最大池化max pooling：
平均池化：计算图像区域的平均值作为该区域池化后的值。
最大池化：选图像区域的最大值作为该区域池化后的值。

（2） 重叠池化（OverlappingPooling）
重叠池化，即相邻池化窗口之间会有重叠区域。如果定义池化窗口的大小为sizeX，定义两个相邻池化窗口的水平位移 / 竖直位移为stride，此时sizeX>stride。

（3）空金字塔池化（Spatial Pyramid Pooling）
空间金字塔池化可以把任何尺度的图像的卷积特征转化成相同维度，这不仅可以让CNN处理任意尺度的图像，还能避免cropping和warping操作，导致一些信息的丢失，具有非常重要的意义。一般的CNN都需要输入图像的大小是固定的，这是因为全连接层的输入需要固定输入维度，但在卷积操作是没有对图像尺度有限制，所有作者提出了空间金字塔池化，先让图像进行卷积操作，然后转化成维度相同的特征输入到全连接层，这个可以把CNN扩展到任意大小的图像。
![](https://ai-studio-static-online.cdn.bcebos.com/4271e7cd6775453a86a326deec24ab41fb6aad0d6cb94a2f997db93dde869ac3)
SPP其实就是一种多个scale的pooling，可以获取图像中的多尺度信息；在CNN中加入SPP后，可以让CNN处理任意大小的输入，这让模型变得更加的flexible。

（4）全局池化(Global Pooling)
Global Pooling就是池化窗口的大小 = 整张特征图的大小。这样，每个 W×H×C 的特征图输入就会被转化为 1×1×C 的输出，也等同于每个位置权重都为 1/(W×H) 的全连接层操作。

（5）随机池化(Stochastic Pooling)
随机池化是一种简单有效的正则化CNN的方法，能够降低max pooling的过拟合现象，提高泛化能力。对于pooling层的输入，根据输入的多项式分布随机选择一个值作为输出。
随机池化可以看作在一个池化窗口内对特征图数值进行归一化， 按照特征图归一化后的概率值大小随机采样选择，即元素值大的被选中的概率也大。

**3. 数据增强方法修改及补充：**
数据增强也叫数据扩增，意思是在不实质性的增加数据的情况下，让有限的数据产生等价于更多数据的价值。数据增强可以分为，有监督的数据增强和无监督的数据增强方法。其中有监督的数据增强又可以分为单样本数据增强和多样本数据增强方法，无监督的数据增强分为生成新的数据和学习增强策略两个方向。
**3.1 有监督的数据增强**
有监督数据增强，即采用预设的数据变换规则，在已有数据的基础上进行数据的扩增，包含单样本数据增强和多样本数据增强，其中单样本又包括几何操作类，颜色变换类。
**（1）单样本数据增强**
所谓单样本数据增强，即增强一个样本的时候，全部围绕着该样本本身进行操作，包括几何变换类，颜色变换类等。
几何变换类：对图像进行几何变换，包括翻转，旋转，裁剪，变形，缩放等各类操作
颜色变换类：对图像内容进行改变，常见的包括噪声、模糊、颜色变换、擦除、填充等。
**（2）多样本数据增强**
不同于单样本数据增强，多样本数据增强方法利用多个样本来产生新的样本，下面介绍几种方法。
**SMOTE**
SMOTE即Synthetic Minority Over-sampling Technique方法，它是通过人工合成新样本来处理样本不平衡问题，从而提升分类器性能。
SMOTE方法是基于插值的方法，它可以为小样本类合成新的样本，主要流程为：
第一步，定义好特征空间，将每个样本对应到特征空间中的某一点，根据样本不平衡比例确定好一个采样倍率N；
第二步，对每一个小样本类样本(x,y)，按欧氏距离找出K个最近邻样本，从中随机选取一个样本点，假设选择的近邻点为(xn,yn)。在特征空间中样本点与最近邻样本点的连线段上随机选取一点作为新样本点，满足以下公式：
![](https://ai-studio-static-online.cdn.bcebos.com/491656c1ac1e425eb32a7a2b3fa1884bafec94eb71304903a73e54f9e481ad4b)
第三步，重复以上的步骤，直到大、小样本数量平衡。
**SamplePairing**
SamplePairing方法是从训练集中随机抽取两张图片分别经过基础数据增强操作(如随机翻转等)处理后经像素以取平均值的形式叠加合成一个新的样本，标签为原样本标签中的一种。这两张图片甚至不限制为同一类别，这种方法对于医学图像比较有效。
![](https://ai-studio-static-online.cdn.bcebos.com/0d789c7ffe9d4498a4b15ae4d97857e969cfb16dbabb4b6f8d4a0004ec3b3c89)
**mixup**
mixup是Facebook人工智能研究院和MIT在“Beyond Empirical Risk Minimization”中提出的基于邻域风险最小化原则的数据增强方法，它使用线性插值得到新样本数据。令(xn,yn)是插值生成的新数据，(xi,yi)和(xj,yj)是训练集随机选取的两个数据，则数据生成方式如下：
![](https://ai-studio-static-online.cdn.bcebos.com/c63c204f309b4d32a7a2c9531867f0217a21325b8eba40e1802d939e3406c1b2)

**3.2 无监督的数据增强**
无监督的数据增强方法包括两类：
（1）通过模型学习数据的分布，随机生成与训练数据集分布一致的图片，代表方法GAN。
**GAN**
GAN包含两个网络，一个是生成网络，一个是对抗网络，基本原理如下：
![](https://ai-studio-static-online.cdn.bcebos.com/ae9aa492bed842b1a4aa924e232c75a04284d39025fb4738aad41be9422e15db)
G是一个生成图片的网络，它接收随机的噪声z，通过噪声生成图片，记做G(z)；
D是一个判别网络，判别一张图片是不是“真实的”，即是真实的图片，还是由G生成的图片。

（2）通过模型，学习出适合当前任务的数据增强方法，代表方法AutoAugment。
**Autoaugmentation**
AutoAugment是Google提出的自动选择最优数据增强方案的研究，这是无监督数据增强的重要研究方向。它的基本思路是使用增强学习从数据本身寻找最佳图像变换策略，对于不同的任务学习不同的增强方法，流程如下：
(1) 准备16个常用的数据增强操作。
(2) 从16个中选择5个操作，随机产生使用该操作的概率和相应的幅度，将其称为一个sub-policy，一共产生5个sub-polices。
(3) 对训练过程中每一个batch的图片，随机采用5个sub-polices操作中的一种。
(4) 通过模型在验证集上的泛化能力来反馈，使用的优化方法是增强学习方法。
(5) 经过80~100个epoch后网络开始学习到有效的sub-policies。
(6) 之后串接这5个sub-policies，然后再进行最后的训练。

**4. 图像分类方法综述**
![](https://ai-studio-static-online.cdn.bcebos.com/d8113e86b9874104afba5fb3ecdfc34bd266a12415ea42b0b981e2ea58223e17)
图像分类是根据图像的语义信息将不同类别图像区分开来，是计算机视觉中重要的基本问题，也是图像检测、图像分割、物体跟踪、行为分析等其他高层视觉任务的基础。图像分类的主要过程包括图像预处理、特征提取和分类器设计。
（1）传统方法：
传统图像分类通过手工提取特征或特征学习方法对整个图像进行全部描述，然后使用分类器判别物体类别，因此如何提取图像的特征至关重要。
![](https://ai-studio-static-online.cdn.bcebos.com/979b8bd4f3734f1c998722ba642b8c0a5098777b55814e269ab01c413ddfac45)
在特征提取方面，主要包括纹理、颜色、形状等底层视觉特征，尺度不变特征变换、局部二值模式、方向梯度直方图等局部不变性特征。
在分类器方面，主要包括k NN(k-nearest neighbor,k最近邻）决策树、SVM(support vector machine，支持向量机）、人工神经网络等方法。
（2）深度学习方法：
传统的图像分类方法往往需要对目标图像进行人工特征描述和提取，对于大数量的复杂数据很难取得低成本的有效结果。然而，深度学习方法通过神经网络自主地从训练样本中学习特征，提取出更高维、抽象的特征，并且这些特征与分类器关系紧密，很好地解决了人工提取特征和分类器选择的难题，是一种端到端的模型。

对于深度学习标准网络模型使用已经非常广泛，在这里对轻量化进行简单介绍：
常用的标准网络模型：Lenet、Alxnet、Vgg系列、Resnet系列、Inception系列、Densenet系列、Googlenet、Nasnet、Xception、Senet(state of art)；
轻量化网络模型：Mobilenet v1,v2、Shufflenet v1,v2,Squeezenet
目前轻量化模型在具体项目应用时反响很好，它主要存在的优缺点如下：
优点：（1）参数模型小，方便部署（2）计算量小，速度快
缺点：（1）轻量化模型在精度上没有Resnet系列、Inception系列、Densenet系列、Senet的accuracy高

