# 深度学习基础知识

## 1 深度学习发展历史

深度学习(Deep Learning, DL)由Hinton等人于2006年提出，是机器学习(Machine Learning, ML)的一个新领域。
深度学习被引入机器学习使其更接近于最初的目标----人工智能（AI，Artificial Intelligence）。深度学习是学习样本数据的内在规律和表示层次，这些学习过程中获得的信息对诸如文字、图像和声音等数据的解释有很大的帮助。它的最终目标是让机器能够像人一样具有分析学习能力，能够识别文字、图像和声音等数据。
深度学习是一个复杂的机器学习算法，在语言和图像识别方面取得的效果，远远超过先前相关技术。它在搜索技术、数据挖掘、机器学习、机器翻译、自然语言处理、多媒体学习、语音、推荐和个性化技术，以及其它相关领域都取得了很多成果。深度学习使机器模仿视听和思考等人类的活动，解决了很多复杂的模式识别难题，使得人工智能相关技术取得了很大进步。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://tse1-mm.cn.bing.net/th/id/R-C.369ee28478c61b514fdb8f5a8ea2aace?rik=hXU8fnn4ET%2bUiQ&riu=http%3a%2f%2fupload-images.jianshu.io%2fupload_images%2f5756726-4fede5addee4e688&ehk=P%2bnYptS9MrwgWhWr2HqYXIraY2Vb5VYrD6jMV4rqdSQ%3d&risl=&pid=ImgRaw"> 
</center>


### 2.1第一代神经网络（1958~1969）
最早的神经网络的思想起源于1943年的MCP人工神经元模型，当时是希望能够用计算机来模拟人的神经元反应的过程，该模型将神经元简化为了三个过程：输入信号线性加权，求和，非线性激活（阈值法）。如下图所示
 <center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://pic3.zhimg.com/80/v2-dc4bc1a7abd9f733569acc93b086a2aa_720w.jpg"> 
</center>
第一次将MCP用于机器学习（分类）的当属1958年Rosenblatt发明的感知器（perceptron）算法。该算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年，该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。1969年，美国数学家及人工智能先驱Minsky在其著作中证明了感知器本质上是一种线性模型，只能处理线性分类问题，就连最简单的XOR（异或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了近20年的停滞。

### 2.2 第二代神经网络（1986~1998）
1986年由神经网络之父 Geoffrey Hinton 在1986年发明了适用于多层感知器（MLP）的反向传播BP（Backpropagation）算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://pic3.zhimg.com/80/v2-fc47c93f6d2402224b636ee2582bfe4e_720w.jpg"> 
</center>
1989年，Robert Hecht-Nielsen证明了MLP的万能逼近定理，即对于任何闭区间内的一个连续函数f，都可以用含有一个隐含层的BP网络来逼近该定理的发现极大的鼓舞了神经网络的研究人员。
然而，在1991年，BP算法被指出存在梯度消失问题，即在误差梯度后向传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数 的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该发现对此时的NN发展雪上加霜。
与此同时，基于统计思想的机器学习方法渐渐获得了业界的主流支持，决策树、SVM、随机森林等算法纷纷诞生，在数据分类和回归问题上取得了良好的效果，成为了机器学习的主流算法。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://pic4.zhimg.com/80/v2-1b47276317a82cc7a138757b19ed68d7_720w.jpg"> 
</center>
①Sigmoid函数是一个在生物学中常见的S型的函数，也称为S型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间。 

### 2.3第三代神经网络（2006-至今）
直到 2006 年，Hinton 教授解决了BP神经网络算法梯度消失的问题，深度学习的思想再次回到了大众的视野之中，也正因为如此，2006 年被称为是深度学习发展的元年。
本阶段又可以大概分为两个阶段 ，第一个阶段为快速发展阶段，第二个阶段为爆发阶段。
#### 快速发展阶段 
正如前文所说，Hinton教授提出了解决梯度消失的方案 ，首先通过无监督方法对神经网络进行初始化，然后使用有标记的数据进行有监督训练学习，进而对网络参数进行微调。2011年，微软公司首次将深度学习方法应用在语音识别领域中，取得了较好的效果。
#### 爆发阶段 
2012年，Hinton教授带领团队参加ImageNet 图像识别比赛。在比赛中，Hinton团队所使用的深度学习算法一举夺魁，其性能达到了碾压第二名 SVM 算法的效果 ，自此深度学习的算法思想受到了业界研究者的广泛关注。深度学习的算法也渐渐在许多领域代替了传统的统计学机器学习方法 ，成为人工智能中最热门的研究领域。



## 2 人工智能、机器学习、深度学习有什么区别和联系

“机器学习”、“人工智能”、“深度学习”这三个词常常被人混淆，但其实它们出现的时间相隔甚远，“人工智能”（Artificial Intelligence，AI）出现于20世纪50年代，“机器学习”（Machine Learning，ML）出现于20世纪80年代，而“深度学习”（Deep Learning，DL）则是近些年才出现的。

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://mmbiz.qpic.cn/mmbiz_png/LSOjyib5giaVftTxJr31s0f6L6icTP4OGrtIuicessfG7e7f62wjDhoia3TYXbEF3FyLdH300wPIQaAv6gaUxzZ6UGg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1"> 
</center>

如上图，人工智能是最早出现的，也是最大、最外侧的同心圆；其次是机器学习，稍晚一点；最内侧，是深度学习，当今人工智能大爆炸的核心驱动。

### **2.1 机器学习与人工智能**

“人工智能”一词出现在1956年的达特茅斯会议上，当时人工智能先驱的梦想是建造具有人类智能体的软硬件系统，该系统具有人类的智能特征，而这里所说的人工智能为“通用人工智能”。

这样的人工智能梦想曾在影视作品中大放异彩，如电影《星球大战》中的C-3PO机器人具有人类的理性和思考能力。不过，迄今为止，这种高层次的推理和思想仍然难以实现，退而求其次，目前能够落地的都属于“狭义的人工智能”，如人脸识别等。

我们将机器学习描述为实现人工智能的一种方式方法。机器学习是基于已有数据、知识或经验自动识别有意义的模式。最基本的机器学习使用算法解析和学习数据，然后在相似的环境里做出决定或预测。简言之，即基于数据学习并做决策。这样的描述将机器学习与传统软件或普通程序区分开来。

机器学习过程中，并没有人为指示机器学习系统如何对未知环境做出决策或预测，这一过程由机器学习中的算法从数据中习得，做出决策的主体是机器学习算法，并且决策或预测是非确定性的结果，一般以概率的形式输出，比如80%的可能性是晴天。

与之不同的是，常规的应用程序需要软件工程师一句句地编写代码（特定的指令集），指示程序或软件做出确定的行为，比如输出0和1分别表示注册成功和失败。做出决策的主体实际是人，程序只是执行动作的工具。正因如此，机器学习可归为间接编程，与之对应的是常规编程。

### **2.2 机器学习与深度学习**

深度学习使用多层（一般多于5层）人工神经网络学习数据内部的复杂关系。人工神经网络是生物科学、认知科学等与人工智能结合的产物，在早期的机器学习中就已开始应用，其初衷是在计算机中模拟人类大脑神经元的工作模式。

人类大脑的神经元在百亿级别，通过突触实现彼此交流，从计算的角度看属于计算密集型，这限制了复杂人工神经网络在实践中的应用。计算机计算能力的大幅提升带来了新的可能，2000年，多伦多大学的Geoffrey Hinton领导的研究小组在不懈研究下，终于在现代超级计算机中验证了深度学习的多层网络结构。

Geoffrey Hinton因在深度学习领域做出巨大贡献而被称为深度学习的鼻祖，并与Yoshua Bengio、Yann LeCun并称机器学习三巨头。（三人因在深度学习领域的贡献而荣获2018年图灵奖。

深度学习可被看作一种实现机器学习的技术，是机器学习的子集。与深度学习相对，过去那些只有单层或少层的神经网络被称为浅层学习。



## 3 神经元、单层感知机、多层感知机

### 3.1 神经元

神经元即神经元细胞，是神经系统最基本的结构和功能单位。分为细胞体和突起两部分。细胞体由细胞核、细胞膜、细胞质组成，具有联络和整合输入信息并传出信息的作用。突起有树突和轴突两种。树突短而分枝多，直接由细胞体扩张突出，形成树枝状，其作用是接受其他神经元轴突传来的冲动并传给细胞体。轴突长而分枝少，为粗细均匀的细长突起，常起于轴丘，其作用是接受外来刺激，再由细胞体传出。轴突除分出侧枝外，其末端形成树枝样的神经末梢。末梢分布于某些组织器官内，形成各种神经末梢装置。感觉神经末梢形成各种感受器；运动神经末梢分布于骨骼肌肉，形成运动终极。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src="https://tse2-mm.cn.bing.net/th/id/OIP-C.yrNYAqu1eQSG04oNXWOJigHaCp?w=293&h=125&c=7&o=5&dpr=1.25&pid=1.7"> 
</center>


### 3.2 单层感知机

单层感知器由一个线性组合器和一个二值阈值元件组成。

![img](https:////upload-images.jianshu.io/upload_images/11345863-4feca33b55dd2350.png?imageMogr2/auto-orient/strip|imageView2/2/w/397/format/webp)

输入向量为x，权重向量为w，w0为偏执。

简单的理解可以解释为：将x0,x1······xn的变量输入，经过组合器的整合，输出1或者-1，也就是通过组合器对输入变量判断其正确与否。

而这个判断的依据就是权重w0,w1······wn。

因为线性组合器是实现加法的方式，根据向量的运算法则，所以以上公式的输入值可以理解为：
 w0+x1w1+······+xnwn

![img](https:////upload-images.jianshu.io/upload_images/11345863-e550f7988665cf3c.png?imageMogr2/auto-orient/strip|imageView2/2/w/95/format/webp)

单个数据的输入判断就是这样，下面我们将它扩展到多个数据，如下图所示：



![img](https:////upload-images.jianshu.io/upload_images/11345863-a303ea43fb9631d9.png?imageMogr2/auto-orient/strip|imageView2/2/w/580/format/webp)

### 3.3 多层感知机

 多层感知机（MLP，Multilayer Perceptron）也叫人工神经网络（ANN，Artificial Neural Network），除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构，如下图：

​                    ![img](https://img-blog.csdnimg.cn/20190623203530221.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZnMTM4MjEyNjc4MzY=,size_16,color_FFFFFF,t_70)

​      从上图可以看到，多层感知机层与层之间是全连接的。多层感知机最底层是输入层，中间是隐藏层，最后是输出层。 

隐藏层的神经元怎么得来？首先它与输入层是全连接的，假设输入层用向量X表示，则隐藏层的输出就是 f (W1X+b1)，W1是权重（也叫连接系数），b1是偏置，函数f 可以是常用的sigmoid函数或者tanh函数：

## 4 前向传播

![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/07e9efff73ad4e8cf528305497236cdf.png)

如图所示，这里讲得已经很清楚了，前向传播的思想比较简单。
举个例子，假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。
最终不断的通过这种方法一层层的运算，得到输出层结果。

 
## 5 反向传播

BackPropagation算法是多层神经网络的训练中举足轻重的算法。简单的理解，它的确就是复合函数的链式法则，但其在实际运算中的意义比链式法则要大的多。要回答题主这个问题“如何直观的解释back propagation算法？” 需要先直观理解多层神经网络的训练。

机器学习可以看做是数理统计的一个应用，在数理统计中一个常见的任务就是拟合，也就是给定一些样本点，用合适的曲线揭示这些样本点随着自变量的变化关系.

深度学习同样也是为了这个目的，只不过此时，样本点不再限定为(x, y)点对，而可以是由向量、矩阵等等组成的广义点对(X,Y)。而此时，(X,Y)之间的关系也变得十分复杂，不太可能用一个简单函数表示。然而，人们发现可以用多层神经网络来表示这样的关系，而多层神经网络的本质就是一个多层复合的函数。借用网上找到的一幅图[1]，来直观描绘一下这种复合关系。

![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/ef43e6a4e39749579a50bbb04bd0f867.png)

其对应的表达式如下：
![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/07d8c80d5512aa9fd6c3a0f18e94e91d.png)

上面式中的Wij就是相邻两层神经元之间的权值，它们就是深度学习需要学习的参数，也就相当于直线拟合y=k*x+b中的待求参数k和b。

和直线拟合一样，深度学习的训练也有一个目标函数，这个目标函数定义了什么样的参数才算一组“好参数”，不过在机器学习中，一般是采用成本函数（cost function），然后，训练目标就是通过调整每一个权值Wij来使得cost达到最小。cost函数也可以看成是由所有待求权值Wij为自变量的复合函数，而且基本上是非凸的，即含有许多局部最小值。但实际中发现，采用我们常用的梯度下降法就可以有效的求解最小化cost函数的问题。