分层Softmax
Hierarchical softmax （H-Softmax）是由Morin和Bengio受到二叉树的启发而提出。H-Softmax本质上是用层级关系替代了扁平化的softmax层，如图1所示，每个叶子节点表示一个词语。于是，计算单个词语概率值的计算过程被拆解为一系列的概率计算，这样可以避免对所有词语进行标准化计算。用H-Softmax替换softmax层之后，词语的预测速度可以提升至少50倍，速度的提升对于低延时要求的实时系统至关重要，比如谷歌新推出的消息应用Allo。


我们可以把原来的softmax看做深度为1的树，词表V中的每一个词语表示一个叶子节点。计算一个词语的softmax概率需要对|V|个节点的概率值做标准化。如果把softmax改为二叉树结构，每个word表示叶子节点，那么只需要沿着通向该词语的叶子节点的路径搜索，而不需要考虑其它的节点。
平衡二叉树的深度是log2(|V|)，因此，最多只需要计算log2(|V|)个节点就能得到目标词语的概率值。注意，得到的概率值已经经过了标准化，因为二叉树所有叶子节点组成一个概率分布，所有叶子节点的概率值总和等于1。我们可以简单地验证一下，在图1的根节点（Node o）处，两个分枝的概率和必须为1。之后的每个节点，它的两个子节点的概率值之和等于节点本身的概率值。因为整条搜索路径没有概率值的损失，所以最底层所有叶子节点的概率值之和必定等于1，hierarchical softmax定义了词表V中所有词语的标准化概率分布。

具体说来，当遍历树的时候，我们需要能够计算左侧分枝或是右侧分枝的概率值。为此，给每个节点分配一个向量表示。与常规的softmax做法不同，这里不是给每个输出词语w生成词向量v’w，而是给每个节点n计算一个向量v’n。总共有|V|-1个节点，每个节点都有自己独一无二的向量表示，H-Softmax方法用到的参数与常规的softmax几乎一样。于是，在给定上下文c时，就能够计算节点n左右两个分枝的概率：![](https://ai-studio-static-online.cdn.bcebos.com/8ccb2613b36a45a38007c7147b5cc331544e01a4ddf14e0490f1f8c2c193b617)

上式与常规的softmax大致相同。现在需要计算h与树的每个节点的向量v’n的内积，而不是与每个输出词语的向量计算。而且，现在只需要计算一个概率值，这里就是偏向n节点右枝的概率值。相反的，偏向左枝的概率值是1−p(right|n,c)

假设已知出现了词语“the”、“dog”、“and”、“the”，则出现词语“cat”的概率值就是在节点1向左偏的概率值、在节点2向右偏的概率以及在节点5向右偏的概率值的乘积。Hugo Lachorelle在他的视频教程中给了更详细的解释。Rong的文章也详细地解释了这些概念，并推导了H-Softmax。

显然，树形的结构非常重要。若我们让模型在各个节点的预测更方便，比如路径相近的节点概率值也相近，那么凭直觉系统的性能肯定还会提升。沿着这个思路，Morin和Bengio使用WordNet的同义词集作为树簇。然而性能依旧不如常规的softmax方法。Mnih和Hinton[8]将聚类算法融入到树形结构的学习过程，递归地将词集分为两个集合，效果终于和softmax方法持平，计算量有所减小。

值得注意的是，此方法只是加速了训练过程，因为我们可以提前知道将要预测的词语（以及其搜索路径）。在测试过程中，被预测词语是未知的，仍然无法避免计算所有词语的概率值。

在实践中，一般不用“左节点”和“右节点”，而是给每个节点赋一个索引向量，这个向量表示该节点的搜索路径。如图2所示，如果约定该位为0表示向左搜索，该位为1表示向右搜索，那词语“cat”的向量就是011。

上文中提到平衡二叉树的深度不超过log2(|V|)。若词表的大小是|V|=10000，那么搜索路径的平均长度就是13.3。因此，词表中的每个词语都能表示为一个平均长度为13.3比特的向量，即信息量为13.3比特。

关于信息量
在信息论中，人们习惯于将词语w概率值的负对数定义为信息量I(w)：

I(w)=−log2p(w)

而熵H则是词表中所有词语的信息量的期望值：

H=∑i∈Vp(wi)I(wi)

熵也代表着根据信息的概率分布对信息编码所需要的最短平均编码长度。 抛硬币事件需要用1比特来编码正反两个时间，对于永恒不变的事件则只需0比特。若用平衡二叉树的节点来表示词表中的词语，还是假设词表的大小|V|=10000，词表中词语的概率值均相等，那么熵H与平均搜索路径的长度恰好相等：![](https://ai-studio-static-online.cdn.bcebos.com/7747887cf7dd4b10ad893e053c55e729caad8e9ee2f649d6adc997cdf8a6894d)




之前我们一再强调了树结构的重要性，因为利用好树结构不仅能提升系统的性能，还能加快运算速度。若我们给树加入额外的信息，就能缩短某些携带信息量少的词语的搜索路径。Morin和Bengio就是利用了词表中各个词语出现概率不相等这一信息。他们认为词表中的一些词语出现的概率总是大于其它词语，那这些词语就应该用更短的向量编码。他们所用的文档集（|V|=10000）的熵大约是9.16。

于是，考虑词频之后，文档集中每个词语的平均编码长度从13.3比特减为9.16比特，运算速度也提升了31%。Mikolov等人在他们关于hierarchical softmax的论文[1]里就用到了霍夫曼树，即词频越高的词语编码长度越短。比如，“the”是英语中最常见的词语，那“the”在霍夫曼树中的编码长度最短，词频第二高的词语编码长度仅次于“the”，以此类推。整篇文档的平均编码长度因此降低。

霍夫曼编码通常也被称作熵编码，因为每个词语的编码长度与它的熵几乎成正比。香农通过实验[5]得出英语字母的信息量通常在0.6~1.3之间。假设单词的平均长度是4.5个字母，那么所携带的信息量就是2.7~5.85比特。

再回到语言模型：衡量语言模型好坏的指标perplexity是2H，H表示熵。熵为9.16的unigram模型的perplexity达到29.16=572.0。我们可以直观的理解为，平均情况下用该unigram模型预测下一个词语时，有572个词语是等可能的候选词语。目前，Jozefowicz在2016年的论文中提到最好的模型perplexity=24.2。因为24.6=24.2，所以这个模型平均只需要4.6比特来表示一个词语，已经非常接近香农记录的实验下限值了。这个模型是否能用于改进网络的hierarchical softmax层，仍需要人们进一步探索。





请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>
Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. 
