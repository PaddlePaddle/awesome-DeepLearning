# 层归一化
#### 概念
Layer Normalizaiton
Layer Normalization的思想与Batch Normalization非常类似，只是Batch Normalization是在每个神经元对一个mini batch大小的样本进行规范化，而Layer Normalization则是在每一层对单个样本的所有神经元节点进行规范化。

 BN、LN、IN和GN这四个归一化的计算流程几乎是一样的，只不过彼此对Sk的定义不同。∣Sk​∣表示的是集合中元素的个数。可以分为四步：

      1.计算出均值

      2.计算出方差

      3.归一化处理到均值为0，方差为1

      4.变化重构，恢复出这一层网络所要学到的分布
   1.Sk就是N∗H∗C内的所有点。LN的计算就是把每个CHW单独拿出来归一化处理，不受batchsize 的影响，也就是说每个sample是独立的，我们对表示每个sample的feature map进行normalization。

 2.常用在RNN网络，但如果输入的特征区别很大，那么就不建议使用它做归一化处理


#### 算法流程
![Alt text](./1626415144753.png)

#### 作用
1.c在batch_size较大时性能  时比不过batch normalization的。

2.layer Normalization可以提高模型的训练速度和精度，使得模型更加稳健。

3.LN是和BN非常近似的一种归一化方法，不同的是BN取的是不同样本的同一个特征，而LN取的是同一个样本的不同特征。在BN和LN都能使用的场景中，BN的效果一般优于LN，原因是基于不同数据，同一特征得到的归一化特征更不容易损失信息。
#### 应用场景
在实际应用中，如果batch size过小或者使用了RNN等动态网络，可以使用Layer Norm进行规范化。其他的场合还是建议优先使用Batch Norm，毕竟实验证明Batch Norm的效果还是优于Layer Norm。