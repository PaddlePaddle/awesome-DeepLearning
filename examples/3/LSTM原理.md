# RNN和LSTM网络的设计思考

RNN和LSTM的设计初衷是部分场景神经网络需要有“记忆”能力才能解决的任务。在自然语言处理任务中，往往一段文字中某个词的语义可能与前一段句子的语义相关，只有记住了上下文的神经网络才能很好的处理句子的语义关系。

如何设计神经网络的记忆功能呢？我们先了解下RNN网络是如何实现具备记忆功能的。RNN相当于将神经网络单元进行了横向连接，处理前一部分输入的RNN单元不仅有正常的模型输出，还会输出“记忆”传递到下一个RNN单元。而处于后一部分的RNN单元，不仅仅有来自于任务数据的输入，同时会接收从前一个RNN单元传递过来的记忆输入，这样就使得整个神经网络具备了“记忆”能力。

但是RNN网络只是初步实现了“记忆”功能，在此基础上科学家们又发明了一些RNN的变体，来加强网络的记忆能力。但RNN对“记忆”能力的设计是比较粗糙的，当网络处理的序列数据过长时，累积的内部信息就会越来越复杂，直到超过网络的承载能力，通俗的说“事无巨细的记录，总有一天大脑会崩溃”。为了解决这个问题，科学家巧妙的设计了一种记忆单元，称之为“长短时记忆网络（Long Short-Term Memory，LSTM）”。在每个处理单元内部，加入了输入门、输出门和遗忘门的设计，三者有明确的任务分工：

输入门：控制有多少输入信号会被融合；
遗忘门：控制有多少过去的记忆会被遗忘；
输出门：控制多少处理后的信息会被输出；
三者的作用与人类的记忆方式有异曲同工之处

# RNN网络结构
RNN是一个非常经典的面向序列的模型，可以对自然语言句子或是其它时序信号进行建模，网络结构如 图1所示。![](https://ai-studio-static-online.cdn.bcebos.com/acbcb428a2e34457b3168ccf7d58938e61817e1f0fb8404897bff5bc0f54c06f)图1.RNN网络结构


  不同于其他常见的神经网络结构，循环神经网络的输入是一个序列信息。假设给定任意一句话[x0,x1,...,xn]其中每个xi都代表了一个词

  循环神经网络从左到右逐词阅读这个句子，并不断调用一个相同的RNN Cell来处理时序信息。每阅读一个单词，循环神经网络会先将本次输入的单词通过embedding lookup转换为一个向量表示。再把这个单词的向量表示和这个模型内部记忆的向量hhh融合起来，形成一个更新的记忆。最后将这个融合后的表示输出出来，作为它当前阅读到的所有内容的语义表示。当循环神经网络阅读过整个句子之后，我们就可以认为它的最后一个输出状态表示了整个句子的语义信息。

循环神经网络开始从左到右阅读这个句子，在未经过任何阅读之前，循环神经网络中的记忆向量是空白的。其处理逻辑如下：

1. 网络阅读单词“我”，并把单词“我”的向量表示和空白记忆相融合，输出一个向量h1用于表示“空白+我”的语义。

1. 网络开始阅读单词“爱”，这时循环神经网络内部存在“空白+我”的记忆。循环神经网络会将“空白+我”和“爱”的向量表示相融合，并输出“空白+我+爱”的向量表示h2用于表示“我爱”这个短语的语义信息。

1. 网络开始阅读单词“人工”，同样经过融合之后，输出“空白+我+爱+人工”的向量表示h3用于表示“空白+我+爱+人工”语义信息。

1. 最终在网络阅读了“智能”单词后，便可以输出“我爱人工智能”这一句子的整体语义信息。

1. 说明：在实现当前输入xt和已有记忆ht−1融合的时候，循环神经网络采用相加并通过一个激活函数tanh的方式实现：ht=tanh(WXt+VHt−1+b)
1. tanh函数是一个值域为（-1,1）的函数，其作用是长期维持内部记忆在一个固定的数值范围内，防止因多次迭代更新导致数值爆炸。同时tanh的导数是一个平滑的函数，会让神经网络的训练变得更加简单。

# LSTM网络结构

1. 上述方法听上去很有效（事实上在有些任务上效果还不错），但是存在一个明显的缺陷，就是当阅读很长的序列时，网络内部的信息会变得越来越复杂，甚至会超过网络的记忆能力，使得最终的输出信息变得混乱无用。
1. 长短时记忆网络（Long Short-Term Memory，LSTM）内部的复杂结构正是为处理这类问题而设计的，其网络结构如 图2 所示。![](https://ai-studio-static-online.cdn.bcebos.com/3b0bb7bf7207435a972bde076a4b63c5510e1a926c544d6c8c1c047e29ab87f5)图2.LSTM网络结构


长短时记忆网络的结构和循环神经网络非常类似，都是通过不断调用同一个cell来逐次处理时序信息。每阅读一个新单词xt，就会输出一个新的输出信号ht，用来表示当前阅读到所有内容的整体向量表示。不过二者又有一个明显区别，长短时记忆网络在不同cell之间传递的是两个记忆信息，而不像循环神经网络一样只有一个记忆信息，此外长短时记忆网络的内部结构也更加复杂，如 图3 所示。


![](https://ai-studio-static-online.cdn.bcebos.com/05b888fb56144fe8a51667f3f31311496af6973ab9fd4a219302fdefa07dcdaf)图3.LSTM网络内部结构示意图

区别于循环神经网络RNN，长短时记忆网络最大的特点是在更新内部记忆时，引入了遗忘机制。即容许网络忘记过去阅读过程中看到的一些无关紧要的信息，只保留有用的历史信息。

长短时记忆网络的Cell有三个输入：
1. 这个网络新看到的输入信号，如下一个单词，记为xt，其中xt是一个向量，t代表了当前时刻。
1. 这个网络在上一步的输出信号，记为ht-1，这是一个向量，维度同xt相同
1. 这个网络在上一步的记忆信号，记为ct-1，这是一个向量，维度同xt相同。

得到这两个信号之后，长短时记忆网络没有立即去融合这两个向量，而是计算了权重。
1. 输入门：it=sigmoid(WiXt+ViHt−1+bi)，控制有多少输入信号会被融合。
1. 遗忘门：ft=sigmoid(WfXt+VfHt−1+bf)，控制有多少过去的记忆会被遗忘。
1. 输出门：ot=sigmoid(WoXt+VoHt−1+bo)，控制最终输出多少融合了记忆的信息。
1. 单元状态：gt=tanh(WgXt+VgHt−1+bg)，输入信号和过去的输入信号做一个信息融合。

通过学习这些门的权重设置，长短时记忆网络可以根据当前的输入信号和记忆信息，有选择性地忽略或者强化当前的记忆或是输入信号，帮助网络更好地学习长句子的语义信息：
记忆信号：ct=ft⋅ct−1+it⋅gtc_{t}
输出信号：ht=ot⋅tanh(ct)h_{t}
