{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 代码编写\n",
    "我将数据的处理，训练和测试进行了集成。用下边提供的工具，只需要用fit预处理数据，然后用train进行训练，就可以训练模型。如果想知道模型的训练效果，可以用evaluate进行测试。如果想进行预测，可以采用predict_prob预测各个样本被分到各个类别的概率。\n",
    "\n",
    "**为了适应Paddle框架，代码编程中用到了几个技巧可能和原文思路有差别：1，softmax沿用Paddle提供的函数，没有引入Huffman树；2，将输入文本全部转换成子结构的集合（subwords），然后直接查询和训练子结构的向量，同时转换后的子结构集合的长度被控制在相同的长度，如果不足则填充0，超过就截断；3，记录每个文本被转换成子结构后子结构的长度，以便于文本向量求取过程中的求平均。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "from hashlib import md5\r\n",
    "from paddle import fluid\r\n",
    "from collections import Counter\r\n",
    "from itertools import chain\r\n",
    "import random\r\n",
    "from multiprocessing import cpu_count, pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SubWordTools:\r\n",
    "    def __init__(self, id2word_dict, threads=0, sub_ngrams=(3,4,5,6), ngrams=(2,), sequence_length=3000, hash_bins=1e6):\r\n",
    "        self.id2word_dict = id2word_dict\r\n",
    "        self.threads = threads\r\n",
    "        self.ngrams = ngrams\r\n",
    "        self.sub_ngrams = sub_ngrams\r\n",
    "        self.sequence_length = sequence_length\r\n",
    "        self.hash_bins = int(hash_bins)\r\n",
    "        \r\n",
    "    def fit(self, data):\r\n",
    "        start = time.time()\r\n",
    "        if self.threads <= 0:\r\n",
    "            threads = cpu_count() - 1\r\n",
    "        else:\r\n",
    "            threads = self.threads\r\n",
    "        p = pool.Pool(threads)\r\n",
    "        data = p.map(self.gen_ngrams, data)\r\n",
    "        p.close()\r\n",
    "        p.join()\r\n",
    "        print('pre process data done. cost time %.4f'%(time.time() - start))\r\n",
    "        return data\r\n",
    "\r\n",
    "    def gen_subs(self, word):\r\n",
    "        word = '<' + word + '>'\r\n",
    "        len_word = len(word)\r\n",
    "        sub_words = []\r\n",
    "        for n in self.sub_ngrams:\r\n",
    "            if len_word >= n:\r\n",
    "                end_index = len_word - n + 1\r\n",
    "                for i in range(end_index):\r\n",
    "                    sub_words.append(word[i:i+n])\r\n",
    "        return sub_words\r\n",
    "\r\n",
    "    def gen_ngrams(self, sequence):\r\n",
    "        \"\"\"\r\n",
    "        get the ngrams elements of the sequence. The sequence should be a list of encoded words.\r\n",
    "        :param sequence:\r\n",
    "        :return: the hashing code of subs, the length of words and ngrams.\r\n",
    "        \"\"\"\r\n",
    "        subs = []\r\n",
    "        len_seq = len(sequence)\r\n",
    "        for w in sequence:\r\n",
    "            subs += [self.gen_hash_code(sub) for sub in self.gen_subs(self.id2word_dict[w])]\r\n",
    "        for n in self.ngrams:\r\n",
    "            for i in range(len_seq - n + 1):\r\n",
    "                grams = sequence[i:i+n]\r\n",
    "                grams = self.concat([self.id2word_dict[id] for id in grams])\r\n",
    "                subs += [self.gen_hash_code(sub) for sub in self.gen_subs(grams)]\r\n",
    "        len_subs = len(subs)\r\n",
    "        if len_subs >= self.sequence_length:\r\n",
    "            subs = subs[:self.sequence_length]\r\n",
    "            length = self.sequence_length\r\n",
    "        else:\r\n",
    "            length = len(subs) + 0.001 # to avoid divided by 0\r\n",
    "            subs = subs + [0] * (self.sequence_length - len_subs)\r\n",
    "        return subs, length\r\n",
    "        \r\n",
    "    def padding(self, subs):\r\n",
    "        len_subs = len(subs)\r\n",
    "        if len_subs >= self.sequence_length:\r\n",
    "            subs = subs[:self.sequence_length]\r\n",
    "            length = self.sequence_length\r\n",
    "        else:\r\n",
    "            length = len(subs) + 0.001 # to avoid divided by 0\r\n",
    "            subs = subs + [0] * (self.sequence_length - len_subs)\r\n",
    "        return subs, length\r\n",
    "\r\n",
    "    def concat(self, word_list):\r\n",
    "        \"\"\"\r\n",
    "        concatenate the string in word_list, with ' ' as the partition.\r\n",
    "        :param word_list:\r\n",
    "        :return: concatenated string\r\n",
    "        \"\"\"\r\n",
    "        grams = ''\r\n",
    "        for ind, word in enumerate(word_list):\r\n",
    "            grams += word\r\n",
    "            if ind < len(word_list) - 1:\r\n",
    "                grams += ' '\r\n",
    "        return grams\r\n",
    "\r\n",
    "    def gen_hash_code(self, label):\r\n",
    "        \"\"\"\r\n",
    "        get the hash code of a label\r\n",
    "        :param label:\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        def hashing(word):\r\n",
    "            return int(md5(word.encode()).hexdigest(), 16)\r\n",
    "        code = hashing(label) % (self.hash_bins - 1) + 1\r\n",
    "        return code\r\n",
    "\r\n",
    "    def build_data(self, data, labels=None, batch_size=32, shuffle=True):\r\n",
    "        \"\"\"\r\n",
    "        to define a generator to reduce the memory cost\r\n",
    "        :param labels:\r\n",
    "        :param data:\r\n",
    "        :param batch_size:\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        shuffle_id = list(range(len(data)))\r\n",
    "        if shuffle:\r\n",
    "            random.shuffle(shuffle_id)\r\n",
    "        batch_data = []\r\n",
    "        if labels is not None:\r\n",
    "            batch_label = []\r\n",
    "        for id in shuffle_id:\r\n",
    "            batch_data.append(data[id])\r\n",
    "            if labels is not None:\r\n",
    "                batch_label.append(labels[id])\r\n",
    "            if len(batch_data) == batch_size:\r\n",
    "                if labels is not None:\r\n",
    "                    yield batch_data, batch_label\r\n",
    "                    batch_data = []\r\n",
    "                    batch_label = []\r\n",
    "                else:\r\n",
    "                    yield batch_data\r\n",
    "                    batch_data = []\r\n",
    "\r\n",
    "        if len(batch_data) > 0:\r\n",
    "            if labels is not None:\r\n",
    "                yield batch_data, batch_label\r\n",
    "            else:\r\n",
    "                yield batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FastTextC_pp:\r\n",
    "    \"\"\"\r\n",
    "    使用paddlepaddle框架编写\r\n",
    "    output_dim: number of classed of text classification\r\n",
    "    min_count: only the words with frequency higher than min_count will be considered to be trained.\r\n",
    "    embedding_size: dimensionality of word embedding\r\n",
    "    sub_ngrams: the value of n to consider ngrams of subwords.\r\n",
    "    ngrams: the value of n to consider ngrams features of documents\r\n",
    "    skip_ratio: for some words with high frequency, the redundant training of them will not get better embeddings while\r\n",
    "    hash_bins: the length of hash features of subwords.\r\n",
    "    sequence_length: the length of subwords to be considered when document is transferred, if the subwords in the document\r\n",
    "    is not enough sequence_length, the last length will be padded with 0\r\n",
    "    skip_ratio: for some words with high frequency, the redundant training of them will not get better embeddings while\r\n",
    "    cost extra time. These words that exceed skip probability will not be trained every meeting. Instead, they will be skipped\r\n",
    "    in training with probability 1 - (skip_ratio / f(w)) ^ 1/2, where f(w) = Count(w) / Count(all words)\r\n",
    "    learning_rate: learning rate of optimizer\r\n",
    "    use_gpu: whether or not to use gpu for training\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, output_dim,\r\n",
    "                 embedding_size=100,\r\n",
    "                 min_count=5,\r\n",
    "                 sub_ngrams=(3,4,5,6),\r\n",
    "                 ngrams=(2,),\r\n",
    "                 skip_ratio=1.0,\r\n",
    "                 hash_bins=1e5,\r\n",
    "                 sequence_length=10000,\r\n",
    "                 learning_rate=0.001,\r\n",
    "                 use_gpu=False,\r\n",
    "                 threads=0,\r\n",
    "                 ):\r\n",
    "        self.output_dim = output_dim\r\n",
    "        self.min_count = min_count\r\n",
    "        self.embedding_size = embedding_size\r\n",
    "        self.sub_ngrams = sub_ngrams\r\n",
    "        self.ngrams = ngrams\r\n",
    "        self.skip_ratio = skip_ratio\r\n",
    "        self.hash_bins = int(hash_bins)\r\n",
    "        self.sequence_length = sequence_length\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        self.use_gpu = use_gpu\r\n",
    "        self.threads = threads\r\n",
    "        self.deployed = False\r\n",
    "        if self.use_gpu:\r\n",
    "            self.place = fluid.CUDAPlace(0)\r\n",
    "        else:\r\n",
    "            self.place = fluid.CPUPlace()\r\n",
    "\r\n",
    "    def fit(self, data):\r\n",
    "        \"\"\"\r\n",
    "        fit the data, get the vocabulary and vocabulary size, the code the data\r\n",
    "        :param data: text string or splitted words. Note that the data should be a (batch, length)\r\n",
    "        form, where the length dim corresponds the text string or splitted words.\r\n",
    "        :return: the fitted data\r\n",
    "        \"\"\"\r\n",
    "        try:\r\n",
    "            data = [line.split(' ') for line in data]\r\n",
    "            print(\"The form of input text is [text1, text2, ...].\")\r\n",
    "        except AttributeError:\r\n",
    "            print(\"The form of input text is [[word11, word12, ...], [word21, word22, ...]].\")\r\n",
    "        data = [[word.lower() for word in line] for line in data]\r\n",
    "        words_counter = [(word, count) for word, count in Counter(chain(*data)).most_common() if count >= self.min_count]\r\n",
    "        self.vocab_size = len(words_counter)\r\n",
    "        # generate 3 different dicts to store:\r\n",
    "        # word to id\r\n",
    "        # word to id's frequency\r\n",
    "        # id to word\r\n",
    "        self.word2id_dict = {word: id for id, (word, _) in enumerate(words_counter)}\r\n",
    "        self.id2word_dict = {id: word for id, (word, _) in enumerate(words_counter)}\r\n",
    "        self.word2id_freq = {id: freq for id, (_, freq) in enumerate(words_counter)}\r\n",
    "        # transfer data to ids\r\n",
    "        data = [[self.word2id_dict[word] for word in line if word in self.word2id_dict.keys()] for line in data]\r\n",
    "        # subsampling, reduce the size of the data to boost the training process\r\n",
    "        data = self.subsampling(data)\r\n",
    "        self.subwordtool = SubWordTools(self.id2word_dict, self.threads, self.sub_ngrams, self.ngrams, self.sequence_length, self.hash_bins)\r\n",
    "        data = self.subwordtool.fit(data)\r\n",
    "        return data\r\n",
    "\r\n",
    "    def subsampling(self, data):\r\n",
    "        \"\"\"\r\n",
    "        discard some words according to their frequency. The higher the more likely to be discarded.\r\n",
    "        :param data:\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        total_num = np.sum([freq for freq in self.word2id_freq.values()])\r\n",
    "        skip_prob = {id: 1 - (self.skip_ratio / freq * total_num) ** 0.5 for (id, freq) in self.word2id_freq.items()}\r\n",
    "        return [[word for word in line if np.random.rand() > skip_prob[word]] for line in data]\r\n",
    "\r\n",
    "    def forward(self, inputs, words_count, label=None):\r\n",
    "        \"\"\"\r\n",
    "        :param inputs:\r\n",
    "        :param words_count:\r\n",
    "        :param label:\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        sequence_emb = fluid.embedding(inputs, size=[self.hash_bins, self.embedding_size], is_sparse=True, padding_idx=0, param_attr='shred_w')\r\n",
    "        sequence_emb = fluid.layers.reduce_sum(sequence_emb, dim=1)\r\n",
    "        words_count = fluid.layers.reshape(words_count, [words_count.shape[0], 1])\r\n",
    "        sequence_emb = fluid.layers.elementwise_div(sequence_emb, words_count)\r\n",
    "        pred = fluid.layers.fc(input=[sequence_emb], size=self.output_dim, act='softmax')\r\n",
    "\r\n",
    "        if labels is not None:\r\n",
    "            label = fluid.layers.argmax(label, axis=1)\r\n",
    "            label = fluid.layers.reshape(label, [-1, 1])\r\n",
    "            acc = fluid.layers.accuracy(input=pred, label=label)\r\n",
    "            return pred, acc\r\n",
    "        else:\r\n",
    "            return pred\r\n",
    "\r\n",
    "    def train(self, inputs, label, epochs=5, batch_size=32):\r\n",
    "        if not self.deployed:\r\n",
    "            input_var = fluid.data(name='inputs', shape=[None, self.sequence_length], dtype='int64')\r\n",
    "            word_count_var = fluid.data(name='word_count', shape=[None], dtype='float32')\r\n",
    "            label_var = fluid.data(name='label', shape=[None, self.output_dim], dtype='int64')\r\n",
    "\r\n",
    "            self.main_program = fluid.default_main_program()\r\n",
    "            self.startup_program = fluid.default_startup_program()\r\n",
    "\r\n",
    "            pred, self.acc = self.forward(input_var, word_count_var, label_var)\r\n",
    "            self.pred = pred\r\n",
    "            label_var = fluid.layers.argmax(label_var, axis=1)\r\n",
    "            label_var = fluid.layers.reshape(label_var, [-1, 1])\r\n",
    "            loss = fluid.layers.cross_entropy(pred, label_var, soft_label=False)\r\n",
    "            self.loss = fluid.layers.reduce_mean(loss)\r\n",
    "            optimizer = fluid.optimizer.Adam(learning_rate=self.learning_rate)\r\n",
    "\r\n",
    "            self.test_program = self.main_program.clone(for_test=True)\r\n",
    "\r\n",
    "            optimizer.minimize(self.loss)\r\n",
    "\r\n",
    "            self.exe = fluid.Executor(self.place)\r\n",
    "            self.exe.run(self.startup_program)\r\n",
    "\r\n",
    "            self.deployed = True\r\n",
    "\r\n",
    "        # 在静态图中输入向量是根据向量的名字识别的，feed_order中名字的顺序对应的就是在执行操作时送入数据的顺序\r\n",
    "        feed_order = ['inputs', 'word_count', 'label']\r\n",
    "        # 以下两行起到了传递数据的作用，注意feed_order和feed_var_list_loop中的量是一一对应的\r\n",
    "        feed_var_list_loop = [self.main_program.global_block().var(var_name) for var_name in feed_order]\r\n",
    "        self.feeder = fluid.DataFeeder(feed_list=feed_var_list_loop, place=self.place)\r\n",
    "\r\n",
    "        self.history = {'loss': [], 'accuracy': []}\r\n",
    "        for epoch in range(epochs):\r\n",
    "            data = self.subwordtool.build_data(inputs, label, batch_size=batch_size)\r\n",
    "            ave_loss = 0\r\n",
    "            ave_acc = 0\r\n",
    "            step = 0\r\n",
    "            start = time.time()\r\n",
    "            total_step = int(np.ceil(len(inputs) / batch_size))\r\n",
    "            for data_count_batch, labels_batch in data:\r\n",
    "                data_batch = [line[0] for line in data_count_batch]\r\n",
    "                num_batch = [line[1] for line in data_count_batch]\r\n",
    "                data_batch = np.array(data_batch).astype('int64')\r\n",
    "                num_batch = np.array(num_batch).astype('float32')\r\n",
    "                labels_batch = np.array(labels_batch).astype('int64')\r\n",
    "                loss_, accuracy = self.exe.run(self.main_program, feed=self.feeder.feed(zip(data_batch, num_batch, labels_batch)), fetch_list=[self.loss, self.acc])\r\n",
    "                step += 1\r\n",
    "                ave_loss = (ave_loss * (float(step) - 1.0) + loss_[0]) / float(step)\r\n",
    "                ave_acc = (ave_acc * (float(step) - 1.0) + accuracy[0]) / float(step)\r\n",
    "                if step % 10 == 0 and step < total_step:\r\n",
    "                    print('epoch: {} - ETA: {:.2f} - step: {} / {} - acc: {:.4f} - loss: {:.4f}'.format(\r\n",
    "                        epoch + 1,  (time.time() - start) / step * (total_step - step), str(step).ljust(len(str(total_step))),\r\n",
    "                        total_step, accuracy[0], loss_[0]))\r\n",
    "                if step == total_step:\r\n",
    "                    print('epoch: {} done - time cost: {:.2f} -ave acc: {:.4f} - ave loss: {:.4f}'.format(\r\n",
    "                        epoch + 1, time.time() - start, ave_acc, ave_loss))\r\n",
    "            self.history['loss'].append(ave_loss)\r\n",
    "            self.history['accuracy'].append(ave_acc)\r\n",
    "\r\n",
    "    def evaluate(self, test_inputs, test_labels, batch_size=32):\r\n",
    "        \"\"\"\r\n",
    "        evaluate the model, return the accuracy.\r\n",
    "        :param test_data:\r\n",
    "        :param test_labels:\r\n",
    "        :param batch_size:\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        test_program = self.test_program\r\n",
    "        data = self.subwordtool.build_data(test_inputs, test_labels, batch_size=batch_size)\r\n",
    "        ave_loss = 0\r\n",
    "        ave_acc = 0\r\n",
    "        step = 0\r\n",
    "        start = time.time()\r\n",
    "        total_step = int(np.ceil(len(test_inputs) / batch_size))\r\n",
    "        for data_count_batch, labels_batch in data:\r\n",
    "            data_batch = [line[0] for line in data_count_batch]\r\n",
    "            num_batch = [line[1] for line in data_count_batch]\r\n",
    "            data_batch = np.array(data_batch).astype('int64')\r\n",
    "            num_batch = np.array(num_batch).astype('float32')\r\n",
    "            labels_batch = np.array(labels_batch).astype('int64')\r\n",
    "            loss_, accuracy = self.exe.run(test_program,\r\n",
    "                                           feed=self.feeder.feed(zip(data_batch, num_batch, labels_batch)),\r\n",
    "                                           fetch_list=[self.loss, self.acc])\r\n",
    "            step += 1\r\n",
    "            ave_loss = (ave_loss * (float(step) - 1.0) + loss_[0]) / float(step)\r\n",
    "            ave_acc = (ave_acc * (float(step) - 1.0) + accuracy[0]) / float(step)\r\n",
    "            if step % 10 == 0 and step < total_step:\r\n",
    "                print('ETA: {:.2f} - step: {} / {} - acc: {:.4f} - loss: {:.4f}'.format(\r\n",
    "                    (time.time() - start) / step * (total_step - step),\r\n",
    "                    str(step).ljust(len(str(total_step))),\r\n",
    "                    total_step, accuracy[0], loss_[0]))\r\n",
    "            if step == total_step:\r\n",
    "                print('Done - time cost: {:.2f} -ave acc: {:.4f} - ave loss: {:.4f}'.format(\r\n",
    "                    time.time() - start, ave_acc, ave_loss))\r\n",
    "\r\n",
    "    def predict_prob(self, inputs, batch_size=32):\r\n",
    "        \"\"\"\r\n",
    "        predict the probability of labels of inputs\r\n",
    "        :param inputs:\r\n",
    "        :param batch_size:\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        test_program = self.test_program\r\n",
    "        data = self.subwordtool.build_data(inputs, batch_size=batch_size, shuffle=False)\r\n",
    "        pred_labels = []\r\n",
    "        for data_count_batch in data:\r\n",
    "            data_batch = [line[0] for line in data_count_batch]\r\n",
    "            num_batch = [line[1] for line in data_count_batch]\r\n",
    "            data_batch = np.array(data_batch).astype('int64')\r\n",
    "            num_batch = np.array(num_batch).astype('float32')\r\n",
    "            labels_batch = np.zeros((data_batch.shape[0], self.output_dim)).astype('int64')\r\n",
    "            pred, = self.exe.run(test_program,\r\n",
    "                                           feed=self.feeder.feed(zip(data_batch, num_batch, labels_batch)),\r\n",
    "                                           fetch_list=[self.pred])\r\n",
    "            pred = pred.tolist()\r\n",
    "            for line in pred:\r\n",
    "                pred_labels.append(line)\r\n",
    "        return pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 运行一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import chardet\r\n",
    "import time\r\n",
    "\r\n",
    "def one_hot_labeling(labels):\r\n",
    "    \"\"\"\r\n",
    "    get the one hot form of labels\r\n",
    "    :param labels:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "    label_set = [label for label in set(labels)]\r\n",
    "    label_code = {label: x for x, label in enumerate(label_set)}\r\n",
    "    one_hots = []\r\n",
    "    for label in labels:\r\n",
    "        tem = [0] * len(label_set)\r\n",
    "        tem[label_code[label]] = 1\r\n",
    "        one_hots.append(tem)\r\n",
    "    return one_hots, label_set\r\n",
    "\r\n",
    "folder_prefix = 'data/data36324/'\r\n",
    "x_train = list(open(folder_prefix + \"r52-train.txt\", 'rb').readlines())\r\n",
    "x_test = list(open(folder_prefix + \"r52-test.txt\", 'rb').readlines())\r\n",
    "x_all = []\r\n",
    "train_size = len(x_train)\r\n",
    "x_all = x_all + x_train + x_test\r\n",
    "\r\n",
    "le = len(x_all)\r\n",
    "for i in range(le):\r\n",
    "    encode_type = chardet.detect(x_all[i])\r\n",
    "    x_all[i] = x_all[i].decode(encode_type['encoding'])  # 进行相应解码，赋给原标识符（变量\r\n",
    "labels = [s.split()[0] for s in x_all]\r\n",
    "labels, label_set = one_hot_labeling(labels)\r\n",
    "x_all = [s.split()[1:] for s in x_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The form of input text is [[word11, word12, ...], [word21, word22, ...]].\n",
      "pre process data done. cost time 17.4581\n",
      "epoch: 1 - ETA: 6.49 - step: 10  / 103 - acc: 0.5625 - loss: 3.1527\n",
      "epoch: 1 - ETA: 5.37 - step: 20  / 103 - acc: 0.5469 - loss: 1.9049\n",
      "epoch: 1 - ETA: 4.67 - step: 30  / 103 - acc: 0.7656 - loss: 1.1624\n",
      "epoch: 1 - ETA: 3.97 - step: 40  / 103 - acc: 0.5938 - loss: 1.6389\n",
      "epoch: 1 - ETA: 3.31 - step: 50  / 103 - acc: 0.6406 - loss: 1.4624\n",
      "epoch: 1 - ETA: 2.68 - step: 60  / 103 - acc: 0.7500 - loss: 1.1319\n",
      "epoch: 1 - ETA: 2.04 - step: 70  / 103 - acc: 0.7969 - loss: 0.9184\n",
      "epoch: 1 - ETA: 1.42 - step: 80  / 103 - acc: 0.8594 - loss: 0.7913\n",
      "epoch: 1 - ETA: 0.80 - step: 90  / 103 - acc: 0.7344 - loss: 0.9819\n",
      "epoch: 1 - ETA: 0.18 - step: 100 / 103 - acc: 0.8125 - loss: 0.7661\n",
      "epoch: 1 done - time cost: 6.30 -ave acc: 0.7113 - ave loss: 1.4522\n",
      "epoch: 2 - ETA: 5.56 - step: 10  / 103 - acc: 0.9531 - loss: 0.2856\n",
      "epoch: 2 - ETA: 5.03 - step: 20  / 103 - acc: 0.8750 - loss: 0.4554\n",
      "epoch: 2 - ETA: 4.41 - step: 30  / 103 - acc: 0.8281 - loss: 0.8001\n",
      "epoch: 2 - ETA: 3.79 - step: 40  / 103 - acc: 0.9219 - loss: 0.3806\n",
      "epoch: 2 - ETA: 3.18 - step: 50  / 103 - acc: 0.9375 - loss: 0.2406\n",
      "epoch: 2 - ETA: 2.58 - step: 60  / 103 - acc: 0.9219 - loss: 0.4723\n",
      "epoch: 2 - ETA: 1.98 - step: 70  / 103 - acc: 0.9219 - loss: 0.4345\n",
      "epoch: 2 - ETA: 1.38 - step: 80  / 103 - acc: 0.9219 - loss: 0.3591\n",
      "epoch: 2 - ETA: 0.78 - step: 90  / 103 - acc: 0.9375 - loss: 0.3250\n",
      "epoch: 2 - ETA: 0.18 - step: 100 / 103 - acc: 0.9375 - loss: 0.2798\n",
      "epoch: 2 done - time cost: 6.18 -ave acc: 0.9050 - ave loss: 0.4239\n",
      "epoch: 3 - ETA: 5.65 - step: 10  / 103 - acc: 0.9531 - loss: 0.2767\n",
      "epoch: 3 - ETA: 5.04 - step: 20  / 103 - acc: 0.9688 - loss: 0.2445\n",
      "epoch: 3 - ETA: 4.44 - step: 30  / 103 - acc: 0.9375 - loss: 0.2331\n",
      "epoch: 3 - ETA: 3.84 - step: 40  / 103 - acc: 1.0000 - loss: 0.0646\n",
      "epoch: 3 - ETA: 3.23 - step: 50  / 103 - acc: 1.0000 - loss: 0.1593\n",
      "epoch: 3 - ETA: 2.62 - step: 60  / 103 - acc: 0.9375 - loss: 0.2472\n",
      "epoch: 3 - ETA: 2.00 - step: 70  / 103 - acc: 0.9844 - loss: 0.0713\n",
      "epoch: 3 - ETA: 1.40 - step: 80  / 103 - acc: 0.9531 - loss: 0.1183\n",
      "epoch: 3 - ETA: 0.79 - step: 90  / 103 - acc: 0.9219 - loss: 0.2912\n",
      "epoch: 3 - ETA: 0.18 - step: 100 / 103 - acc: 0.9844 - loss: 0.0983\n",
      "epoch: 3 done - time cost: 6.21 -ave acc: 0.9601 - ave loss: 0.1879\n",
      "epoch: 4 - ETA: 5.63 - step: 10  / 103 - acc: 0.9844 - loss: 0.0863\n",
      "epoch: 4 - ETA: 4.97 - step: 20  / 103 - acc: 0.9688 - loss: 0.0989\n",
      "epoch: 4 - ETA: 4.39 - step: 30  / 103 - acc: 0.9688 - loss: 0.1134\n",
      "epoch: 4 - ETA: 3.79 - step: 40  / 103 - acc: 1.0000 - loss: 0.0432\n",
      "epoch: 4 - ETA: 3.18 - step: 50  / 103 - acc: 1.0000 - loss: 0.0508\n",
      "epoch: 4 - ETA: 2.59 - step: 60  / 103 - acc: 0.9844 - loss: 0.0824\n",
      "epoch: 4 - ETA: 1.99 - step: 70  / 103 - acc: 0.9844 - loss: 0.0493\n",
      "epoch: 4 - ETA: 1.39 - step: 80  / 103 - acc: 1.0000 - loss: 0.0440\n",
      "epoch: 4 - ETA: 0.78 - step: 90  / 103 - acc: 1.0000 - loss: 0.0665\n",
      "epoch: 4 - ETA: 0.18 - step: 100 / 103 - acc: 0.9844 - loss: 0.0724\n",
      "epoch: 4 done - time cost: 6.19 -ave acc: 0.9841 - ave loss: 0.0878\n",
      "epoch: 5 - ETA: 5.65 - step: 10  / 103 - acc: 1.0000 - loss: 0.0394\n",
      "epoch: 5 - ETA: 5.01 - step: 20  / 103 - acc: 1.0000 - loss: 0.0516\n",
      "epoch: 5 - ETA: 4.40 - step: 30  / 103 - acc: 1.0000 - loss: 0.0443\n",
      "epoch: 5 - ETA: 3.83 - step: 40  / 103 - acc: 1.0000 - loss: 0.0535\n",
      "epoch: 5 - ETA: 3.23 - step: 50  / 103 - acc: 1.0000 - loss: 0.0667\n",
      "epoch: 5 - ETA: 2.62 - step: 60  / 103 - acc: 1.0000 - loss: 0.0447\n",
      "epoch: 5 - ETA: 2.00 - step: 70  / 103 - acc: 1.0000 - loss: 0.0202\n",
      "epoch: 5 - ETA: 1.39 - step: 80  / 103 - acc: 0.9844 - loss: 0.1077\n",
      "epoch: 5 - ETA: 0.79 - step: 90  / 103 - acc: 0.9844 - loss: 0.0459\n",
      "epoch: 5 - ETA: 0.18 - step: 100 / 103 - acc: 1.0000 - loss: 0.0322\n",
      "epoch: 5 done - time cost: 6.19 -ave acc: 0.9948 - ave loss: 0.0436\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 100\r\n",
    "learning_rate = 0.01\r\n",
    "output_dim = len(labels[0])\r\n",
    "sequence_length = 1000\r\n",
    "hash_bins = 5*1e6\r\n",
    "epochs = 5\r\n",
    "\r\n",
    "ftc = FastTextC_pp(output_dim=output_dim, embedding_size=embedding_size, ngrams=(2,),\r\n",
    "                   sequence_length=sequence_length, min_count=5, hash_bins=hash_bins, \r\n",
    "                   use_gpu=True, sub_ngrams=(3, 4, 5), skip_ratio=0.001, learning_rate=learning_rate)\r\n",
    "data = ftc.fit(x_all)\r\n",
    "ftc.train(data[:train_size], labels[:train_size], epochs=epochs, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-23 20:52:26,439-INFO: font search path ['/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/afm', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/pdfcorefonts']\n",
      "2021-07-23 20:52:26,953-INFO: generated new fontManager\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘出loss，accuracy的变化趋势\r\n",
    "%matplotlib inline\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "epochs = [i+1 for i in range(epochs)]\r\n",
    "history = ftc.history\r\n",
    "plt.plot(epochs, history['loss'], 'r-*')\r\n",
    "plt.xlabel('epochs')\r\n",
    "plt.ylabel('loss')\r\n",
    "plt.title('loss trend')\r\n",
    "plt.show()\r\n",
    "plt.plot(epochs, history['accuracy'], 'b->')\r\n",
    "plt.xlabel('epochs')\r\n",
    "plt.ylabel('accuracy')\r\n",
    "plt.title('accuracy trend')\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETA: 0.51 - step: 10 / 81 - acc: 0.8750 - loss: 0.4942\n",
      "ETA: 0.42 - step: 20 / 81 - acc: 0.8438 - loss: 0.6506\n",
      "ETA: 0.35 - step: 30 / 81 - acc: 1.0000 - loss: 0.0775\n",
      "ETA: 0.28 - step: 40 / 81 - acc: 0.9688 - loss: 0.1666\n",
      "ETA: 0.21 - step: 50 / 81 - acc: 0.8438 - loss: 0.5006\n",
      "ETA: 0.14 - step: 60 / 81 - acc: 0.9688 - loss: 0.1155\n",
      "ETA: 0.08 - step: 70 / 81 - acc: 0.9375 - loss: 0.2690\n",
      "ETA: 0.01 - step: 80 / 81 - acc: 0.9062 - loss: 0.4072\n",
      "Done - time cost: 0.57 -ave acc: 0.9309 - ave loss: 0.2848\n"
     ]
    }
   ],
   "source": [
    "ftc.evaluate(data[train_size:], labels[train_size:], batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label is earn, Predicted label is earn\n",
      "True label is earn, Predicted label is earn\n",
      "True label is earn, Predicted label is earn\n",
      "True label is earn, Predicted label is earn\n",
      "True label is earn, Predicted label is earn\n",
      "True label is cpi, Predicted label is cpi\n",
      "True label is copper, Predicted label is copper\n",
      "True label is earn, Predicted label is earn\n",
      "True label is acq, Predicted label is acq\n",
      "True label is iron-steel, Predicted label is iron-steel\n"
     ]
    }
   ],
   "source": [
    "test_docs = data[500:510]\r\n",
    "test_labels = labels[500:510]\r\n",
    "prob = ftc.predict_prob(test_docs)\r\n",
    "test_labels = np.argmax(test_labels, axis=1)\r\n",
    "pred_labels = np.argmax(prob, axis=1)\r\n",
    "for i, j in zip(test_labels, pred_labels):\r\n",
    "    print('True label is {}, Predicted label is {}'.format(label_set[i], label_set[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 总结\n",
    "本项目用Paddle静态图模式实现了fast text分类的相关论文。从结果可以看出基于Paddle实现后得到了比较好的分类效果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.8.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
