### LSTM-DSSM

#### 1.概念

针对 CNN-DSSM 无法捕获较远距离上下文特征的缺点，有人提出了用LSTM-DSSM（Long-Short-Term Memory）来解决。在介绍LSTM 之前，首先需要介绍RNN。

如下两图：RNN（Recurrent Neural Networks）可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。如果将这个循环展开：假设输入 xi 为一个 query 中几个连续的词，hi 为输出。那么上一个神经元的输出 h(t-1) 与当前细胞的输入 Xt 拼接后经过 tanh 函数会输出 ht，同时把 ht 传递给下一个细胞。不幸的是，在这个间隔不断增大时，RNN 会逐渐丧失学习到远距离信息的能力。因为 RNN 随着距离的加长，会导致梯度消失。简单来说，由于求导的链式法则，直接导致梯度被表示为连乘的形式，以至梯度消失（几个小于 1 的数相乘会逐渐趋向于 0）。

| ![来自CSDN](img/LSTM-1.png) | ![来自CSDN](img/LSTM-2.png) |
| --------------------------- | --------------------------- |

#### 2.LSTM模型

LSTM（Long-Short-Term Memory）是一种 RNN 特殊的类型，可以学习长期依赖信息。我们分别介绍它最重要的几个模块：

- **细胞状态：**细胞状态可以理解成是一条信息的传送带，只有一些少量的线性交互。在上面流动可以保持信息的不变性。

- **遗忘门：**遗忘门]由 Gers 提出，用来控制细胞状态 cell 有哪些信息可以通过，继续往下传递。如下公式：其中上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（遗忘门）产生一个从 0 到 1 的数值 ft，然后与细胞状态 C(t-1) 相乘，最终决定有多少细胞状态可以继续往后传递。
  $$
  f_{t}=\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+b_{f}\right)
  $$

- **输入门：**输入门决定要新增什么信息到细胞状态，这里包含两部分：一个 sigmoid 输入门和一个 tanh 函数。sigmoid 决定输入的信号控制，tanh 决定输入什么内容。如下公式所示：上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（输入门）产生一个从 0 到 1 的数值 it，同样的信息经过 tanh 网络做非线性变换得到结果 Ct，sigmoid 的结果和 tanh 的结果相乘，最终决定有哪些信息可以输入到细胞状态里。
  $$
  \begin{aligned}
  i_{t} &=\sigma\left(W_{i} \cdot\left[h_{t-1}, x_{t}\right]+b_{i}\right) \\
  \tilde{C}_{t} &=\tanh \left(W_{C} \cdot\left[h_{t-1}, x_{t}\right]+b_{C}\right)
  \end{aligned}
  $$

- **输出门：**输出门决定从细胞状态要输出什么信息，这里也包含两部分：一个 sigmoid 输出门和一个 tanh 函数。sigmoid 决定输出的信号控制，tanh 决定输出什么内容。上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（输出门）产生一个从 0 到 1 的数值 Ot，细胞状态 Ct 经过 tanh 网络做非线性变换，得到结果再与 sigmoid 的结果 Ot 相乘，最终决定有哪些信息可以输出，输出的结果 ht 会作为这个细胞的输出，也会作为传递个下一个细胞。
  $$
  \begin{aligned}
  o_{t} &=\sigma\left(W_{o}\left[h_{t-1}, x_{t}\right]+b_{o}\right) \\
  h_{t} &=o_{t} * \tanh \left(C_{t}\right)
  \end{aligned}
  $$

#### 3.网络结构

LSTM-DSSM 其实是 LSTM 加入了peephole的变种。其整体网络结构如下：红色的部分可以清晰的看到残差传递的方向。

![来自CSDN](img/LSTM-3.png)

#### 4.场景

LSTM-DSSM模型与DSSM的本质区别在于把DSSM里的全连接改为LSTM。输入层直接把每个单词映射到一个word representation，就是embedding，然后把整个句子送入LSTM并训练，拿出最后输出的状态作为隐语义向量，用最后的语义向量后就和DSSM模型一样，进行相似度度量，softmax计算概率等等。

#### 5.优缺点

- **优点：**LSTM-DSSM 可以捕获较远距离上下文特征。
- **缺点：**梯度消失的风险，也许会使得该模型逐渐丧失学习远距离信息的能力。此外，DSSM是弱监督模型，其训练样本都是点击曝光日志里的Query与Tiltle，但点两者不一定是语义匹配的，因此想从这种非常弱的信号里提取出语义相似性，就需要海量的训练样本。

#### 6.引用

[DSSM、CNN-DSSM、LSTM-DSSM等深度学习模型在计算语义相似度上的应用+距离运算]: http://blog.csdn.net/u013074302/article/details/76422551

[【深度语义匹配模型 】原理篇一：表示型]: https://cloud.tencent.com/developer/article/1632997

