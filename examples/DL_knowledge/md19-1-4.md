### ShareBottom多任务学习

#### 1.概念

多任务学习（Multi-Task Learning，MTL）是推荐领域研究的热点。以推荐视频为例，我们可能需要提升很多label，例如提升用户的点赞率、分享率等，在单任务学习中，预测用户点赞动作的模型和预测用户点击分享的模型是两个模型；而在多任务学习中，一个模型一次性就输出了这两个label的预测结果。

多任务模型的一个重要的优点就是节省了计算开销，一个模型干了之前多个模型的事情，这在实时任务中是至关重要的，你给用户推荐，用户不可能刷新一下网页，等个10分钟（夸张一下）才看到你推荐的结果；同时，多任务模型由于联合学习了多个label，附带的也学习了label之间的相关性，这其实是提升了模型的鲁棒性和准确性的，因为单任务模型是无法学习label间的相关性的。多任务学习通常通过 Hard 或 Soft 参数共享来完成。

顾名思义，ShareBottom模型指的就是“共享底部结构的模型”。


#### 2.模型

ShareBottom模型的结构图如下：

![来自CSDN](img/Share-1.png)

如上图可以直观的看到，这是一个双塔模型/多塔模型，在共享底部结构中，学习相似性，每个独立的塔在学习一下每个label的自己独立的特性。该模型一个很重要的前提就是任务相关性，如果多个label的相关性比较差，那共享的底部结构就是一种相互负面影响，会对每个label的预测带来一定的负面影响。但是相似性问题本身就没有一个明确的度量，在全世界范围内都是一个开放问题，那么多相似性度量指标咱们怎么选，为何这样能衡量相似性，一些研究就尝试利用神经网络学习的方法，自动解决这个相似性的问题，其中一种方法是引入门结构（gating structures），这里有一些很有名的改进型，例如MMOE（Multi-gate Mixture-of-Experts）和SNR（Sub-Network Routing）。

#### 3.公式

对于简单的share bottom，x为模型的输入，shared-bottom网络位于底部，多个任务共用这一层；K个子任务分别对应一个tower Network；每个子任务的输出为：$y_{k}=h^{k}(f(x))$​。

#### 4.网络

ShareBottom模型网络结构如下：将id特征embedding，和dense特征concat一起，作为share bottom网络输入，id特征embedding可以使用end2end和预训练两种方式。预训练可以使用word2vec，GraphSAGE等工业界落地的算法，训练全站id embedding特征，在训练dnn或则multi task的过程中fine-tune。end2end训练简单，可以很快就将模型train起来，直接输入id特征，模型从头开始学习id的embedding向量。

![来自CSDN](img/Share-2.png)

#### 5.优缺点

- **优点：**节省计算开销。

- **缺点：**多任务模型对任务之间的相关性很敏感，如果子任务之前的关联性不大，采用share bottom的网络，share部分的参数会产生较大的噪音。通常相似的子任务也拥有比较接近的底层特征，那么在多任务学习中，他们就可以很好地进行底层特征共享；而对于不相似的子任务，他们的底层表示差异很大，在进行参数共享时很有可能会互相冲突或噪声太多，导致多任务学习的模型效果不佳。

#### 6.引用

[推荐系统 | Google多目标学习MMOE]: https://zhuanlan.zhihu.com/p/143166131

[多任务学习在推荐中的探索]: https://blog.csdn.net/horizonheart/article/details/104781499

[多任务学习（Multi-Task Learning，MTL）：SB，MMOE，SNR]: https://blog.csdn.net/Leon_winter/article/details/104314441

