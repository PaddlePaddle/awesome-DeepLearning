### MMoE多任务学习

#### 1.概念

多任务学习模型，旨在构建单个模型来同时学习多个目标和任务。但是，通常任务之间的关系会极大地影响多任务模型的预测质量。因此学习task-specific、objectives和inter-task relationships之间的权衡也非常重要。MMOE模型，全称为：Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts，该模型就可以清晰地从数据中学习任务之间的关系 。

#### 2.模型

MMoE模型的结构图如下：

![来自论文](img/MMoE-1.png)

其中，**模型 (a)** 最为常见，两个任务直接共享模型的bottom部分，只在最后处理时做区分。图 (a) 中使用了Tower A和Tower B，然后分别接损失函数。函数表达式如下：
$$
y^{k}=h^{k}(f(x))
$$
如上，**模型 (b)** 是常见的多任务学习模型。将input分别输入给三个Expert，但三个Expert并不共享参数。同时将input输出给Gate，Gate输出每个Expert被选择的概率，然后将三个Expert的输出加权求和，输出给Tower。函数表达式如下，式中k表示k个任务，n表示n个expert network。
$$
y^{k}=h^{k}\left(\sum_{i=1}^{n} g_{i} f_{i}(x)\right)
$$
如上，**模型 (c)** 是作者新提出的方法，对于不同的任务，模型的权重选择是不同的，所以作者为每个任务都配备一个Gate模型。对于不同的任务，特定的Gate k的输出表示不同的Expert 被选择的概率，将多个Expert加权求和，得到 $f^{k}(x)$​，并输出给特定的Tower模型，用于最终的输出。函数表达式如下：$g(x)$​​表示gate门的输出，为多层感知机模型，实现时为简单的线性变换加softmax层。
$$
\begin{aligned}
f^{k}(x) &=\sum_{i=1}^{n} g_{i}^{k}(x) f_{i}(x) \\
g^{k}(x) &=\operatorname{softmax}\left(\mathbb{W}_{g^{k}(x)}\right)
\end{aligned}
$$

#### 3.对比

MMoE和其他多任务模型做的一个对比：

![来自论文](img/MMoE-2.png)

#### 4.场景

MMoE模型跳出了Shared Bottom那种将整个隐藏层一股脑的共享的思维定式，选择将共享层有意识的（按照数据领域之类的）划分成了多个Expert，并引入了gate机制，得以个性化组合使用共享层。

#### 5.优缺点

- **优点：**MMoE模型是MOE的改进，相对于 MOE的结构中所有任务共享一个门控网络，MMoE的结构优化为每个任务都单独使用一个门控网络。这种改进可以针对不同任务得到不同的 Experts 权重，从而实现对 Experts 的选择性利用，不同任务对应的门控网络可以学习到不同的Experts 组合模式，模型也更容易捕捉到子任务间的相关性和差异性。
- **缺点：**MMoE模型中所有Expert是被所有任务所共享的，这可能无法捕捉到任务间更复杂的关系，从而给部分任务带来一定的噪声。此外，不同的Expert之间没有交互，联合优化的效果有所折扣。

#### 6.引用

[多任务学习之MMOE模型]: https://zhuanlan.zhihu.com/p/145288000

[Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts]: https://dl.acm.org/doi/10.1145/3219819.3220007

[多目标学习(MMOE/ESMM/PLE)在推荐系统的实战经验分享]: https://blog.csdn.net/abcdefg90876/article/details/111399012

