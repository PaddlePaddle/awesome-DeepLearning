{
"cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###LSTM原理:\n",
    "\n",
    "LSTM（Long Short-Term Memory）算法作为深度学习方法的一种，在介绍LSTM算法之前，有必要介绍一下深度学习（Deep Learning）的一些基本背景。\n",
    "\n",
    "目前在机器学习领域，最大的热点毫无疑问是深度学习，从谷歌大脑（Google Brain）的猫脸识别[1]，到ImageNet比赛中深度卷积神经网络的获胜[2]，再到Alphago大胜李世石[3]，深度学习受到媒体、学者以及相关研究人员越来越多的热捧。这背后的原因无非是深度学习方法的效果确实超越了传统机器学习方法许多。从2012年Geoffrey E. Hinton的团队在ImageNet比赛（图像识别中规模最大影响最大的比赛之一）中使用深度学习方法获胜[4]之后，关于深度学习的研究就呈井喷之势；在2012年以前，该比赛结果的准确率一直处于缓慢提升的状态，这一年突然有质的飞越，而从此之后深度学习方法也成为了ImageNet比赛中的不二选择。同时，深度学习的影响却不仅局限于图像识别比赛，也深刻影响了学术界和工业界，顶级的学术会议中关于深度学习的研究越来越多，如CVPR、ICML等等，而工业级也为深度学习立下了汗马功劳，贡献了越来越多的计算支持或者框架，如Nivdia的cuda、cuDnn，Google的tensorflow，Facebook的torch和微软的DMTK等等。\n",
    "\n",
    "深度学习技术发展的背后是广大研究人员的付出，目前该领域内最著名的研究人员莫过于Yoshua Bengio，Geoffrey E. Hinton，Yann LeCun以及Andrew Ng。最近Yoshua Bengio等出版了《Deep Learning》[5]一书，其中对深度学习的历史发展以及该领域内的主要技术做了很系统的论述，其关于深度学习历史发展趋势的总结非常精辟，书中总结的深度学习历史发展趋势的几个关键点分别：\n",
    "\n",
    "a）深度学习本身具有丰富悠久的历史，但是从不同的角度出发有很多不同得名，所以历史上其流行有过衰减趋势。\n",
    "b）随着可以使用的训练数据量逐渐增加，深度学习的应用空间必将越来越大。\n",
    "c）随着计算机硬件和深度学习软件基础架构的改善，深度学习模型的规模必将越来越大。\n",
    "d）随着时间的推移，深度学习解决复杂应用的精度必将越来越高。\n",
    "而深度学习的历史大体可以分为三个阶段。一是在20世纪40年代至60年代，当时深度学习被称为控制论；二是在上世纪80年代至90年代，此期间深度学习被誉为联结学习；三是从2006年开始才以深度学习这个名字开始复苏（起点是2006年，Geoffrey Hinton发现深度置信网可以通过逐层贪心预训练的策略有效地训练）。总而言之，深度学习作为机器学习的一种方法，在过去几十年中有了长足的发展。随着基础计算架构性能的提升，更大的数据集和更好的优化训练技术，可以预见深度学习在不远的未来一定会取得更多的成果。\n",
    "\n",
    "LSTM算法全称为Long short-term memory，最早由 Sepp Hochreiter和Jürgen Schmidhuber于1997年提出[6]，是一种特定形式的RNN（Recurrent neural network，循环神经网络），而RNN是一系列能够处理序列数据的神经网络的总称。这里要注意循环神经网络和递归神经网络（Recursive neural network）的区别。\n",
    "\n",
    "一般地，RNN包含如下三个特性：\n",
    "\n",
    "a）循环神经网络能够在每个时间节点产生一个输出，且隐单元间的连接是循环的；\n",
    "b）循环神经网络能够在每个时间节点产生一个输出，且该时间节点上的输出仅与下一时间节点的隐单元有循环连接；\n",
    "c）循环神经网络包含带有循环连接的隐单元，且能够处理序列数据并输出单一的预测。\n",
    "RNN还有许多变形，例如双向RNN（Bidirectional RNN）等。然而，RNN在处理长期依赖（时间序列上距离较远的节点）时会遇到巨大的困难，因为计算距离较远的节点之间的联系时会涉及雅可比矩阵的多次相乘，这会带来梯度消失（经常发生）或者梯度膨胀（较少发生）的问题，这样的现象被许多学者观察到并独立研究。为了解决该问题，研究人员提出了许多解决办法，例如ESN（Echo State Network），增加有漏单元（Leaky Units）等等。其中最成功应用最广泛的就是门限RNN（Gated RNN），而LSTM就是门限RNN中最著名的一种。有漏单元通过设计连接间的权重系数，从而允许RNN累积距离较远节点间的长期联系；而门限RNN则泛化了这样的思想，允许在不同时刻改变该系数，且允许网络忘记当前已经累积的信息。\n",
    "\n",
    "LSTM就是这样的门限RNN，其单一节点的结构如下图1所示。LSTM的巧妙之处在于通过增加输入门限，遗忘门限和输出门限，使得自循环的权重是变化的，这样一来在模型参数固定的情况下，不同时刻的积分尺度可以动态改变，从而避免了梯度消失或者梯度膨胀的问题。\n",
    "\n",
    "图1 LSTM的CELL示意图\n",
    "\n",
    "https://img-blog.csdn.net/20170313102101605?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveHVhbnl1YW5zZW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\n",
    "\n",
    "根据LSTM网络的结构，每个LSTM单元的计算公式如下图2所示，其中Ft表示遗忘门限，It表示输入门限， ̃Ct表示前一时刻cell状态、Ct表示cell状态（这里就是循环发生的地方），Ot表示输出门限，Ht表示当前单元的输出，Ht-1表示前一时刻单元的输出。\n",
    "\n",
    "图2 LSTM计算公式\n",
    "\n",
    "https://img-blog.csdn.net/20170313102116809?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveHVhbnl1YW5zZW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\n",
    "\n",
    "介绍完LSTM算法的原理之后，自然要了解如何训练LSTM网络。与前馈神经网络类似，LSTM网络的训练同样采用的是误差的反向传播算法（BP），不过因为LSTM处理的是序列数据，所以在使用BP的时候需要将整个时间序列上的误差传播回来。LSTM本身又可以表示为带有循环的图结构，也就是说在这个带有循环的图上使用反向传播时我们称之为BPTT（back-propagation through time）。下面我们通过图3和图4来解释BPTT的计算过程。从图3中LSTM的结构可以看到，当前cell的状态会受到前一个cell状态的影响，这体现了LSTM的recurrent特性。同时在误差反向传播计算时，可以发现h(t)的误差不仅仅包含当前时刻T的误差，也包括T时刻之后所有时刻的误差，即back-propagation through time的含义，这样每个时刻的误差都可以经由h(t)和c(t+1)迭代计算。\n",
    "\n",
    "图3 LSTM网络示意图\n",
    "\n",
    "https://img-blog.csdn.net/20170313102132168?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveHVhbnl1YW5zZW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\n",
    "\n",
    "为了直观地表示整个计算过程，在参考神经网络计算图的基础上，LSTM的计算图如图4所示，从计算图上面可以清晰地看出LSTM的forward propagation和back propagation过程。如图，H(t-1)的误差由H(t)决定，且要对所有的gate layer传播回来的梯度求和，c(t-1)由c(t)决定，而c(t)的误差由两部分，一部分是h(t)，另一部分是c(t+1)。所以在计算c(t)反向传播误差的时候，需要传入h(t)和c(t+1)，而h(t)在更新的时候需要加上h(t+1)。这样就可以从时刻T向后计算任一时刻的梯度，利用随机梯度下降完成权重系数的更新。\n",
    "\n",
    "https://img-blog.csdn.net/20170313102144772?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveHVhbnl1YW5zZW4=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast\n",
    "\n",
    "图4 BPTT示意图\n",
    "\n",
    "四、LSTM算法的一些变形\n",
    "LSTM算法的变形有很多，最主要的有两种，分别如下：\n",
    "\n",
    "a）GRU\n",
    "\n",
    "LSTM算法的变形里面GRU（Gated Recurrent Unit）是使用最为广泛的一种，最早由Cho等人于2014年提出[7]。GRU与LSTM的区别在于使用同一个门限来代替输入门限和遗忘门限，即通过一个“更新”门限来控制cell的状态，该做法的好处是计算得以简化，同时模型的表达能力也很强，所以GRU也因此越来越流行。\n",
    "\n",
    "b）Peephole LSTM\n",
    "\n",
    "Peephole LSTM由Gers和Schmidhuber在2000年提出[8]，Peephole的含义是指允许当前时刻的门限Gate“看到”前一时刻cell的状态，这样在计算输入门限，遗忘门限和输出门限时需要加入表示前一时刻cell状态的变量。同时，另外一些Peephole LSTM的变种会允许不同的门限“看到”前一时刻cell的状态。\n",
    "\n",
    "不同的研究者提出了许多LSTM的改进，然而并没有特定类型的LSTM在任何任务上都能够由于其他变种，仅能在部分特定任务上取得最佳的效果。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddle import fluid\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "\r\n",
    "\r\n",
    "class LSTM:\r\n",
    "    \"\"\"\r\n",
    "    emb_dim: 词向量维度\r\n",
    "    vocab_size: 词典大小，不能小于训练数据中所有词的总数\r\n",
    "    num_layers: 隐含层的数量\r\n",
    "    hidden_size: 隐含层的大小\r\n",
    "    num_steps: LSTM 一次接收数据的最大长度，样本的timestamp\r\n",
    "    use_gpu: 是否使用gpu进行训练\r\n",
    "    dropout_prob: 如果大于0，就启用dropout，值在0-1区间\r\n",
    "    init_scale: 训练参数的初始化范围\r\n",
    "    lr：学习速率\r\n",
    "    vocab: 默认为None，占位，暂时没用\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self,\r\n",
    "                 vocab_size,\r\n",
    "                 num_layers,\r\n",
    "                 hidden_size,\r\n",
    "                 num_steps,\r\n",
    "                 use_gpu=True,\r\n",
    "                 dropout_prob=None,\r\n",
    "                 init_scale=0.1,\r\n",
    "                 lr=0.001,\r\n",
    "                 vocab=None):\r\n",
    "        self.vocab_size = vocab_size\r\n",
    "        self.num_layers = num_layers\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.num_steps = num_steps\r\n",
    "        self.dropout_prob = dropout_prob\r\n",
    "        self.use_gpu = use_gpu\r\n",
    "        self.init_scale = init_scale\r\n",
    "        self.vocab = vocab\r\n",
    "        self.lr = lr\r\n",
    "\r\n",
    "    def forward(self, x, batch_size):\r\n",
    "        self.init_hidden = fluid.layers.data(name='init_hidden',\r\n",
    "                                             shape=[self.num_layers, batch_size, self.hidden_size],\r\n",
    "                                             append_batch_size=False)\r\n",
    "        self.init_cell = fluid.layers.data(name='init_cell',\r\n",
    "                                           shape=[self.num_layers, batch_size, self.hidden_size],\r\n",
    "                                           append_batch_size=False)\r\n",
    "        x_emb = fluid.embedding(input=x, size=[self.vocab_size, self.hidden_size],\r\n",
    "                                dtype='float32', is_sparse=False,\r\n",
    "                                param_attr=fluid.ParamAttr(\r\n",
    "                                    name='embedding_para',\r\n",
    "                                    initializer=fluid.initializer.UniformInitializer(\r\n",
    "                                        low=-self.init_scale, high=self.init_scale\r\n",
    "                                    )\r\n",
    "                                ))\r\n",
    "        x_emb = fluid.layers.reshape(x_emb, shape=[-1, self.num_steps, self.hidden_size])\r\n",
    "        if self.dropout_prob is not None and self.dropout_prob > 0.0:\r\n",
    "            x_emb = fluid.layers.dropout(x_emb, dropout_prob=self.dropout_prob,\r\n",
    "                                         dropout_implementation=\"upscale_in_train\")\r\n",
    "\r\n",
    "        rnn_out, last_hidden, last_cell = fluid.contrib.layers.basic_lstm(x_emb, self.init_hidden, self.init_cell,\r\n",
    "                                                                          self.hidden_size, self.num_layers,\r\n",
    "                                                                          dropout_prob=self.dropout_prob)\r\n",
    "        rnn_out = fluid.layers.reshape(rnn_out, shape=[-1, self.num_steps, self.hidden_size])\r\n",
    "        softmax_weight = fluid.layers.create_parameter(\r\n",
    "            [self.hidden_size, self.vocab_size],\r\n",
    "            dtype=\"float32\",\r\n",
    "            name=\"softmax_weight\",\r\n",
    "            default_initializer=fluid.initializer.UniformInitializer(\r\n",
    "                low=-self.init_scale, high=self.init_scale))\r\n",
    "        softmax_bias = fluid.layers.create_parameter(\r\n",
    "            [self.vocab_size],\r\n",
    "            dtype=\"float32\",\r\n",
    "            name='softmax_bias',\r\n",
    "            default_initializer=fluid.initializer.UniformInitializer(\r\n",
    "                low=-self.init_scale, high=self.init_scale))\r\n",
    "\r\n",
    "        proj = fluid.layers.matmul(rnn_out, softmax_weight)\r\n",
    "        proj = fluid.layers.elementwise_add(proj, softmax_bias)\r\n",
    "        proj = fluid.layers.reshape(proj, shape=[-1, self.vocab_size], inplace=True)\r\n",
    "        # 更新 init_hidden, init_cell\r\n",
    "        fluid.layers.assign(input=last_cell, output=self.init_cell)\r\n",
    "        fluid.layers.assign(input=last_hidden, output=self.init_hidden)\r\n",
    "        return proj, last_hidden, last_cell\r\n",
    "\r\n",
    "    def train(self, x, epochs=3, batch_size=32, log_interval=100):\r\n",
    "        \"\"\"\r\n",
    "        :param log_interval: 输出信息的间隔\r\n",
    "        :param x: 输入，一维list，文本需要经过编码\r\n",
    "        :param epochs: 训练回合数\r\n",
    "        :param batch_size: 训练batch大小\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        self.batch_size = batch_size\r\n",
    "        # 定义训练的program\r\n",
    "        main_program = fluid.default_main_program()\r\n",
    "        startup_program = fluid.default_startup_program()\r\n",
    "        train_loss, train_proj, self.last_hidden, self.last_cell, py_reader = self.build_train_model(main_program, startup_program)\r\n",
    "\r\n",
    "        # 定义测试的program, 写成全局的，以便留给测试函数\r\n",
    "        self.test_program = fluid.Program()\r\n",
    "        self.test_startup_program = fluid.Program()\r\n",
    "        self.test_loss, self.test_proj, _, _ = self.build_test_model(self.test_program, self.test_startup_program)\r\n",
    "        self.test_program = self.test_program.clone(for_test=True)\r\n",
    "\r\n",
    "        place = fluid.CUDAPlace(0) if self.use_gpu else fluid.CPUPlace()\r\n",
    "        self.exe = fluid.Executor(place)\r\n",
    "        self.exe.run(startup_program)\r\n",
    "\r\n",
    "        def data_gen():\r\n",
    "            batches = self.get_data_iter(x)\r\n",
    "            for batch in batches:\r\n",
    "                x_, y_ = batch\r\n",
    "                yield x_, y_\r\n",
    "\r\n",
    "        py_reader.decorate_tensor_provider(data_gen)\r\n",
    "\r\n",
    "        for epoch in range(epochs):\r\n",
    "            batch_times = []\r\n",
    "            epoch_start_time = time.time()\r\n",
    "            total_loss = 0\r\n",
    "            iters = 0\r\n",
    "            py_reader.start()\r\n",
    "            batch_id = 0\r\n",
    "            batch_start_time = time.time()\r\n",
    "            # 初始化init_hidden, init_cell\r\n",
    "            init_hidden = np.zeros((self.num_layers, self.batch_size, self.hidden_size), dtype='float32')\r\n",
    "            init_cell = np.zeros((self.num_layers, self.batch_size, self.hidden_size), dtype='float32')\r\n",
    "\r\n",
    "            data_len = len(x)\r\n",
    "            batch_len = data_len // self.batch_size\r\n",
    "            batch_num = (batch_len - 1) // self.num_steps\r\n",
    "\r\n",
    "            # 送入数据，抓取结果\r\n",
    "            try:\r\n",
    "                while True:\r\n",
    "                    # 送入数据\r\n",
    "                    data_feeds = {}\r\n",
    "                    data_feeds['init_hidden'] = init_hidden\r\n",
    "                    data_feeds['init_cell'] = init_cell\r\n",
    "                    fetch_outs = self.exe.run(main_program, feed=data_feeds,\r\n",
    "                                         fetch_list=[train_loss.name, self.last_hidden.name, self.last_cell.name])\r\n",
    "                    t_loss = np.array(fetch_outs[0])\r\n",
    "                    init_hidden = np.array(fetch_outs[1])\r\n",
    "                    init_cell = np.array(fetch_outs[2])\r\n",
    "\r\n",
    "                    total_loss += t_loss\r\n",
    "                    batch_time = time.time() - batch_start_time\r\n",
    "                    batch_times.append(batch_time)\r\n",
    "                    batch_start_time = time.time()\r\n",
    "\r\n",
    "                    batch_id += 1\r\n",
    "                    iters += self.num_steps\r\n",
    "                    if batch_id % log_interval == 0:\r\n",
    "                        ppl = np.exp(total_loss / iters)\r\n",
    "                        print(\"-- Epoch: %d - Batch: %d / %d - Cost Time: %.2f s -ETA: %.2f s- ppl: %.5f\"\r\n",
    "                              % (epoch + 1, batch_id, batch_num, sum(batch_times),\r\n",
    "                                 sum(batch_times) / batch_id * (batch_num - batch_id), ppl[0]))\r\n",
    "            except fluid.core.EOFException:\r\n",
    "                py_reader.reset()\r\n",
    "\r\n",
    "            epoch_time = time.time() - epoch_start_time\r\n",
    "            ppl = np.exp(total_loss / iters)\r\n",
    "            print(\"Epoch %d Done. Cost Time: %.2f s. ppl: %.5f.\" % (epoch + 1, epoch_time, ppl))\r\n",
    "\r\n",
    "    def evaluate(self, x):\r\n",
    "        \"\"\"\r\n",
    "        测试模型的效果\r\n",
    "        :param x:\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        eval_data_gen = self.get_data_iter(x)\r\n",
    "        total_loss = 0.0\r\n",
    "        iters = 0\r\n",
    "        # 初始化init_hidden, init_cell\r\n",
    "        init_hidden = np.zeros((self.num_layers, self.batch_size, self.hidden_size), dtype='float32')\r\n",
    "        init_cell = np.zeros((self.num_layers, self.batch_size, self.hidden_size), dtype='float32')\r\n",
    "\r\n",
    "        for batch_id, batch in enumerate(eval_data_gen):\r\n",
    "            x, y = batch\r\n",
    "            data_feeds = {}\r\n",
    "            data_feeds['init_hidden'] = init_hidden\r\n",
    "            data_feeds['init_cell'] = init_cell\r\n",
    "            data_feeds['x'] = x\r\n",
    "            data_feeds['y'] = y\r\n",
    "            fetch_outs = self.exe.run(self.test_program, feed=data_feeds,\r\n",
    "                                      fetch_list=[self.test_loss.name, self.last_hidden.name, self.last_cell.name])\r\n",
    "            cost_test = np.array(fetch_outs[0])\r\n",
    "            init_hidden = np.array(fetch_outs[1])\r\n",
    "            init_cell = np.array(fetch_outs[2])\r\n",
    "\r\n",
    "            total_loss += cost_test\r\n",
    "            iters += self.num_steps\r\n",
    "            ppl = np.exp(total_loss / iters)\r\n",
    "            print(\"-- Batch: %d - ppl: %.5f\" % (batch_id, ppl[0]))\r\n",
    "        print(\"ppl: %.5f\" % (ppl[0]))\r\n",
    "        return ppl\r\n",
    "\r\n",
    "    def get_data_iter(self, raw_data):\r\n",
    "        \"\"\"\r\n",
    "        处理原始文本，生成训练数据\r\n",
    "        对于RNN来说，一般为读取前n个词，然后预测下一个词，这里简化为每读一个词，预测下一个词。\r\n",
    "        由于LSTM考虑了长依赖，所以也可以做到读取n个词，预测下一个词\r\n",
    "        :param raw_data: 一个一维数组，list，\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        data_len = len(raw_data)\r\n",
    "        raw_data = np.asarray(raw_data, dtype='int64')\r\n",
    "        batch_len = data_len // self.batch_size\r\n",
    "        # 将一维数组变为二维数组，第一维是batch的数量，第二维是每个batch的数据，这里对后边不足batch_len的数据进行了裁剪，弃掉不用\r\n",
    "        data = raw_data[0:self.batch_size * batch_len].reshape((self.batch_size, batch_len))\r\n",
    "\r\n",
    "        # 为了保证每个batch最后一个词能够被预测，x的词最多被分到batch_len-1\r\n",
    "        batch_num = (batch_len - 1) // self.num_steps\r\n",
    "        for i in range(batch_num):\r\n",
    "            x = np.copy(data[:, i * self.num_steps:(i + 1) * self.num_steps])\r\n",
    "            y = np.copy(data[:, i * self.num_steps + 1:(i + 1) * self.num_steps + 1])\r\n",
    "            x = x.reshape((-1, self.num_steps, 1))\r\n",
    "            y = y.reshape((-1, 1))\r\n",
    "            yield x, y\r\n",
    "\r\n",
    "    def build_train_model(self, main_program, startup_program):\r\n",
    "        \"\"\"\r\n",
    "        读取数据，构建网络\r\n",
    "        :param main_program:\r\n",
    "        :param startup_program:\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        with fluid.program_guard(main_program, startup_program):\r\n",
    "            feed_shapes = [[self.batch_size, self.num_steps, 1],\r\n",
    "                           [self.batch_size * self.num_steps, 1]]\r\n",
    "            py_reader = fluid.layers.py_reader(capacity=64, shapes=feed_shapes, dtypes=['int64', 'int64'])\r\n",
    "            x, y = fluid.layers.read_file(py_reader)\r\n",
    "            # 使用unique_name.guard创建变量空间，以便在test时共享参数\r\n",
    "            with fluid.unique_name.guard():\r\n",
    "                proj, last_hidden, last_cell = self.forward(x, self.batch_size)\r\n",
    "\r\n",
    "                loss = self.get_loss(proj, y)\r\n",
    "                optimizer = fluid.optimizer.Adam(learning_rate=self.lr,\r\n",
    "                                                 grad_clip=fluid.clip.GradientClipByGlobalNorm(clip_norm=1000))\r\n",
    "                optimizer.minimize(loss)\r\n",
    "\r\n",
    "                # 不知道有什么用，先写上\r\n",
    "                #loss.persistable = True\r\n",
    "                #proj.persistable = True\r\n",
    "                #last_cell.persistable = True\r\n",
    "                #last_hidden.persistable = True\r\n",
    "\r\n",
    "                return loss, proj, last_hidden, last_cell, py_reader\r\n",
    "\r\n",
    "    def build_test_model(self, main_program, startup_program):\r\n",
    "        \"\"\"\r\n",
    "        验证模型效果\r\n",
    "        :param main_program:\r\n",
    "        :param startup_program:\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        with fluid.program_guard(main_program, startup_program):\r\n",
    "            x = fluid.layers.data(name='x', shape=[self.batch_size, self.num_steps, 1], dtype='int64', append_batch_size=False)\r\n",
    "            y = fluid.layers.data(name='y', shape=[self.batch_size * self.num_steps, 1], dtype='int64', append_batch_size=False)\r\n",
    "            # 使用unique_name.guard创建变量空间，和train共享参数\r\n",
    "            with fluid.unique_name.guard():\r\n",
    "                proj, last_hidden, last_cell = self.forward(x, self.batch_size)\r\n",
    "                loss = self.get_loss(proj, y)\r\n",
    "\r\n",
    "                # 不知道有什么用，先写上\r\n",
    "                #loss.persistable = True\r\n",
    "                #proj.persistable = True\r\n",
    "                #last_cell.persistable = True\r\n",
    "                #last_hidden.persistable = True\r\n",
    "\r\n",
    "                return loss, proj, last_hidden, last_cell\r\n",
    "\r\n",
    "    def get_loss(self, proj, y):\r\n",
    "        loss = fluid.layers.softmax_with_cross_entropy(logits=proj, label=y, soft_label=False)\r\n",
    "        loss = fluid.layers.reshape(loss, shape=[-1, self.num_steps])\r\n",
    "        loss = fluid.layers.reduce_mean(loss, dim=[0])\r\n",
    "        loss = fluid.layers.reduce_sum(loss)\r\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.3.1 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\r\n",
    "from collections import Counter\r\n",
    "import itertools\r\n",
    "\r\n",
    "def clean_str(string):\r\n",
    "    \"\"\"\r\n",
    "    将文本中的特定字符串做修改和替换处理\r\n",
    "    :param string:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "    string = re.sub(r\"[^A-Za-z0-9:(),!?\\'\\`]\", \" \", string)\r\n",
    "    string = re.sub(r\":\", \" : \", string)\r\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\r\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\r\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\r\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\r\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\r\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\r\n",
    "    string = re.sub(r\",\", \" , \", string)\r\n",
    "    string = re.sub(r\"!\", \" ! \", string)\r\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\r\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\r\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\r\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\r\n",
    "    return string.strip().lower()\r\n",
    "\r\n",
    "\r\n",
    "def build_vocab(sentences, EOS='</eos>'):\r\n",
    "    \"\"\"\r\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\r\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\r\n",
    "    \"\"\"\r\n",
    "    # Build vocabulary\r\n",
    "    word_counts = Counter(itertools.chain(*sentences))\r\n",
    "    # Mapping from index to word\r\n",
    "    # vocabulary_inv=['<PAD/>', 'the', ....]\r\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\r\n",
    "    # Mapping from word to index\r\n",
    "    # vocabulary = {'<PAD/>': 0, 'the': 1, ',': 2, 'a': 3, 'and': 4, ..}\r\n",
    "    vocabulary = {x: i+1 for i, x in enumerate(vocabulary_inv)}\r\n",
    "    vocabulary[EOS] = 0\r\n",
    "    return [vocabulary, vocabulary_inv]\r\n",
    "\r\n",
    "\r\n",
    "def file_to_ids(src_file, src_vocab):\r\n",
    "    \"\"\"\r\n",
    "    将文章单词序列转化成词典id序列\r\n",
    "    :param src_file:\r\n",
    "    :param src_vocab:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "    src_data = []\r\n",
    "    for line in src_file:\r\n",
    "        ids = [src_vocab[w] for w in line if w in src_vocab]\r\n",
    "        src_data += ids + [0]\r\n",
    "    return src_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_text = list(open(\"text8\", \"r\").readlines())\r\n",
    "x_text = [clean_str(sent) for sent in x_text]\r\n",
    "vocabulary, vocabulary_inv = build_vocab(x_text)\r\n",
    "x_text = file_to_ids(x_text, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.3.2 训练 \n",
    "训练的结果与数据的吻合程度用困惑度指标ppl来衡量，参考了[基于LSTM的语言模型实现](https://aistudio.baidu.com/aistudio/projectdetail/592038)。ppl的值即e为底，平均交叉熵损失为指数的幂指数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_test = LSTM(vocab_size=len(vocabulary), num_layers=1, hidden_size=100, num_steps=20, use_gpu=True, dropout_prob=0.2, init_scale=0.1, lr=0.01)\r\n",
    "lstm_test.train(x_text[:1000000], epochs=3, batch_size=32, log_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.3.3 测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ppl = lstm_test.evaluate(x_text[1000000:1005000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
  