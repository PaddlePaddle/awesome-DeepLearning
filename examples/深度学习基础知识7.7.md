深度学习2

1损失函数方法补充：

①均方误差损失定义：

均方差损失函数常用在最小二乘法中。它的思想是使得各个训练点到最优拟合线的距离最小（平方和最小）。均方差损失函数也是我们最常见的损失函数了，相信大很熟悉了，我们用神经网络中激活函数的形式表达一下，定义如下：

![[公式]](https://www.zhihu.com/equation?tex=J%28w%2Cb%29%3D%5Cfrac%7B1%7D%7B2N%7D%5Csum_%7B1%7D%5E%7BN%7D%7B%7C%7Cy-a%7C%7C%5E%7B2%7D%7D)

其中， ![[公式]](https://www.zhihu.com/equation?tex=a%3Df%28z%29%3Df%28w%C2%B7x%2Bb%29) ：x是输入、w和b是网络的参数、 ![[公式]](https://www.zhihu.com/equation?tex=f%28%C2%B7%29) 是激活函数。

②交叉熵损失(Cross Entropy，CE)

多用于分类的损失函数。

交叉熵损失定义：

交叉熵损失的计算分为两个部分。

（1）softmax多分类器：

交叉熵损失是基于softmax计算来的，softmax将网络最后输出z通过指数转变成概率形式。首先看一下softmax计算公式： ![[公式]](https://www.zhihu.com/equation?tex=p_%7Bi%7D%3D%5Cfrac%7Be%5E%7Bz_%7Bi%7D%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bk%7D%7Be%7D%5E%7Bz_%7Bj%7D%7D%7D)

其中， 分子![[公式]](https://www.zhihu.com/equation?tex=e%5E%7Bz_%7Bi%7D%7D) 是要计算的类别 ![[公式]](https://www.zhihu.com/equation?tex=i) 的网络输出的指数；分母是所有类别网络输出的指数和，共k个类别。这样就得到了类别i的输出概率 ![[公式]](https://www.zhihu.com/equation?tex=p_%7Bi%7D) 。

（2）交叉熵损失：

公式定义如下： ![[公式]](https://www.zhihu.com/equation?tex=J%3D-%5Cfrac%7B1%7D%7BN%7D%5Csum_%7B1%7D%5E%7BN%7D%7B%5Csum_%7Bi%3D1%7D%5E%7Bk%7D%7By_%7Bi%7D%C2%B7log%28p_%7Bi%7D%29%7D%7D)

其中， ![[公式]](https://www.zhihu.com/equation?tex=y_%7Bi%7D) 是类别 ![[公式]](https://www.zhihu.com/equation?tex=i) 的真实标签；![[公式]](https://www.zhihu.com/equation?tex=p_%7Bi%7D)是上面softmax计算出的类别 ![[公式]](https://www.zhihu.com/equation?tex=i) 的概率值；k是类别数，N是样本总数。

③SVM合页损失

定义：

合页损失函数想让正确分类的“得分”比其他错误分类的“得分”高出至少一个边界值![[公式]](https://www.zhihu.com/equation?tex=%5CDelta)。

如果正确分类的得分与错误分类的得分差值比边界值还要高，就会认为损失值是0；如果没有

就要计算损失了。看一下计算公式和示意图：

![[公式]](https://www.zhihu.com/equation?tex=J%3D-%5Cfrac%7B1%7D%7BN%7D%5Csum_%7B1%7D%5E%7BN%7D%7B%5Csum_%7B1%7D%5E%7Bk%7D%7Bmax%280%2C%5CDelta-%28z_%7Bcorrect%7D-z_%7Bother%7D%29%29%7D%7D)

其中， ![[公式]](https://www.zhihu.com/equation?tex=z_%7Bcorrect%7D) 是正确分类的得分、 ![[公式]](https://www.zhihu.com/equation?tex=z_%7Bother%7D) 是其他错误分类的得分； ![[公式]](https://www.zhihu.com/equation?tex=%5CDelta) 是指想要正确类别的分类得分比其他错误分类类别的得分要高且至少高出 ![[公式]](https://www.zhihu.com/equation?tex=%5CDelta) 的边界值；k是类别数（对应other错误类别数），N是样本总数。

示意图如下：

![img](https://pic3.zhimg.com/80/v2-31bc50376905f1219479407d5bc45c52_720w.jpg)

④Smooth L1损失：

Smooth L1损失函数是在Fast R-CNN中被提出，主要目的是为了防止梯度爆炸。

对于目标检测中的回归问题，最初大多采用均方误差损失 ![[公式]](https://www.zhihu.com/equation?tex=%7C%7Cy-f%28z%29%7C%7C%5E%7B2%7D) ，这样反向传播对w或者b求导时仍存在 ![[公式]](https://www.zhihu.com/equation?tex=y-f%28z%29) 。那么当预测值和目标值相差很大时，就容易造成梯度爆炸。

所以我们将 ![[公式]](https://www.zhihu.com/equation?tex=%7C%7Cy-f%28z%29%7C%7C%5E%7B2%7D) 这种均方误差形式，转变成 ![[公式]](https://www.zhihu.com/equation?tex=Smooth_%7BL1%7D%28y-f%28z%29%29) 这种形式，其中：

![img](https://pic1.zhimg.com/80/v2-3c8e2374fdb43666560924189362f9c4_720w.jpg)

通过上式可以看出：

①当 ![[公式]](https://www.zhihu.com/equation?tex=%7Cy-f%28z%29%7C%3C1) 时，即预测值和目标值相差小于1，不易造成梯度爆炸，此时还原成均方误差损失形式并给一个0.5的平滑系数，即 ![[公式]](https://www.zhihu.com/equation?tex=0.5%7C%7Cy-f%28z%29%7C%7C%5E%7B2%7D) ；

②当 ![[公式]](https://www.zhihu.com/equation?tex=%7Cy-f%28z%29%7C%5Cgeq1) 时，即预测值和目标值相差大于等于1，易造成梯度爆炸，此时降低损失次幂数，变成 ![[公式]](https://www.zhihu.com/equation?tex=%7Cy-f%28z%29%7C-0.5) ，这时候反向传播求导时候就不存在 ![[公式]](https://www.zhihu.com/equation?tex=y-f%28z%29) 这一项了，从而防止了梯度爆炸。

2损失函数python代码实现：

①均方误差损失代码实现

import numpy as np

y_hat = np.dot(X, w) + b

loss = np.sum((y_hat - y) ** 2) / num_train

dw = np.dot(X.T, (y_hat - y)) / num_train
db = np.sum((y_hat - y)) / num_train

②softmax+交叉熵损失函数代码实现：

```python
'''
输入层：（z1,z2,z3）
输出层：（y1,y2,y3）
label：（t1,t2,t3）
反向传播结果：（y1-t1,y2-t2,y3-t3）
'''
class SoftmaxWithEntropyLoss(object):
    def __init__(self):
        self.loss = None
        self.y = None
        self.t = None
    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
    def backward(self, dout):
        batch_size = self.t.shape[0]
        if self.t.size == self.y.size:
            dx = (self.y - self.t) / batch_size
        else:
            dx = self.y.copy()
            dx[np.arange(batch_size), self.t] -= 1
            dx = dx / batch_size
        return dx
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    if t.size == y.size:
        t = t.argmax(axis=1)
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```

3池化方法补充：

①一般池化

我们定义池化窗口的大小为sizeX，即下图中红色正方形的边长，定义两个相邻池化窗口的水平位移/竖直位移为stride。一般池化由于每一池化窗口都是不重复的，所以sizeX=stride。

 ![img](https://img-blog.csdn.net/20150105213214237?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZGFuaWVsamlhbmZlbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

最常见的池化操作为平均池化mean pooling和最大池化max pooling：

平均池化：计算图像区域的平均值作为该区域池化后的值。

最大池化：选图像区域的最大值作为该区域池化后的值。

②重叠池化

重叠池化正如其名字所说的，相邻池化窗口之间会有重叠区域，此时sizeX>stride。

论文中[2]中，作者使用了重叠池化，其他的设置都不变的情况下， top-1和top-5 的错误率分别减少了0.4% 和0.3%。

 ③空金字塔池化

空间金字塔池化可以把任何尺度的图像的卷积特征转化成相同维度，这不仅可以让CNN处理任意尺度的图像，还能避免cropping和warping操作，导致一些信息的丢失，具有非常重要的意义。

一般的CNN都需要输入图像的大小是固定的，这是因为全连接层的输入需要固定输入维度，但在卷积操作是没有对图像尺度有限制，所有作者提出了空间金字塔池化，先让图像进行卷积操作，然后转化成维度相同的特征输入到全连接层，这个可以把CNN扩展到任意大小的图像。

4数据增强方法修改及补充：

数据增强可以分为，有监督的数据增强和无监督的数据增强方法。其中有监督的数据增强又可以分为单样本数据增强和多样本数据增强方法，无监督的数据增强分为生成新的数据和学习增强策略两个方向。

①有监督的数据增强

有监督数据增强，即采用预设的数据变换规则，在已有数据的基础上进行数据的扩增，包含单样本数据增强和多样本数据增强，其中单样本又包括几何操作类，颜色变换类。

单样本数据增强

所谓单样本数据增强，即增强一个样本的时候，全部围绕着该样本本身进行操作，包括几何变换类，颜色变换类等。

(1) 几何变换类

几何变换类即对图像进行几何变换，包括翻转，旋转，裁剪，变形，缩放等各类操作，下面展示其中的若干个操作。

(2) 颜色变换类

上面的几何变换类操作，没有改变图像本身的内容，它可能是选择了图像的一部分或者对像素进行了重分布。如果要改变图像本身的内容，就属于颜色变换类的数据增强了，常见的包括噪声、模糊、颜色变换、擦除、填充等等。

多样本数据增强

不同于单样本数据增强，多样本数据增强方法利用多个样本来产生新的样本，下面介绍几种方法。

(1) SMOTE

SMOTE即Synthetic Minority Over-sampling Technique方法，它是通过人工合成新样本来处理样本不平衡问题，从而提升分类器性能。

(2) SamplePairing

SamplePairing方法的原理非常简单，从训练集中随机抽取两张图片分别经过基础数据增强操作(如随机翻转等)处理后经像素以取平均值的形式叠加合成一个新的样本，标签为原样本标签中的一种。这两张图片甚至不限制为同一类别，这种方法对于医学图像比较有效。

(3) mixup

mixup是Facebook人工智能研究院和MIT在“Beyond Empirical Risk Minimization”中提出的基于邻域风险最小化原则的数据增强方法，它使用线性插值得到新样本数据。

②无监督的数据增强

无监督的数据增强方法包括两类：

(1) 通过模型学习数据的分布，随机生成与训练数据集分布一致的图片，代表方法GAN。

(2) 通过模型，学习出适合当前任务的数据增强方法，代表方法AutoAugment。

GAN

关于GAN(generative adversarial networks)，我们已经说的太多了。它包含两个网络，一个是生成网络，一个是对抗网络，基本原理如下：

(1) G是一个生成图片的网络，它接收随机的噪声z，通过噪声生成图片，记做G(z) 。

(2) D是一个判别网络，判别一张图片是不是“真实的”，即是真实的图片，还是由G生成的图片。

Autoaugmentation

AutoAugment是Google提出的自动选择最优数据增强方案的研究，这是无监督数据增强的重要研究方向。它的基本思路是使用增强学习从数据本身寻找最佳图像变换策略，对于不同的任务学习不同的增强方法，流程如下：

(1) 准备16个常用的数据增强操作。

(2) 从16个中选择5个操作，随机产生使用该操作的概率和相应的幅度，将其称为一个sub-policy，一共产生5个sub-polices。

(3) 对训练过程中每一个batch的图片，随机采用5个sub-polices操作中的一种。

(4) 通过模型在验证集上的泛化能力来反馈，使用的优化方法是增强学习方法。

(5) 经过80~100个epoch后网络开始学习到有效的sub-policies。

(6) 之后串接这5个sub-policies，然后再进行最后的训练。

总的来说，就是学习已有数据增强的组合策略，对于门牌数字识别等任务，研究表明剪切和平移等几何变换能够获得最佳效果。

5图像分类方法综述：

图像分类是根据图像的语义信息对不同类别图像进行区分，是计算机视觉的核心，是物体检测、图像分割、物体跟踪、行为分析、人脸识别等其他高层次视觉任务的基础。图像分类在许多领域都有着广泛的应用，如：安防领域的人脸识别和智能视频分析等，交通领域的交通场景识别，互联网领域基于内容的图像检索和相册自动归类，医学领域的图像识别等。

涵盖如下卷积神经网络：

- LeNet：Yann LeCun等人于1998年第一次将卷积神经网络应用到图像分类任务上[1]，在手写数字识别任务上取得了巨大成功。
- AlexNet：Alex Krizhevsky等人在2012年提出了AlexNet[2], 并应用在大尺寸图片数据集ImageNet上，获得了2012年ImageNet比赛冠军(ImageNet Large Scale Visual Recognition Challenge，ILSVRC）。
- VGG：Simonyan和Zisserman于2014年提出了VGG网络结构[3]，是当前最流行的卷积神经网络之一，由于其结构简单、应用性极强而深受广大研究者欢迎。
- GoogLeNet：Christian Szegedy等人在2014提出了GoogLeNet[4]，并取得了2014年ImageNet比赛冠军。
- ResNet：Kaiming He等人在2015年提出了ResNet[5]，通过引入残差模块加深网络层数，在ImagNet数据集上的错误率降低到3.6%，超越了人眼识别水平。ResNet的设计思想深刻地影响了后来的深度神经网络的设计。

传统方法：图像预处理、特征提取和分类器设计，每一种都有多种方法。

深度学习方法：图像预处理和图像识别，其中图像识别中的骨干网络实现特征提取的功能，后面的softmax等实现图像分类
