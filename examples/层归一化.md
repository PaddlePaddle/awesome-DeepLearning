一、概念： Batch Normalization 的过程就是将输入的数据进行归一化，使其服从均值为 0，方差为 1 的分布。这样使得输入具有相同的均值和方差，固定了每一层的输入分布，从而加速网络的收敛。

二、算法流程：
![](https://ai-studio-static-online.cdn.bcebos.com/79b11c60092e4d52858936e05cbd71caa6cb7063a97d4c5a80fb1fc005a4265a)

​其中x为输入数据，μ为对一个 batch 的输入数据取平均值，σ为方差，xi是归一化之后的输入数据，其满足均值为0，方差为1，是一个标准正态分布。其中ϵ是一个很小的正数，仅仅是为了保证分母不为零。第四个方程是在xi基础上做了一个γ倍的缩放 和β的偏移。而这里的 γ和β是超参数，也就是需要通过训练来的学习。

三、作用
1.缓解梯度消失，可以增大 learning rate，使训练更快收敛。
2. 简化调参，网络更稳定。在调参时，学习率调得过大容易出现震荡于不收敛，而 BN 抑制了参数微小变化随网络加深而被放大的问题，因此对于参数变化的适应能力更强。
3.更不容易过拟合， 因此可以替代 dropout，或者少用 dropout，可以减少 L2 正则化的权重，同时可以减少 argument 的操作，可以不用太操心 initialization 的操作。

四、应用场景
应用场景：Conv -> ReLU -> Batch Norm -> Pooling
