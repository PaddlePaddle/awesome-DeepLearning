深度学习发展历史
第一代神经网络（1958~1969）
最早的神经网络的思想起源于1943年的MCP人工神经元模型，当时是希望能够用计算机来模拟人的神经元反应的过程，该模型将神经元简化为了三个过程：输入信号线性加权，求和，非线性激活（阈值法）。

第一次将MCP用于机器学习（分类）的当属1958年Rosenblatt发明的感知器（perceptron）算法。该算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年，该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。

然而学科发展的历史不总是一帆风顺的。

1969年，美国数学家及人工智能先驱Minsky在其著作中证明了感知器本质上是一种线性模型，只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了近20年的停滞。

第二代神经网络（1986~1998）
第一次打破非线性诅咒的当属现代DL大牛Hinton，其在1986年发明了适用于多层感知器（MLP）的BP算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。

1989年，Robert Hecht-Nielsen证明了MLP的万能逼近定理，即对于任何闭区间内的一个连续函数f，都可以用含有一个隐含层的BP网络来逼近该定理的发现极大的鼓舞了神经网络的研究人员。

也是在1989年，LeCun发明了卷积神经网络-LeNet，并将其用于数字识别，且取得了较好的成绩，不过当时并没有引起足够的注意。

值得强调的是在1989年以后由于没有特别突出的方法被提出，且NN一直缺少相应的严格的数学理论支持，神经网络的热潮渐渐冷淡下去。冰点来自于1991年，BP算法被指出存在梯度消失问题，即在误差梯度后向传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该发现对此时的NN发展雪上加霜。

1997年，LSTM模型被发明，尽管该模型在序列建模上的特性非常突出，但由于正处于NN的下坡期，也没有引起足够的重视。

统计学习方法的春天（1986~2006）
1986年，决策树方法被提出，很快ID3，ID4，CART等改进的决策树方法相继出现，到目前仍然是非常常用的一种机器学习方法。该方法也是符号学习方法的代表。
1995年，线性SVM被统计学家Vapnik提出。该方法的特点有两个：由非常完美的数学理论推导而来（统计学与凸优化等），符合人的直观感受（最大间隔）。不过，最重要的还是该方法在线性分类的问题上取得了当时最好的成绩。
1997年，AdaBoost被提出，该方法是PAC（Probably Approximately Correct）理论在机器学习实践上的代表，也催生了集成方法这一类。该方法通过一系列的弱分类器集成，达到强分类器的效果。
2000年，KernelSVM被提出，核化的SVM通过一种巧妙的方式将原空间线性不可分的问题，通过Kernel映射成高维空间的线性可分问题，成功解决了非线性分类的问题，且分类效果非常好。至此也更加终结了NN时代。
2001年，随机森林被提出，这是集成方法的另一代表，该方法的理论扎实，比AdaBoost更好的抑制过拟合问题，实际效果也非常不错。
2001年，一种新的统一框架-图模型被提出，该方法试图统一机器学习混乱的方法，如朴素贝叶斯，SVM，隐马尔可夫模型等，为各种学习方法提供一个统一的描述框架。

第三代神经网络-DL（2006-至今）
该阶段又分为两个时期：快速发展期（2006~2012）与爆发期（2012~至今）

快速发展期（2006~2012）
2006年，DL元年。是年，Hinton提出了深层网络训练中梯度消失问题的解决方案：无监督预训练对权值进行初始化+有监督训练微调。其主要思想是先通过自学习的方法学习到训练数据的结构（自动编码器），然后在该结构上进行有监督训练微调。但是由于没有特别有效的实验验证，该论文并没有引起重视。

2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。

2011年，微软首次将DL应用在语音识别上，取得了重大突破。

爆发期（2012~至今）
2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。
AlexNet的创新点：
（1）首次采用ReLU激活函数，极大增大收敛速度且从根本上解决了梯度消失问题；（2）由于ReLU方法可以很好抑制梯度消失问题，AlexNet抛弃了“预训练+微调”的方法，完全采用有监督训练。也正因为如此，DL的主流学习方法也因此变为了纯粹的有监督学习；（3）扩展了LeNet5结构，添加Dropout层减小过拟合，LRN层增强泛化能力/减小过拟合；（4）首次采用GPU对计算进行加速；

2013,2014,2015年，通过ImageNet图像识别比赛，DL的网络结构，训练方法，GPU硬件的不断进步，促使其在其他领域也在不断的征服战场

2015年，Hinton，LeCun，Bengio论证了局部极值问题对于DL的影响，结果是Loss的局部极值问题对于深层网络来说影响可以忽略。该论断也消除了笼罩在神经网络上的局部极值问题的阴霾。具体原因是深层网络虽然局部极值非常多，但是通过DL的BatchGradientDescent优化方法很难陷进去，而且就算陷进去，其局部极小值点与全局极小值点也是非常接近，但是浅层网络却不然，其拥有较少的局部极小值点，但是却很容易陷进去，且这些局部极小值点与全局极小值点相差较大。论述原文其实没有证明，只是简单叙述，严密论证是猜的。。。

2015，DeepResidualNet发明。分层预训练，ReLU和BatchNormalization都是为了解决深度神经网络优化时的梯度消失或者爆炸问题。但是在对更深层的神经网络进行优化时，又出现了新的Degradation问题，即”通常来说，如果在VGG16后面加上若干个单位映射，网络的输出特性将和VGG16一样，这说明更深次的网络其潜在的分类性能只可能>=VGG16的性能，不可能变坏，然而实际效果却是只是简单的加深VGG16的话，分类性能会下降（不考虑模型过拟合问题）“Residual网络认为这说明DL网络在学习单位映射方面有困难，因此设计了一个对于单位映射（或接近单位映射）有较强学习能力的DL网络，极大的增强了DL网络的表达能力。此方法能够轻松的训练高达150层的网络。



1.三者关系：
人工智能、机器学习和深度学习覆盖的技术范畴是逐层递减的
人工智能是最宽泛的概念
机器学习是当前比较有效的一种实现人工智能的方式
深度学习是机器学习算法中最热门的一个分支，替代了大多数传统
机器学习算法
2.人工智能：
研发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统
的一门新的技术科学
3.关键区别：
人工智能是一个大杂烩，机器学习（监督学习）则有更加明确的指代
![](https://ai-studio-static-online.cdn.bcebos.com/1f256c2a39584b27aff0b6a14591f1d8ceb213e539054eb0a4b954de9753710a)
下面一张图能更加细分其关系：
![](https://ai-studio-static-online.cdn.bcebos.com/9c79d869cc264db295940feabed85d467224ce898fe54f0bbacc38b371485dd7)


神经元 神经网络中每个节点称为神经元,由加权和、非线性变换（激活函数）组成
![](https://ai-studio-static-online.cdn.bcebos.com/95bddb4ebef74d069b82388f6e62c569f66e5995368f46ac9508fbf7aa593d23)
单层感知器由一个线性组合器和一个二值阈值元件组成。输入向量的各个分量先与权值相乘，然后在线性组合器中进行叠加，得到一个标量结果，其输出是线性组合结果经过一个二值阈值函数。二值阈值元件通常是一个上升函数，典型功能是非负数映射为1，负数映射为0或负一。
输入是一个N维向量 x=[x1,x2,...,xn]，其中每一个分量对应一个权值wi，隐含层输出叠加为一个标量值：![](https://ai-studio-static-online.cdn.bcebos.com/eb9d502f3bce4c8fb58f6071a23a544f97d6ad5a3521402e940691fba4335e7c)
随后在二值阈值元件中对得到的v值进行判断，产生二值输出：![](https://ai-studio-static-online.cdn.bcebos.com/f8b7ed1ae40741da8948c46c53b209f784fb22bb57b5441ebd380fd715c6a409)
可以将数据分为两类。实际应用中，还加入偏置，值恒为1，权值为b。这时，y输出为![](https://ai-studio-static-online.cdn.bcebos.com/60f43f16af554a7f81bd13638be645fdb4510e0578ca4f5bbeb4197f3a29504e)把偏置值当作特殊权值
单层感知器结构图：![](https://ai-studio-static-online.cdn.bcebos.com/f4e9a1724df146a8adea3b672cee191e2cb080f3f49e491888e1a351feb29286)
单层感知器进行模式识别的超平面由下式决定：![](https://ai-studio-static-online.cdn.bcebos.com/5d882ec787a74091a098de883f0015b154a91eb1029340c9a98eee129d15cc63)
当维数N=2时，输入向量可以表示为平面直角坐标系中的一个点。此时分类超平面是一条直线:![](https://ai-studio-static-online.cdn.bcebos.com/f7aa8991a15d4088b2fee3b3968e5b3214047c9eee0e42d5a162cf5f4af3cec4)
这样就可以将点沿直线划分成两类。


多层感知机(multi-layer perceptron)/神经网络(neural network)
1. 相比于感知器，引入了隐层(hidden layer)概念；
    隐层, 不包括输入层和输出层，在输入层和输出层中间的所有N层神经元就称作隐层！通常输入层不算作神经网络的一部分，
对于有一层隐层的神经网络，叫做单隐层神经网络或二层感知机；对于第L个隐层，通常有以下一些特性：
    a) L层的每一个神经元与 L-1 层的每一个神经元的输出相连；
    b) L层的每一个神经元互相没有连接；

2. 引入了新的非线性激活函数(sigmoid/tanh等)
3. 反向传播算法(back propagation)
4. 优化算法(梯度下降，随机梯度下降，mini-batch)
![](https://ai-studio-static-online.cdn.bcebos.com/748341c968ab4f9986a45b1daaecdf4d20ce41c13bdf40809c86acc8e4f0a48a)


前向传播：通过输入层输入，一路向前，通过输出层输出的一个结果。如图指的是1 、 x1、x2、xn、与权重（weights）相乘，并且加上偏置值b0，然后进行总的求和，同时通过激活函数激活之后算出结果。这个过程就是前向传播。![](https://ai-studio-static-online.cdn.bcebos.com/7778d5b093d54dc6ab7ece8d5f2ea45e8e28f6a9447f4946823958be8db68df1)


反向传播：通过输出反向更新权重的过程。具体的说输出位置会产生一个模型的输出，通过这个输出以及原数据计算一个差值。将前向计算过程反过来计算。通过差值和学习率更新权重。
![](https://ai-studio-static-online.cdn.bcebos.com/04beac49a60b4eb5b0485c3dde9a30a38424dc8b72f8444ea574c48db052c043)


请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>
Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. 
