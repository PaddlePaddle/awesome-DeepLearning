# 一、CNN-DSSM知识点补充
## 概念
DSSM（Deep Structured Semantic Models）的原理是通过搜索引擎里 Query 和 Title 的海量的点击曝光日志，用 DNN 把 Query 和 Title 表达为低纬语义向量，并通过 cosine 距离来计算两个语义向量的距离，最终训练出语义相似度模型。该模型既可以用来预测两个句子的语义相似度，又可以获得某句子的低纬语义向量表达。

## 模型：
DSSM 从下往上可以分为三层结构：输入层、表示层、匹配层

![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/864857e027344cbdcee38029df80bab46ba9ff7c/examples/D1.jpg)

（1）输入层
 输入层把句子映射到一个向量空间里并输入到 DNN 中，其中对英语和中文的处理有所不同。

对于英语，英文的输入层处理方式是通过word hashing。举个例子，假设用 letter-trigams 来切分单词（3 个字母为一组，#表示开始和结束符），boy 这个单词会被切为 #-b-o, b-o-y, o-y-#。
这样做的好处有两个：首先是压缩空间，50 万个词的 one-hot 向量空间可以通过 letter-trigram 压缩为一个 3 万维的向量空间。其次是增强范化能力，三个字母的表达往往能代表英文中的前缀和后缀，而前缀后缀往往具有通用的语义。
这里之所以用 3 个字母的切分粒度，是综合考虑了向量空间和单词冲突：
以 50 万个单词的词库为例，2 个字母的切分粒度的单词冲突为 1192（冲突的定义：至少有两个单词的 letter-bigram 向量完全相同），而 3 个字母的单词冲突降为 22 效果很好，且转化后的向量空间 3 万维不是很大，综合考虑选择 3 个字母的切分粒度。
 
 对于中文而言，中文的输入层处理方式与英文有很大不同，首先中文分词是个让所有 NLP 从业者头疼的事情，即便业界号称能做到 95%左右的分词准确性，但分词结果极为不可控，往往会在分词阶段引入误差。所以这里我们不分词，而是仿照英文的处理方式，对应到中文的最小粒度就是单字了。（曾经有人用偏旁部首切的，感兴趣的朋友可以试试）
 
 由于常用的单字为 1.5 万左右，而常用的双字大约到百万级别了，所以这里出于向量空间的考虑，采用字向量（one-hot）作为输入，向量空间约为 1.5 万维。

（2）DSSM 的表示层采用 BOW（Bag of words）的方式，相当于把字向量的位置信息抛弃了，整个句子里的词都放在一个袋子里了，不分先后顺序。当然这样做会有问题，我们先为 CNN-DSSM 和 LSTM-DSSM 埋下一个伏笔。
紧接着是一个含有多个隐层的 DNN，如下图所示：

![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/864857e027344cbdcee38029df80bab46ba9ff7c/examples/D2.jpg)


用 Wi 表示第 i 层的权值矩阵，bi 表示第 i 层的 bias 项。则第一隐层向量 l1（300 维），第 i 个隐层向量 li（300 维），输出向量 y（128 维）可以分别表示为：

![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/864857e027344cbdcee38029df80bab46ba9ff7c/examples/D3.jpg)

用 tanh 作为隐层和输出层的激活函数：

![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/864857e027344cbdcee38029df80bab46ba9ff7c/examples/D4.jpg)

最终输出一个 128 维的低纬语义向量。

（3）匹配层

Query 和 Doc 的语义相似性可以用这两个语义向量(128 维) 的 cosine 距离来表示：

![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/864857e027344cbdcee38029df80bab46ba9ff7c/examples/D5.jpg)

通过softmax 函数可以把Query 与正样本 Doc 的语义相似性转化为一个后验概率：

![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/864857e027344cbdcee38029df80bab46ba9ff7c/examples/D6.jpg)

其中 r 为 softmax 的平滑因子，D 为 Query 下的正样本，D-为 Query 下的负样本（采取随机负采样），D 为 Query 下的整个样本空间。

在训练阶段，通过极大似然估计，我们最小化损失函数：

![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/864857e027344cbdcee38029df80bab46ba9ff7c/examples/D7.jpg)

残差会在表示层的 DNN 中反向传播，最终通过随机梯度下降（SGD）使模型收敛，得到各网络层的参数{Wi,bi}。

## 优缺点

优点：DSSM 用字向量作为输入既可以减少切词的依赖，又可以提高模型的范化能力，因为每个汉字所能表达的语义是可以复用的。另一方面，传统的输入层是用 Embedding 的方式（如 Word2Vec 的词向量）或者主题模型的方式（如 LDA 的主题向量）来直接做词的映射，再把各个词的向量累加或者拼接起来，由于 Word2Vec 和 LDA 都是无监督的训练，这样会给整个模型引入误差，DSSM 采用统一的有监督训练，不需要在中间过程做无监督模型的映射，因此精准度会比较高。

缺点：上文提到 DSSM 采用词袋模型（BOW），因此丧失了语序信息和上下文信息。另一方面，DSSM 采用弱监督、端到端的模型，预测结果不可控。


# LSTM-DSSM

针对 CNN-DSSM 无法捕获较远距离上下文特征的缺点，有人提出了用LSTM-DSSM（Long-Short-Term Memory）来解决该问题。

LSTM（Long-Short-Term Memory）是一种 RNN 特殊的类型，可以学习长期依赖信息。我们分别来介绍它最重要的几个模块：

![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/864857e027344cbdcee38029df80bab46ba9ff7c/examples/D8.jpg)

（0）细胞状态

细胞状态这条线可以理解成是一条信息的传送带，只有一些少量的线性交互。在上面流动可以保持信息的不变性。

![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/864857e027344cbdcee38029df80bab46ba9ff7c/examples/D9.jpg)

（1）遗忘门

遗忘门由 Gers 提出，它用来控制细胞状态 cell 有哪些信息可以通过，继续往下传递。如下图所示，上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（遗忘门）产生一个从 0 到 1 的数值 ft，然后与细胞状态 C(t-1) 相乘，最终决定有多少细胞状态可以继续往后传递。

![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/864857e027344cbdcee38029df80bab46ba9ff7c/examples/D10.jpg)

（2）输入门

输入门决定要新增什么信息到细胞状态，这里包含两部分：一个 sigmoid 输入门和一个 tanh 函数。sigmoid 决定输入的信号控制，tanh 决定输入什么内容。如下图所示，上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（输入门）产生一个从 0 到 1 的数值 it，同样的信息经过 tanh 网络做非线性变换得到结果 Ct，sigmoid 的结果和 tanh 的结果相乘，最终决定有哪些信息可以输入到细胞状态里。

![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/864857e027344cbdcee38029df80bab46ba9ff7c/examples/D11.jpg)

（3）输出门

输出门决定从细胞状态要输出什么信息，这里也包含两部分：一个 sigmoid 输出门和一个 tanh 函数。sigmoid 决定输出的信号控制，tanh 决定输出什么内容。如下图所示，上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（输出门）产生一个从 0 到 1 的数值 Ot，细胞状态 Ct 经过 tanh 网络做非线性变换，得到结果再与 sigmoid 的结果 Ot 相乘，最终决定有哪些信息可以输出，输出的结果 ht 会作为这个细胞的输出，也会作为传递个下一个细胞。

![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/864857e027344cbdcee38029df80bab46ba9ff7c/examples/D12.jpg)

## LSTM-DSSM

LSTM-DSSM 其实用的是 LSTM 的一个变种——加入了peephole的 LSTM。如下图所示：

![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/864857e027344cbdcee38029df80bab46ba9ff7c/examples/D13.jpg)

这里三条黑线就是所谓的 peephole，传统的 LSTM 中遗忘门、输入门和输出门只用了 h(t-1) 和 xt 来控制门缝的大小，peephole 的意思是说不但要考虑 h(t-1) 和 xt，也要考虑 Ct-1 和 Ct，其中遗忘门和输入门考虑了 Ct-1，而输出门考虑了 Ct。总体来说需要考虑的信息更丰富了。

一个 LSTM-DSSM 整体的网络结构如下;

![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/864857e027344cbdcee38029df80bab46ba9ff7c/examples/D14.jpg)

红色的部分可以清晰的看到残差传递的方向。

## 缺点：

1. DSSM 是端到端的模型，虽然省去了人工特征转化、特征工程和特征组合，但端到端的模型有个问题就是效果不可控。对于一些要保证较高的准确率的场景，用有监督人工标注的 query 分类作为打底，再结合无监督的 word2vec、LDA 等进行语义特征的向量化，显然比较可控（至少 query 分类的准确率可以达到 95%以上）。

2. DSSM 是弱监督模型，因为引擎的点击曝光日志里 Query 和 Title 的语义信息比较弱。举个例子，搜索引擎第一页的信息往往都是 Query 的包含匹配，笔者统计过，完全的语义匹配只有不到 2%。这就意味着几乎所有的标题里都包含用户 Query 里的关键词，而仅用点击和曝光就能作为正负样例的判断？显然不太靠谱，因为大部分的用户进行点击时越靠前的点击的概率越大，而引擎的排序又是由 pCTR、CVR、CPC 等多种因素决定的。从这种非常弱的信号里提取出语义的相似性或者差别，那就需要有海量的训练样本。DSSM 论文中提到，实验的训练样本超过 1 亿。笔者和同事也亲测过，用传统 CTR 预估模型千万级的样本量来训练，模型无法收敛。可是这样海量的训练样本，恐怕只有搜索引擎才有吧？普通的搜索业务 query 有上千万，可资源顶多只有几百万，像论文中说需要挑出点击和曝光置信度比较高且资源热度也比较高的作为训练样本，这样就过滤了 80%的长尾 query 和 Title 结果对，所以也只有搜索引擎才有这样的训练语料了吧。另一方面，超过 1 亿的训练样本作为输入，用深度学习模型做训练，需要大型的 GPU 集群，这个对于很多业务来说也是不具备的条件。

# MMoE多任务学习
## 1 Shared-Bottom model
先简单结下 shared-bottom 模型，ESMM 模型就是基于 shared-bottom 的多任务模型。这篇文章把该框架作为多任务模型的 baseline，其结构如下图所示：
 
![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/0cbcbe67153e3a56035499654d6c5b5baa93f28d/examples/D15.jpg)

## 2 One-gate MoE Layer

而 One-gate MoE layer 则是将隐藏层划分为三个专家（expert）子网，同时接入一个 Gate 网络将各个子网的输出和输入信息进行组合，并将得到的结果进行相加。
 
![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/0cbcbe67153e3a56035499654d6c5b5baa93f28d/examples/D16.jpg)
 
MoE 的主要目标是实现条件计算，对于每个数据而言，只有部分网络是活跃的，该模型可以通过限制输入的门控网络来选择专家网络的子集。

## 3 Multi-gate MoE model

MoE 能够实现不同数据多样化使用共享层，但针对不同任务而言，其使用的共享层是一致的。这种情况下，如果任务相关性较低，则会导致模型性能下降。

所以，作者在 MoE 的基础上提出了 MMoE 模型，为每个任务都设置了一个 Gate 网路，旨在使得不同任务和不同数据可以多样化的使用共享层，其模型结构如下：
 
![](https://github.com/Eric-Zhang-Composing/awesome-DeepLearning/blob/0cbcbe67153e3a56035499654d6c5b5baa93f28d/examples/D17.jpg)

这种情况下，每个 Gate 网络都可以根据不同任务来选择专家网络的子集，所以即使两个任务并不是十分相关，那么经过 Gate 后也可以得到不同的权重系数，此时，MMoE 可以充分利用部分 expert 网络的信息，近似于单个任务；而如果两个任务相关性高，那么 Gate 的权重分布相差会不大，会类似于一般的多任务学习。
优缺点： MMoE通过多个 Gate 网络来自适应学习不同数据在不同任务下的与专家子网的权重关系系数，从而在相关性较低的多任务学习中取得不错的成绩。
共享网络节省了大量计算资源，且 Gate 网络参数较少，所以 MMoE 模型很大程度上也保持了计算优势。


