{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像数据形状和对应数据为: (28, 28)\n",
      "图像标签形状和对应数据为: (1,) [5]\n",
      "\n",
      "打印第一个batch的第一个图像，对应标签数字为[5]\n"
     ]
    }
   ],
   "source": [
    "#加载飞桨和相关类库\r\n",
    "import paddle\r\n",
    "from paddle.nn import Linear\r\n",
    "import paddle.nn.functional as F\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "train_dataset = paddle.vision.datasets.MNIST(mode='train')\r\n",
    "\r\n",
    "train_data0 = np.array(train_dataset[0][0])\r\n",
    "train_label_0 = np.array(train_dataset[0][1])\r\n",
    "\r\n",
    "# 显示第一batch的第一个图像\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "plt.figure(\"Image\") # 图像窗口名称\r\n",
    "plt.figure(figsize=(2,2))\r\n",
    "plt.imshow(train_data0, cmap=plt.cm.binary)\r\n",
    "plt.axis('on') # 关掉坐标轴为 off\r\n",
    "plt.title('image') # 图像题目\r\n",
    "plt.show()\r\n",
    "\r\n",
    "print(\"图像数据形状和对应数据为:\", train_data0.shape)\r\n",
    "print(\"图像标签形状和对应数据为:\", train_label_0.shape, train_label_0)\r\n",
    "print(\"\\n打印第一个batch的第一个图像，对应标签数字为{}\".format(train_label_0))\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "# 确保从paddle.vision.datasets.MNIST中加载的图像数据是np.ndarray类型\r\n",
    "paddle.vision.set_image_backend('cv2')\r\n",
    "\r\n",
    "#数据处理部分之前的代码，保持不变\r\n",
    "import os\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from PIL import Image\r\n",
    "import paddle.nn.functional as F\r\n",
    "import gzip\r\n",
    "import json\r\n",
    "import paddle.fluid as fluid\r\n",
    "\r\n",
    "# 定义数据集读取器\r\n",
    "def load_data(mode='train'):\r\n",
    "\r\n",
    "    # 加载数据\r\n",
    "    datafile = 'data/data17155/mnist.json.gz'\r\n",
    "    print('loading mnist dataset from {} ......'.format(datafile))\r\n",
    "    data = json.load(gzip.open(datafile))\r\n",
    "    print('mnist dataset load done')\r\n",
    "\r\n",
    "    # 读取到的数据区分训练集，验证集，测试集\r\n",
    "    train_set, val_set, eval_set = data\r\n",
    "\r\n",
    "    # 数据集相关参数，图片高度IMG_ROWS, 图片宽度IMG_COLS\r\n",
    "    # IMG_ROWS = 28\r\n",
    "    # IMG_COLS = 28\r\n",
    "\r\n",
    "    if mode == 'train':\r\n",
    "        # 获得训练数据集\r\n",
    "        imgs, labels = train_set[0], train_set[1]\r\n",
    "    elif mode == 'valid':\r\n",
    "        # 获得验证数据集\r\n",
    "        imgs, labels = val_set[0], val_set[1]\r\n",
    "    elif mode == 'eval':\r\n",
    "        # 获得测试数据集\r\n",
    "        imgs, labels = eval_set[0], eval_set[1]\r\n",
    "    else:\r\n",
    "        raise Exception(\"mode can only be one of ['train', 'valid', 'eval']\")\r\n",
    "\r\n",
    "    #校验数据\r\n",
    "    imgs_length = len(imgs)\r\n",
    "    assert len(imgs) == len(labels), \\\r\n",
    "          \"length of train_imgs({}) should be the same as train_labels({})\".format(\r\n",
    "                  len(imgs), len(labels))\r\n",
    "\r\n",
    "    # 定义数据集每个数据的序号， 根据序号读取数据\r\n",
    "    index_list = list(range(imgs_length))\r\n",
    "    # 读入数据时用到的batchsize\r\n",
    "    BATCHSIZE = 100\r\n",
    "\r\n",
    "    # 定义数据生成器\r\n",
    "    def data_generator():\r\n",
    "        if mode == 'train':\r\n",
    "            random.shuffle(index_list)\r\n",
    "        imgs_list = []\r\n",
    "        labels_list = []\r\n",
    "        for i in index_list:\r\n",
    "            img = np.array(imgs[i]).astype('float32')\r\n",
    "            label = np.array(labels[i]).astype('float32')\r\n",
    "            # img = np.reshape(imgs[i], [1, IMG_ROWS, IMG_COLS]).astype('float32')\r\n",
    "            # label = np.reshape(labels[i], [1]).astype('float32')\r\n",
    "            imgs_list.append(img) \r\n",
    "            labels_list.append(label)\r\n",
    "            if len(imgs_list) == BATCHSIZE:\r\n",
    "                yield np.array(imgs_list), np.array(labels_list)\r\n",
    "                imgs_list = []\r\n",
    "                labels_list = []\r\n",
    "\r\n",
    "        # 如果剩余数据的数目小于BATCHSIZE，\r\n",
    "        # 则剩余数据一起构成一个大小为len(imgs_list)的mini-batch\r\n",
    "        if len(imgs_list) > 0:\r\n",
    "            yield np.array(imgs_list), np.array(labels_list)\r\n",
    "\r\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/2\n",
      "step 938/938 [==============================] - loss: 0.1892 - acc: 0.9317 - 13ms/step         \n",
      "Epoch 2/2\n",
      "step 938/938 [==============================] - loss: 0.0064 - acc: 0.9725 - 12ms/step         \n",
      "Eval begin...\n",
      "step  30/157 [====>.........................] - loss: 0.1455 - acc: 0.9708 - ETA: 0s - 7ms/st"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "from paddle.vision.transforms import Compose, Normalize\r\n",
    "import paddle.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "from paddle.metric import Accuracy\r\n",
    "import random\r\n",
    "from paddle import fluid\r\n",
    "from visualdl import LogWriter\r\n",
    "\r\n",
    "log_writer=LogWriter(\"./data/log/train\") #log记录器\r\n",
    "\r\n",
    "\r\n",
    "transform = Compose([Normalize(mean=[127.5],\r\n",
    "                               std=[127.5],\r\n",
    "                               data_format='CHW')])\r\n",
    "#归一化\r\n",
    "\r\n",
    "#读取训练集 测试集数据\r\n",
    "train_dataset = paddle.vision.datasets.MNIST(mode='train', transform=transform)\r\n",
    "test_dataset = paddle.vision.datasets.MNIST(mode='test', transform=transform)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# 确保从paddle.vision.datasets.MNIST中加载的图像数据是np.ndarray类型\r\n",
    "paddle.vision.set_image_backend('cv2')\r\n",
    "\r\n",
    "#数据处理部分之前的代码，保持不变\r\n",
    "import os\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from PIL import Image\r\n",
    "import paddle.nn.functional as F\r\n",
    "import gzip\r\n",
    "import json\r\n",
    "import paddle.fluid as fluid\r\n",
    "from paddle.fluid.dygraph import Conv2D, Pool2D, Linear, Dropout, BatchNorm\r\n",
    "\r\n",
    "\r\n",
    "class InceptionA(paddle.nn.Layer):  #作为网络一层\r\n",
    "    def __init__(self,in_channels):\r\n",
    "        super(InceptionA,self).__init__()\r\n",
    "        self.branch3x3_1=paddle.nn.Conv2D(in_channels,16,kernel_size=1) #第一个分支\r\n",
    "        self.branch3x3_2=paddle.nn.Conv2D( 16,24,kernel_size=3,padding=1)\r\n",
    "        self.branch3x3_3=paddle.nn.Conv2D(24,24,kernel_size=3,padding=1)\r\n",
    "\r\n",
    "        self.branch5x5_1=paddle.nn.Conv2D(in_channels, 16,kernel_size=1) #第二个分支\r\n",
    "        self.branch5x5_2=paddle.nn.Conv2D( 16,24,kernel_size=5,padding=2)\r\n",
    "\r\n",
    "        self.branch1x1=paddle.nn.Conv2D(in_channels, 16,kernel_size=1) #第三个分支\r\n",
    "\r\n",
    "        self.branch_pool=paddle.nn.Conv2D(in_channels,24,kernel_size= 1) #第四个分支\r\n",
    "\r\n",
    "    def forward(self,x):\r\n",
    "        #分支1处理过程\r\n",
    "        branch3x3= self.branch3x3_1(x)\r\n",
    "        branch3x3= self.branch3x3_2(branch3x3)\r\n",
    "        branch3x3= self.branch3x3_3(branch3x3)\r\n",
    "        #分支2处理过程\r\n",
    "        branch5x5=self.branch5x5_1(x)\r\n",
    "        branch5x5=self.branch5x5_2(branch5x5)\r\n",
    "        #分支3处理过程\r\n",
    "        branch1x1=self.branch1x1(x)\r\n",
    "        #分支4处理过程\r\n",
    "        branch_pool=F.avg_pool2d(x,kernel_size=3,stride=1,padding= 1)\r\n",
    "        branch_pool=self.branch_pool(branch_pool)\r\n",
    "        outputs=[branch1x1,branch5x5,branch3x3,branch_pool]     #将4个分支的输出拼接起来\r\n",
    "        return fluid.layers.concat(outputs,axis=1) #横着拼接， 共有24+24+16+24=88个通道\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "class MNIST(paddle.nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(MNIST,self).__init__()\r\n",
    "        #定义两个卷积层\r\n",
    "        self.conv1=paddle.nn.Conv2D(1,10,kernel_size=5)\r\n",
    "        self.conv2=paddle.nn.Conv2D(88,20,kernel_size=5)\r\n",
    "        #Inception模块的输出均为88通道\r\n",
    "        self.incep1=InceptionA(in_channels=10 )\r\n",
    "        self.incep2=InceptionA(in_channels=20)\r\n",
    "        self.mp=paddle.nn.MaxPool2D(2)\r\n",
    "        self.fc=paddle.nn.Linear(1408,10) #5*5* 88 =2200，图像高*宽*通道数\r\n",
    "    def forward(self,x):\r\n",
    "        x=F.relu(self.mp(self.conv1(x)))# 卷积池化，relu  输出x为图像尺寸14*14*10\r\n",
    "        x =self.incep1(x)               #图像尺寸14*14*88\r\n",
    "\r\n",
    "        x =F.relu(self.mp(self.conv2(x)))# 卷积池化，relu  输出x为图像尺寸5*5*20\r\n",
    "        x = self.incep2(x)              #图像尺寸5*5*88\r\n",
    "\r\n",
    "        x = paddle.flatten(x, start_axis=1,stop_axis=-1)\r\n",
    "        x = self.fc(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "model = paddle.Model(MNIST())   # 封装模型\r\n",
    "optim = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters()) # adam优化器\r\n",
    "\r\n",
    "# 配置模型\r\n",
    "model.prepare(\r\n",
    "    optim,\r\n",
    "    paddle.nn.CrossEntropyLoss(),\r\n",
    "    Accuracy()\r\n",
    "    )\r\n",
    "# 训练模型\r\n",
    "model.fit(train_dataset,epochs=2,batch_size=64,verbose=1)\r\n",
    "#评估\r\n",
    "model.evaluate(test_dataset, batch_size=64, verbose=1)\r\n",
    "\r\n",
    "#训练\r\n",
    "def train(model,Batch_size=64):\r\n",
    "    train_loader = paddle.io.DataLoader(train_dataset, batch_size=Batch_size, shuffle=True)\r\n",
    "    model.train()\r\n",
    "    iterator = 0\r\n",
    "    epochs = 10\r\n",
    "    total_steps = (int(50000//Batch_size)+1)*epochs\r\n",
    "    lr = paddle.optimizer.lr.PolynomialDecay(learning_rate=0.01,decay_steps=total_steps,end_lr=0.001)\r\n",
    "    optim = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())\r\n",
    "    # 用Adam作为优化函数\r\n",
    "    for epoch in range(epochs):\r\n",
    "        for batch_id, data in enumerate(train_loader()):\r\n",
    "            x_data = data[0]\r\n",
    "            y_data = data[1]\r\n",
    "            predicts = model(x_data)\r\n",
    "            loss = F.cross_entropy(predicts, y_data)\r\n",
    "            # 计算损失\r\n",
    "            acc = paddle.metric.accuracy(predicts, y_data)\r\n",
    "            loss.backward()\r\n",
    "            if batch_id % 200 == 0:\r\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}, acc is: {}\".format(epoch, batch_id, loss.numpy(), acc.numpy()))\r\n",
    "                log_writer.add_scalar(tag='acc',step=iterator,value=acc.numpy())\r\n",
    "                log_writer.add_scalar(tag='loss',step=iterator,value=loss.numpy())\r\n",
    "                iterator+=200\r\n",
    "            optim.step()\r\n",
    "            optim.clear_grad()\r\n",
    "        paddle.save(model.state_dict(),'./data/checkpoint/mnist_epoch{}'.format(epoch)+'.pdparams')\r\n",
    "        paddle.save(optim.state_dict(),'./data/checkpoint/mnist_epoch{}'.format(epoch)+'.pdopt')\r\n",
    "\r\n",
    "\r\n",
    "#测试\r\n",
    "def test(model):\r\n",
    "    # 加载测试数据集\r\n",
    "    test_loader = paddle.io.DataLoader(test_dataset, places=paddle.CPUPlace(), batch_size=64)\r\n",
    "    model.eval()\r\n",
    "    for batch_id, data in enumerate(test_loader()):\r\n",
    "        x_data = data[0]\r\n",
    "        y_data = data[1]\r\n",
    "        predicts = model(x_data)\r\n",
    "        # 获取预测结果\r\n",
    "        loss = F.cross_entropy(predicts, y_data)\r\n",
    "        acc = paddle.metric.accuracy(predicts, y_data)\r\n",
    "        if batch_id % 20 == 0:\r\n",
    "            print(\"batch_id: {}, loss is: {}, acc is: {}\".format(batch_id, loss.numpy(), acc.numpy()))\r\n",
    "\r\n",
    "#随机抽取100张图片进行测试\r\n",
    "def random_test(model,num=100):\r\n",
    "    select_id = random.sample(range(1, 10000), 100) #生成一百张测试图片的下标\r\n",
    "    test_loader = paddle.io.DataLoader(test_dataset, places=paddle.CPUPlace(), batch_size=64)\r\n",
    "    for batch_id, data in enumerate(test_loader()):\r\n",
    "        x_data = data[0]\r\n",
    "        label = data[1]\r\n",
    "    predicts = model(x_data)\r\n",
    "    #返回正确率\r\n",
    "    acc = paddle.metric.accuracy(predicts, label)\r\n",
    "    print(\"正确率为：{}\".format(acc.numpy()))\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    model = MNIST()\r\n",
    "    train(model)\r\n",
    "    test(model)\r\n",
    "    random_test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
