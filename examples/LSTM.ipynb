{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/data36219/text8.zip\n",
      "  inflating: text8                   \n"
     ]
    }
   ],
   "source": [
    "!unzip -o data/data36219/text8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddle import fluid\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "\r\n",
    "\r\n",
    "class LSTM:\r\n",
    "    \"\"\"\r\n",
    "    emb_dim: 词向量维度\r\n",
    "    vocab_size: 词典大小，不能小于训练数据中所有词的总数\r\n",
    "    num_layers: 隐含层的数量\r\n",
    "    hidden_size: 隐含层的大小\r\n",
    "    num_steps: LSTM 一次接收数据的最大长度，样本的timestamp\r\n",
    "    use_gpu: 是否使用gpu进行训练\r\n",
    "    dropout_prob: 如果大于0，就启用dropout，值在0-1区间\r\n",
    "    init_scale: 训练参数的初始化范围\r\n",
    "    lr：学习速率\r\n",
    "    vocab: 默认为None，占位，暂时没用\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self,\r\n",
    "                 vocab_size,\r\n",
    "                 num_layers,\r\n",
    "                 hidden_size,\r\n",
    "                 num_steps,\r\n",
    "                 use_gpu=True,\r\n",
    "                 dropout_prob=None,\r\n",
    "                 init_scale=0.1,\r\n",
    "                 lr=0.001,\r\n",
    "                 vocab=None):\r\n",
    "        self.vocab_size = vocab_size\r\n",
    "        self.num_layers = num_layers\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.num_steps = num_steps\r\n",
    "        self.dropout_prob = dropout_prob\r\n",
    "        self.use_gpu = use_gpu\r\n",
    "        self.init_scale = init_scale\r\n",
    "        self.vocab = vocab\r\n",
    "        self.lr = lr\r\n",
    "\r\n",
    "    def forward(self, x, batch_size):\r\n",
    "        self.init_hidden = fluid.layers.data(name='init_hidden',\r\n",
    "                                             shape=[self.num_layers, batch_size, self.hidden_size],\r\n",
    "                                             append_batch_size=False)\r\n",
    "        self.init_cell = fluid.layers.data(name='init_cell',\r\n",
    "                                           shape=[self.num_layers, batch_size, self.hidden_size],\r\n",
    "                                           append_batch_size=False)\r\n",
    "        x_emb = fluid.embedding(input=x, size=[self.vocab_size, self.hidden_size],\r\n",
    "                                dtype='float32', is_sparse=False,\r\n",
    "                                param_attr=fluid.ParamAttr(\r\n",
    "                                    name='embedding_para',\r\n",
    "                                    initializer=fluid.initializer.UniformInitializer(\r\n",
    "                                        low=-self.init_scale, high=self.init_scale\r\n",
    "                                    )\r\n",
    "                                ))\r\n",
    "        x_emb = fluid.layers.reshape(x_emb, shape=[-1, self.num_steps, self.hidden_size])\r\n",
    "        if self.dropout_prob is not None and self.dropout_prob > 0.0:\r\n",
    "            x_emb = fluid.layers.dropout(x_emb, dropout_prob=self.dropout_prob,\r\n",
    "                                         dropout_implementation=\"upscale_in_train\")\r\n",
    "\r\n",
    "        rnn_out, last_hidden, last_cell = fluid.contrib.layers.basic_lstm(x_emb, self.init_hidden, self.init_cell,\r\n",
    "                                                                          self.hidden_size, self.num_layers,\r\n",
    "                                                                          dropout_prob=self.dropout_prob)\r\n",
    "        rnn_out = fluid.layers.reshape(rnn_out, shape=[-1, self.num_steps, self.hidden_size])\r\n",
    "        softmax_weight = fluid.layers.create_parameter(\r\n",
    "            [self.hidden_size, self.vocab_size],\r\n",
    "            dtype=\"float32\",\r\n",
    "            name=\"softmax_weight\",\r\n",
    "            default_initializer=fluid.initializer.UniformInitializer(\r\n",
    "                low=-self.init_scale, high=self.init_scale))\r\n",
    "        softmax_bias = fluid.layers.create_parameter(\r\n",
    "            [self.vocab_size],\r\n",
    "            dtype=\"float32\",\r\n",
    "            name='softmax_bias',\r\n",
    "            default_initializer=fluid.initializer.UniformInitializer(\r\n",
    "                low=-self.init_scale, high=self.init_scale))\r\n",
    "\r\n",
    "        proj = fluid.layers.matmul(rnn_out, softmax_weight)\r\n",
    "        proj = fluid.layers.elementwise_add(proj, softmax_bias)\r\n",
    "        proj = fluid.layers.reshape(proj, shape=[-1, self.vocab_size], inplace=True)\r\n",
    "        # 更新 init_hidden, init_cell\r\n",
    "        fluid.layers.assign(input=last_cell, output=self.init_cell)\r\n",
    "        fluid.layers.assign(input=last_hidden, output=self.init_hidden)\r\n",
    "        return proj, last_hidden, last_cell\r\n",
    "\r\n",
    "    def train(self, x, epochs=3, batch_size=32, log_interval=100):\r\n",
    "        \"\"\"\r\n",
    "        :param log_interval: 输出信息的间隔\r\n",
    "        :param x: 输入，一维list，文本需要经过编码\r\n",
    "        :param epochs: 训练回合数\r\n",
    "        :param batch_size: 训练batch大小\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        self.batch_size = batch_size\r\n",
    "        # 定义训练的program\r\n",
    "        main_program = fluid.default_main_program()\r\n",
    "        startup_program = fluid.default_startup_program()\r\n",
    "        train_loss, train_proj, self.last_hidden, self.last_cell, py_reader = self.build_train_model(main_program, startup_program)\r\n",
    "\r\n",
    "        # 定义测试的program, 写成全局的，以便留给测试函数\r\n",
    "        self.test_program = fluid.Program()\r\n",
    "        self.test_startup_program = fluid.Program()\r\n",
    "        self.test_loss, self.test_proj, _, _ = self.build_test_model(self.test_program, self.test_startup_program)\r\n",
    "        self.test_program = self.test_program.clone(for_test=True)\r\n",
    "\r\n",
    "        place = fluid.CUDAPlace(0) if self.use_gpu else fluid.CPUPlace()\r\n",
    "        self.exe = fluid.Executor(place)\r\n",
    "        self.exe.run(startup_program)\r\n",
    "\r\n",
    "        def data_gen():\r\n",
    "            batches = self.get_data_iter(x)\r\n",
    "            for batch in batches:\r\n",
    "                x_, y_ = batch\r\n",
    "                yield x_, y_\r\n",
    "\r\n",
    "        py_reader.decorate_tensor_provider(data_gen)\r\n",
    "\r\n",
    "        for epoch in range(epochs):\r\n",
    "            batch_times = []\r\n",
    "            epoch_start_time = time.time()\r\n",
    "            total_loss = 0\r\n",
    "            iters = 0\r\n",
    "            py_reader.start()\r\n",
    "            batch_id = 0\r\n",
    "            batch_start_time = time.time()\r\n",
    "            # 初始化init_hidden, init_cell\r\n",
    "            init_hidden = np.zeros((self.num_layers, self.batch_size, self.hidden_size), dtype='float32')\r\n",
    "            init_cell = np.zeros((self.num_layers, self.batch_size, self.hidden_size), dtype='float32')\r\n",
    "\r\n",
    "            data_len = len(x)\r\n",
    "            batch_len = data_len // self.batch_size\r\n",
    "            batch_num = (batch_len - 1) // self.num_steps\r\n",
    "\r\n",
    "            # 送入数据，抓取结果\r\n",
    "            try:\r\n",
    "                while True:\r\n",
    "                    # 送入数据\r\n",
    "                    data_feeds = {}\r\n",
    "                    data_feeds['init_hidden'] = init_hidden\r\n",
    "                    data_feeds['init_cell'] = init_cell\r\n",
    "                    fetch_outs = self.exe.run(main_program, feed=data_feeds,\r\n",
    "                                         fetch_list=[train_loss.name, self.last_hidden.name, self.last_cell.name])\r\n",
    "                    t_loss = np.array(fetch_outs[0])\r\n",
    "                    init_hidden = np.array(fetch_outs[1])\r\n",
    "                    init_cell = np.array(fetch_outs[2])\r\n",
    "\r\n",
    "                    total_loss += t_loss\r\n",
    "                    batch_time = time.time() - batch_start_time\r\n",
    "                    batch_times.append(batch_time)\r\n",
    "                    batch_start_time = time.time()\r\n",
    "\r\n",
    "                    batch_id += 1\r\n",
    "                    iters += self.num_steps\r\n",
    "                    if batch_id % log_interval == 0:\r\n",
    "                        ppl = np.exp(total_loss / iters)\r\n",
    "                        print(\"-- Epoch: %d - Batch: %d / %d - Cost Time: %.2f s -ETA: %.2f s- ppl: %.5f\"\r\n",
    "                              % (epoch + 1, batch_id, batch_num, sum(batch_times),\r\n",
    "                                 sum(batch_times) / batch_id * (batch_num - batch_id), ppl[0]))\r\n",
    "            except fluid.core.EOFException:\r\n",
    "                py_reader.reset()\r\n",
    "\r\n",
    "            epoch_time = time.time() - epoch_start_time\r\n",
    "            ppl = np.exp(total_loss / iters)\r\n",
    "            print(\"Epoch %d Done. Cost Time: %.2f s. ppl: %.5f.\" % (epoch + 1, epoch_time, ppl))\r\n",
    "\r\n",
    "    def evaluate(self, x):\r\n",
    "        \"\"\"\r\n",
    "        测试模型的效果\r\n",
    "        :param x:\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        eval_data_gen = self.get_data_iter(x)\r\n",
    "        total_loss = 0.0\r\n",
    "        iters = 0\r\n",
    "        # 初始化init_hidden, init_cell\r\n",
    "        init_hidden = np.zeros((self.num_layers, self.batch_size, self.hidden_size), dtype='float32')\r\n",
    "        init_cell = np.zeros((self.num_layers, self.batch_size, self.hidden_size), dtype='float32')\r\n",
    "\r\n",
    "        for batch_id, batch in enumerate(eval_data_gen):\r\n",
    "            x, y = batch\r\n",
    "            data_feeds = {}\r\n",
    "            data_feeds['init_hidden'] = init_hidden\r\n",
    "            data_feeds['init_cell'] = init_cell\r\n",
    "            data_feeds['x'] = x\r\n",
    "            data_feeds['y'] = y\r\n",
    "            fetch_outs = self.exe.run(self.test_program, feed=data_feeds,\r\n",
    "                                      fetch_list=[self.test_loss.name, self.last_hidden.name, self.last_cell.name])\r\n",
    "            cost_test = np.array(fetch_outs[0])\r\n",
    "            init_hidden = np.array(fetch_outs[1])\r\n",
    "            init_cell = np.array(fetch_outs[2])\r\n",
    "\r\n",
    "            total_loss += cost_test\r\n",
    "            iters += self.num_steps\r\n",
    "            ppl = np.exp(total_loss / iters)\r\n",
    "            print(\"-- Batch: %d - ppl: %.5f\" % (batch_id, ppl[0]))\r\n",
    "        print(\"ppl: %.5f\" % (ppl[0]))\r\n",
    "        return ppl\r\n",
    "\r\n",
    "    def get_data_iter(self, raw_data):\r\n",
    "        \"\"\"\r\n",
    "        处理原始文本，生成训练数据\r\n",
    "        对于RNN来说，一般为读取前n个词，然后预测下一个词，这里简化为每读一个词，预测下一个词。\r\n",
    "        由于LSTM考虑了长依赖，所以也可以做到读取n个词，预测下一个词\r\n",
    "        :param raw_data: 一个一维数组，list，\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        data_len = len(raw_data)\r\n",
    "        raw_data = np.asarray(raw_data, dtype='int64')\r\n",
    "        batch_len = data_len // self.batch_size\r\n",
    "        # 将一维数组变为二维数组，第一维是batch的数量，第二维是每个batch的数据，这里对后边不足batch_len的数据进行了裁剪，弃掉不用\r\n",
    "        data = raw_data[0:self.batch_size * batch_len].reshape((self.batch_size, batch_len))\r\n",
    "\r\n",
    "        # 为了保证每个batch最后一个词能够被预测，x的词最多被分到batch_len-1\r\n",
    "        batch_num = (batch_len - 1) // self.num_steps\r\n",
    "        for i in range(batch_num):\r\n",
    "            x = np.copy(data[:, i * self.num_steps:(i + 1) * self.num_steps])\r\n",
    "            y = np.copy(data[:, i * self.num_steps + 1:(i + 1) * self.num_steps + 1])\r\n",
    "            x = x.reshape((-1, self.num_steps, 1))\r\n",
    "            y = y.reshape((-1, 1))\r\n",
    "            yield x, y\r\n",
    "\r\n",
    "    def build_train_model(self, main_program, startup_program):\r\n",
    "        \"\"\"\r\n",
    "        读取数据，构建网络\r\n",
    "        :param main_program:\r\n",
    "        :param startup_program:\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        with fluid.program_guard(main_program, startup_program):\r\n",
    "            feed_shapes = [[self.batch_size, self.num_steps, 1],\r\n",
    "                           [self.batch_size * self.num_steps, 1]]\r\n",
    "            py_reader = fluid.layers.py_reader(capacity=64, shapes=feed_shapes, dtypes=['int64', 'int64'])\r\n",
    "            x, y = fluid.layers.read_file(py_reader)\r\n",
    "            # 使用unique_name.guard创建变量空间，以便在test时共享参数\r\n",
    "            with fluid.unique_name.guard():\r\n",
    "                proj, last_hidden, last_cell = self.forward(x, self.batch_size)\r\n",
    "\r\n",
    "                loss = self.get_loss(proj, y)\r\n",
    "                optimizer = fluid.optimizer.Adam(learning_rate=self.lr,\r\n",
    "                                                 grad_clip=fluid.clip.GradientClipByGlobalNorm(clip_norm=1000))\r\n",
    "                optimizer.minimize(loss)\r\n",
    "\r\n",
    "                # 不知道有什么用，先写上\r\n",
    "                #loss.persistable = True\r\n",
    "                #proj.persistable = True\r\n",
    "                #last_cell.persistable = True\r\n",
    "                #last_hidden.persistable = True\r\n",
    "\r\n",
    "                return loss, proj, last_hidden, last_cell, py_reader\r\n",
    "\r\n",
    "    def build_test_model(self, main_program, startup_program):\r\n",
    "        \"\"\"\r\n",
    "        验证模型效果\r\n",
    "        :param main_program:\r\n",
    "        :param startup_program:\r\n",
    "        :return:\r\n",
    "        \"\"\"\r\n",
    "        with fluid.program_guard(main_program, startup_program):\r\n",
    "            x = fluid.layers.data(name='x', shape=[self.batch_size, self.num_steps, 1], dtype='int64', append_batch_size=False)\r\n",
    "            y = fluid.layers.data(name='y', shape=[self.batch_size * self.num_steps, 1], dtype='int64', append_batch_size=False)\r\n",
    "            # 使用unique_name.guard创建变量空间，和train共享参数\r\n",
    "            with fluid.unique_name.guard():\r\n",
    "                proj, last_hidden, last_cell = self.forward(x, self.batch_size)\r\n",
    "                loss = self.get_loss(proj, y)\r\n",
    "\r\n",
    "                # 不知道有什么用，先写上\r\n",
    "                #loss.persistable = True\r\n",
    "                #proj.persistable = True\r\n",
    "                #last_cell.persistable = True\r\n",
    "                #last_hidden.persistable = True\r\n",
    "\r\n",
    "                return loss, proj, last_hidden, last_cell\r\n",
    "\r\n",
    "    def get_loss(self, proj, y):\r\n",
    "        loss = fluid.layers.softmax_with_cross_entropy(logits=proj, label=y, soft_label=False)\r\n",
    "        loss = fluid.layers.reshape(loss, shape=[-1, self.num_steps])\r\n",
    "        loss = fluid.layers.reduce_mean(loss, dim=[0])\r\n",
    "        loss = fluid.layers.reduce_sum(loss)\r\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.3.1 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\r\n",
    "from collections import Counter\r\n",
    "import itertools\r\n",
    "\r\n",
    "def clean_str(string):\r\n",
    "    \"\"\"\r\n",
    "    将文本中的特定字符串做修改和替换处理\r\n",
    "    :param string:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "    string = re.sub(r\"[^A-Za-z0-9:(),!?\\'\\`]\", \" \", string)\r\n",
    "    string = re.sub(r\":\", \" : \", string)\r\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\r\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\r\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\r\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\r\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\r\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\r\n",
    "    string = re.sub(r\",\", \" , \", string)\r\n",
    "    string = re.sub(r\"!\", \" ! \", string)\r\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\r\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\r\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\r\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\r\n",
    "    return string.strip().lower()\r\n",
    "\r\n",
    "\r\n",
    "def build_vocab(sentences, EOS='</eos>'):\r\n",
    "    \"\"\"\r\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\r\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\r\n",
    "    \"\"\"\r\n",
    "    # Build vocabulary\r\n",
    "    word_counts = Counter(itertools.chain(*sentences))\r\n",
    "    # Mapping from index to word\r\n",
    "    # vocabulary_inv=['<PAD/>', 'the', ....]\r\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\r\n",
    "    # Mapping from word to index\r\n",
    "    # vocabulary = {'<PAD/>': 0, 'the': 1, ',': 2, 'a': 3, 'and': 4, ..}\r\n",
    "    vocabulary = {x: i+1 for i, x in enumerate(vocabulary_inv)}\r\n",
    "    vocabulary[EOS] = 0\r\n",
    "    return [vocabulary, vocabulary_inv]\r\n",
    "\r\n",
    "\r\n",
    "def file_to_ids(src_file, src_vocab):\r\n",
    "    \"\"\"\r\n",
    "    将文章单词序列转化成词典id序列\r\n",
    "    :param src_file:\r\n",
    "    :param src_vocab:\r\n",
    "    :return:\r\n",
    "    \"\"\"\r\n",
    "    src_data = []\r\n",
    "    for line in src_file:\r\n",
    "        ids = [src_vocab[w] for w in line if w in src_vocab]\r\n",
    "        src_data += ids + [0]\r\n",
    "    return src_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_text = list(open(\"text8\", \"r\").readlines())\r\n",
    "x_text = [clean_str(sent) for sent in x_text]\r\n",
    "vocabulary, vocabulary_inv = build_vocab(x_text)\r\n",
    "x_text = file_to_ids(x_text, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.3.2 训练 \n",
    "训练的结果与数据的吻合程度用困惑度指标ppl来衡量，参考了[基于LSTM的语言模型实现](https://aistudio.baidu.com/aistudio/projectdetail/592038)。ppl的值即e为底，平均交叉熵损失为指数的幂指数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-20 15:32:47,661-WARNING: paddle.fluid.layers.py_reader() may be deprecated in the near future. Please use paddle.fluid.io.DataLoader.from_generator() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch: 1 - Batch: 100 / 1562 - Cost Time: 3.56 s -ETA: 52.05 s- ppl: 11.80868\n",
      "-- Epoch: 1 - Batch: 200 / 1562 - Cost Time: 6.34 s -ETA: 43.19 s- ppl: 9.78493\n",
      "-- Epoch: 1 - Batch: 300 / 1562 - Cost Time: 9.07 s -ETA: 38.17 s- ppl: 8.85931\n",
      "-- Epoch: 1 - Batch: 400 / 1562 - Cost Time: 11.84 s -ETA: 34.39 s- ppl: 8.23703\n",
      "-- Epoch: 1 - Batch: 500 / 1562 - Cost Time: 14.58 s -ETA: 30.96 s- ppl: 7.85555\n",
      "-- Epoch: 1 - Batch: 600 / 1562 - Cost Time: 18.18 s -ETA: 29.15 s- ppl: 7.57556\n",
      "-- Epoch: 1 - Batch: 700 / 1562 - Cost Time: 20.93 s -ETA: 25.77 s- ppl: 7.38541\n",
      "-- Epoch: 1 - Batch: 800 / 1562 - Cost Time: 24.00 s -ETA: 22.86 s- ppl: 7.20709\n",
      "-- Epoch: 1 - Batch: 900 / 1562 - Cost Time: 27.15 s -ETA: 19.97 s- ppl: 7.08078\n",
      "-- Epoch: 1 - Batch: 1000 / 1562 - Cost Time: 30.28 s -ETA: 17.02 s- ppl: 6.95750\n",
      "-- Epoch: 1 - Batch: 1100 / 1562 - Cost Time: 33.40 s -ETA: 14.03 s- ppl: 6.85713\n",
      "-- Epoch: 1 - Batch: 1200 / 1562 - Cost Time: 36.60 s -ETA: 11.04 s- ppl: 6.77589\n",
      "-- Epoch: 1 - Batch: 1300 / 1562 - Cost Time: 40.52 s -ETA: 8.17 s- ppl: 6.69210\n",
      "-- Epoch: 1 - Batch: 1400 / 1562 - Cost Time: 43.59 s -ETA: 5.04 s- ppl: 6.61734\n",
      "-- Epoch: 1 - Batch: 1500 / 1562 - Cost Time: 46.72 s -ETA: 1.93 s- ppl: 6.54857\n",
      "Epoch 1 Done. Cost Time: 48.78 s. ppl: 6.50548.\n",
      "-- Epoch: 2 - Batch: 100 / 1562 - Cost Time: 3.00 s -ETA: 43.89 s- ppl: 5.59537\n",
      "-- Epoch: 2 - Batch: 200 / 1562 - Cost Time: 6.01 s -ETA: 40.91 s- ppl: 5.59269\n",
      "-- Epoch: 2 - Batch: 300 / 1562 - Cost Time: 9.86 s -ETA: 41.47 s- ppl: 5.58961\n",
      "-- Epoch: 2 - Batch: 400 / 1562 - Cost Time: 12.72 s -ETA: 36.96 s- ppl: 5.52860\n",
      "-- Epoch: 2 - Batch: 500 / 1562 - Cost Time: 15.54 s -ETA: 33.00 s- ppl: 5.50956\n",
      "-- Epoch: 2 - Batch: 600 / 1562 - Cost Time: 18.47 s -ETA: 29.61 s- ppl: 5.50064\n",
      "-- Epoch: 2 - Batch: 700 / 1562 - Cost Time: 21.43 s -ETA: 26.39 s- ppl: 5.50716\n",
      "-- Epoch: 2 - Batch: 800 / 1562 - Cost Time: 24.38 s -ETA: 23.22 s- ppl: 5.49576\n",
      "-- Epoch: 2 - Batch: 900 / 1562 - Cost Time: 27.36 s -ETA: 20.12 s- ppl: 5.50313\n",
      "-- Epoch: 2 - Batch: 1000 / 1562 - Cost Time: 30.97 s -ETA: 17.41 s- ppl: 5.49932\n",
      "-- Epoch: 2 - Batch: 1100 / 1562 - Cost Time: 33.74 s -ETA: 14.17 s- ppl: 5.49350\n",
      "-- Epoch: 2 - Batch: 1200 / 1562 - Cost Time: 36.51 s -ETA: 11.01 s- ppl: 5.49490\n",
      "-- Epoch: 2 - Batch: 1300 / 1562 - Cost Time: 39.37 s -ETA: 7.93 s- ppl: 5.48674\n",
      "-- Epoch: 2 - Batch: 1400 / 1562 - Cost Time: 42.14 s -ETA: 4.88 s- ppl: 5.47724\n",
      "-- Epoch: 2 - Batch: 1500 / 1562 - Cost Time: 44.90 s -ETA: 1.86 s- ppl: 5.46795\n",
      "Epoch 2 Done. Cost Time: 46.69 s. ppl: 5.45791.\n",
      "-- Epoch: 3 - Batch: 100 / 1562 - Cost Time: 3.71 s -ETA: 54.21 s- ppl: 5.34223\n",
      "-- Epoch: 3 - Batch: 200 / 1562 - Cost Time: 6.56 s -ETA: 44.64 s- ppl: 5.34592\n",
      "-- Epoch: 3 - Batch: 300 / 1562 - Cost Time: 9.43 s -ETA: 39.68 s- ppl: 5.34064\n",
      "-- Epoch: 3 - Batch: 400 / 1562 - Cost Time: 12.40 s -ETA: 36.02 s- ppl: 5.29089\n",
      "-- Epoch: 3 - Batch: 500 / 1562 - Cost Time: 15.28 s -ETA: 32.45 s- ppl: 5.28373\n",
      "-- Epoch: 3 - Batch: 600 / 1562 - Cost Time: 18.20 s -ETA: 29.19 s- ppl: 5.27816\n",
      "-- Epoch: 3 - Batch: 700 / 1562 - Cost Time: 21.17 s -ETA: 26.07 s- ppl: 5.29117\n",
      "-- Epoch: 3 - Batch: 800 / 1562 - Cost Time: 24.91 s -ETA: 23.73 s- ppl: 5.28815\n",
      "-- Epoch: 3 - Batch: 900 / 1562 - Cost Time: 27.86 s -ETA: 20.49 s- ppl: 5.30161\n",
      "-- Epoch: 3 - Batch: 1000 / 1562 - Cost Time: 30.81 s -ETA: 17.32 s- ppl: 5.30364\n",
      "-- Epoch: 3 - Batch: 1100 / 1562 - Cost Time: 33.76 s -ETA: 14.18 s- ppl: 5.30080\n",
      "-- Epoch: 3 - Batch: 1200 / 1562 - Cost Time: 36.68 s -ETA: 11.06 s- ppl: 5.30615\n",
      "-- Epoch: 3 - Batch: 1300 / 1562 - Cost Time: 39.64 s -ETA: 7.99 s- ppl: 5.30172\n",
      "-- Epoch: 3 - Batch: 1400 / 1562 - Cost Time: 42.59 s -ETA: 4.93 s- ppl: 5.29959\n",
      "-- Epoch: 3 - Batch: 1500 / 1562 - Cost Time: 46.32 s -ETA: 1.91 s- ppl: 5.29445\n",
      "Epoch 3 Done. Cost Time: 48.16 s. ppl: 5.28739.\n"
     ]
    }
   ],
   "source": [
    "lstm_test = LSTM(vocab_size=len(vocabulary), num_layers=1, hidden_size=100, num_steps=20, use_gpu=True, dropout_prob=0.2, init_scale=0.1, lr=0.01)\r\n",
    "lstm_test.train(x_text[:1000000], epochs=3, batch_size=32, log_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.3.3 测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Batch: 0 - ppl: 6.02640\n",
      "-- Batch: 1 - ppl: 5.05699\n",
      "-- Batch: 2 - ppl: 4.67704\n",
      "-- Batch: 3 - ppl: 4.67142\n",
      "-- Batch: 4 - ppl: 4.59714\n",
      "-- Batch: 5 - ppl: 4.55684\n",
      "-- Batch: 6 - ppl: 4.52244\n",
      "ppl: 4.52244\n"
     ]
    }
   ],
   "source": [
    "ppl = lstm_test.evaluate(x_text[1000000:1005000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Batch: 0 - ppl: 6.82386\n",
      "-- Batch: 1 - ppl: 5.83328\n",
      "-- Batch: 2 - ppl: 5.45274\n",
      "-- Batch: 3 - ppl: 5.52978\n",
      "-- Batch: 4 - ppl: 5.42611\n",
      "-- Batch: 5 - ppl: 5.44866\n",
      "-- Batch: 6 - ppl: 5.42464\n",
      "ppl: 5.42464\n"
     ]
    }
   ],
   "source": [
    "ppl = gru_test.evaluate(x_text[1000000:1005000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.8.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
