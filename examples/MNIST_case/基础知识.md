绝对值损失函数 Loss = |predict-label|


```python
def loss(predict,label):
    return abs(predict-label)
```

重叠池化：与一般池化操作相同，但是池化范围size与滑窗步长stride的关系为size>stride，同一区域内的像素特征可以参与多次滑窗提取，得到的特征表达能力更强，但计算量更大。

autoaugment方法是无监督的数据增强方法。它的基本思路是使用增强学习从数据本身寻找最佳图像变换策略，对于不同的任务学习不同的增强方法，流程如下：
(1) 准备16个常用的数据增强操作。
(2) 从16个中选择5个操作，随机产生使用该操作的概率和相应的幅度，将其称为一个sub-policy，一共产生5个sub-polices。
(3) 对训练过程中每一个batch的图片，随机采用5个sub-polices操作中的一种。
(4) 通过模型在验证集上的泛化能力来反馈，使用的优化方法是增强学习方法。
(5) 经过80~100个epoch后网络开始学习到有效的sub-policies。
(6) 之后串接这5个sub-policies，然后再进行最后的训练。

目前常用的图像分类方法有KNN、SVM、深度神经网络等。KNN的思想很简单：对每张图像，找与其最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。SVM还包括核技巧，这使它成为实质上的非线性分类器。深度神经网络主要有LeNet、AlexNet、VGG等。LeNet通过连续使用卷积和池化层的组合提取图像特征，在手写数字识别数据集上取得了很好的结果，但在更大的数据集上表现却并不好。与LeNet相比，AlexNet具有更深的网络结构，包含5层卷积和3层全连接，同时使用了如下三种方法改进模型的训练过程：数据增广、使用Dropout抑制过拟合、使用ReLU激活函数减少梯度消失现象。AlexNet模型通过构造多层网络，取得了较好的效果，但是并没有给出深度神经网络设计的方向。VGG通过使用一系列大小为3x3的小尺寸卷积核和池化层构造深度卷积神经网络，并取得了较好的效果。尤其是它的网络结构设计方法，为构建深度神经网络提供了方向。
