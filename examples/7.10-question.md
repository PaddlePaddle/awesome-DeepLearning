```python
1.损失函数补充：

0-1损失函数(zero-one loss)
实际输出值与预期相等时，loss=0，否者loss=1
0-1损失对每个错分类点都施以相同的惩罚，这样那些“错的离谱“ (即 margin→−∞)的点并不会收到大的关注，这在直觉上不是很合适。另外0-1损失不连续、非凸，优化困难，因而常使用其他的代理损失函数进行优化。

Python实现：
### Y为预期值，f(X)为实际输出

output=f(X)
if output != Y:
    loss = 1
else:
    loss = 0

2.池化方法补充：

（1）Stochastic-pooling（随机池化）：模板内元素值大的被选中的概率也大，这种方法既不会一直选择max值。但这种池化效果并不稳定即不能保证池化的结果一定是好的，可能产生更坏的结果。随机池化伴随着概率矩阵，每个元素对应一个被选取的概率，模板内概率和为1。
（2）overlapping pooling（重叠池化）：池化方法一般设置stride和size_PL相等，可以称之为一般方法。如果步长和池化模板尺寸不相等且两个池化区域存在重叠，这种池化方法称之为重叠池化。
（3）金字塔池化(spatial pyramid pooling)：一般CNN对输入的图像尺寸有着特定的要求，因为这是全卷积层的神经元个数对输入的特征维度是固定的。但采用金字塔池化，则可以将任意图像的卷积特征图像转化为所指定维度的特征向量输入给全卷积层。这就解决了CNN输入图像可以是任意尺寸的问题。空间金字塔池化是将池化层转化为多尺度的池化，即利用多个不同大小尺度的池化模板来进行池化操作。

3.数据增强

（1）随机放大： 对图片进行随机的放大。

（2）颜色抖动： 改变图片的颜色，通过对颜色通道的数值偏移，改变图片的整体的颜色。

（3）rescale： rescale的作用是对图片的每个像素值均乘上这个放缩因子，这个操作在所有其它变换操作之前执行，在一些模型当中，直接输入原图的像素值可能会落入激活函数的饱和区，因此设置放缩因子为1/255，把像素值放缩到0和1之间有利于模型的收敛，避免神经元饱和。图片经过rescale之后，保存到本地的图片用肉眼看是没有任何区别的。

（4）fill_mode： fill_mode为填充模式，如前面提到，当对图片进行平移、放缩、错切等操作时，图片中会出现一些缺失的地方，就由fill_mode中的参数确定。包括：“constant”、“nearest”（默认）、“reflect”和“wrap”。

4.图像分类算法综述

（1）KNN（K Near Neighbor）：k个最近的邻居，即每个样本都可以用它最接近的 K 个邻居 
来代表。其思想大概是，在空间中先放置好所有用于训练的样品，把测试样品置于该空间中。 用距离公式计算出离测试样品最近的K个样品，假如K个样品中属于A类的最多，那测试样品也算输入 A 类。

（2）SVM 分类 ：SVM(Support Vector Machine，支持向量机)，是一种二类分类模型，其基本模型定义为 特征空间上的即那个最大的线性分类器，器学习策略是间隔最大化，最终可转化为一个凸二次规划问题的解决。(线性支持向量机、非线性支持向量机)。 线性 SVM 的主要思想是建立一个超平面作为决策曲面，是的正例和反例之间的隔离边 缘被最大化。对于二维线性可分情况，令 H 为把两类训练样本没有错误地分开的分类县， H1、H2 分别为过各类中离分类线最近的样本且平行于分类线的直线，它们之间的距离讲座 分类间隔。所谓最优分类线就是要求分类线不但能将两类正确分开，而且使分类间隔最大。在高维空间，最优分类线就成为最优分类线。
 
（3）BPNN（后向传播神经网络方法：BPNN全称Back Propagation Neural Network，后向传播神经网络。它是属于前馈型神经网络的一种。BP 神经网络就是在前馈型网络的结构上增加了后向传播算法（Back Propagation）。 BP 网络的前馈表现为输入信号从输入层（输入层不参加计算）开始，每一层的神经元 计算出该层各神经元的输出并向下一层传递直到输出层计算出网络的输出结果，前馈只是用 于计算出网络的输出，不对网络的参数进行调整。 而后向传播是用于训练时网络权值和阈值的调整，该过程是需要监督学习的。在你的网 络没有训练好的时候，输出肯定和你想象的不一样，那么我们会得到一个偏差，并且把偏差 一级一级向前传递，逐层得到δ(i)，这就是反馈。

（4）CNN(卷积神经网络)：卷积神经网络是一种前馈多层网络，信息的流动只有一个方向，即从输入到输出，每个 层使用一组卷积核执行多个转换。CNN 模型主要包含卷积层、池化层、全连接层。以 CNN 模 型为基础，将多层卷积和多层池化结合产生新的网络模型，可提高网络结构的准确度。经典 的卷积神经网络模型有 GoogLeNet、AlexNet、VGGNet 。

（5）迁移学习： 在神经网络模型不断发展与完善下，由于模型的构建和训练往往需要花费大量的时间和 空间，然而生活中的问题在不断被需求着解决，研究者们试着通过对已有网络结构做适当的 调整用于新问题的求解优化，迁移学习在这种背景下孕育而生。 迁移学习是一种机器学习方法，就是把为任务 A 开发的模型作为初始点，重新使用在 为任务 B 开发模型的过程中。 迁移学习在深度学习上的应用有两种策略，但目前这两种策略的命名还没有统一。一种策略是微调（finetuning）——其中包括使用基础数据集上的预训练网络以及在目标数据集中训练所有层；另一种则是冻结与训练（freeze and train）——其中包括冻结除最后一层 的所有层（权重不更新）并训练最后一层。


```


      File "<ipython-input-2-b0d9b4f6647f>", line 1
        1.损失函数补充：
                ^
    SyntaxError: invalid character in identifier






```python

```


```python

```


```python

```


```python

```

请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>
Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. 
