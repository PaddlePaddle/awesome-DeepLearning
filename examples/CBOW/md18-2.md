### CBOW方式训练词向量

#### 1.概念

2013年，Mikolov提出的经典word2vec算法就是通过上下文来学习语义信息。word2vec包含两个经典模型：CBOW（Continuous Bag-of-Words）和Skip-gram，如下图所示。CBOW：通过上下文的词向量推理中心词。Skip-gram：根据中心词推理上下文。

假设有一个句子“Pineapples are spiked and yellow”，CBOW模型的推理方式如下：先在句子中选定一个中心词，并把其它词作为这个中心词的上下文。如下图，把“Spiked”作为中心词，“Pineapples、are、and、yellow”作为中心词的上下文。在学习过程中，使用上下文的词向量推理中心词，这样中心词的语义就被传递到上下文的词向量中，从而达到学习语义信息的目的。

![来自飞浆课程](img/CBOW-1.jpg)

一般来说，CBOW比Skip-gram训练速度快，训练过程更加稳定，原因是CBOW使用上下文average的方式进行训练，每个训练step会见到更多样本。而在生僻字（出现频率低的字）处理上，skip-gram比CBOW效果更好，原因是skip-gram不会刻意回避生僻字(CBOW结构中输入中存在生僻字时，生僻字会被其它非生僻字的权重冲淡)。

#### 2.网络结构

如下图所示，CBOW是一个具有3层结构的神经网络，分别是：输入层、隐藏层、输出层。

![来自飞浆课程](img/CBOW-2.jpg)

**输入层：** 一个形状为C×V的one-hot张量，其中C代表上线文中词的个数，通常是一个偶数，我们假设为4；V表示词表大小，我们假设为5000，该张量的每一行都是一个上下文词的one-hot向量表示，比如“Pineapples, are, and, yellow”。
**隐藏层：** 一个形状为V×N的参数张量W1，一般称为word-embedding，N表示每个词的词向量长度，我们假设为128。输入张量和word embedding W1进行矩阵乘法，就会得到一个形状为C×N的张量。综合考虑上下文中所有词的信息去推理中心词，因此将上下文中C个词相加得一个1×N的向量，是整个上下文的一个隐含表示。
**输出层：** 创建另一个形状为N×V的参数张量，将隐藏层得到的1×N的向量乘以该N×V的参数张量，得到了一个形状为1×V的向量。最终，1×V的向量代表了使用上下文去推理中心词，每个候选词的打分，再经过softmax函数的归一化，即得到了对中心词的推理概率：
$$
\operatorname{softmax}\left(O_{i}\right)=\frac{\exp \left(O_{i}\right)}{\sum_{j} \exp \left(O_{j}\right)}
$$

#### 3.实现

需要说明的是，在本例中并没有采用上下文average或者将上下文求和之后作为输入的方式，而是先遍历上下文，依次将每个单词作为输入，以提高CBOW算法对生僻字处理的能力。

使用神经网络实现CBOW中，模型接收的输入应该有2个不同的tensor：代表当前上下文的tensor：假设我们称之为context_words V，一般来说，这个tensor是一个形状为[batch_size, vocab_size]的one-hot tensor，表示在一个mini-batch中，每组上下文中每一个单词的ID。代表目标词的tensor：假设我们称之为target_words T，一般来说，这个tensor是一个形状为[batch_size, 1]的整型tensor，这个tensor中的每个元素是一个[0, vocab_size-1]的值，代表目标词的ID。

在理想情况下，我们可以把上下文中的每一个单词，依次作为输入，把当前句子中的中心词作为标签，构建神经网络进行学习，实现上下文预测中心词；但在实际中，为避免过于庞大的计算量，我们通常采用负采样的方法，来避免查询整个此表，从而将多分类问题转换为二分类问题。实现流程如下图：

![来自飞浆：https://aistudio.baidu.com/aistudio/projectdetail/306925](img/CBOW-3.png)

在实现的过程中，通常会让模型接收3个tensor输入：

- 代表上下文单词的tensor：假设我们称之为context_words V，一般来说，这个tensor是一个形状为[batch_size, vocab_size]的one-hot tensor，表示在一个mini-batch中每个中心词具体的ID。


- 代表目标词的tensor：假设我们称之为target_words T，一般来说，这个tensor同样是一个形状为[batch_size, vocab_size]的one-hot tensor，表示在一个mini-batch中每个目标词具体的ID。


- 代表目标词标签的tensor：假设我们称之为labels LLL，一般来说，这个tensor是一个形状为[batch_size, 1]的tensor，每个元素不是0就是1（0：负样本，1：正样本）。

模型训练过程如下流程图：

![](img/CBOW-4.png)

#### 4.使用飞浆实现CBOW

使用飞桨实现CBOW模型的方法的基本流程如下：**数据处理**（选择需要使用的数据，并做好必要的预处理工作）；**网络定义**（使用飞桨定义好网络结构，包括输入层，中间层，输出层，损失函数和优化算法）；**网络训练**（将准备好的数据送入神经网络进行学习，并观察学习的过程是否正常，如损失函数值是否在降低，也可以打印一些中间步骤的结果出来等）；**训练结果与总结**（使用测试集合测试训练好的神经网络，看看训练效果如何）。

##### 4.1 数据处理

首先找到一个合适的语料用于训练word2vec模型，这里选择text8数据集，该数据集里包含了大量从维基百科收集到的英文语料，下载后的文件被保存在当前目录的‘text8.txt‘文件内。

接下来，把下载的语料读取到程序里，并打印前500个字符观察语料的样子。一般来说，在自然语言处理中，还需要先对语料进行切词。在经过切词后，需要对语料进行统计，为每个词构造ID。一般来说，可以根据每个词在语料中出现的频次构造ID，频次越高，ID越小，便于对词典进行管理。

然后需要使用二次采样法处理原始文本。二次采样法的主要思想是降低高频词在语料中出现的频次，从而优化整个词表的词向量训练效果。在完成语料数据预处理之后，需要构造训练数据。根据上面的描述，我们需要使用一个滑动窗口对语料从左到右扫描，在每个窗口内，通过上下文预测中心词，并形成训练数据。在实际操作中，由于词表往往很大（50000，100000等），对大词表的一些矩阵运算（如softmax）需要消耗巨大的资源，因此可以通过负采样的方式模拟softmax的结果。具体来说，给定上下文和需要预测的中心词，把中心词作为正样本；通过词表随机采样的方式，选择若干个负样本。这样就把一个大规模分类问题转化为一个2分类问题，通过这种方式优化计算速度。

训练数据准备好后，把训练数据都组装成mini-batch，并准备输入到网络中进行训练。

##### 4.2 网络定义&网络训练

定义CBOW的网络结构用于模型训练。在飞桨动态图中，对于任意网络，都需要定义一个继承自fluid.dygraph.Layer的类来搭建网络结构、参数等数据的声明。同时需要在forward函数中定义网络的计算逻辑。值得注意的是，我们仅需要定义网络的前向计算逻辑，飞桨会自动完成神经网络的反向计算。

完成网络定义后就可以启动模型训练。这里定义每隔100步打印一次loss，以确保当前的网络是正常收敛的。同时，每隔1000步观察一下计算出来的同义词，从而可视化网络的训练效果。

##### 4.3 训练结果与总结

从下面两张loss的打印结果可以看到，经过一定步骤的训练，loss逐渐下降并趋于稳定。

| ![](img/loss-1.png) | ![](img/loss-2.png) |
| ------------------- | ------------------- |

此外，训练趋于后期还可以发现模型学习到了一些有趣的语言现象。比如：与one比较接近的词有"one, nine, eight, six, five"；与who比较接近的词有"who, whom,nobility,him,caravaggio"；与king比较接近的词有"king,brother,constantine,married,son"等。从结果可以看出来，对于出现频率最高的数字以及who, he等代词，模型可以很快地学习出它们之间的相似性，但对于king这类出现频率相对较低的词来说，模型很难学习出它们的相似性。这也是CBOW算法的局限性。

#### 5.与Skip-gram区别

![](img/difference.png)

#### 6.引用

[词向量Word Embedding]: https://aistudio.baidu.com/aistudio/projectdetail/2205123
[词向量word2vec之CBOW算法]: https://aistudio.baidu.com/aistudio/projectdetail/306925

