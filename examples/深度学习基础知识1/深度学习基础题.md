# 深度学习基础题

## 基于层次softmax训练词向量

通过寻找一个模型，使他在给定一个词向量时，计算出这个词附近出现每个词的概率，即将一个词看成一个类别，词典中有多少词，就是多少个类别，在给定输入下每个类别的概率。此时可以通过softmax函数建立模型。

将词典大小设为D，w表示词典中的每个词，有

![1](C:\Users\1\Desktop\3\1.jpg)

通过该流程和softmax的定义，可以得出模型的假设函数：![2](C:\Users\1\Desktop\3\2.png)

在定义了假设函数后，要建立损失函数评估训练的效果。

我们可以定义模型输入的词为中心词，将这个词两边的词记为目标词。假如我们只将中心词附近的m个词认为是它的共现词，则目标词一共有2m个。

由于这里类别特别多，所以算出来的每个概率都可能非常小，为了避免浮点下溢（值太小，容易在计算机中被当成0，而且容易被存储浮点数的噪声淹没），更明智的选择是使用对数似然函数。所以对于一段长度为T的训练文本，损失函数即：

![3](C:\Users\1\Desktop\3\3.png)

这里要让长度为m的窗口滑过训练文本中的每个词，滑到每个词时，都要计算2m次后验概率。而每次计算后验概率都要用到softmax函数，而回顾一下softmax函数的分母：

![4](C:\Users\1\Desktop\3\4.png)

一种很巧妙的方法是将原来计算复杂度为D的分母（要计算D次指数函数）通过构造一棵“胡夫曼二叉树(Huffman binary tree)”来将原来扁平的“softmax”给变成树状的softmax，从而将softmax的分母给优化成计算复杂度为log D。这种树形的softmax也叫分层（Hierarchical Softmax）。

### 分层softmax

分层softmax的计算过程如下图：

![4](C:\Users\1\Desktop\3\4.webp)

从图中可以看出，hidden layer到output layer的连接原本是一个简单的softmax，有V个神经元和所有的hidden layer两两连接，现在变成了一个树，有V-1个神经元和所有的hidden layer两两连接。计算概率的方法就发生了变化：

![5](C:\Users\1\Desktop\3\5.svg)

## LSTM可实现其他类型的NLP任务

### 序列到类别——中文情感分类

* 准备数据集
* 将数据集中的所有字映射为字典，使得每个字都有唯一的标号对应
* 实现embedding层、
* 添加LSTM层进行特征抽取

### 同步的序列到序列——中英翻译

* 准备数据集
* 使用LSTM编码，由encoder得到整句话的embedding，将输入转化成了一个向量
* 将上述得到的向量放到decoder中解码，即将得到的embedding逐次解码映射到词典中的某个词，找出概率最高的词，然后作为输出

### 异步的序列到序列——古诗生成

* 数据处理，按照字的出现频率建立字符集词典，根据词典得到每个字对应的索引号，建立从字符到索引号和索引号到字符两个字典。
* 生成训练集
* 建立模型，使用两个LSTM叠加上一个全连接层再进行训练
* 输入文本，开始预测
