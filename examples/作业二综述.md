一、损失函数
1、log对数损失函数（逻辑回归）
有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到，而逻辑回归得到的并不是平方损失。在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即max F(y, f(x)) —-> min -F(y, f(x)))。从损失函数的视角来看，它就成了log损失函数了。

log损失函数的标准形式：

L(Y,P(Y|X))=−logP(Y|X)L(Y,P(Y|X))=−log⁡P(Y|X)


刚刚说到，取对数是为了方便计算极大似然估计，因为在MLE中，直接求导比较困难，所以通常都是先取对数再求导找极值点。损失函数L(Y, P(Y|X))表达的是样本X在分类Y的情况下，使概率P(Y|X)达到最大值（换言之，就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大）。因为log函数是单调递增的，所以logP(Y|X)也会达到最大值，因此在前面加上负号之后，最大化P(Y|X)就等价于最小化L了。
逻辑回归的P(Y=y|x)表达式如下：

P(Y=y|x)=11+exp(−yf(x))P(Y=y|x)=11+exp(−yf(x))


将它带入到上式，通过推导可以得到logistic的损失函数表达式，如下：

L(y,P(Y=y|x))=log(1+exp(−yf(x)))L(y,P(Y=y|x))=log⁡(1+exp(−yf(x)))


逻辑回归最后得到的目标式子如下：



 

如果是二分类的话，则m值等于2，如果是多分类，m就是相应的类别总个数。这里需要解释一下：之所以有人认为逻辑回归是平方损失，是因为在使用梯度下降来求最优解的时候，它的迭代式子与平方损失求导后的式子非常相似，从而给人一种直观上的错觉。

这里有个PDF可以参考一下：Lecture 6: logistic regression.pdf.

2、平方损失函数（最小二乘法, Ordinary Least Squares ）
最小二乘法是线性回归的一种，OLS将问题转化成了一个凸优化问题。在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是中心极限定理，可以参考【central limit theorem】），最后通过极大似然估计（MLE）可以推导出最小二乘式子。最小二乘的基本原则是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。换言之，OLS是基于距离的，而这个距离就是我们用的最多的欧几里得距离。为什么它会选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因：

简单，计算方便；
欧氏距离是一种很好的相似性度量标准；
在不同的表示域变换后特征性质不变。
平方损失（Square loss）的标准形式如下：

L(Y,f(X))=(Y−f(X))2L(Y,f(X))=(Y−f(X))2


当样本个数为n时，此时的损失函数变为：

Y-f(X)表示的是残差，整个式子表示的是残差的平方和，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和（residual sum of squares，RSS）。

 

而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下：

MSE=1n∑i=1n(Yi~−Yi)2MSE=1n∑i=1n(Yi~−Yi)2


上面提到了线性回归，这里额外补充一句，我们通常说的线性有两种情况，一种是因变量y是自变量x的线性函数，一种是因变量y是参数αα的线性函数。在机器学习中，通常指的都是后一种情况。

 

3、指数损失函数（Adaboost）
学过Adaboost算法的人都知道，它是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，可以得到fm(x)fm(x):



Adaboost每次迭代时的目的是为了找到最小化下列式子时的参数αα 和G：

 



而指数损失函数(exp-loss）的标准形式如下

 



可以看出，Adaboost的目标式子就是指数损失，在给定n个样本的情况下，Adaboost的损失函数为：



关于Adaboost的推导，可以参考Wikipedia：AdaBoost或者《统计学习方法》P145.

4、Hinge损失函数（SVM）
在机器学习算法中，hinge损失函数和SVM是息息相关的。在线性支持向量机中，最优化问题可以等价于下列式子：

下面来对式子做个变形，令：

于是，原式就变成了：

如若取λ=12Cλ=12C，式子就可以表示成：

可以看出，该式子与下式非常相似：



前半部分中的ll就是hinge损失函数，而后面相当于L2正则项。

Hinge 损失函数的标准形式

L(y)=max(0,1−yy~),y=±1L(y)=max(0,1−yy~),y=±1


可以看出，当|y|>=1时，L(y)=0。

 

更多内容，参考Hinge-loss。

补充一下：在libsvm中一共有4中核函数可以选择，对应的是-t参数分别是：

0-线性核；
1-多项式核；
2-RBF核；
3-sigmoid核。


二、池化方法总结

1.  一般池化（General Pooling）
池化作用于图像中不重合的区域（这与卷积操作不同），过程如下图。
![](https://ai-studio-static-online.cdn.bcebos.com/31f62acd27a9490eb4f21a3f2e1c7409e093464c640d42a9b67ec91bdbfecb05)
我们定义池化窗口的大小为sizeX，即下图中红色正方形的边长，定义两个相邻池化窗口的水平位移/竖直位移为stride。一般池化由于每一池化窗口都是不重复的，所以sizeX=stride。
最常见的池化操作为平均池化mean pooling和最大池化max pooling：
平均池化：计算图像区域的平均值作为该区域池化后的值。
最大池化：选图像区域的最大值作为该区域池化后的值。
2. 重叠池化（OverlappingPooling）[2]
重叠池化正如其名字所说的，相邻池化窗口之间会有重叠区域，此时sizeX>stride。
论文中[2]中，作者使用了重叠池化，其他的设置都不变的情况下， top-1和top-5 的错误率分别减少了0.4% 和0.3%
3. 空金字塔池化（Spatial Pyramid Pooling）[3] 
空间金字塔池化可以把任何尺度的图像的卷积特征转化成相同维度，这不仅可以让CNN处理任意尺度的图像，还能避免cropping和warping操作，导致一些信息的丢失，具有非常重要的意义。


三、数据增强方法修改
在线增强
在训练前对数据集进行处理，往往能得到多倍的数据集。
离线增强
在线增强是在训练时对加载数据进行预处理，不改变训练数据的数量。


