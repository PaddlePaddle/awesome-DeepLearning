{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 波士顿房价预测任务 \n",
    "\n",
    "## 线性回归模型\n",
    "\n",
    "假设房价和各影响因素之间能够用线性关系来描述：\n",
    "\n",
    "$$y = {\\sum_{j=1}^Mx_j w_j} + b$$\n",
    "\n",
    "模型的求解即是通过数据拟合出每个$w_j$和$b$。其中，$w_j$和$b$分别表示该线性模型的权重和偏置。一维情况下，$w_j$ 和 $b$ 是直线的斜率和截距。\n",
    "\n",
    "线性回归模型使用均方误差作为损失函数（Loss），用以衡量预测房价和真实房价的差异，公式如下：\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat{Y_i} - {Y_i})^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# python+numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据处理\n",
    "\n",
    "数据处理包含五个部分：数据导入、数据形状变换、数据集划分、数据归一化处理和封装`load data`函数。数据预处理后，才能被模型调用。\n",
    "#### 读入数据\n",
    "通过如下代码读入数据，了解下波士顿房价的数据集结构，数据存放在本地目录下housing.data文件中。\n",
    "#### 数据形状变换\n",
    "由于读入的原始数据是1维的，所有数据都连在一起。因此需要我们将数据的形状进行变换，形成一个2维的矩阵，每行为一个数据样本（14个值），每个数据样本包含13个$X$（影响房价的特征）和一个$Y$（该类型房屋的均价）。\n",
    "#### 数据集划分\n",
    "将数据集划分成训练集和测试集，其中训练集用于确定模型的参数，测试集用于评判模型的效果。\n",
    "在本案例中，我们将80%的数据用作训练集，20%用作测试集，实现代码如下。通过打印训练集的形状，可以发现共有404个样本，每个样本含有13个特征和1个预测值。\n",
    "#### 数据归一化处理\n",
    "对每个特征进行归一化处理，使得每个特征的取值缩放到0~1之间。这样做有两个好处：一是模型训练更高效；二是特征前的权重大小可以代表该变量对预测结果的贡献度（因为每个特征值本身的范围相同）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # 从文件导入数据\n",
    "    datafile = './work/housing.data'\n",
    "    data = np.fromfile(datafile, sep=' ')\n",
    "\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "    feature_num = len(feature_names)\n",
    "\n",
    "    # 数据形状变换\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
    "\n",
    "    # 将原数据集拆分成训练集和测试集\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\n",
    "    # 测试集和训练集必须是没有交集的\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0] * ratio)\n",
    "    training_data = data[:offset]\n",
    "\n",
    "    # 计算训练集的最大值，最小值，平均值\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\n",
    "\n",
    "    # 对数据进行归一化处理\n",
    "    for i in range(feature_num):\n",
    "        #print(maximums[i], minimums[i], avgs[i])\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\n",
    "\n",
    "    # 训练集和测试集的划分比例\n",
    "    training_data = data[:offset]\n",
    "    test_data = data[offset:]\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "training_data, test_data = load_data()\n",
    "x = training_data[:, :-1]\n",
    "y = training_data[:, -1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型设计\n",
    "\n",
    "模型设计是深度学习模型关键要素之一，也称为网络结构设计，相当于模型的假设空间，即实现模型“前向计算”（从输入到输出）的过程。\n",
    "\n",
    "如果将输入特征和输出预测值均以向量表示，输入特征$x$有13个分量，$y$有1个分量，那么参数权重的形状（shape）是$13\\times1$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练过程\n",
    "\n",
    "上述计算过程描述了如何构建神经网络，通过神经网络完成预测值和损失函数的计算。接下来介绍如何求解参数$w$和$b$的数值，这个过程也称为模型训练过程。训练过程是深度学习模型的关键要素之一，其目标是让定义的损失函数$Loss$尽可能的小，也就是说找到一个参数解$w$和$b$，使得损失函数取得极小值。\n",
    "\n",
    "### 梯度下降法\n",
    "\n",
    "在现实中存在大量的函数正向求解容易，但反向求解较难，被称为单向函数，这种函数在密码学中有大量的应用。密码锁的特点是可以迅速判断一个密钥是否是正确的(已知$x$，求$y$很容易)，但是即使获取到密码锁系统，无法破解出正确的密钥是什么（已知$y$，求$x$很难）。\n",
    "\n",
    "这种情况特别类似于一位想从山峰走到坡谷的盲人，他看不见坡谷在哪（无法逆向求解出$Loss$导数为0时的参数值），但可以伸脚探索身边的坡度（当前点的导数值，也称为梯度）。那么，求解Loss函数最小值可以这样实现：从当前的参数取值，一步步的按照下坡的方向下降，直到走到最低点。这就是“梯度下降法”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / iter   0, loss = 6.3256\n",
      "Epoch   0 / iter   1, loss = 5.8511\n",
      "Epoch   0 / iter   2, loss = 2.0103\n",
      "Epoch   0 / iter   3, loss = 0.9236\n",
      "Epoch   0 / iter   4, loss = 0.3894\n",
      "Epoch   1 / iter   0, loss = 0.2812\n",
      "Epoch   1 / iter   1, loss = 0.1347\n",
      "Epoch   1 / iter   2, loss = 0.0898\n",
      "Epoch   1 / iter   3, loss = 0.0485\n",
      "Epoch   1 / iter   4, loss = 0.0057\n",
      "Epoch   2 / iter   0, loss = 0.0407\n",
      "Epoch   2 / iter   1, loss = 0.0237\n",
      "Epoch   2 / iter   2, loss = 0.0176\n",
      "Epoch   2 / iter   3, loss = 0.0231\n",
      "Epoch   2 / iter   4, loss = 0.0047\n",
      "Epoch   3 / iter   0, loss = 0.0253\n",
      "Epoch   3 / iter   1, loss = 0.0200\n",
      "Epoch   3 / iter   2, loss = 0.0177\n",
      "Epoch   3 / iter   3, loss = 0.0210\n",
      "Epoch   3 / iter   4, loss = 0.0478\n",
      "Epoch   4 / iter   0, loss = 0.0237\n",
      "Epoch   4 / iter   1, loss = 0.0232\n",
      "Epoch   4 / iter   2, loss = 0.0172\n",
      "Epoch   4 / iter   3, loss = 0.0205\n",
      "Epoch   4 / iter   4, loss = 0.0076\n",
      "Epoch   5 / iter   0, loss = 0.0220\n",
      "Epoch   5 / iter   1, loss = 0.0174\n",
      "Epoch   5 / iter   2, loss = 0.0253\n",
      "Epoch   5 / iter   3, loss = 0.0175\n",
      "Epoch   5 / iter   4, loss = 0.0440\n",
      "Epoch   6 / iter   0, loss = 0.0265\n",
      "Epoch   6 / iter   1, loss = 0.0204\n",
      "Epoch   6 / iter   2, loss = 0.0211\n",
      "Epoch   6 / iter   3, loss = 0.0173\n",
      "Epoch   6 / iter   4, loss = 0.0014\n",
      "Epoch   7 / iter   0, loss = 0.0200\n",
      "Epoch   7 / iter   1, loss = 0.0230\n",
      "Epoch   7 / iter   2, loss = 0.0201\n",
      "Epoch   7 / iter   3, loss = 0.0202\n",
      "Epoch   7 / iter   4, loss = 0.0058\n",
      "Epoch   8 / iter   0, loss = 0.0215\n",
      "Epoch   8 / iter   1, loss = 0.0212\n",
      "Epoch   8 / iter   2, loss = 0.0175\n",
      "Epoch   8 / iter   3, loss = 0.0233\n",
      "Epoch   8 / iter   4, loss = 0.0055\n",
      "Epoch   9 / iter   0, loss = 0.0162\n",
      "Epoch   9 / iter   1, loss = 0.0144\n",
      "Epoch   9 / iter   2, loss = 0.0367\n",
      "Epoch   9 / iter   3, loss = 0.0154\n",
      "Epoch   9 / iter   4, loss = 0.0425\n",
      "Epoch  10 / iter   0, loss = 0.0187\n",
      "Epoch  10 / iter   1, loss = 0.0250\n",
      "Epoch  10 / iter   2, loss = 0.0193\n",
      "Epoch  10 / iter   3, loss = 0.0209\n",
      "Epoch  10 / iter   4, loss = 0.0093\n",
      "Epoch  11 / iter   0, loss = 0.0253\n",
      "Epoch  11 / iter   1, loss = 0.0173\n",
      "Epoch  11 / iter   2, loss = 0.0202\n",
      "Epoch  11 / iter   3, loss = 0.0204\n",
      "Epoch  11 / iter   4, loss = 0.0022\n",
      "Epoch  12 / iter   0, loss = 0.0213\n",
      "Epoch  12 / iter   1, loss = 0.0209\n",
      "Epoch  12 / iter   2, loss = 0.0212\n",
      "Epoch  12 / iter   3, loss = 0.0195\n",
      "Epoch  12 / iter   4, loss = 0.0075\n",
      "Epoch  13 / iter   0, loss = 0.0226\n",
      "Epoch  13 / iter   1, loss = 0.0241\n",
      "Epoch  13 / iter   2, loss = 0.0160\n",
      "Epoch  13 / iter   3, loss = 0.0206\n",
      "Epoch  13 / iter   4, loss = 0.0148\n",
      "Epoch  14 / iter   0, loss = 0.0186\n",
      "Epoch  14 / iter   1, loss = 0.0234\n",
      "Epoch  14 / iter   2, loss = 0.0233\n",
      "Epoch  14 / iter   3, loss = 0.0170\n",
      "Epoch  14 / iter   4, loss = 0.0239\n",
      "Epoch  15 / iter   0, loss = 0.0185\n",
      "Epoch  15 / iter   1, loss = 0.0193\n",
      "Epoch  15 / iter   2, loss = 0.0258\n",
      "Epoch  15 / iter   3, loss = 0.0224\n",
      "Epoch  15 / iter   4, loss = 0.0105\n",
      "Epoch  16 / iter   0, loss = 0.0185\n",
      "Epoch  16 / iter   1, loss = 0.0244\n",
      "Epoch  16 / iter   2, loss = 0.0232\n",
      "Epoch  16 / iter   3, loss = 0.0166\n",
      "Epoch  16 / iter   4, loss = 0.0038\n",
      "Epoch  17 / iter   0, loss = 0.0195\n",
      "Epoch  17 / iter   1, loss = 0.0166\n",
      "Epoch  17 / iter   2, loss = 0.0233\n",
      "Epoch  17 / iter   3, loss = 0.0236\n",
      "Epoch  17 / iter   4, loss = 0.0048\n",
      "Epoch  18 / iter   0, loss = 0.0263\n",
      "Epoch  18 / iter   1, loss = 0.0175\n",
      "Epoch  18 / iter   2, loss = 0.0237\n",
      "Epoch  18 / iter   3, loss = 0.0151\n",
      "Epoch  18 / iter   4, loss = 0.0136\n",
      "Epoch  19 / iter   0, loss = 0.0215\n",
      "Epoch  19 / iter   1, loss = 0.0203\n",
      "Epoch  19 / iter   2, loss = 0.0194\n",
      "Epoch  19 / iter   3, loss = 0.0208\n",
      "Epoch  19 / iter   4, loss = 0.0123\n",
      "Epoch  20 / iter   0, loss = 0.0215\n",
      "Epoch  20 / iter   1, loss = 0.0180\n",
      "Epoch  20 / iter   2, loss = 0.0191\n",
      "Epoch  20 / iter   3, loss = 0.0222\n",
      "Epoch  20 / iter   4, loss = 0.0397\n",
      "Epoch  21 / iter   0, loss = 0.0246\n",
      "Epoch  21 / iter   1, loss = 0.0220\n",
      "Epoch  21 / iter   2, loss = 0.0207\n",
      "Epoch  21 / iter   3, loss = 0.0150\n",
      "Epoch  21 / iter   4, loss = 0.0299\n",
      "Epoch  22 / iter   0, loss = 0.0266\n",
      "Epoch  22 / iter   1, loss = 0.0193\n",
      "Epoch  22 / iter   2, loss = 0.0221\n",
      "Epoch  22 / iter   3, loss = 0.0159\n",
      "Epoch  22 / iter   4, loss = 0.0148\n",
      "Epoch  23 / iter   0, loss = 0.0237\n",
      "Epoch  23 / iter   1, loss = 0.0189\n",
      "Epoch  23 / iter   2, loss = 0.0146\n",
      "Epoch  23 / iter   3, loss = 0.0245\n",
      "Epoch  23 / iter   4, loss = 0.0263\n",
      "Epoch  24 / iter   0, loss = 0.0239\n",
      "Epoch  24 / iter   1, loss = 0.0143\n",
      "Epoch  24 / iter   2, loss = 0.0224\n",
      "Epoch  24 / iter   3, loss = 0.0213\n",
      "Epoch  24 / iter   4, loss = 0.0032\n",
      "Epoch  25 / iter   0, loss = 0.0197\n",
      "Epoch  25 / iter   1, loss = 0.0195\n",
      "Epoch  25 / iter   2, loss = 0.0226\n",
      "Epoch  25 / iter   3, loss = 0.0181\n",
      "Epoch  25 / iter   4, loss = 0.0362\n",
      "Epoch  26 / iter   0, loss = 0.0211\n",
      "Epoch  26 / iter   1, loss = 0.0217\n",
      "Epoch  26 / iter   2, loss = 0.0203\n",
      "Epoch  26 / iter   3, loss = 0.0165\n",
      "Epoch  26 / iter   4, loss = 0.0532\n",
      "Epoch  27 / iter   0, loss = 0.0165\n",
      "Epoch  27 / iter   1, loss = 0.0203\n",
      "Epoch  27 / iter   2, loss = 0.0216\n",
      "Epoch  27 / iter   3, loss = 0.0191\n",
      "Epoch  27 / iter   4, loss = 0.0864\n",
      "Epoch  28 / iter   0, loss = 0.0232\n",
      "Epoch  28 / iter   1, loss = 0.0170\n",
      "Epoch  28 / iter   2, loss = 0.0211\n",
      "Epoch  28 / iter   3, loss = 0.0220\n",
      "Epoch  28 / iter   4, loss = 0.0313\n",
      "Epoch  29 / iter   0, loss = 0.0214\n",
      "Epoch  29 / iter   1, loss = 0.0185\n",
      "Epoch  29 / iter   2, loss = 0.0201\n",
      "Epoch  29 / iter   3, loss = 0.0222\n",
      "Epoch  29 / iter   4, loss = 0.0059\n",
      "Epoch  30 / iter   0, loss = 0.0208\n",
      "Epoch  30 / iter   1, loss = 0.0198\n",
      "Epoch  30 / iter   2, loss = 0.0160\n",
      "Epoch  30 / iter   3, loss = 0.0227\n",
      "Epoch  30 / iter   4, loss = 0.0422\n",
      "Epoch  31 / iter   0, loss = 0.0177\n",
      "Epoch  31 / iter   1, loss = 0.0204\n",
      "Epoch  31 / iter   2, loss = 0.0253\n",
      "Epoch  31 / iter   3, loss = 0.0160\n",
      "Epoch  31 / iter   4, loss = 0.0015\n",
      "Epoch  32 / iter   0, loss = 0.0154\n",
      "Epoch  32 / iter   1, loss = 0.0159\n",
      "Epoch  32 / iter   2, loss = 0.0226\n",
      "Epoch  32 / iter   3, loss = 0.0250\n",
      "Epoch  32 / iter   4, loss = 0.0053\n",
      "Epoch  33 / iter   0, loss = 0.0186\n",
      "Epoch  33 / iter   1, loss = 0.0179\n",
      "Epoch  33 / iter   2, loss = 0.0223\n",
      "Epoch  33 / iter   3, loss = 0.0200\n",
      "Epoch  33 / iter   4, loss = 0.0040\n",
      "Epoch  34 / iter   0, loss = 0.0140\n",
      "Epoch  34 / iter   1, loss = 0.0212\n",
      "Epoch  34 / iter   2, loss = 0.0201\n",
      "Epoch  34 / iter   3, loss = 0.0226\n",
      "Epoch  34 / iter   4, loss = 0.0049\n",
      "Epoch  35 / iter   0, loss = 0.0150\n",
      "Epoch  35 / iter   1, loss = 0.0243\n",
      "Epoch  35 / iter   2, loss = 0.0121\n",
      "Epoch  35 / iter   3, loss = 0.0259\n",
      "Epoch  35 / iter   4, loss = 0.0040\n",
      "Epoch  36 / iter   0, loss = 0.0164\n",
      "Epoch  36 / iter   1, loss = 0.0232\n",
      "Epoch  36 / iter   2, loss = 0.0179\n",
      "Epoch  36 / iter   3, loss = 0.0184\n",
      "Epoch  36 / iter   4, loss = 0.0087\n",
      "Epoch  37 / iter   0, loss = 0.0140\n",
      "Epoch  37 / iter   1, loss = 0.0188\n",
      "Epoch  37 / iter   2, loss = 0.0234\n",
      "Epoch  37 / iter   3, loss = 0.0185\n",
      "Epoch  37 / iter   4, loss = 0.0198\n",
      "Epoch  38 / iter   0, loss = 0.0164\n",
      "Epoch  38 / iter   1, loss = 0.0208\n",
      "Epoch  38 / iter   2, loss = 0.0221\n",
      "Epoch  38 / iter   3, loss = 0.0166\n",
      "Epoch  38 / iter   4, loss = 0.0076\n",
      "Epoch  39 / iter   0, loss = 0.0159\n",
      "Epoch  39 / iter   1, loss = 0.0173\n",
      "Epoch  39 / iter   2, loss = 0.0237\n",
      "Epoch  39 / iter   3, loss = 0.0160\n",
      "Epoch  39 / iter   4, loss = 0.0169\n",
      "Epoch  40 / iter   0, loss = 0.0130\n",
      "Epoch  40 / iter   1, loss = 0.0205\n",
      "Epoch  40 / iter   2, loss = 0.0188\n",
      "Epoch  40 / iter   3, loss = 0.0202\n",
      "Epoch  40 / iter   4, loss = 0.0058\n",
      "Epoch  41 / iter   0, loss = 0.0099\n",
      "Epoch  41 / iter   1, loss = 0.0170\n",
      "Epoch  41 / iter   2, loss = 0.0199\n",
      "Epoch  41 / iter   3, loss = 0.0240\n",
      "Epoch  41 / iter   4, loss = 0.0032\n",
      "Epoch  42 / iter   0, loss = 0.0178\n",
      "Epoch  42 / iter   1, loss = 0.0205\n",
      "Epoch  42 / iter   2, loss = 0.0178\n",
      "Epoch  42 / iter   3, loss = 0.0134\n",
      "Epoch  42 / iter   4, loss = 0.0130\n",
      "Epoch  43 / iter   0, loss = 0.0192\n",
      "Epoch  43 / iter   1, loss = 0.0158\n",
      "Epoch  43 / iter   2, loss = 0.0213\n",
      "Epoch  43 / iter   3, loss = 0.0134\n",
      "Epoch  43 / iter   4, loss = 0.0091\n",
      "Epoch  44 / iter   0, loss = 0.0120\n",
      "Epoch  44 / iter   1, loss = 0.0252\n",
      "Epoch  44 / iter   2, loss = 0.0161\n",
      "Epoch  44 / iter   3, loss = 0.0135\n",
      "Epoch  44 / iter   4, loss = 0.0067\n",
      "Epoch  45 / iter   0, loss = 0.0157\n",
      "Epoch  45 / iter   1, loss = 0.0132\n",
      "Epoch  45 / iter   2, loss = 0.0134\n",
      "Epoch  45 / iter   3, loss = 0.0217\n",
      "Epoch  45 / iter   4, loss = 0.0519\n",
      "Epoch  46 / iter   0, loss = 0.0156\n",
      "Epoch  46 / iter   1, loss = 0.0184\n",
      "Epoch  46 / iter   2, loss = 0.0168\n",
      "Epoch  46 / iter   3, loss = 0.0149\n",
      "Epoch  46 / iter   4, loss = 0.0116\n",
      "Epoch  47 / iter   0, loss = 0.0150\n",
      "Epoch  47 / iter   1, loss = 0.0155\n",
      "Epoch  47 / iter   2, loss = 0.0134\n",
      "Epoch  47 / iter   3, loss = 0.0203\n",
      "Epoch  47 / iter   4, loss = 0.0094\n",
      "Epoch  48 / iter   0, loss = 0.0126\n",
      "Epoch  48 / iter   1, loss = 0.0118\n",
      "Epoch  48 / iter   2, loss = 0.0151\n",
      "Epoch  48 / iter   3, loss = 0.0245\n",
      "Epoch  48 / iter   4, loss = 0.0091\n",
      "Epoch  49 / iter   0, loss = 0.0216\n",
      "Epoch  49 / iter   1, loss = 0.0151\n",
      "Epoch  49 / iter   2, loss = 0.0135\n",
      "Epoch  49 / iter   3, loss = 0.0122\n",
      "Epoch  49 / iter   4, loss = 0.0100\n",
      "The testing loss = 0.0072\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # sigmoid激活函数\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    # sigmoid激活函数的导数\n",
    "    return x*(1-x)\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, num_of_weights,hidden_sum):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w_1 = np.random.randn(num_of_weights, hidden_sum)  # 第一个全连接层的网络参数\n",
    "        self.b_1 = np.zeros(hidden_sum)\n",
    "        self.w_2 = np.random.randn(hidden_sum,1) # 第二个全连接层的网络参数\n",
    "        self.b_2 = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w_1) + self.b_1  # 全连接层\n",
    "        z = sigmoid(z)  # sigmoid激活层\n",
    "        z = np.dot(z, self.w_2) + self.b_2  # 全连接层\n",
    "        return z\n",
    "    \n",
    "    # 损失函数 均方误差\n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        cost = error * error\n",
    "        num_samples = error.shape[0]\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return 0.5*cost\n",
    "    \n",
    "    # 计算梯度\n",
    "    def gradient(self, x, y):\n",
    "        # 梯度计算\n",
    "        o_1 = sigmoid(np.dot(x, self.w_1) + self.b_1)  # 第一个全连接层的输出\n",
    "        z = self.forward(x)  # 第一个全连接层的输出\n",
    "\n",
    "        gradient_w_1 = x.T.dot((z-y).dot(self.w_2.T) * dsigmoid(o_1))   # 第一个全连接层参数的梯度\n",
    "        gradient_b_1 = np.mean((z-y).dot(self.w_2.T) * dsigmoid(o_1), axis=0) \n",
    "\n",
    "        gradient_w_2 = np.mean((z-y)*o_1,axis=0)  # 第二个全连接层参数的梯度\n",
    "        gradient_w_2 = gradient_w_2[:,np.newaxis]\n",
    "        gradient_b_2 = np.mean((z-y))\n",
    "        return gradient_w_1, gradient_b_1, gradient_w_2, gradient_b_2\n",
    "    \n",
    "    def update(self, gradient_w_1, gradient_w_2, gradient_b_1, gradient_b_2,eta):\n",
    "        self.w_1 = self.w_1 - eta * gradient_w_1\n",
    "        self.b_1 = self.b_1 - eta * gradient_b_1\n",
    "        self.w_2 = self.w_2 - eta * gradient_w_2\n",
    "        self.b_2 = self.b_2 - eta * gradient_b_2\n",
    "            \n",
    "    # 随机梯度下降法（ Stochastic Gradient Descent）            \n",
    "    def train(self, training_data, num_epochs, batch_size=100, eta=0.1):\n",
    "        n = len(training_data)\n",
    "        losses = []\n",
    "        for epoch_id in range(num_epochs):\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\n",
    "            # 然后再按每次取batch_size条数据的方式取出\n",
    "            np.random.shuffle(training_data)\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\n",
    "                #print(self.w.shape)\n",
    "                #print(self.b)\n",
    "                x = mini_batch[:, :-1]\n",
    "                y = mini_batch[:, -1:]\n",
    "                a = self.forward(x)\n",
    "                loss = self.loss(a, y)\n",
    "                gradient_w_1, gradient_b_1, gradient_w_2, gradient_b_2 = self.gradient(x, y)  # 参数梯度\n",
    "                self.update(gradient_w_1, gradient_w_2, gradient_b_1, gradient_b_2, eta)  # 更新参数\n",
    "                losses.append(loss)\n",
    "                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\n",
    "                                 format(epoch_id, iter_id, loss))\n",
    "        \n",
    "        return losses\n",
    "\n",
    "    def test(self,test_data):\n",
    "        x = test_data[:, :-1]   # 特征\n",
    "        y = test_data[:, -1:]   # 标签\n",
    "        a = self.forward(x)      # 网络输出\n",
    "        loss = self.loss(a, y)   # 网络损失\n",
    "        print('The testing loss = {:.4f}'.format(loss))\n",
    "\n",
    "\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "\n",
    "# 创建网络\n",
    "net = Network(13,8)\n",
    "# 启动训练\n",
    "losses = net.train(train_data, num_epochs=50, batch_size=100, eta=0.1)\n",
    "net.test(test_data)  # 测试集的效果\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(len(losses))\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 使用飞桨构建波士顿房价预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#在数据处理之前，需要先加载飞桨框架的相关类库。\n",
    "import paddle\n",
    "from paddle.nn import Linear\n",
    "import paddle.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "代码中参数含义如下：\n",
    "\n",
    "* paddle：飞桨的主库，paddle 根目录下保留了常用API的别名，当前包括：paddle.tensor、paddle.framework目录下的所有API。\n",
    "\n",
    "* paddle.nn：组网相关的API，例如 Linear 、卷积 Conv2D 、 循环神经网络 LSTM 、损失函数 CrossEntropyLoss 、 激活函数 ReLU 等。\n",
    "\n",
    "* Linear：神经网络的全连接层函数，即包含所有输入权重相加的基本神经元结构。在房价预测任务中，使用只有一层的神经网络（全连接层）来实现线性回归模型。\n",
    "\n",
    "* paddle.nn.functional：与paddle.nn一样，包含组网相关的API，例如Linear、激活函数ReLu等。两者下的同名模块功能相同，运行性能也基本一致。 但是，paddle.nn下的模块均是类，每个类下可以自带模块参数；paddle.nn.functional下的模块均是函数，需要手动传入模块计算需要的参数。在实际使用中，卷积、全连接层等层本身具有可学习的参数，建议使用paddle.nn模块，而激活函数、池化等操作没有可学习参数，可以考虑直接使用paddle.nn.functional下的函数代替。\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "数据处理\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    datafile = './work/housing.data'\r\n",
    "    data = np.fromfile(datafile, sep=' ', dtype=np.float32)\r\n",
    "\r\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\r\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\r\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\r\n",
    "    feature_num = len(feature_names)\r\n",
    "\r\n",
    "    # 数据形状变换\r\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\r\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\r\n",
    "\r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算训练集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\r\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\r\n",
    "\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        #print(maximums[i], minimums[i], avgs[i])\r\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型设计\n",
    "实现过程分如下两步：\n",
    "\n",
    "1. **定义init函数**：在类的初始化函数中声明每一层网络的实现函数。在房价预测模型中，只需要定义一层全连接层。\n",
    "1. **定义forward函数**：构建神经网络结构，实现前向计算过程，并返回预测结果，在本任务中返回的是房价预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Regressor(paddle.nn.Layer):\r\n",
    "\r\n",
    "    # self代表类的实例自身\r\n",
    "    def __init__(self):\r\n",
    "        # 初始化父类中的一些参数\r\n",
    "        super(Regressor, self).__init__()\r\n",
    "        \r\n",
    "        # 定义2层全连接层，第一层输入维度是13，输出维度是8，第二层输入为8，输出为1\r\n",
    "        self.fc1 = Linear(in_features=13, out_features=8)\r\n",
    "        self.fc2 = Linear(in_features=8, out_features=1)\r\n",
    "    \r\n",
    "    # 网络的前向计算\r\n",
    "    def forward(self, inputs):\r\n",
    "        x = self.fc1(inputs)\r\n",
    "        x = F.sigmoid(x)\r\n",
    "        x = self.fc2(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 声明定义好的线性回归模型\r\n",
    "model = Regressor()\r\n",
    "# 开启模型训练模式\r\n",
    "model.train()\r\n",
    "# 加载数据\r\n",
    "training_data, test_data = load_data()\r\n",
    "# 定义优化算法，使用随机梯度下降SGD\r\n",
    "# 学习率设置为0.1\r\n",
    "opt = paddle.optimizer.SGD(learning_rate=0.1, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/ iter: 0, loss is: 0.013656497932970524\n",
      "epoch: 0/ iter: 1, loss is: 0.009776579216122627\n",
      "epoch: 0/ iter: 2, loss is: 0.01251570601016283\n",
      "epoch: 0/ iter: 3, loss is: 0.01221397239714861\n",
      "epoch: 0/ iter: 4, loss is: 0.14746616780757904\n",
      "epoch: 1/ iter: 0, loss is: 0.029910515993833542\n",
      "epoch: 1/ iter: 1, loss is: 0.020103542134165764\n",
      "epoch: 1/ iter: 2, loss is: 0.009747287258505821\n",
      "epoch: 1/ iter: 3, loss is: 0.014623909257352352\n",
      "epoch: 1/ iter: 4, loss is: 0.002632832620292902\n",
      "epoch: 2/ iter: 0, loss is: 0.014526795595884323\n",
      "epoch: 2/ iter: 1, loss is: 0.007702362723648548\n",
      "epoch: 2/ iter: 2, loss is: 0.014702584594488144\n",
      "epoch: 2/ iter: 3, loss is: 0.016314875334501266\n",
      "epoch: 2/ iter: 4, loss is: 0.02050354704260826\n",
      "epoch: 3/ iter: 0, loss is: 0.021532204002141953\n",
      "epoch: 3/ iter: 1, loss is: 0.01183726079761982\n",
      "epoch: 3/ iter: 2, loss is: 0.008866339921951294\n",
      "epoch: 3/ iter: 3, loss is: 0.01304091140627861\n",
      "epoch: 3/ iter: 4, loss is: 0.006978966295719147\n",
      "epoch: 4/ iter: 0, loss is: 0.020720789209008217\n",
      "epoch: 4/ iter: 1, loss is: 0.013769150711596012\n",
      "epoch: 4/ iter: 2, loss is: 0.009907962754368782\n",
      "epoch: 4/ iter: 3, loss is: 0.009041111916303635\n",
      "epoch: 4/ iter: 4, loss is: 0.002078067511320114\n",
      "epoch: 5/ iter: 0, loss is: 0.01412498950958252\n",
      "epoch: 5/ iter: 1, loss is: 0.013271071016788483\n",
      "epoch: 5/ iter: 2, loss is: 0.017989736050367355\n",
      "epoch: 5/ iter: 3, loss is: 0.01027832180261612\n",
      "epoch: 5/ iter: 4, loss is: 0.013158595189452171\n",
      "epoch: 6/ iter: 0, loss is: 0.010778163559734821\n",
      "epoch: 6/ iter: 1, loss is: 0.01405747514218092\n",
      "epoch: 6/ iter: 2, loss is: 0.013647631742060184\n",
      "epoch: 6/ iter: 3, loss is: 0.01857406087219715\n",
      "epoch: 6/ iter: 4, loss is: 0.007821359671652317\n",
      "epoch: 7/ iter: 0, loss is: 0.01402253657579422\n",
      "epoch: 7/ iter: 1, loss is: 0.013639582321047783\n",
      "epoch: 7/ iter: 2, loss is: 0.011404627934098244\n",
      "epoch: 7/ iter: 3, loss is: 0.016094161197543144\n",
      "epoch: 7/ iter: 4, loss is: 0.0073583186604082584\n",
      "epoch: 8/ iter: 0, loss is: 0.015957830473780632\n",
      "epoch: 8/ iter: 1, loss is: 0.012599872425198555\n",
      "epoch: 8/ iter: 2, loss is: 0.015634000301361084\n",
      "epoch: 8/ iter: 3, loss is: 0.009150320664048195\n",
      "epoch: 8/ iter: 4, loss is: 0.004909131675958633\n",
      "epoch: 9/ iter: 0, loss is: 0.011396128684282303\n",
      "epoch: 9/ iter: 1, loss is: 0.021150512620806694\n",
      "epoch: 9/ iter: 2, loss is: 0.008050295524299145\n",
      "epoch: 9/ iter: 3, loss is: 0.0128333056345582\n",
      "epoch: 9/ iter: 4, loss is: 0.0021159902680665255\n",
      "epoch: 10/ iter: 0, loss is: 0.013355663046240807\n",
      "epoch: 10/ iter: 1, loss is: 0.017130747437477112\n",
      "epoch: 10/ iter: 2, loss is: 0.012525566853582859\n",
      "epoch: 10/ iter: 3, loss is: 0.01115333754569292\n",
      "epoch: 10/ iter: 4, loss is: 0.004987969063222408\n",
      "epoch: 11/ iter: 0, loss is: 0.01532826293259859\n",
      "epoch: 11/ iter: 1, loss is: 0.009273102506995201\n",
      "epoch: 11/ iter: 2, loss is: 0.01998165249824524\n",
      "epoch: 11/ iter: 3, loss is: 0.01009319443255663\n",
      "epoch: 11/ iter: 4, loss is: 0.007117416709661484\n",
      "epoch: 12/ iter: 0, loss is: 0.011032789945602417\n",
      "epoch: 12/ iter: 1, loss is: 0.01766127161681652\n",
      "epoch: 12/ iter: 2, loss is: 0.012141269631683826\n",
      "epoch: 12/ iter: 3, loss is: 0.014755879528820515\n",
      "epoch: 12/ iter: 4, loss is: 0.012452391907572746\n",
      "epoch: 13/ iter: 0, loss is: 0.01701178401708603\n",
      "epoch: 13/ iter: 1, loss is: 0.012669989839196205\n",
      "epoch: 13/ iter: 2, loss is: 0.01404732558876276\n",
      "epoch: 13/ iter: 3, loss is: 0.009724571369588375\n",
      "epoch: 13/ iter: 4, loss is: 0.011414241045713425\n",
      "epoch: 14/ iter: 0, loss is: 0.016699425876140594\n",
      "epoch: 14/ iter: 1, loss is: 0.010826353915035725\n",
      "epoch: 14/ iter: 2, loss is: 0.007785639259964228\n",
      "epoch: 14/ iter: 3, loss is: 0.019299862906336784\n",
      "epoch: 14/ iter: 4, loss is: 0.0005084688309580088\n",
      "epoch: 15/ iter: 0, loss is: 0.009127872996032238\n",
      "epoch: 15/ iter: 1, loss is: 0.013322867453098297\n",
      "epoch: 15/ iter: 2, loss is: 0.012450003996491432\n",
      "epoch: 15/ iter: 3, loss is: 0.017892882227897644\n",
      "epoch: 15/ iter: 4, loss is: 0.01570168137550354\n",
      "epoch: 16/ iter: 0, loss is: 0.01080675981938839\n",
      "epoch: 16/ iter: 1, loss is: 0.016569625586271286\n",
      "epoch: 16/ iter: 2, loss is: 0.016258200630545616\n",
      "epoch: 16/ iter: 3, loss is: 0.011035766452550888\n",
      "epoch: 16/ iter: 4, loss is: 0.015864677727222443\n",
      "epoch: 17/ iter: 0, loss is: 0.014222614467144012\n",
      "epoch: 17/ iter: 1, loss is: 0.008280463516712189\n",
      "epoch: 17/ iter: 2, loss is: 0.01687382534146309\n",
      "epoch: 17/ iter: 3, loss is: 0.015470939688384533\n",
      "epoch: 17/ iter: 4, loss is: 0.005082518327981234\n",
      "epoch: 18/ iter: 0, loss is: 0.015093792229890823\n",
      "epoch: 18/ iter: 1, loss is: 0.017776785418391228\n",
      "epoch: 18/ iter: 2, loss is: 0.010889686644077301\n",
      "epoch: 18/ iter: 3, loss is: 0.009769515134394169\n",
      "epoch: 18/ iter: 4, loss is: 0.007255195640027523\n",
      "epoch: 19/ iter: 0, loss is: 0.01260917168110609\n",
      "epoch: 19/ iter: 1, loss is: 0.016227465122938156\n",
      "epoch: 19/ iter: 2, loss is: 0.010447927750647068\n",
      "epoch: 19/ iter: 3, loss is: 0.014049909077584743\n",
      "epoch: 19/ iter: 4, loss is: 0.008876407518982887\n",
      "epoch: 20/ iter: 0, loss is: 0.015706682577729225\n",
      "epoch: 20/ iter: 1, loss is: 0.014784404076635838\n",
      "epoch: 20/ iter: 2, loss is: 0.017712097615003586\n",
      "epoch: 20/ iter: 3, loss is: 0.006420065648853779\n",
      "epoch: 20/ iter: 4, loss is: 0.003296333597972989\n",
      "epoch: 21/ iter: 0, loss is: 0.013422607444226742\n",
      "epoch: 21/ iter: 1, loss is: 0.015795458108186722\n",
      "epoch: 21/ iter: 2, loss is: 0.01110892090946436\n",
      "epoch: 21/ iter: 3, loss is: 0.013666925020515919\n",
      "epoch: 21/ iter: 4, loss is: 0.011791826225817204\n",
      "epoch: 22/ iter: 0, loss is: 0.009467883966863155\n",
      "epoch: 22/ iter: 1, loss is: 0.013634410686790943\n",
      "epoch: 22/ iter: 2, loss is: 0.01669413223862648\n",
      "epoch: 22/ iter: 3, loss is: 0.013639496639370918\n",
      "epoch: 22/ iter: 4, loss is: 0.009468224830925465\n",
      "epoch: 23/ iter: 0, loss is: 0.014710239134728909\n",
      "epoch: 23/ iter: 1, loss is: 0.012467199936509132\n",
      "epoch: 23/ iter: 2, loss is: 0.01158473826944828\n",
      "epoch: 23/ iter: 3, loss is: 0.014312796294689178\n",
      "epoch: 23/ iter: 4, loss is: 0.0028118188492953777\n",
      "epoch: 24/ iter: 0, loss is: 0.008321203291416168\n",
      "epoch: 24/ iter: 1, loss is: 0.013972469605505466\n",
      "epoch: 24/ iter: 2, loss is: 0.013384678401052952\n",
      "epoch: 24/ iter: 3, loss is: 0.01696288399398327\n",
      "epoch: 24/ iter: 4, loss is: 0.007085734512656927\n",
      "epoch: 25/ iter: 0, loss is: 0.010463573969900608\n",
      "epoch: 25/ iter: 1, loss is: 0.011676503345370293\n",
      "epoch: 25/ iter: 2, loss is: 0.01986943930387497\n",
      "epoch: 25/ iter: 3, loss is: 0.011168736964464188\n",
      "epoch: 25/ iter: 4, loss is: 0.004612788092344999\n",
      "epoch: 26/ iter: 0, loss is: 0.009172231890261173\n",
      "epoch: 26/ iter: 1, loss is: 0.010841608978807926\n",
      "epoch: 26/ iter: 2, loss is: 0.016123848035931587\n",
      "epoch: 26/ iter: 3, loss is: 0.011718771420419216\n",
      "epoch: 26/ iter: 4, loss is: 0.1278482973575592\n",
      "epoch: 27/ iter: 0, loss is: 0.02526479959487915\n",
      "epoch: 27/ iter: 1, loss is: 0.013163603842258453\n",
      "epoch: 27/ iter: 2, loss is: 0.008458618074655533\n",
      "epoch: 27/ iter: 3, loss is: 0.016708172857761383\n",
      "epoch: 27/ iter: 4, loss is: 0.0007807555375620723\n",
      "epoch: 28/ iter: 0, loss is: 0.015653489157557487\n",
      "epoch: 28/ iter: 1, loss is: 0.008289478719234467\n",
      "epoch: 28/ iter: 2, loss is: 0.014793715439736843\n",
      "epoch: 28/ iter: 3, loss is: 0.014123613014817238\n",
      "epoch: 28/ iter: 4, loss is: 0.0015750715974718332\n",
      "epoch: 29/ iter: 0, loss is: 0.011224258691072464\n",
      "epoch: 29/ iter: 1, loss is: 0.018174372613430023\n",
      "epoch: 29/ iter: 2, loss is: 0.010074173100292683\n",
      "epoch: 29/ iter: 3, loss is: 0.013334623537957668\n",
      "epoch: 29/ iter: 4, loss is: 0.0012388990726321936\n",
      "epoch: 30/ iter: 0, loss is: 0.018497873097658157\n",
      "epoch: 30/ iter: 1, loss is: 0.013342873193323612\n",
      "epoch: 30/ iter: 2, loss is: 0.00997589435428381\n",
      "epoch: 30/ iter: 3, loss is: 0.010931987315416336\n",
      "epoch: 30/ iter: 4, loss is: 0.003538716584444046\n",
      "epoch: 31/ iter: 0, loss is: 0.01596628502011299\n",
      "epoch: 31/ iter: 1, loss is: 0.015269482508301735\n",
      "epoch: 31/ iter: 2, loss is: 0.010697266086935997\n",
      "epoch: 31/ iter: 3, loss is: 0.01188262552022934\n",
      "epoch: 31/ iter: 4, loss is: 0.0048185004852712154\n",
      "epoch: 32/ iter: 0, loss is: 0.008813286200165749\n",
      "epoch: 32/ iter: 1, loss is: 0.012809252366423607\n",
      "epoch: 32/ iter: 2, loss is: 0.01196268480271101\n",
      "epoch: 32/ iter: 3, loss is: 0.01861272193491459\n",
      "epoch: 32/ iter: 4, loss is: 0.006618367973715067\n",
      "epoch: 33/ iter: 0, loss is: 0.01649671606719494\n",
      "epoch: 33/ iter: 1, loss is: 0.008383605629205704\n",
      "epoch: 33/ iter: 2, loss is: 0.012438632547855377\n",
      "epoch: 33/ iter: 3, loss is: 0.01512096542865038\n",
      "epoch: 33/ iter: 4, loss is: 0.0002417344512650743\n",
      "epoch: 34/ iter: 0, loss is: 0.008336981758475304\n",
      "epoch: 34/ iter: 1, loss is: 0.007037680130451918\n",
      "epoch: 34/ iter: 2, loss is: 0.018381519243121147\n",
      "epoch: 34/ iter: 3, loss is: 0.018803542479872704\n",
      "epoch: 34/ iter: 4, loss is: 0.00230320543050766\n",
      "epoch: 35/ iter: 0, loss is: 0.00815995130687952\n",
      "epoch: 35/ iter: 1, loss is: 0.013665549457073212\n",
      "epoch: 35/ iter: 2, loss is: 0.012917430140078068\n",
      "epoch: 35/ iter: 3, loss is: 0.017659204080700874\n",
      "epoch: 35/ iter: 4, loss is: 0.0031004741322249174\n",
      "epoch: 36/ iter: 0, loss is: 0.013515192084014416\n",
      "epoch: 36/ iter: 1, loss is: 0.009844723157584667\n",
      "epoch: 36/ iter: 2, loss is: 0.010684398002922535\n",
      "epoch: 36/ iter: 3, loss is: 0.018956487998366356\n",
      "epoch: 36/ iter: 4, loss is: 0.0019366831984370947\n",
      "epoch: 37/ iter: 0, loss is: 0.010875936597585678\n",
      "epoch: 37/ iter: 1, loss is: 0.014874310232698917\n",
      "epoch: 37/ iter: 2, loss is: 0.006760809570550919\n",
      "epoch: 37/ iter: 3, loss is: 0.02029835619032383\n",
      "epoch: 37/ iter: 4, loss is: 0.0030408489983528852\n",
      "epoch: 38/ iter: 0, loss is: 0.008860810659825802\n",
      "epoch: 38/ iter: 1, loss is: 0.022360561415553093\n",
      "epoch: 38/ iter: 2, loss is: 0.009225670248270035\n",
      "epoch: 38/ iter: 3, loss is: 0.012899070978164673\n",
      "epoch: 38/ iter: 4, loss is: 0.008582775481045246\n",
      "epoch: 39/ iter: 0, loss is: 0.013101036660373211\n",
      "epoch: 39/ iter: 1, loss is: 0.017979104071855545\n",
      "epoch: 39/ iter: 2, loss is: 0.008022191002964973\n",
      "epoch: 39/ iter: 3, loss is: 0.01397773064672947\n",
      "epoch: 39/ iter: 4, loss is: 0.00958389975130558\n",
      "epoch: 40/ iter: 0, loss is: 0.010452893562614918\n",
      "epoch: 40/ iter: 1, loss is: 0.015440184623003006\n",
      "epoch: 40/ iter: 2, loss is: 0.015425404533743858\n",
      "epoch: 40/ iter: 3, loss is: 0.011501641012728214\n",
      "epoch: 40/ iter: 4, loss is: 0.015425563789904118\n",
      "epoch: 41/ iter: 0, loss is: 0.019897932186722755\n",
      "epoch: 41/ iter: 1, loss is: 0.014787886291742325\n",
      "epoch: 41/ iter: 2, loss is: 0.011824510060250759\n",
      "epoch: 41/ iter: 3, loss is: 0.0076888687908649445\n",
      "epoch: 41/ iter: 4, loss is: 0.011804481968283653\n",
      "epoch: 42/ iter: 0, loss is: 0.009693005122244358\n",
      "epoch: 42/ iter: 1, loss is: 0.018486740067601204\n",
      "epoch: 42/ iter: 2, loss is: 0.011643819510936737\n",
      "epoch: 42/ iter: 3, loss is: 0.012036582455039024\n",
      "epoch: 42/ iter: 4, loss is: 0.03154875338077545\n",
      "epoch: 43/ iter: 0, loss is: 0.0224239993840456\n",
      "epoch: 43/ iter: 1, loss is: 0.010338306427001953\n",
      "epoch: 43/ iter: 2, loss is: 0.012521486729383469\n",
      "epoch: 43/ iter: 3, loss is: 0.014023768715560436\n",
      "epoch: 43/ iter: 4, loss is: 0.0070082987658679485\n",
      "epoch: 44/ iter: 0, loss is: 0.007013896480202675\n",
      "epoch: 44/ iter: 1, loss is: 0.015959903597831726\n",
      "epoch: 44/ iter: 2, loss is: 0.014543640427291393\n",
      "epoch: 44/ iter: 3, loss is: 0.013780223205685616\n",
      "epoch: 44/ iter: 4, loss is: 0.03017805516719818\n",
      "epoch: 45/ iter: 0, loss is: 0.019758112728595734\n",
      "epoch: 45/ iter: 1, loss is: 0.008131253533065319\n",
      "epoch: 45/ iter: 2, loss is: 0.014857904985547066\n",
      "epoch: 45/ iter: 3, loss is: 0.009057015180587769\n",
      "epoch: 45/ iter: 4, loss is: 0.021670322865247726\n",
      "epoch: 46/ iter: 0, loss is: 0.014662769623100758\n",
      "epoch: 46/ iter: 1, loss is: 0.017938153818249702\n",
      "epoch: 46/ iter: 2, loss is: 0.00908712949603796\n",
      "epoch: 46/ iter: 3, loss is: 0.013196988962590694\n",
      "epoch: 46/ iter: 4, loss is: 0.0070266807451844215\n",
      "epoch: 47/ iter: 0, loss is: 0.018820367753505707\n",
      "epoch: 47/ iter: 1, loss is: 0.009189566597342491\n",
      "epoch: 47/ iter: 2, loss is: 0.005585582461208105\n",
      "epoch: 47/ iter: 3, loss is: 0.01819954439997673\n",
      "epoch: 47/ iter: 4, loss is: 0.010976653546094894\n",
      "epoch: 48/ iter: 0, loss is: 0.008754502050578594\n",
      "epoch: 48/ iter: 1, loss is: 0.02242124266922474\n",
      "epoch: 48/ iter: 2, loss is: 0.01040855422616005\n",
      "epoch: 48/ iter: 3, loss is: 0.011857148259878159\n",
      "epoch: 48/ iter: 4, loss is: 0.006196592468768358\n",
      "epoch: 49/ iter: 0, loss is: 0.011345036327838898\n",
      "epoch: 49/ iter: 1, loss is: 0.011339468881487846\n",
      "epoch: 49/ iter: 2, loss is: 0.01365527044981718\n",
      "epoch: 49/ iter: 3, loss is: 0.016472771763801575\n",
      "epoch: 49/ iter: 4, loss is: 0.0007244937587529421\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCH_NUM = 50   # 设置外层循环次数\r\n",
    "BATCH_SIZE = 100  # 设置batch大小\r\n",
    "def train(EPOCH_NUM,BATCH_SIZE):\r\n",
    "    # 定义外层循环\r\n",
    "    losses = []\r\n",
    "    for epoch_id in range(EPOCH_NUM):\r\n",
    "        # 在每轮迭代开始之前，将训练数据的顺序随机的打乱\r\n",
    "        np.random.shuffle(training_data)\r\n",
    "        # 将训练数据进行拆分，每个batch包含100条数据\r\n",
    "        mini_batches = [training_data[k:k+BATCH_SIZE] for k in range(0, len(training_data), BATCH_SIZE)]\r\n",
    "        # 定义内层循环\r\n",
    "        for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "            x = np.array(mini_batch[:, :-1]) # 获得当前批次训练数据\r\n",
    "            y = np.array(mini_batch[:, -1:]) # 获得当前批次训练标签（真实房价）\r\n",
    "            house_features = paddle.to_tensor(x)\r\n",
    "            prices = paddle.to_tensor(y)\r\n",
    "\r\n",
    "            # 前向计算\r\n",
    "            predicts = model(house_features)\r\n",
    "\r\n",
    "            # 计算损失\r\n",
    "            loss = F.square_error_cost(predicts, label=prices)\r\n",
    "            avg_loss = paddle.mean(loss)\r\n",
    "            losses.append(np.squeeze(avg_loss.numpy()))\r\n",
    "            print(\"epoch: {}/ iter: {}, loss is: {}\".format(epoch_id, iter_id, np.squeeze(avg_loss.numpy())))\r\n",
    "\r\n",
    "            # 反向传播\r\n",
    "            avg_loss.backward()\r\n",
    "            # 最小化loss,更新参数\r\n",
    "            opt.step()\r\n",
    "            # 清除梯度\r\n",
    "            opt.clear_grad()\r\n",
    "    return losses\r\n",
    "\r\n",
    "\r\n",
    "losses = train(EPOCH_NUM, BATCH_SIZE)\r\n",
    "\r\n",
    "plot_x = np.arange(len(losses))\r\n",
    "plot_y = np.array(losses)\r\n",
    "plt.plot(plot_x, plot_y)\r\n",
    "plt.show()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存成功，模型参数保存在LR_model.pdparams中\n"
     ]
    }
   ],
   "source": [
    "# 保存模型参数，文件名为LR_model.pdparams\r\n",
    "paddle.save(model.state_dict(), 'LR_model.pdparams')\r\n",
    "print(\"模型保存成功，模型参数保存在LR_model.pdparams中\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 测试集效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test loss is: 0.012102206237614155\n"
     ]
    }
   ],
   "source": [
    "    x = np.array(test_data[:, :-1]) # 获得当前批次训练数据\r\n",
    "    y = np.array(test_data[:, -1:]) # 获得当前批次训练标签（真实房价）\r\n",
    "    # 将numpy数据转为飞桨动态图tensor形式\r\n",
    "    house_features = paddle.to_tensor(x)\r\n",
    "    prices = paddle.to_tensor(y)\r\n",
    "\r\n",
    "    # 前向计算\r\n",
    "    predicts = model(house_features)\r\n",
    "\r\n",
    "    # 计算损失\r\n",
    "    loss = F.square_error_cost(predicts, label=prices)\r\n",
    "    avg_loss = paddle.mean(loss)\r\n",
    "    print(\"The test loss is: {}\".format(np.squeeze(avg_loss.numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 测试模型\n",
    "\n",
    "下面我们选择一条数据样本，测试下模型的预测效果。测试过程和在应用场景中使用模型的过程一致，主要可分成如下三个步骤：\n",
    "\n",
    "1. 配置模型预测的机器资源。本案例默认使用本机，因此无需写代码指定。\n",
    "1. 将训练好的模型参数加载到模型实例中。由两个语句完成，第一句是从文件中读取模型参数；第二句是将参数内容加载到模型。加载完毕后，需要将模型的状态调整为``eval()``（校验）。上文中提到，训练状态的模型需要同时支持前向计算和反向传导梯度，模型的实现较为臃肿，而校验和预测状态的模型只需要支持前向计算，模型的实现更加简单，性能更好。\n",
    "1. 将待预测的样本特征输入到模型中，打印输出的预测结果。\n",
    "\n",
    "通过``load_one_example``函数实现从数据集中抽一条样本作为测试样本，具体实现代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_one_example():\r\n",
    "    # 从上边已加载的测试集中，随机选择一条作为测试数据\r\n",
    "    idx = np.random.randint(0, test_data.shape[0])\r\n",
    "    idx = -10\r\n",
    "    one_data, label = test_data[idx, :-1], test_data[idx, -1]\r\n",
    "    # 修改该条数据shape为[1,13]\r\n",
    "    one_data =  one_data.reshape([1,-1])\r\n",
    "\r\n",
    "    return one_data, label\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference result is [[33.81375]], the corresponding label is 38.87574005126953\n"
     ]
    }
   ],
   "source": [
    "# 参数为保存模型参数的文件地址\r\n",
    "model_dict = paddle.load('LR_model.pdparams')\r\n",
    "model.load_dict(model_dict)\r\n",
    "model.eval()\r\n",
    "\r\n",
    "# 参数为数据集的文件地址\r\n",
    "one_data, label = load_one_example()\r\n",
    "# 将数据转为动态图的variable格式 \r\n",
    "one_data = paddle.to_tensor(one_data)\r\n",
    "predict = model(one_data)\r\n",
    "\r\n",
    "# 对结果做反归一化处理\r\n",
    "predict = predict * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "# 对label数据做反归一化处理\r\n",
    "label = label * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "\r\n",
    "print(\"Inference result is {}, the corresponding label is {}\".format(predict.numpy(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "对比两次结果可知，结果相似"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
