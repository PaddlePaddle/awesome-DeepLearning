{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1.深度学习发展历史：\n",
    "1.1第一代神经网络（1958-1969）\n",
    "最早的神经网络的思想起源于1943年的MCP人工神经元模型，当时是希望能够用计算机来模拟人的神经元反应的过程，该模型将神经元简化为了三个过程：输入信号线性加权，求和，非线性激活（阈值法）。如下图所示\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/4ba1cbdf71d64b4d8c4978dfa557b28a07c4fd0479094db59ec40b8420672d7d)\n",
    "\n",
    "第一次将MCP用于机器学习（分类）的当属1958年Rosenblatt发明的感知器（perceptron）算法。该算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年，该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。\n",
    "然而学科发展的历史不总是一帆风顺的。\n",
    "1969年，美国数学家及人工智能先驱Minsky在其著作中证明了感知器本质上是一种线性模型，只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了近20年的停滞。\n",
    "1.2第二代神经网络（1986-1998）\n",
    "第一次打破非线性诅咒的当属现代DL大牛Hinton，其在1986年发明了适用于多层感知器（MLP）的BP算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。\n",
    "1989年，Robert Hecht-Nielsen证明了MLP的万能逼近定理，即对于任何闭区间内的一个连续函数f，都可以用含有一个隐含层的BP网络来逼近该定理的发现极大的鼓舞了神经网络的研究人员。\n",
    "也是在1989年，LeCun发明了卷积神经网络-LeNet，并将其用于数字识别，且取得了较好的成绩，不过当时并没有引起足够的注意。\n",
    "值得强调的是在1989年以后由于没有特别突出的方法被提出，且NN一直缺少相应的严格的数学理论支持，神经网络的热潮渐渐冷淡下去。冰点来自于1991年，BP算法被指出存在梯度消失问题，即在误差梯度后向传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该发现对此时的NN发展雪上加霜。\n",
    "1997年，LSTM模型被发明，尽管该模型在序列建模上的特性非常突出，但由于正处于NN的下坡期，也没有引起足够的重视。\n",
    "1.3统计学习方法的春天（1986-2006）\n",
    "1986年，决策树方法被提出，很快ID3，ID4，CART等改进的决策树方法相继出现，到目前仍然是非常常用的一种机器学习方法。该方法也是符号学习方法的代表。 \n",
    "1995年，线性SVM被统计学家Vapnik提出。该方法的特点有两个：由非常完美的数学理论推导而来（统计学与凸优化等），符合人的直观感受（最大间隔）。不过，最重要的还是该方法在线性分类的问题上取得了当时最好的成绩。 \n",
    "1997年，AdaBoost被提出，该方法是PAC（Probably Approximately Correct）理论在机器学习实践上的代表，也催生了集成方法这一类。该方法通过一系列的弱分类器集成，达到强分类器的效果。 \n",
    "2000年，KernelSVM被提出，核化的SVM通过一种巧妙的方式将原空间线性不可分的问题，通过Kernel映射成高维空间的线性可分问题，成功解决了非线性分类的问题，且分类效果非常好。至此也更加终结了NN时代。 \n",
    "2001年，随机森林被提出，这是集成方法的另一代表，该方法的理论扎实，比AdaBoost更好的抑制过拟合问题，实际效果也非常不错。 \n",
    "2001年，一种新的统一框架-图模型被提出，该方法试图统一机器学习混乱的方法，如朴素贝叶斯，SVM，隐马尔可夫模型等，为各种学习方法提供一个统一的描述框架。\n",
    "1.4第三代神经网络-DL（2006-至今）\n",
    "2006年，DL元年。是年，Hinton提出了深层网络训练中梯度消失问题的解决方案：无监督预训练对权值进行初始化+有监督训练微调。其主要思想是先通过自学习的方法学习到训练数据的结构（自动编码器），然后在该结构上进行有监督训练微调。但是由于没有特别有效的实验验证，该论文并没有引起重视。\n",
    "2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。\n",
    "2011年，微软首次将DL应用在语音识别上，取得了重大突破。\n",
    "2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。 \n",
    "AlexNet的创新点： \n",
    "（1）首次采用ReLU激活函数，极大增大收敛速度且从根本上解决了梯度消失问题；（2）由于ReLU方法可以很好抑制梯度消失问题，AlexNet抛弃了“预训练+微调”的方法，完全采用有监督训练。也正因为如此，DL的主流学习方法也因此变为了纯粹的有监督学习；（3）扩展了LeNet5结构，添加Dropout层减小过拟合，LRN层增强泛化能力/减小过拟合；（4）首次采用GPU对计算进行加速；\n",
    "\n",
    "2013,2014,2015年，通过ImageNet图像识别比赛，DL的网络结构，训练方法，GPU硬件的不断进步，促使其在其他领域也在不断的征服战场\n",
    "2015年，Hinton，LeCun，Bengio论证了局部极值问题对于DL的影响，结果是Loss的局部极值问题对于深层网络来说影响可以忽略。该论断也消除了笼罩在神经网络上的局部极值问题的阴霾。具体原因是深层网络虽然局部极值非常多，但是通过DL的BatchGradientDescent优化方法很难陷进去，而且就算陷进去，其局部极小值点与全局极小值点也是非常接近，但是浅层网络却不然，其拥有较少的局部极小值点，但是却很容易陷进去，且这些局部极小值点与全局极小值点相差较大。论述原文其实没有证明，只是简单叙述，严密论证是猜的。。。\n",
    "2015，DeepResidualNet发明。分层预训练，ReLU和BatchNormalization都是为了解决深度神经网络优化时的梯度消失或者爆炸问题。但是在对更深层的神经网络进行优化时，又出现了新的Degradation问题，即”通常来说，如果在VGG16后面加上若干个单位映射，网络的输出特性将和VGG16一样，这说明更深次的网络其潜在的分类性能只可能>=VGG16的性能，不可能变坏，然而实际效果却是只是简单的加深VGG16的话，分类性能会下降（不考虑模型过拟合问题）“Residual网络认为这说明DL网络在学习单位映射方面有困难，因此设计了一个对于单位映射（或接近单位映射）有较强学习能力的DL网络，极大的增强了DL网络的表达能力。此方法能够轻松的训练高达150层的网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2.人工智能、机器学习、深度学习的区别与联系：\n",
    " 2.1人工智能（Artificial Intelligence）\n",
    "人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门技术科学。“人工智能”是“一门技术科学”，它研究与开发的对象是“理论、技术及应用系统”，研究的目的是为了“模拟、延伸和扩展人的智能”。人工智能在50年代就提出了。\n",
    " 2.2 机器学习   \n",
    "随着人对计算机科学的期望越来越高，要求它解决的问题越来越复杂，已经远远不能满足人们的诉求了。于是有人提出了一个新的思路——能否不为难研究者，让机器自己去学习呢？机器学习就是用算法解析数据，不断学习，对世界中发生的事做出判断和预测的一项技术。研究人员不会亲手编写软件、确定特殊指令集、然后让程序完成特殊任务；相反，研究人员会用大量数据和算法“训练”机器，让机器学会如何执行任务。这里有三个重要的信息：\n",
    "- 1、“机器学习”是“模拟、延伸和扩展人的智能”的一条路径，所以是人工智能的一个子集；\n",
    "- 2、“机器学习”是要基于大量数据的，也就是说它的“智能”是用大量数据喂出来的；\n",
    "- 3、正是因为要处理海量数据，所以大数据技术尤为重要；“机器学习”只是大数据技术上的一个应用。常用的10大机器学习算法有：决策树、随机森林、逻辑回归、SVM、朴素贝叶斯、K最近邻算法、K均值算法、Adaboost算法、神经网络、马尔科夫。\n",
    " 2.3 深度学习\n",
    "相较而言，深度学习是一个比较新的概念，严格地说是2006年提出的。深度学习是用于建立、模拟人脑进行分析学习的神经网络，并模仿人脑的机制来解释数据的一种机器学习技术。它的基本特点，是试图模仿大脑的神经元之间传递，处理信息的模式。最显著的应用是计算机视觉和自然语言处理(NLP)领域。显然，“深度学习”是与机器学习中的“神经网络”是强相关，“神经网络”也是其主要的算法和手段；或者我们可以将“深度学习”称之为“改良版的神经网络”算法。深度学习又分为卷积神经网络（Convolutional neural networks，简称CNN）和深度置信网（Deep Belief Nets，简称DBN）。其主要的思想就是模拟人的神经元，每个神经元接受到信息，处理完后传递给与之相邻的所有神经元即可。\n",
    "联系：深度学习是一种机器学习技术，机器学习是人工智能研究中的重要分支。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3.神经元、单层感知机、多层感知机：\n",
    "3.1神经元\n",
    "即神经元细胞，是神经系统最基本的结构和功能单位。分为细胞体和突起两部分。细胞体由细胞核、细胞膜、细胞质组成，具有联络和整合输入信息并传出信息的作用。突起有树突和轴突两种。树突短而分枝多，直接由细胞体扩张突出，形成树枝状，其作用是接受其他神经元轴突传来的冲动并传给细胞体。轴突长而分枝少，为粗细均匀的细长突起，常起于轴丘，其作用是接受外来刺激，再由细胞体传出。轴突除分出侧枝外，其末端形成树枝样的神经末梢。末梢分布于某些组织器官内，形成各种神经末梢装置。感觉神经末梢形成各种感受器；运动神经末梢分布于骨骼肌肉，形成运动终极。\n",
    "3.2单层感知机\n",
    "单层感知机是二分类的线性分类模型，输入是被感知数据集的特征向量，输出时数据集的类别{+1,-1}。单层感知机的函数近似非常有限，其决策边界必须是一个超平面，严格要求数据是线性可分的。\n",
    "3.3多层感知机\n",
    "多层感知机（MLP，Multilayer Perceptron）也叫人工神经网络（ANN，Artificial Neural Network），除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构.。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4.什么是前向传播？\n",
    "所谓前向传播，就是给网络输入一个样本向量，该样本向量的各元素，经过各隐藏层的逐级加权求和+非线性激活，最终由输出层输出一个预测向量的过程。\n",
    "如下图，图右上角是f(x,y,z)=(x+y)*z 的计算tu\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/ee2da10825f24ad7be7be8398bd58a427ec049006e18463db0f33f1ab0fe3fc6)\n",
    "分别赋值 x = − 2 ， y = 5 ， z = − 4 x = -2，y = 5， z = -4x=−2，y=5，z=−4，从计算图的左边开始，数据开始流动，依次计算出 q 、 f q、fq、f。\n",
    "\n",
    "最终得到计算图中那 6 个绿色的数字，这就是前向传播的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5.什么是反向传播？\n",
    "按照我的拙见，反向传播是损失函数对权重的偏导数，为了求出来具体的数值，用了函数求导时候的链式法则。根据求出来的导数值大小来看权重大小对损失函数的影响，方便下一步进行权重更新，进而最小化损失函数，求出较为合适的权重，即找到在选择的模型空间下的一个较为优的解！\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c196204c7d0c41b5845fc653a803917eb35b5f9ff40740858f76869c6e484ce7)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/8f25a27338444458870f7029f6cb5ee90a6bc573b3da4f3fad4fdcb0c0e85aaf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "房价预测python+numpy实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston \r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "def load_data():   \r\n",
    "    datafile = './data/housing.data'\r\n",
    "    data = np.fromfile(datafile, sep=' ')\r\n",
    "\r\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\r\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\r\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\r\n",
    "    feature_num = len(feature_names)\r\n",
    "\r\n",
    "    # 2.将原始数据进行Reshape，变成[N, 14]这样的形状\r\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\r\n",
    "\r\n",
    "    # 3.将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算train数据集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\r\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\r\n",
    "\r\n",
    "    # 4.对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        #print(maximums[i], minimums[i], avgs[i])\r\n",
    "        data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data\r\n",
    "\r\n",
    "\r\n",
    "class Network(object):\r\n",
    "    def __init__(self, num_of_weights):\r\n",
    "        # 随机产生w的初始值\r\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\r\n",
    "        np.random.seed(0)       \r\n",
    "        self.n_hidden = 10\r\n",
    "        self.w1 = np.random.randn(num_of_weights,10)  # 设置随机的权重\r\n",
    "        self.b1 = np.zeros(10)  # 这里偏置为0\r\n",
    "        self.w2 = np.random.rand(10,1)  # 这里因为输出只有一个模型，所以输出维度为1\r\n",
    "        self.b2 = np.zeros(1)\r\n",
    "                   \r\n",
    "    def Relu(self,x):\r\n",
    "        return np.where(x < 0,0,x)\r\n",
    "    \r\n",
    "    def MSE_loss(self, y,y_pred):\r\n",
    "        return np.mean(np.square(y_pred - y))\r\n",
    "        \r\n",
    "    def Linear(self,x,w,b):\r\n",
    "        z = x.dot(w) + b\r\n",
    "        return z\r\n",
    "    \r\n",
    "    def back_gradient(self, y_pred, y,s1,l1):\r\n",
    "        grad_y_pred = 2.0 * (y_pred - y)\r\n",
    "        grad_w2 = s1.T.dot(grad_y_pred)\r\n",
    "        grad_temp_relu = grad_y_pred.dot(self.w2.T)\r\n",
    "        grad_temp_relu[l1 < 0] = 0\r\n",
    "        grad_w1 = x.T.dot(grad_temp_relu) \r\n",
    "        return grad_w1, grad_w2\r\n",
    "    \r\n",
    "    def update(self, grad_w1,grad_w2,learning_rate):\r\n",
    "        self.w1 -= learning_rate * grad_w1\r\n",
    "        self.w2 -= learning_rate * grad_w2      \r\n",
    "            \r\n",
    "    def train(self, x, y, iterations, learning_rate):\r\n",
    "        losses = []  # 记录每次迭代损失值\r\n",
    "        for t in range(num_iterations):\r\n",
    "            # 前向传播\r\n",
    "            l1 = self.Linear(x,self.w1,self.b1)\r\n",
    "            s1 = self.Relu(l1)\r\n",
    "            y_pred = self.Linear(s1,self.w2,self.b2)\r\n",
    "            # 计算损失函数\r\n",
    "            loss = self.MSE_loss(y,y_pred)\r\n",
    "            losses.append(loss)\r\n",
    "            # 反向传播\r\n",
    "            grad_w1,grad_w2 = self.back_gradient(y_pred, y,s1,l1)\r\n",
    "            # 权重更新\r\n",
    "            self.update(grad_w1,grad_w2,learning_rate)          \r\n",
    "        return losses\r\n",
    "        \r\n",
    "# 获取数据\r\n",
    "train_data, test_data = load_data()\r\n",
    "x = train_data[:, :-1]\r\n",
    "y = train_data[:, -1:]\r\n",
    "\r\n",
    "# 创建网络\r\n",
    "net = Network(13)\r\n",
    "num_iterations=50000\r\n",
    "# 启动训练\r\n",
    "losses = net.train(x,y, iterations = num_iterations, learning_rate = 1e-6)\r\n",
    "\r\n",
    "# 画出损失函数的变化趋势    \r\n",
    "plot_x = np.arange(num_iterations)\r\n",
    "plot_y = np.array(losses)\r\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\r\n",
    "plt.plot(plot_x, plot_y)\r\n",
    "plt.xlabel('iterations')\r\n",
    "plt.ylabel('loss')\r\n",
    "plt.show()\r\n",
    "#print('w1 = {}\\n w2 = {}'.format(w1,w2))\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "paddle实现房价预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss is: [0.06904466]\n",
      "epoch: 0, iter: 20, loss is: [0.0605798]\n",
      "epoch: 0, iter: 40, loss is: [0.08289625]\n",
      "epoch: 1, iter: 0, loss is: [0.08934028]\n",
      "epoch: 1, iter: 20, loss is: [0.06063873]\n",
      "epoch: 1, iter: 40, loss is: [0.04450267]\n",
      "epoch: 2, iter: 0, loss is: [0.06582879]\n",
      "epoch: 2, iter: 20, loss is: [0.04792718]\n",
      "epoch: 2, iter: 40, loss is: [0.09931071]\n",
      "epoch: 3, iter: 0, loss is: [0.09746288]\n",
      "epoch: 3, iter: 20, loss is: [0.0253105]\n",
      "epoch: 3, iter: 40, loss is: [0.1328317]\n",
      "epoch: 4, iter: 0, loss is: [0.02627663]\n",
      "epoch: 4, iter: 20, loss is: [0.03310246]\n",
      "epoch: 4, iter: 40, loss is: [0.04306072]\n",
      "epoch: 5, iter: 0, loss is: [0.02750561]\n",
      "epoch: 5, iter: 20, loss is: [0.04128321]\n",
      "epoch: 5, iter: 40, loss is: [0.13451238]\n",
      "epoch: 6, iter: 0, loss is: [0.02393558]\n",
      "epoch: 6, iter: 20, loss is: [0.02832639]\n",
      "epoch: 6, iter: 40, loss is: [0.03099384]\n",
      "epoch: 7, iter: 0, loss is: [0.06580763]\n",
      "epoch: 7, iter: 20, loss is: [0.04271714]\n",
      "epoch: 7, iter: 40, loss is: [0.0485788]\n",
      "epoch: 8, iter: 0, loss is: [0.04531223]\n",
      "epoch: 8, iter: 20, loss is: [0.07685316]\n",
      "epoch: 8, iter: 40, loss is: [0.02358524]\n",
      "epoch: 9, iter: 0, loss is: [0.07324368]\n",
      "epoch: 9, iter: 20, loss is: [0.05917173]\n",
      "epoch: 9, iter: 40, loss is: [0.0629088]\n",
      "模型保存成功，模型参数保存在LR_model中\n",
      "Inference result is [[15.3298645]], the corresponding label is 19.7\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "import paddle.fluid as fluid\r\n",
    "import paddle.fluid.dygraph as dygraph\r\n",
    "from paddle.fluid.dygraph import Linear\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import random\r\n",
    "\r\n",
    "#数据处理\r\n",
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    datafile = './work/housing.data'\r\n",
    "    data = np.fromfile(datafile, sep=' ')\r\n",
    "\r\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\r\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\r\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\r\n",
    "    feature_num = len(feature_names)\r\n",
    "\r\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\r\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\r\n",
    "\r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算train数据集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\r\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\r\n",
    "    \r\n",
    "    # 记录数据的归一化参数，在预测时对数据做归一化\r\n",
    "    global max_values\r\n",
    "    global min_values\r\n",
    "    global avg_values\r\n",
    "    max_values = maximums\r\n",
    "    min_values = minimums\r\n",
    "    avg_values = avgs\r\n",
    "\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        #print(maximums[i], minimums[i], avgs[i])\r\n",
    "        data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    #ratio = 0.8\r\n",
    "    #offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data\r\n",
    "\r\n",
    "#模型设计\r\n",
    "class Regressor(fluid.dygraph.Layer):\r\n",
    "    #声明每一层网络的实现函数\r\n",
    "    def __init__(self):\r\n",
    "        super(Regressor, self).__init__()\r\n",
    "        \r\n",
    "        # 定义一层全连接层，输出维度是1，激活函数为None，即不使用激活函数\r\n",
    "        self.fc = Linear(input_dim=13, output_dim=1, act=None)\r\n",
    "    \r\n",
    "    # 构建神经网络结构，实现前向计算过程，并返回预测结果\r\n",
    "    def forward(self, inputs):\r\n",
    "        x = self.fc(inputs)\r\n",
    "        return x\r\n",
    "\r\n",
    "#训练配置\r\n",
    "# 定义飞桨动态图的工作环境\r\n",
    "with fluid.dygraph.guard():\r\n",
    "    # 声明定义好的线性回归模型\r\n",
    "    model = Regressor()\r\n",
    "    # 开启模型训练模式\r\n",
    "    model.train()\r\n",
    "    # 加载数据\r\n",
    "    training_data, test_data = load_data()\r\n",
    "    # 定义优化算法，这里使用随机梯度下降-SGD\r\n",
    "    # 学习率设置为0.01\r\n",
    "    opt = fluid.optimizer.SGD(learning_rate=0.01, parameter_list=model.parameters())\r\n",
    "\r\n",
    "#训练过程\r\n",
    "with dygraph.guard(fluid.CPUPlace()):\r\n",
    "    EPOCH_NUM = 10   # 设置外层循环次数\r\n",
    "    BATCH_SIZE = 10  # 设置batch大小\r\n",
    "    \r\n",
    "    # 定义外层循环\r\n",
    "    for epoch_id in range(EPOCH_NUM):\r\n",
    "        # 在每轮迭代开始之前，将训练数据的顺序随机的打乱\r\n",
    "        np.random.shuffle(training_data)\r\n",
    "        # 将训练数据进行拆分，每个batch包含10条数据\r\n",
    "        mini_batches = [training_data[k:k+BATCH_SIZE] for k in range(0, len(training_data), BATCH_SIZE)]\r\n",
    "        # 定义内层循环\r\n",
    "        for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "            x = np.array(mini_batch[:, :-1]).astype('float32') # 获得当前批次训练数据\r\n",
    "            y = np.array(mini_batch[:, -1:]).astype('float32') # 获得当前批次训练标签（真实房价）\r\n",
    "            # 将numpy数据转为飞桨动态图variable形式\r\n",
    "            house_features = dygraph.to_variable(x)\r\n",
    "            prices = dygraph.to_variable(y)\r\n",
    "            \r\n",
    "            # 前向计算\r\n",
    "            predicts = model(house_features)\r\n",
    "            \r\n",
    "            # 计算损失\r\n",
    "            loss = fluid.layers.square_error_cost(predicts, label=prices)\r\n",
    "            avg_loss = fluid.layers.mean(loss)\r\n",
    "            if iter_id%20==0:\r\n",
    "                print(\"epoch: {}, iter: {}, loss is: {}\".format(epoch_id, iter_id, avg_loss.numpy()))\r\n",
    "            \r\n",
    "            # 反向传播\r\n",
    "            avg_loss.backward()\r\n",
    "            # 最小化loss,更新参数\r\n",
    "            opt.minimize(avg_loss)\r\n",
    "            # 清除梯度\r\n",
    "            model.clear_gradients()\r\n",
    "    # 保存模型\r\n",
    "    fluid.save_dygraph(model.state_dict(), 'LR_model')\r\n",
    "\r\n",
    "#保存模型\r\n",
    "# 定义飞桨动态图工作环境\r\n",
    "with fluid.dygraph.guard():\r\n",
    "    # 保存模型参数，文件名为LR_model\r\n",
    "    fluid.save_dygraph(model.state_dict(), 'LR_model')\r\n",
    "    print(\"模型保存成功，模型参数保存在LR_model中\")\r\n",
    "\r\n",
    "#测试模型\r\n",
    "def load_one_example(data_dir):\r\n",
    "    f = open(data_dir, 'r')\r\n",
    "    datas = f.readlines()\r\n",
    "    # 选择倒数第10条数据用于测试\r\n",
    "    tmp = datas[-10]\r\n",
    "    tmp = tmp.strip().split()\r\n",
    "    one_data = [float(v) for v in tmp]\r\n",
    "\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(len(one_data)-1):\r\n",
    "        one_data[i] = (one_data[i] - avg_values[i]) / (max_values[i] - min_values[i])\r\n",
    "\r\n",
    "    data = np.reshape(np.array(one_data[:-1]), [1, -1]).astype(np.float32)\r\n",
    "    label = one_data[-1]\r\n",
    "    return data, label\r\n",
    "\r\n",
    "with dygraph.guard():\r\n",
    "    # 参数为保存模型参数的文件地址\r\n",
    "    model_dict, _ = fluid.load_dygraph('LR_model')\r\n",
    "    model.load_dict(model_dict)\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    # 参数为数据集的文件地址\r\n",
    "    test_data, label = load_one_example('./work/housing.data')\r\n",
    "    # 将数据转为动态图的variable格式\r\n",
    "    test_data = dygraph.to_variable(test_data)\r\n",
    "    results = model(test_data)\r\n",
    "\r\n",
    "    # 对结果做反归一化处理\r\n",
    "    results = results * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "    print(\"Inference result is {}, the corresponding label is {}\".format(results.numpy(), label))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "通过对比，不难看出，numpy由于训练次数较多，损失函数最终较小；\n",
    "两者预测结果对比，大致相同。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
