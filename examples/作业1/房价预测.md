### 实验步骤

(1) 加载数据。

(2) 划分训练集和验证集：用验证集去评估模型的稳健性，防止过拟合。

(3) 数据归一化：目的是消除数据间量纲的影响，使数据具有可比性。

(4) 构建神经网络与训练。

(5) 训练历史可视化。

(6) 保存模型。

(7) 模型的预测功能与反归一化。

**1) 导入所需模块**

```python
1.  # /chapter/4_7_1_MLP.ipynb
2.  from keras.preprocessing import sequence  
3.  from keras.models import Sequential  
4.  from keras.datasets import boston_housing  
5.  from keras.layers import Dense, Dropout  
6.  from keras.utils import multi_gpu_model  
7.  from keras import regularizers  # 正则化  
8.  import matplotlib.pyplot as plt  
9.  import numpy as np  
10.  from sklearn.preprocessing import MinMaxScaler  
11.  import pandas as pd
```

**2) 加载数据**

```python
1.  (x_train, y_train), (x_valid, y_valid) = boston_housing.load_data()  # 加载数据  
2.    
3.  # 转成DataFrame格式方便数据处理  
4.  x_train_pd = pd.DataFrame(x_train)  
5.  y_train_pd = pd.DataFrame(y_train)  
6.  x_valid_pd = pd.DataFrame(x_valid)  
7.  y_valid_pd = pd.DataFrame(y_valid)  
8.  print(x_train_pd.head(5))  
9.  print('-------------------')  
10.  print(y_train_pd.head(5))
```

**3) 数据归一化**

```python
1.  # 训练集归一化  
2.  min_max_scaler = MinMaxScaler()  
3.  min_max_scaler.fit(x_train_pd)  
4.  x_train = min_max_scaler.transform(x_train_pd)  
5.    
6.  min_max_scaler.fit(y_train_pd)  
7.  y_train = min_max_scaler.transform(y_train_pd)  
8.    
9.  # 验证集归一化  
10.  min_max_scaler.fit(x_valid_pd)  
11.  x_valid = min_max_scaler.transform(x_valid_pd)  
12.    
13.  min_max_scaler.fit(y_valid_pd)  
14.  y_valid = min_max_scaler.transform(y_valid_pd)
```

**4) 训练模型**

```python
1.  # 单CPU or GPU版本，若有GPU则自动切换  
2.  model = Sequential()  # 初始化，很重要！
3.  model.add(Dense(units = 10,   # 输出大小  
4.                  activation='relu',  # 激励函数  
5.                  input_shape=(x_train_pd.shape[1],)  # 输入大小, 也就是列的大小  
6.                 )  
7.           )  
8.    
9.  model.add(Dropout(0.2))  # 丢弃神经元链接概率  
10.    
11.  model.add(Dense(units = 15,  
12.  #                 kernel_regularizer=regularizers.l2(0.01),  # 施加在权重上的正则项  
13.  #                 activity_regularizer=regularizers.l1(0.01),  # 施加在输出上的正则项  
14.                  activation='relu' # 激励函数  
15.                  # bias_regularizer=keras.regularizers.l1_l2(0.01)  # 施加在偏置向量上的正则项  
16.                 )  
17.           )  
18.    
19.  model.add(Dense(units = 1,     
20.                  activation='linear'  # 线性激励函数 回归一般在输出层用这个激励函数    
21.                 )  
22.           )  
23.    
24.  print(model.summary())  # 打印网络层次结构  
25.    
26.  model.compile(loss='mse',  # 损失均方误差  
27.                optimizer='adam',  # 优化器  
28.               )  
29.  history = model.fit(x_train, y_train,  
30.            epochs=200,  # 迭代次数  
31.            batch_size=200,  # 每次用来梯度下降的批处理数据大小  
32.            verbose=2,  # verbose：日志冗长度，int：冗长度，0：不输出训练过程，1：输出训练进度，2：输出每一个epoch  
33.            validation_data = (x_valid, y_valid)  # 验证集  
34.          )
```

**5) 训练过程可视化**

```python
1.  import matplotlib.pyplot as plt  
2.  # 绘制训练 & 验证的损失值  
3.  plt.plot(history.history['loss'])  
4.  plt.plot(history.history['val_loss'])  
5.  plt.title('Model loss')  
6.  plt.ylabel('Loss')  
7.  plt.xlabel('Epoch')  
8.  plt.legend(['Train', 'Test'], loc='upper left')  
9.  plt.show()
```

**6) 保存模型 & 模型可视化 & 加载模型**

```python
1.  from keras.utils import plot_model  
2.  from keras.models import load_model  
3.  # 保存模型  
4.  model.save('model_MLP.h5')  # 生成模型文件 'my_model.h5'  
5.    
6.  # 模型可视化 需要安装pydot pip install pydot  
7.  plot_model(model, to_file='model_MLP.png', show_shapes=True)  
8.    
9.  # 加载模型  
10.  model = load_model('model_MLP.h5')
```


**7) 模型的预测功能**

```python
1.  # 预测  
2.  y_new = model.predict(x_valid)  
3.  # 反归一化还原原始量纲  
4.  min_max_scaler.fit(y_valid_pd)  
5.  y_new = min_max_scaler.inverse_transform(y_new)  
3. 结果分析
在迭代了200个epochs之后，训练集和验证集的损失loss，趋于平稳，这时，我们得到的模型已经是最优的了。所以将epoch设置为200即可。
```