### 深度学习发展历史

- **1943年**

由神经科学家**麦卡洛克(W.S.McCilloch)** 和**数学家皮兹（W.Pitts）**在《数学生物物理学公告》上发表论文《神经活动中内在思想的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity）。建立了神经网络和数学模型，称为MCP模型。所谓**MCP**模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，也就诞生了所谓的“模拟大脑”，人工神经网络的大门由此开启。



![img](images\1.jpg)

**麦卡洛克(W.S.McCilloch)**

![img](images\2.jpg)

**皮兹（W.Pitts）**

MCP当时是希望能够用计算机来模拟人的神经元反应的过程，该模型将神经元简化为了三个过程：输入信号线性加权，求和，非线性激活（阈值法）。如下图所示

![img](images\3.png)

- **1958年**

计算机科学家**罗森布拉特（ Rosenblatt）**提出了两层神经元组成的神经网络，称之为**“感知器”(Perceptrons)**。第一次将MCP用于**机器学习（machine learning）分类(classification)**。“感知器”算法算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年,该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。

- **1969年**

纵观科学发展史，无疑都是充满曲折的，深度学习也毫不例外。 1969年，美国数学家及人工智能先驱 **Marvin Minsky** 在其著作中证明了感知器本质上是一种**线性模型（linear model）**，只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了将近20年的停滞。

- **1986年**

由神经网络之父 **Geoffrey Hinton** 在1986年发明了适用于多层感知器（MLP）的**BP（Backpropagation）**算法，并采用**Sigmoid**进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。

注：**Sigmoid** 函数是一个在生物学中常见的S型的函数，也称为S型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间。



![img](images\4.png)

- **90年代时期**

1991年BP算法被指出存在梯度消失问题，也就是说在误差梯度后项传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该问题直接阻碍了深度学习的进一步发展。

此外90年代中期，支持向量机算法诞生（SVM算法）等各种浅层机器学习模型被提出，SVM也是一种有监督的学习模型，应用于模式识别，分类以及回归分析等。支持向量机以统计学为基础，和神经网络有明显的差异，支持向量机等算法的提出再次阻碍了深度学习的发展。

- **发展期 2006年 - 2012年**

2006年，加拿大多伦多大学教授、机器学习领域泰斗、神经网络之父—— Geoffrey Hinton 和他的学生 Ruslan Salakhutdinov 在顶尖学术刊物《科学》上发表了一篇文章，该文章提出了深层网络训练中梯度消失问题的解决方案：**无监督预训练对权值进行初始化+有监督训练微调**。斯坦福大学、纽约大学、加拿大蒙特利尔大学等成为研究深度学习的重镇，至此开启了深度学习在学术界和工业界的浪潮。

![img](images\5.jpg)

Geoffrey Hinton



2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。2011年以来，微软首次将DL应用在语音识别上，取得了重大突破。微软研究院和Google的语音识别研究人员先后采用DNN技术降低语音识别错误率20％~30％，是语音识别领域十多年来最大的突破性进展。2012年，DNN技术在图像识别领域取得惊人的效果，在ImageNet评测上将错误率从26％降低到15％。在这一年，DNN还被应用于制药公司的DrugeActivity预测问题，并获得世界最好成绩。

- **爆发期 2012 - 2017**

2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。

![img](images\6.jpg)

**AlexNet**的创新点在于:

**(1)首次采用ReLU激活函数，极大增大收敛速度且从根本上解决了梯度消失问题。**

**(2)由于ReLU方法可以很好抑制梯度消失问题，AlexNet抛弃了“预训练+微调”的方法，完全采用有监督训练。也正因为如此，DL的主流学习方法也因此变为了纯粹的有监督学习。**

**(3)扩展了LeNet5结构，添加Dropout层减小过拟合，LRN层增强泛化能力/减小过拟合。**

**(4)第一次使用GPU加速模型计算。**

2013、2014、2015、2016年，通过ImageNet图像识别比赛，DL的网络结构，训练方法，GPU硬件的不断进步，促使其在其他领域也在不断的征服战场。

2016年3月，由谷歌（Google）旗下DeepMind公司开发的AlphaGo(基于深度学习)与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜；2016年末2017年初，该程序在中国棋类网站上以“大师”（Master）为注册帐号与中日韩数十位围棋高手进行快棋对决，连续60局无一败绩；2017年5月，在中国乌镇围棋峰会上，它与排名世界第一的世界围棋冠军柯洁对战，以3比0的总比分获胜。围棋界公认阿尔法围棋的棋力已经超过人类职业围棋顶尖水平。



### 人工智能、机器学习、深度学习有什么区别和联系？

**人工智能（Artificial Intelligence）**

**人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门技术科学。**“人工智能”是“一门技术科学”，它研究与开发的对象是“理论、技术及应用系统”，研究的目的是为了“模拟、延伸和扩展人的智能”。我们现在看到的貌似很高端的技术，如图像识别、NLP，其实依然没有脱离这个范围，就是“模拟人在看图方面的智能”和“模拟人在听话方面的智能”，本质上和“模拟人在计算方面的智能”没啥两样，虽然难度有高低，但目的是一样的——模拟、延伸和扩展人的智能。另外，人工智能在50年代就提出了。

**机器学习**

   随着人对计算机科学的期望越来越高，要求它解决的问题越来越复杂，已经远远不能满足人们的诉求了。于是有人提出了一个新的思路——能否不为难码农，让机器自己去学习呢？

**机器学习就是用算法解析数据，不断学习，对世界中发生的事做出判断和预测的一项技术。**研究人员不会亲手编写软件、确定特殊指令集、然后让程序完成特殊任务；相反，研究人员会用大量数据和算法“训练”机器，让机器学会如何执行任务。这里有三个重要的信息：1、“机器学习”是“模拟、延伸和扩展人的智能”的一条路径，所以**是人工智能的一个子集**；2、“机器学习”是要基于大量数据的，也就是说它的“智能”是用大量数据喂出来的；3、正是因为要处理海量数据，所以大数据技术尤为重要；“机器学习”只是大数据技术上的一个应用。常用的10大机器学习算法有：决策树、随机森林、逻辑回归、SVM、朴素贝叶斯、K最近邻算法、K均值算法、Adaboost算法、神经网络、马尔科夫。

**深度学习**

  相较而言，深度学习是一个比较新的概念，严格地说是2006年提出的。**深度学习是用于建立、模拟人脑进行分析学习的神经网络，并模仿人脑的机制来解释数据的一种机器学习技术。**它的基本特点，是试图模仿大脑的神经元之间传递，处理信息的模式。最显著的应用是计算机视觉和自然语言处理(NLP)领域。显然，**“深度学习”是与机器学习中的“神经网络”是强相关**，“神经网络”也是其主要的算法和手段；或者我们可以将“深度学习”称之为“改良版的神经网络”算法。深度学习又分为卷积神经网络（Convolutional neural networks，简称CNN）和深度置信网（Deep Belief Nets，简称DBN）。其主要的思想就是模拟人的神经元，每个神经元接受到信息，处理完后传递给与之相邻的所有神经元即可。所以看起来的处理方式有点像下图（想深入了解的同学可以自行google）。

![img](images\7.jpg)

神经网络的计算量非常大，事实上在很长时间里由于基础设施技术的限制进展并不大。而GPU的出现让人看到了曙光，也造就了深度学习的蓬勃发展，“深度学习”才一下子火热起来。击败李世石的Alpha go即是深度学习的一个很好的示例。Google的TensorFlow是开源深度学习系统一个比较好的实现，支持CNN、RNN和LSTM算法，是目前在图像识别、自然语言处理方面最流行的深度神经网络模型。事实上，提出“深度学习”概念的Hinton教授加入了google，而Alpha go也是google家的。

**总结：人工智能是一个很老的概念，机器学习是人工智能的一个子集，深度学习又是机器学习的一个子集。机器学习与深度学习都是需要大量数据来“喂”的，是大数据技术上的一个应用，同时深度学习还需要更高的运算能力支撑，如GPU。**

![img](images\8.jpg)



### 神经元、单层感知机、多层感知机

#### 神经元是什么？

正如其名字所表明，神经网络的灵感来源于人类大脑的神经结构，像在一个人类大脑中，最基本的构件就叫做神经元。它的功能和人的神经元很相似，换句话说，它有一些输入，然后给一个输出。在数学上，在机器学习中的神经元就是一个数学函数的占位符，它仅有的工作就是对输入使用一个函数，然后给一个输出。

![理解神经网络：从神经元到RNN、CNN、深度学习](images\9.jpg)

这种神经元中使用的函数，在术语上通常叫做激活函数。主要的激活函数有5种，date,step,sigmoid,tanh和ReLU。

#### 单层感知机

单层感知器由一个线性组合器和一个二值阈值元件组成。



![img](images\10.webp)



输入向量为x，权重向量为w，w0为偏执。

简单的理解可以解释为：将x0,x1······xn的变量输入，经过组合器的整合，输出1或者-1，也就是通过组合器对输入变量判断其正确与否。

而这个判断的依据就是权重w0,w1······wn。

因为线性组合器是实现加法的方式，根据向量的运算法则，所以以上公式的输入值可以理解为：
 w0+x1w1+······+xnwn



![img](images\12.webp)





单个数据的输入判断就是这样，下面我们将它扩展到多个数据，如下图所示：



![img](images\11.webp)



在整个的感知器算法中，是有明确的数学公式，通过线性组合器的组装进行分类判断：



![img](images\13.webp)



这就是详细的组合器算法。其中偏振因子b，一般会用w0表示，这时会加入一个偏振输入变量x0,不过x0恒等于1,也就是以上所描述的公式。

下面整体的介绍一下单层感知器算法模型：



![img](images\14.webp)



感知器算法模型

> 神经元期望的输出值已知;
>  根据实际的输入值向量X，和初始的权值向量W(已知)，经过线性感知器求得实际的输出值（一般为值是1或者-1的向量）。
>  使用神经元期望的输出值减去实机的输出值，求得差值，再和设定的学习率相乘后，再和输入向量相乘，求得权值变化的向量。（也就是得到对输入向量的调整后的向量）
>  将权值向量W和得到的变化向量相加，重复以上动作，直到期望输出和实际输出相等。

因为期望输出的值为（1或者-1）
 即：w0+w1x1+······+wnxn>0或w0+w1x1+······+wnxn<0
 所以它们的分界线为：
 w0+w1x1+······+wnxn=0

二维时为：
 w0+w1x1+w2x2=0

w2x2=-w1x1-w0
 x2=-(w1/w2)x1-w0/w2

x2=kx1+b

![img](images\15.webp)

#### 多层感知机

多层感知器（Multi Layer Perceptron，即 MLP）包括至少一个隐藏层（除了一个输入层和一个输出层以外）。单层感知器只能学习线性函数，而多层感知器也可以学习非线性函数。

![img](images\16.png)

*图 4：有一个隐藏层的多层感知器*

图 4 表示了含有一个隐藏层的多层感知器。注意，所有的连接都有权重，但在图中只标记了三个权重（w0,，w1，w2）。

输入层：输入层有三个节点。偏置节点值为 1。其他两个节点从 X1 和 X2 取外部输入（皆为根据输入数据集取的数字值）。和上文讨论的一样，在输入层不进行任何计算，所以输入层节点的输出是 1、X1 和 X2 三个值被传入隐藏层。

隐藏层：隐藏层也有三个节点，偏置节点输出为 1。隐藏层其他两个节点的输出取决于输入层的输出（1，X1，X2）以及连接（边界）所附的权重。图 4 显示了隐藏层（高亮）中一个输出的计算。其他隐藏节点的输出计算同理。需留意 *f *指代激活函数。这些输出被传入输出层的节点。

输出层：输出层有两个节点，从隐藏层接收输入，并执行类似高亮出的隐藏层的计算。这些作为计算结果的计算值（Y1 和 Y2）就是多层感知器的输出。

给出一系列特征 X = (x1, x2, ...) 和目标 Y，一个多层感知器可以以分类或者回归为目的，学习到特征和目标之间的关系。



### 前向传播和反向传播

## 1.前向传播

![这里写图片描述](images\17.png)

如图所示，这里讲得已经很清楚了，前向传播的思想比较简单。
举个例子，假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。
最终不断的通过这种方法一层层的运算，得到输出层结果。

对于前向传播来说，不管维度多高，其过程都可以用如下公式表示：
a 2 = σ ( z 2 ) = σ ( a 1 ∗ W 2 + b 2 ) a^2 = \sigma(z^2) = \sigma(a^1 * W^2 + b^2)*a*2=*σ*(*z*2)=*σ*(*a*1∗*W*2+*b*2)

其中，上标代表层数，星号表示卷积，b表示偏置项bias，σ \sigma*σ*表示激活函数。

## 2.反向传播算法(Back propagation)

BackPropagation算法是多层神经网络的训练中举足轻重的算法。简单的理解，它的确就是复合函数的链式法则，但其在实际运算中的意义比链式法则要大的多。要回答题主这个问题“如何直观的解释back propagation算法？” 需要先直观理解多层神经网络的训练。

机器学习可以看做是数理统计的一个应用，在数理统计中一个常见的任务就是拟合，也就是给定一些样本点，用合适的曲线揭示这些样本点随着自变量的变化关系.

深度学习同样也是为了这个目的，只不过此时，样本点不再限定为(x, y)点对，而可以是由向量、矩阵等等组成的广义点对(X,Y)。而此时，(X,Y)之间的关系也变得十分复杂，不太可能用一个简单函数表示。然而，人们发现可以用多层神经网络来表示这样的关系，而多层神经网络的本质就是一个多层复合的函数。借用网上找到的一幅图[1]，来直观描绘一下这种复合关系。

![这里写图片描述](images\18.png)

其对应的表达式如下：
![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/07d8c80d5512aa9fd6c3a0f18e94e91d.png)

上面式中的Wij就是相邻两层神经元之间的权值，它们就是深度学习需要学习的参数，也就相当于直线拟合y=k*x+b中的待求参数k和b。

和直线拟合一样，深度学习的训练也有一个目标函数，这个目标函数定义了什么样的参数才算一组“好参数”，不过在机器学习中，一般是采用成本函数（cost function），然后，训练目标就是通过调整每一个权值Wij来使得cost达到最小。cost函数也可以看成是由所有待求权值Wij为自变量的复合函数，而且基本上是非凸的，即含有许多局部最小值。但实际中发现，采用我们常用的梯度下降法就可以有效的求解最小化cost函数的问题。

梯度下降法需要给定一个初始点，并求出该点的梯度向量，然后以负梯度方向为搜索方向，以一定的步长进行搜索，从而确定下一个迭代点，再计算该新的梯度方向，如此重复直到cost收敛。那么如何计算梯度呢？

假设我们把cost函数表示为H ( W 11 , W 12 , ⋯   , W i j , ⋯   , W m n ) H(W_{11}, W_{12}, \cdots , W_{ij}, \cdots, W_{mn})*H*(*W*11,*W*12,⋯,*W**i**j*,⋯,*W**m**n*),那么它的梯度向量[2]就等于∇ H = ∂ H ∂ W 11 e 11 + ⋯ + ∂ H ∂ W m n e m n \nabla H = \frac{\partial H}{\partial W_{11} }\mathbf{e}_{11} + \cdots + \frac{\partial H}{\partial W_{mn} }\mathbf{e}_{mn}∇*H*=∂*W*11∂*H***e**11+⋯+∂*W**m**n*∂*H***e***m**n*, 其中e i j \mathbf{e}_{ij}**e***i**j*表示正交单位向量。为此，我们需求出cost函数H对每一个权值Wij的偏导数。而BP算法正是用来求解这种多层复合函数的所有变量的偏导数的利器。

我们以求e=(a+b)*(b+1)的偏导[3]为例。
它的复合关系画出图可以表示如下：
![这里写图片描述](images\19.png)

在图中，引入了中间变量c,d。
为了求出a=2, b=1时，e的梯度，我们可以先利用偏导数的定义求出不同层之间相邻节点的偏导关系，如下图所示。
![这里写图片描述](images\20.png)

利用链式法则我们知道：
∂ e ∂ a = ∂ e ∂ c ⋅ ∂ c ∂ a 以 及 ∂ e ∂ b = ∂ e ∂ c ⋅ ∂ c ∂ b + ∂ e ∂ d ⋅ ∂ d ∂ b \frac{\partial e}{\partial a}=\frac{\partial e}{\partial c}\cdot \frac{\partial c}{\partial a}以及\frac{\partial e}{\partial b}=\frac{\partial e}{\partial c}\cdot \frac{\partial c}{\partial b}+\frac{\partial e}{\partial d}\cdot \frac{\partial d}{\partial b}∂*a*∂*e*​=∂*c*∂*e*​⋅∂*a*∂*c*​以及∂*b*∂*e*​=∂*c*∂*e*​⋅∂*b*∂*c*​+∂*d*∂*e*​⋅∂*b*∂*d*​

链式法则在上图中的意义是什么呢？其实不难发现，∂ e ∂ a \frac{\partial e}{\partial a}∂*a*∂*e*的值等于从a到e的路径上的偏导值的乘积，而∂ e ∂ b \frac{\partial e}{\partial b}∂*b*∂*e*的值等于从b到e的路径1(b-c-e)上的偏导值的乘积加上路径2(b-d-e)上的偏导值的乘积。也就是说，对于上层节点p和下层节点q，要求得∂ p ∂ q \frac{\partial p}{\partial q}∂*q*∂*p*，需要找到从q节点到p节点的所有路径，并且对每条路径，求得该路径上的所有偏导数之乘积，然后将所有路径的 “乘积” 累加起来才能得到∂ p ∂ q \frac{\partial p}{\partial q}∂*q*∂*p*的值。

大家也许已经注意到，这样做是十分冗余的，因为很多路径被重复访问了。比如上图中，a-c-e和b-c-e就都走了路径c-e。对于权值动则数万的深度模型中的神经网络，这样的冗余所导致的计算量是相当大的。

同样是利用链式法则，BP算法则机智地避开了这种冗余，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。
正如反向传播(BP)算法的名字说的那样，BP算法是反向(自上往下)来寻找路径的。

从最上层的节点e开始，初始值为1，以层为单位进行处理。对于e的下一层的所有子节点，将1乘以e到某个节点路径上的偏导值，并将结果“堆放”在该子节点中。等e所在的层按照这样传播完毕后，第二层的每一个节点都“堆放"些值，然后我们针对每个节点，把它里面所有“堆放”的值求和，就得到了顶点e对该节点的偏导。然后将这些第二层的节点各自作为起始顶点，初始值设为顶点e对它们的偏导值，以"层"为单位重复上述传播过程，即可求出顶点e对每一层节点的偏导数。

以上图为例，节点c接受e发送的1*2并堆放起来，节点d接受e发送的1*3并堆放起来，至此第二层完毕，求出各节点总堆放量并继续向下一层发送。节点c向a发送2*1并对堆放起来，节点c向b发送2*1并堆放起来，节点d向b发送3*1并堆放起来，至此第三层完毕，节点a堆放起来的量为2，节点b堆放起来的量为2*1+3*1=5, 即顶点e对b的偏导数为5.

举个不太恰当的例子，如果把上图中的箭头表示欠钱的关系，即c→e表示e欠c的钱。以a, b为例，直接计算e对它们俩的偏导相当于a, b各自去讨薪。a向c讨薪，c说e欠我钱，你向他要。于是a又跨过c去找e。b先向c讨薪，同样又转向e，b又向d讨薪，再次转向e。可以看到，追款之路，充满艰辛，而且还有重复，即a, b 都从c转向e。

而BP算法就是主动还款。e把所欠之钱还给c，d。c，d收到钱，乐呵地把钱转发给了a，b，皆大欢喜。

## 3.反向传播具体计算过程推导

为了方便起见，这里我定义了三层网络，输入层（第0层），隐藏层（第1层），输出层（第二层）。并且每个结点没有偏置（有偏置原理完全一样），激活函数为sigmod函数（不同的激活函数，求导不同），符号说明如下：
![这里写图片描述](images\21.png)

对应网络如下：

![这里写图片描述](images\22.png)

其中对应的矩阵表示如下

![这里写图片描述](images\23.png)

首先我们先走一遍正向传播，公式与相应的数据对应如下：

![这里写图片描述](images\24.png)

那么：

![这里写图片描述](images\25.png)

同理可以得到：

![这里写图片描述](images\26.png)

那么最终的损失为

![这里写图片描述](images\27.png)

，我们当然是希望这个值越小越好。这也是我们为什么要进行训练，调节参数，使得最终的损失最小。这就用到了我们的反向传播算法，实际上反向传播就是梯度下降法中链式法则的使用。

下面我们看如何反向传播

根据公式，我们有：

![这里写图片描述](images\28.png)

这个时候我们需要求出C对w的偏导，则根据链式法则有：

![这里写图片描述](images\29.png)

同理有：

![这里写图片描述](images\30.png)

到此我们已经算出了最后一层的参数偏导了.我们继续往前面链式推导：

我们现在还需要求

![这里写图片描述](images\31.png)

，下面给出一个推导其它全都类似

![这里写图片描述](images\32.png)

同理可得其它几个式子：

则最终的结果为：

![这里写图片描述](images\33.png)

再按照这个权重参数进行一遍正向传播得出来的Error为0.165

而这个值比原来的0.19要小，则继续迭代，不断修正权值，使得代价函数越来越小，预测值不断逼近0.5.我迭代了100次的结果，Error为5.92944818e-07（已经很小了，说明预测值与真实值非常接近了），最后的权值为：

![这里写图片描述](images\34.png)

### 