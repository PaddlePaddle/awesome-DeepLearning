# 7.6深度学习基础知识作业（石瑜恺）

## 1.深度学习发展历史

1943年，心理学家Warren Mcculloch和数理逻辑学家Walter Pitts在合作的论文中提出并给出了人工神经网络的概念及人工神神经元的数学模型，从而开创了人类神经网络研究的时代。

1949年，心理学家Donald Hebb在论文[8]中提出了**神经心理学理论**，Hebb认为神经网络的学习过程最终是发生在神经元之间的突触部位，突触的联结强度随着突触前后神经元的活动而变化，变化的量与两个神经元的活性之和成正比。

1956年，心理学家Frank Rosenblatt受到这种思想的启发，认为这个简单想法足以创造一个可以学习识别物体的机器，并设计了算法和硬件。直到1957年，Frank Rosenblatt在《New York Times》上发表文章《Electronic ‘Brain’ Teaches Itself》，首次提出了可以模型人类感知能力的机器，并称之为**感知机**（**Perceptron**）。

1982年，著名物理学家约翰·霍普菲尔德发明了Hopfield神经网络。Hopfield神经网络是一种结合存储系统和二元系统的循环神经网络。Hopfield网络也可以模拟人类的记忆，根据激活函数的选取不同，有连续型和离散型两种类型，分别用于优化计算和联想记忆。但由于容易陷入局部最小值的缺陷，该算法并未在当时引起很大的轰动。

1986年，深度学习之父杰弗里·辛顿提出了一种适用于多层感知器的反向传播算法——BP算法。BP算法在传统神经网络正向传播的基础上，增加了误差的反向传播过程。反向传播过程不断地调整神经元之间的权值和阈值，直到输出的误差达到减小到允许的范围之内，或达到预先设定的训练次数为止。BP算法完美的解决了非线性分类问题。

2006年，杰弗里·辛顿以及他的学生鲁斯兰·萨拉赫丁诺夫正式提出了深度学习的概念。他们在世界顶级学术期刊《科学》发表的一篇文章中详细的给出了“梯度消失”问题的解决方案——通过无监督的学习方法逐层训练算法，再使用有监督的反向传播算法进行调优。该深度学习方法的提出，立即在学术圈引起了巨大的反响，以斯坦福大学、多伦多大学为代表的众多世界知名高校纷纷投入巨大的人力、财力进行深度学习领域的相关研究。而后又在迅速蔓延到工业界中。

## 2.人工智能、机器学习和深度学习的区别与联系

人工智能：研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，该领域的研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。

机器学习：机器学习是一种实现人工智能的方法。机器学习是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。

深度学习：深度学习是一种实现机器学习的技术。深度学习是学习样本数据的内在规律和表示层次，这些学习过程中获得的信息对诸如文字，图像和声音等数据的解释有很大的帮助。它的最终目标是让机器能够像人一样具有分析学习能力，能够识别文字、图像和声音等数据。 

## 3.神经元、单层感知机、多层感知机

神经元：神经元通过对n个输入信号，通过带权重的连接（connection）进行传递，将总的输入与阈值进行比较，通过激活函数处理产生输出。

![](.\神经元.png)
$$
y^1 = f(\sum_{i=1}^n w_i^1x_i^1+b^1)
$$
单层感知机：单层感知机目标是将被感知数据集划分为两类的分离超平面，并计算出该超平面。单层感知机是二分类的线性分类模型，输入是被感知数据集的特征向量，输出时数据集的类别{+1,-1}。

多层感知机：多层感知机的一个重要特点就是多层，我们将第一层称之为输入层，最后一层称之有输出层，中间的层称之为隐层。MLP并没有规定隐层的数量，因此可以根据各自的需求选择合适的隐层层数。且对于输出层神经元的个数也没有限制。MLP神经网络结构模型如下,本文中只涉及了一个隐层，输入只有三个变量[x1,x2,x3][x1,x2,x3]和一个偏置量b，输出层有三个神经元。相比于感知机算法中的神经元模型对其进行了集成。

## 4.前向传播

<img src=".\多层感知机.png" style="zoom: 50%;" />

基于神经元的传播原理，我们得到前馈公式：
$$
y^1 = f^1(\sum_{i=1}^n w_i^1x_i^1+b^1)
$$
$$
y^2 = f^2(\sum_{i=1}^n w_i^2y_i^1+b^2)
$$

$$
y^j= f^j(\sum_{i=1}^n w_i^jy_i^{j-1}+b^j)
$$

## 5.反向传播

设置损失函数，利用链式求导逐步得到每个权重的梯度。
$$
\frac{\partial Loss}{\partial w^i}=\frac{\partial Loss}{\partial y^j}*\frac{\partial y^j}{\partial f^j}*\frac{\partial f^j}{\partial y^{j-1}}*...*\frac{\partial y^{i}}{\partial w^i}
$$
梯度迭代：
$$
w^i=w^i+\alpha*\frac{\partial Loss}{\partial w^i}
$$
