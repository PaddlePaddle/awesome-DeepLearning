```python
# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原
# View dataset directory. 
# This directory will be recovered automatically after resetting environment. 
!ls /home/aistudio/data
```


```python
# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.
# View personal work directory. 
# All changes under this directory will be kept even after reset. 
# Please clean unnecessary files in time to speed up environment loading. 
!ls /home/aistudio/work
```


```python
# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:
# If a persistence installation is required, 
# you need to use the persistence path as the following: 
!mkdir /home/aistudio/external-libraries
!pip install beautifulsoup4 -t /home/aistudio/external-libraries
```


```python
# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: 
# Also add the following code, 
# so that every time the environment (kernel) starts, 
# just run the following code: 
import sys 
sys.path.append('/home/aistudio/external-libraries')
```

请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>
Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. 

基于层次softmax训练词向量
假设有一份训练文档集，它包括了T个训练词语w1,w2,w3,⋯,wT，它们构成大小为|V|的词语集合V。语言模型通常只考虑由当前词语wi的左右n个词语组成的上下文ci。每个词语有一个d维的输入词向量vW(即embedding层的词向量)和输出词向量v’W（即softmax层的权重矩阵所表示的词语）。最后，针对模型参数θ来优化目标函数Jθ。
若指定上下文c，用softmax方法计算词语w出现的概率可以用公式表示为：
![](https://ai-studio-static-online.cdn.bcebos.com/866900721ed74c36b36fe94693873d534a4050943f744e899622ec74fff956d2)
h是网络倒数第二层的输出向量。为了简化表示，上式中用c表示上下文内容，并且省略了目标词语wt的下标t。为了得到上式的分母部分，需要计算向量h与词典V中每个词语向量之间的内积。因此，计算softmax的代价非常昂贵。
接下来，我们将讨论几种能够近似替代softmax的策略。这些方法可以归纳为基于softmax的和基于sampling的两大类。基于softmax的方法仍旧保留了模型的softmax层，但是通过调整其结构来提高效率。
基于softmax的方法
分层Softmax
Hierarchical softmax （H-Softmax）是由Morin和Bengio[3]受到二叉树的启发而提出。H-Softmax本质上是用层级关系替代了扁平化的softmax层，如图1所示，每个叶子节点表示一个词语。于是，计算单个词语概率值的计算过程被拆解为一系列的概率计算，这样可以避免对所有词语进行标准化计算。用H-Softmax替换softmax层之后，词语的预测速度可以提升至少50倍，速度的提升对于低延时要求的实时系统至关重要，比如谷歌新推出的消息应用Allo。
我们可以把原来的softmax看做深度为1的树，词表V中的每一个词语表示一个叶子节点。计算一个词语的softmax概率需要对|V|个节点的概率值做标准化。如果把softmax改为二叉树结构，每个word表示叶子节点，那么只需要沿着通向该词语的叶子节点的路径搜索，而不需要考虑其它的节点。
平衡二叉树的深度是log2(|V|)，因此，最多只需要计算log2(|V|)个节点就能得到目标词语的概率值。注意，得到的概率值已经经过了标准化，因为二叉树所有叶子节点组成一个概率分布，所有叶子节点的概率值总和等于1。我们可以简单地验证一下，在图1的根节点（Node o）处，两个分枝的概率和必须为1。之后的每个节点，它的两个子节点的概率值之和等于节点本身的概率值。因为整条搜索路径没有概率值的损失，所以最底层所有叶子节点的概率值之和必定等于1，hierarchical softmax定义了词表V中所有词语的标准化概率分布。
具体说来，当遍历树的时候，我们需要能够计算左侧分枝或是右侧分枝的概率值。为此，给每个节点分配一个向量表示。与常规的softmax做法不同，这里不是给每个输出词语w生成词向量v’w，而是给每个节点n计算一个向量v’n。总共有|V|-1个节点，每个节点都有自己独一无二的向量表示，H-Softmax方法用到的参数与常规的softmax几乎一样。于是，在给定上下文c时，就能够计算节点n左右两个分枝的概率：
![](https://ai-studio-static-online.cdn.bcebos.com/945e8bbf37474d968ee06632d4db07f2196e417587ea48bb8bff4fadb5c4aa09)
上式与常规的softmax大致相同。现在需要计算h与树的每个节点的向量v’n的内积，而不是与每个输出词语的向量计算。而且，现在只需要计算一个概率值，这里就是偏向n节点右枝的概率值。相反的，偏向左枝的概率值是1−p(right|n,c)
假设已知出现了词语“the”、“dog”、“and”、“the”，则出现词语“cat”的概率值就是在节点1向左偏的概率值、在节点2向右偏的概率以及在节点5向右偏的概率值的乘积。Hugo Lachorelle在他的视频教程中给了更详细的解释。Rong[7]的文章也详细地解释了这些概念，并推导了H-Softmax。
显然，树形的结构非常重要。若我们让模型在各个节点的预测更方便，比如路径相近的节点概率值也相近，那么凭直觉系统的性能肯定还会提升。沿着这个思路，Morin和Bengio使用WordNet的同义词集作为树簇。然而性能依旧不如常规的softmax方法。Mnih和Hinton[8]将聚类算法融入到树形结构的学习过程，递归地将词集分为两个集合，效果终于和softmax方法持平，计算量有所减小。
值得注意的是，此方法只是加速了训练过程，因为我们可以提前知道将要预测的词语（以及其搜索路径）。在测试过程中，被预测词语是未知的，仍然无法避免计算所有词语的概率值。
在实践中，一般不用“左节点”和“右节点”，而是给每个节点赋一个索引向量，这个向量表示该节点的搜索路径。如图2所示，如果约定该位为0表示向左搜索，该位为1表示向右搜索，那词语“cat”的向量就是011。
上文中提到平衡二叉树的深度不超过log2(|V|)。若词表的大小是|V|=10000，那么搜索路径的平均长度就是13.3。因此，词表中的每个词语都能表示为一个平均长度为13.3比特的向量，即信息量为13.3比特。
关于信息量
在信息论中，人们习惯于将词语w概率值的负对数定义为信息量I(w)：
I(w)=−log2p(w)
而熵H则是词表中所有词语的信息量的期望值：
H=∑i∈Vp(wi)I(wi)
熵也代表着根据信息的概率分布对信息编码所需要的最短平均编码长度。 抛硬币事件需要用1比特来编码正反两个时间，对于永恒不变的事件则只需0比特。若用平衡二叉树的节点来表示词表中的词语，还是假设词表的大小|V|=10000，词表中词语的概率值均相等，那么熵H与平均搜索路径的长度恰好相等：
![](https://ai-studio-static-online.cdn.bcebos.com/cfeefb05990146a5afd04ee24d20baf69ef4480fc77840f8984a08b8a4efad9e)
之前我们一再强调了树结构的重要性，因为利用好树结构不仅能提升系统的性能，还能加快运算速度。若我们给树加入额外的信息，就能缩短某些携带信息量少的词语的搜索路径。Morin和Bengio就是利用了词表中各个词语出现概率不相等这一信息。他们认为词表中的一些词语出现的概率总是大于其它词语，那这些词语就应该用更短的向量编码。他们所用的文档集（|V|=10000）的熵大约是9.16。
于是，考虑词频之后，文档集中每个词语的平均编码长度从13.3比特减为9.16比特，运算速度也提升了31%。Mikolov等人在他们关于hierarchical softmax的论文[1]里就用到了霍夫曼树，即词频越高的词语编码长度越短。比如，“the”是英语中最常见的词语，那“the”在霍夫曼树中的编码长度最短，词频第二高的词语编码长度仅次于“the”，以此类推。整篇文档的平均编码长度因此降低。
霍夫曼编码通常也被称作熵编码，因为每个词语的编码长度与它的熵几乎成正比。香农通过实验[5]得出英语字母的信息量通常在0.6~1.3之间。假设单词的平均长度是4.5个字母，那么所携带的信息量就是2.7~5.85比特。
再回到语言模型：衡量语言模型好坏的指标perplexity是2H，H表示熵。熵为9.16的unigram模型的perplexity达到29.16=572.0。我们可以直观的理解为，平均情况下用该unigram模型预测下一个词语时，有572个词语是等可能的候选词语。目前，Jozefowicz在2016年的论文中提到最好的模型perplexity=24.2。因为24.6=24.2，所以这个模型平均只需要4.6比特来表示一个词语，已经非常接近香农记录的实验下限值了。这个模型是否能用于改进网络的hierarchical softmax层，仍需要人们进一步探索。
分片Softmax
Chen等人在论文中介绍了一种传统softmax层的变换形式，称作Differentiated Softmax (D-Softmax)。D-Softmax基于的假设是并不是所有词语都需要相同数量的参数：多次出现的高频词语需要更多的参数去拟合，而较少见的词语就可以用较少的参数。
传统的softmax层用到了dx|V|的稠密矩阵来存放输出的词向量表示v′w∈ℝd，论文中采用了稀疏矩阵。他们将词向量v′w按照词频分块，每块区域的向量维度各不相同。分块数量和对应的维度是超参数，可以根据需要调整。
A区域的词向量维度是dA（这个分块是高频词语，向量的维度较高），B和C区域的词向量维度分别是dB和dC。其余空白区域的值为0。
隐藏层h的输出被视为是各个分块的级联，比如图3中h层的输出是由三个长度分别为dA、dB、dC的向量级联而成。D-Softmax只需计算各个向量段与h对应位置的内积，而不需整个矩阵和向量参与计算。
由于大多数的词语只需要相对较少的参数，计算softmax的复杂度得到降低，训练速度因此提升。相对于H-Softmax方法，D-Softmax的优化方法在测试阶段仍然有效。Chen在2015年的论文中提到D-Softmax是测试阶段最快的方法，同时也是准确率最高的之一。但是，由于低频词语的参数较少，D-Softmax对这部分数据的建模能力较弱。
CNN-Softmax
传统softmax层的另一种改进是受到Kim[3]的论文启发，Kim对输入词向量vw采用了字符级别的CNN模型。相反，Jozefowicz在2016年将同样的方法用于输出词向量v′w，并将这种方法称为CNN-Softmax。如图4所示，如果我们在输入端和输出端加上CNN模型，输出端CNN生成的向量v′w与输入端CNN生成的向量必然不相同，因为输入和输出的词向量矩阵就不一样。
尽管这个方法仍需要计算常规softmax的标准化，但模型的参数却大大的减少：只需要保留CNN模型的参数，不需要保存整个词向量矩阵dx|V|。在测试阶段，输出词向量v′w可以提前计算，所以性能损失不大。
但是，由于字符串是在连续空间内表示，而且模型倾向于用平滑函数将字符串映射到词向量，因此基于字符的模型往往无法区分拼写相似但是含义不同的词语。为了消除上述影响，论文的作者增加了一个矫正因数，显著地缩小了CNN-Softmax与传统方法的性能差距。通过调整矫正因数的维度，作者可以方便地取舍模型的大小与计算性能。
论文的作者还提到，前一层h的输出可以传入字符级别的LSTM模型，这个模型每次预测输出词语的一个字母。但是，这个方法的性能并不令人满意。Ling等人在2014年的论文中采用类类似的方法来解决机器翻译任务，取得了不错的效果。

LSTM可实现的NLP任务
序列到类别模式：主要用于序列数据的分类问题：
   输入为序列，输出为类别，如文本分类中，输入数据为单词的序列，输出数据为文本的类别

将一个样本 ![](https://ai-studio-static-online.cdn.bcebos.com/9e8a3508f031448e98c0b0e11d20f9013d70641ff27840a4ae34e95b1a52e0a0)，输出为y={1,2,3,4....C}  将x按照不同时刻输入到循环神经网络中，得到不同时刻的隐状态ht ，ht+1等等，  可以将看作整个序列的最终表示，然后再输入给分类器进行分类。


除了将最后时刻的状态作为整个序列的表示之后，我们还可以对所有序列的状态进行平均，利用这个平均的状态作为整个序列的表示。
同步的序列到序列模型：用于序列标注问题
每一个时刻都有输入和输出，且其长度相同。如在词性标注中，每一个单词都需要标注其对应的词性标签。

输入为一个长度T的序列，输出为序列y（1-T）  样本x按不同时刻输入到循环神经网络中，并得到不同时刻的隐状态![](https://ai-studio-static-online.cdn.bcebos.com/d81772411b9f422f87ae0cac09bb87ea323c82f7beaf4cb4814c05b96632d25d)，每个时刻的ht都代表了当前时刻xt信息和前一时刻隐状态的历史信息。 （全部的包含信息？与后来的遗忘门f来做比较。）
然后再输入给g分类器 得到序列y。对真是标签y和预测的y_hat进行交叉熵得到损失函数，在进行梯度下降更新参数和梯度。
异步的序列到序列模型：
seq2seq中用到此类，也称为编码器-解码器 输入和输出不需要有严格的对应关系，也不需要有一致的长度。 如在机器翻译中，输入源语言序列，输出为目标语言序列，其长度不一定等长。
一般通过先编码后解码的过程来进行，先将输入序列x按不同时刻输入到编码器中得到其编码,然后再使用另外一个循环神经网络（解码器）进行解码得到输出y1_hat,  再将y1_hat和传递下来的隐状态 一起进行f2函数的传递，  采用自回归模型，每个时刻的输入都是上一时刻的预测结果。
