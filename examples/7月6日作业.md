# 7月6日作业

## 一、深度学习发展史

### 1深度学习的起源

​      **人工智能**（Artificial Intelligence）就像长生不老和星际漫游一样，是人类最美好的梦想之一。虽然计算机技术已经取得了长足的进步，但是到目前为止，还没有一台电脑能产生“自我”的意识。计算机能够具有人的意识起源于图灵测试（Turing Testing）问题的产生，由“计算机科学之父”及“人工智能之父”英国数学家**阿兰·图灵**在1950年的一篇著名论文**《机器会思考吗？》**里提出图灵测试的设想：

​      把一个人和一台计算机分别隔离在两间屋子，然后让屋外的一个提问者对两者进行问答测试。如果提问者无法判断哪边是人，哪边是机器，那就证明计算机已具备人的智能。

​      但是半个世纪过去了，人工智能的进展，远远没有达到图灵试验的标准。这不仅让多年翘首以待的人们心灰意冷，认为人工智能是忽悠，相关领域是“伪科学”。直到深度学习（Deep Learning）的出现，让人们看到了一丝曙光。至少，图灵测试已不再是那么遥不可及了。2013年4月，《麻省理工学院技术评论》杂志将深度学习列为2013年十大突破性技术之首。

​     了解深度学习的起源，首先让我们先来了解一下人类的大脑是如何工作的。1981年的诺贝尔医学奖，分发给了David Hubel、Torsten Wiesel和Roger Sperry。前两位的主要贡献是，**发现了人的视觉系统的信息处理是分级。**如图1所示，从视网膜（Retina）出发，经过低级的V1区提取边缘特征，到V2区的基本形状或目标的局部，再到高层V4的整个目标（如判定为一张人脸），以及到更高层的PFC（前额叶皮层）进行分类判断等。也就是说高层的特征是低层特征的组合，从低层到高层的特征表达越来越抽象和概念化。

​        ![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720110716951-1388773395.jpg)               

​                        图1 人的视觉处理系统

​        **这个发现激发了人们对于神经系统的进一步思考。**大脑的工作过程，是一个对接收信号不断迭代、不断抽象概念化的过程，如图2所示。例如，从原始信号摄入开始（瞳孔摄入像素），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定眼前物体的形状，比如是椭圆形的），然后进一步抽象（大脑进一步判定该物体是张人脸），最后识别人脸。这个过程其实和我们的常识是相吻合的，因为复杂的图形，往往就是由一些基本结构组合而成的。同时我们还可以看出：**大脑是一个深度架构，认知过程也是深度的。**

 ![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720110744685-736941965.jpg)

![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720110807997-430892409.jpg)

​                                           图2 视觉系统分层处理结构

​        **而深度学习，恰恰就是通过组合低层特征形成更加抽象的高层特征**（或属性类别）。例如，在计算机视觉领域，深度学习算法从原始图像去学习得到一个低层次表达，例如边缘检测器、小波滤波器等，然后在这些低层次表达的基础上，通过线性或者非线性组合，来获得一个高层次的表达。此外，不仅图像存在这个规律，声音也是类似的。

### 2深度学习的发展

#### 2.1从感知机到神经网络

##### 2.1.1 最简单的神经网络结构—感知机

​        1943年，心理学家Warren Mcculloch和数理逻辑学家Walter Pitts在合作的论文[7]中提出并给出了人工神经网络的概念及人工神神经元的数学模型，从而开创了人类神经网络研究的时代。

​        1949年，心理学家Donald Hebb在论文[8]中提出了**神经心理学理论**，Hebb认为神经网络的学习过程最终是发生在神经元之间的突触部位，突触的联结强度随着突触前后神经元的活动而变化，变化的量与两个神经元的活性之和成正比。

​        1956年，心理学家Frank Rosenblatt受到这种思想的启发，认为这个简单想法足以创造一个可以学习识别物体的机器，并设计了算法和硬件（如图3所示）。直到1957年，Frank Rosenblatt在《New York Times》上发表文章《Electronic ‘Brain’ Teaches Itself》，首次提出了可以模型人类感知能力的机器，并称之为**感知机**（**Perceptron**）[2]。

 ![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720110844560-1606545946.jpg)

![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720110857419-1307290957.jpg)

图3 Frank Rosenblatt和感知机的提出

​       **感知机是有单层计算单元的神经网络**，由线性元件及阈值元件组成。感知机的逻辑图如图4所示。

 ![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720110945669-1584491879.jpg)

图4 感知机模型

​        Frank Rosenblatt对Hebb的理论猜想提出了数学论证方法：

感知机的数学模型（ 是阈值）：

 ![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720111014107-276590950.jpg)

其中，f[.]是阶跃函数，并且有：

 ![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720111037029-1974730047.jpg)

感知器的做大作用就是对输入的样本**分类**，故它可以作为分类器，感知器对输入信号的分类如下（A类，B类）：

 ![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720111100638-161408501.jpg)

当感知器的输出为1时，输入样本为A类；输出为-1时，输入样本为B类。由此可知感知器的分类边界是：

![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720111129404-1265441691.jpg) 

在输入样本只有两个分量x1和x2时，则分类边界条件：

![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720111202435-737517514.jpg) 

即：

 ![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720111223044-1917131043.jpg)

从坐标轴上表示如图5所示：

 ![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720111242497-1004342292.jpg)

图5 感知机的二元线性分类

##### 2.1.2感知机算法

​        感知机的学习算法：目的在于计算出恰当的权系数（w1,w2,…,wn），使系统对一个特定的样本（x1,x2,…,xn）能产生期望值d。

感知机学习算法步骤如下：

1) 对权系数设置初值；

2) 输入一个样本（x1,x2,…,xn）以及它的期望输出d；

3) 计算实际输出值：

![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720111332044-702070275.jpg) 

4) 根据实际输出求误差e：

e=d-Y 

5) 用误差e去修改权系数：

![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720111405779-625930675.jpg) 

6) 转到第2步，一直执行到一切样本均稳定为止。

​        感知机是整个神经网络的基础，神经元通过激励函数确定输出，神经元之间通过权值进行传递能量，权重的确定根据误差来进行调节，这个方法的前景是整个网络是收敛的。这个问题，Frank Rosenblatt在1957年证明了这个结论。

​        有关感知机的成果，由Frank Rosenblatt在1958年发表在文章[9]里。1962年，他又出版了[10]一书，向大众深入解释感知机的理论知识及背景假设。此书介绍了一些重要的概念及定理证明，例如感知机收敛定理。

##### 2.1.3 单层感知机的局限性

​        单层感知机仅对**线性问题**具有分类能力，即仅用一条直线可分的图形，如图6所示。还有逻辑“**与**”或逻辑“**或**”，采用一条直线分割0和1，如图7所示。

![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720111446326-776710112.png) 

图6 线性可分问题

**![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720111458294-1866376022.jpg)** 

（a）逻辑“与”的真值表和二维样本图

![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720111516419-400709418.jpg)

（b）逻辑“或”的真值表和二维样本图

 

图7逻辑“与”和“或”的线性划分

​       但是，如果让感知机解决**非线性**问题，单层感知机就无能为力了，如图8所示。例如，“异或”就是非线性运算，无法用一条直线分割开来，如图9所示。

 ![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720111546122-1442697837.jpg)

图8 非线性不可分问题

 ![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720111553419-2121278739.jpg)

图9 逻辑“异或”的非线性不可分

##### 2.1.4 多层感知机的瓶颈

​       虽然，感知机最初被认为有着良好的发展潜能，但是感知机最终被证明不能处理诸多的模式识别问题。1969年，Marvin Minsky和Seymour Papery在[11]中，仔细分析了以感知机为代表的单层感知机在计算能力上的局限性，**证明感知机不能解决简单的异或（XOR）等线性不可分问题，但Rosenblatt和Minsky及Papery等人在当时已经了解到多层神经网络能够解决线性不可分的问题。

​       既然一条直线无法解决分类问题，当然就会有人想到用弯曲的折线来分类样本，因此在单层感知机的输入层和输出层之间加入隐藏层，就构成了多层感知机，目的是通过**凸域**能够正确分类样本。多层感知机结构如图10所示。

 

![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720095638404-1243111254.jpg)

 

图10 多层感知机

 

​        对单层感知机和多层感知机的分类能力进行比较，如表1所示：

 

![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720100051560-1836066233.jpg)

   

​        由表1可知，随着隐藏层的层数增多，凸域将可以形成任意的形状，因此可以解决任何复杂的分类问题。虽然多层感知机缺失是非常理想的分类器，**但是问题也随之而来**：隐藏层的权值怎么训练？对于各隐层的节点来说，它们并不存在期望输出，所以也无法通过感知机的学习规则来训练多层感知机。因此，多层感知机的训练也遇到了瓶颈，人工神经网络的发展进入了低潮期。

​        通过图11可见人工神经网络最初的发展史。1969年Marvin Minsky和Seymour Papery在[11]一书中提出了上述的感知机的研究瓶颈，指出理论上还不能证明将感知机模型扩展到多层网络是有意义的。这在人工神经网络的历史上书写了及其灰暗的一章。对于ANN的研究，始于1890年开始于美国心理学家W.James对于人脑结构与功能的研究，半个世纪后W.S.McCulloch和W.A.Pitts提出了M-P模型，之后的1958年Frank Rosenblatt在这个基础上又提出了感知机，此时对ANN的演技正处在升温阶段，[11]这本书的出现（1988有所更正并更名为[12]）为这刚刚燃起的人工神经网络之火泼了一大盆冷水。一时间人们仿佛感觉以感知机为基础的ANN的研究突然走到尽头。于是，几乎所有为ANN提供的研究基金都枯竭了，很多领域的专家纷纷放弃了这方面课题的研究。

 

![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720100340107-1258047511.jpg)

图11 ANN简史

##### 2.1.5神经网络的崛起

​       真理的果实总是垂青于能够坚持研究的科学家。尽管ANN的研究陷入了前所未有的低谷，但仍有为数不多的学者致力于ANN的研究。直到1982年美国加州理工学院的物理学家John J.Hopfield博士提出的Hopfield网络和David E.Rumelhart以及James L.McCelland研究小组发表的《并行分布处理》。这两个成果重新激起了人们对ANN的研究兴趣，使人们对模仿脑信息处理的智能计 算机的研究重新充满了希望。

​       前者暂不讨论，后者对具有**非线性连续变换函数的多层感知器的误差反向传播****(Error Back Propagation)****算法**进行了详尽的分析，实现了 Minsky 关于多层网络的设想。误差反向传播即**反向传播算法**（Backpropagation algorithm，BP）。

​       前面我们说到，多层感知器在如何获取隐层的权值的问题上遇到了瓶颈。既然我们无法直接得到隐层的权值，能否先通过输出层得到输出结果和期望输出的误差来间接调整隐层的权值呢？BP算法就是采用这样的思想设计出来的算法，它的基本思想：学习过程由**信号的正向传播**与**误差的反向传播**两个过程组成。如图12所示。

 

![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720100449029-565404047.jpg)

图12 反向传播的基本思想

 

1) 正向传播时，输入样本从输入层传入,经各隐层逐层处理后,传向输出层。若输出层的实际输出与期望的输出不符,则转入误差的反向传播阶段。

2) 反向传播时，将输出以某种形式通过隐层向输入层逐层反传,并将误差分摊给各层的所有单元，从而获得各层单元的误差信号,此误差信号即作为修正各单元权值的依据。

​       结合了BP算法的神经网络称为BP神经网络，BP神经网路模型中采用**反向传播算法所带来的问题是**：基于局部梯度下降对权值进行调整容易出现**梯度弥散**（Gradient Diffusion）现象，根源在于非凸目标代价函数导致求解陷入局部最优，而不是全局最优。而且，随着网络层数的增多，这种情况会越来越严重。这一问题的产生制约了神经网络的发展。

#### 2.2 神经网络之后的又一突破—深度学习

​       直至2006年，加拿大多伦多大学教授**Geoffrey Hinton**对**深度学习的提出以及模型训练方法的改进打破了****BP****神经网络发展的瓶颈**。Hinton在世界顶级学术期刊《科学》上的一篇论文[1]中提出了**两个观点**：（1）多层人工神经网络模型有很强的特征学习能力，深度学习模型学习得到的特征数据对原始数据有更本质的代表性，这将大大便于分类和可视化问题；（2）**对于深度神经网络很难训练达到最优的问题，可以采用逐层训练方法解决。**将上层训练好的结果作为下层训练过程中的初始化参数。在这一文献中深度模型的训练过程中逐层初始化采用无监督学习方式。

​       值得一提的是，从感知机诞生到神经网络的发展，再到深度学习的萌芽，深度学习的发展并非一帆风顺。直到2006年，Geoffrey Hinton提出深度置信网（Deep Belief Net：DBN）[2]，其由一系列受限波尔兹曼机（Restricted Boltzmann Machine：RBM）[3]组成，提出**非监督贪心逐层训练**（Layerwise Pre-Training）算法，应用效果才取得突破性进展，其与之后Ruslan Salakhutdinov提出的深度波尔兹曼机（Deep Boltzmann Machine：DBM）[4]重新点燃了人工智能领域对于神经网络（Neural Network）和波尔兹曼机（Boltzmann Machine）[5]的热情，才由此掀起了深度学习的浪潮。从目前的最新研究进展来看，只要数据足够大、隐藏层足够深，即便不加“Pre-Training”预处理，深度学习也可以取得很好的结果，反映了大数据和深度学习相辅相成的内在联系。此外，虽说非监督（如DBM方法）是深度学习的一个优势，深度学习当然也可用于带监督的情况（也即给予了用户手动标注的机会），实际上带监督的卷积神经网络（Convolutional Neural Network：CNN）[6]方法目前就应用得越来越多，乃至正在超越DBM。

​       深度学习是一列在信息处理阶段利用非监督特征学习和模型分析分类功能的，具有多层分层体系结构的机器学习技术。深度学习的本质是对观察数据进行分层特征表示，实现将低级特征进一步抽象成高级特征表示。

​       深度学习可以分为**三类**：（1）**生成型深度结构**：生成型深度结构旨在模式分析过程中描述观察到的课件数据的高阶相关属性，或者描述课件数据和其相关类别的联合概率分布。由于不关心数据的标签，人们经常使用非监督特征学习。当应用生成模型结构到模式识别中时，一个重要的任务就是**预训练**。但是当训练数据有限时，学习较低层的网络是困难的。因此，一般采用先学习每一个较低层，然后在学习较高层的方式，通过贪婪地逐层训练，实现从底向上分层学习。属于生成型深度结构的深度学习模型有：**自编码器**、受限玻尔兹曼机、**深度置信网络**等。（2）**判别型深度结构**：判别型深度结构的目的是通过描述可见数据的类别的后验概率分布为模式分类提供辨别力。属于判别型深度结构的深度学习模型主要有**卷积神经网络**和深凸网络等。（3）**混合型深度结构**：混合型深度结构的目的是对数据进行判别，是一种包含了生成和判别两部分结构的模型。在应用生成型深度结构解决分类问题时，因为现有的生成型结构大多数都是用于对数据的判别，可以结合判别型模型在预训练阶段对网络的所有权值进行优化。例如通过深度置信网络进行预训练后的深度神经网络。

### 3 .什么是深度学习

​       深度学习作为机器学习算法研究中的一个新的技术，其动机在于建立、模拟人脑进行分析学习的神经网络。**深度学习是相对于简单学习而言的，**目前多数分类、回归等学习算法都属于简单学习或者浅层结构，**浅层结构**通常只包含1层或2层的非线性特征转换层，典型的浅层结构有高斯混合模型(GMM)、隐马尔科夫模型(HMM)、条件随机域(CRF)、最大熵模型(MEM)、逻辑回归(LR)、支持向量机(SVM)和多层感知器(MLP)。（其中，最成功的分类模型是SVM，SVM使用一个浅层线性模式分离模型，当不同类别的数据向量在低维空间无法划分时，SVM会将它们通过核函数映射到高维空间中并寻找分类最优超平面。）浅层结构学习模型的**相同点**是采用一层简单结构将原始输入信号或特征转换到特定问题的特征空间中。浅层模型的**局限性对复杂函数的表示能力有限，针对复杂分类问题其泛化能力受到一定的制约，比较难解决一些更加复杂的自然信号处理问题，例如人类语音和自然图像等。**而**深度学习**可通过学习一种深层非线性网络结构，表征输入数据，实现复杂函数逼近，并展现了强大的从少数样本集中学习数据集本质特征的能力。

​       **深度学习可以简单理解为传统神经网络的拓展。**如图13所示，深度学习与传统的神经网络之间有相同的地方，二者的相同之处在于，深度学习采用了与神经网络相似的分层结构：系统是一个包括输入层、隐层（可单层、可多层）、输出层的多层网络，只有相邻层的节点之间有连接，而同一层以及跨层节点之间相互无连接。

 

![img](https://images2015.cnblogs.com/blog/901086/201607/901086-20160720100924372-972784315.jpg)

 

图13 传统的神经网络和深度神经网络

 

​        深度学习框架将特征和分类器结合到一个框架中，用数据去学习特征，在使用中减少了手工设计特征的巨大工作量。看它的一个别名：无监督特征学习（Unsupervised Feature Learning），就可以顾名思义了。无监督（Unsupervised）学习的意思就是不需要通过人工方式进行样本类别的标注来完成学习。因此，深度学习是一种可以自动地学习特征的方法。（准确地说，深度学习首先利用无监督学习对每一层进行逐层预训练（Layerwise Pre-Training）去学习特征；每次单独训练一层，并将训练结果作为更高一层的输入；然后到最上层改用监督学习从上到下进行微调（Fine-Tune）去学习模型。）

​       深度学习通过学习一种深层非线性网络结构，只需简单的网络结构即可实现复杂函数的逼近，并展现了强大的从大量无标注样本集中学习数据集本质特征的能力。深度学习能够获得可更好地表示数据的特征，同时由于模型的层次深（通常有5层、6层，甚至10多层的隐藏层节点）、表达能力强，因此有能力表示大规模数据。对于图像、语音这种特征不明显（需要手工设计且很多没有直观的物理含义）的问题，深度模型能够在大规模训练数据上取得更好的效果。**相比于传统的神经网络，深度神经网络作出了重大的改进，在训练上的难度（如梯度弥散问题）可以通过“逐层预训练”来有效降低。**

​      值的注意的是，**深度学习不是万能的**，像很多其他方法一样，**它需要**结合特定领域的先验知识，需要和其他模型结合才能得到最好的结果。此外，类似于神经网络，**深度学习的另一局限性**是可解释性不强，像个“黑箱子”一样不知为什么能取得好的效果，以及不知如何有针对性地去具体改进，而这有可能成为产品升级过程中的阻碍。

​      近年来，深度学习的发展逐渐成熟。2012年6月，《纽约时报》披露了Google Brain项目，吸引了公众的广泛关注。这个项目是由著名的斯坦福大学的机器学习教授Andrew Ng和在大规模计算机系统方面的世界顶尖专家Jeff Dean共同主导，用16,000个CPU Core的并行计算平台去训练含有10亿个节点的深度神经网络（Deep Neural Networks，DNN），使其能够自我训练，对2万个不同物体的1,400万张图片进行辨识。在开始分析数据前，并不需要向系统手工输入任何诸如“脸、肢体、猫的长相是什么样子”这类特征。Jeff Dean说：“我们在训练的时候从来不会告诉机器：‘这是一只猫’（即无标注样本）。系统其实是自己发明或领悟了‘猫’的概念。

​      2014年3月，同样也是基于深度学习方法，Facebook的DeepFace项目使得人脸识别技术的识别率已经达到了97.25%，只比人类识别97.5%的正确率略低那么一点点，准确率几乎可媲美人类。该项目利用了9层的神经网络来获得脸部表征，神经网络处理的参数高达1.2亿。

​      以及2016年3月人工智能围棋比赛，由位于英国伦敦的谷歌（Google）旗下DeepMind公司的戴维·西尔弗、艾佳·黄和[戴密斯·哈萨比斯](http://baike.baidu.com/item/戴密斯·哈萨比斯)与他们的团队开发的AlphaGo战胜了世界围棋冠军、职业九段选手李世石，并以4:1的总比分获胜。AlphaGo的主要工作原理就是深度学习，通过两个不同神经网络“大脑”合作来改进下棋：第一大脑：落子选择器 （Move Picker）和第二大脑：棋局评估器 （Position Evaluator）。这些大脑是多层神经网络跟那些Google图片搜索引擎识别图片在结构上是相似的。它们从多层启发式二维过滤器开始，去处理围棋棋盘的定位，就像图片分类器网络处理图片一样。经过过滤，13个完全连接的神经网络层产生对它们看到的局面判断。这些层能够做分类和逻辑推理。

### 4.深度学习的研究现状

​       深度学习极大地促进了机器学习的发展，收到世界各国相关领域研究人员和高科技公司的重视，语音、图像和自然语言处理是深度学习算法应用最广泛的三个主要研究领域：

#### 4.1深度学习在语音识别领域研究现状

​      长期以来，语音识别系统大多是采用高斯混合模型（GMM）来描述每个建模单元的概率模型。由于这种模型估计简单，方便使用大规模数据对其训练，该模型有较好的区分度训练算法，保证了该模型能够很好的训练。在很长时间内占据了语音识别应用领域主导性地位。但是GMM实质上一种浅层学习网络模型，特征的状态空间分布不能够被充分描述。而且，使用GMM建模数据的特征为数通常只有几十维，这使得特征之间的相关性不能被充分描述。最后GMM建模实质上是一种似然概率建模方式，即使一些模式分类之间的区分性能够通过区分度训练模拟得到，但是效果有限。

​      从2009年开始，微软亚洲研究院的语音识别专家们和深度学习领军人物Hinton合作。2011年微软公司推出基于深度神经网络的语音识别系统，这一成果将语音识别领域已有的技术框架完全改变。采用深度神经网络后，样本数据特征间相关性信息得以充分表示，将连续的特征信息结合构成高维特征，通过高维特征样本对深度神经网络模型进行训练。由于深度神经网络采用了模拟人脑神经架构，通过逐层的进行数据特征提取，最终得到适合进行模式分类处理的理想特征。

#### 4.2深度学习在图像识别领域研究现状

​      对于图像的处理是深度学习算法最早尝试应用的领域。早在1989年，加拿大多伦多大学教授Yann LeCun就和他的同事提出了卷积神经网络（Convolutional Neural Networks, CNN）它是一种包含卷积层的深度神经网络模型。通常一个卷机神经网络架构包含两个可以通过训练产生的非线性卷积层，两个固定的子采样层和一个全连接层，隐藏层的数量一般至少在5个以上。CNN的架构设计是受到生物学家Hube和Wiesel的动物视觉模型启发而发明的，尤其是模拟动物视觉皮层的V1层和V2层中简单细胞和复杂细胞在视觉系统的功能。起初卷积神经网络在小规模的问题上取得了当时世界最好成果。但是在很长一段时间里一直没有取得重大突破。主要原因是卷积神经网络应用在大尺寸图像上一直不能取得理想结果，比如对于像素数很大的自然图像内容的理解，这使得它没有引起计算机视觉研究领域足够的重视。2012年10月，Hinton教授以及他的学生采用更深的卷神经网络模型在著名的ImageNet问题上取得了世界最好结果，使得对于图像识别的领域研究更进一步。

​      自卷积神经网络提出以来，在图像识别问题上并没有取得质的提升和突破，直到2012年Hinton构建深度神经网络才去的惊人的成果。这主要是因为对算法的改进，在网络的训练中引入了权重衰减的概念，有效的减小权重幅度，防止网络过拟合。更关键的是计算机计算能力的提升，GPU加速技术的发展，使得在训练过程中可以产生更多的训练数据，使网络能够更好的拟合训练数据。2012年国内互联网巨头百度公司将相关最新技术成功应用到人脸识别和自然图像识别问题，并推出相应的产品。现在的深度学习网络模型已经能够理解和识别一般的自然图像。深度学习模型不仅大幅提高了图像识别的精度，同时也避免了需要消耗大量时间进行人工特征的提取，使得在线运行效率大大提升。

#### 4.3深度学习在自然语言处理领域研究现状

​     自然语言处理问题是深度学习在除了语音和图像处理之外的另一个重要的应用领域。数十年以来，自然语言处理的主流方法是基于统计的模型，人工神经网络也是基于统计方法模型之一，但在自然语言处理领域却一直没有被重视。语言建模时最早采用神经网络进行自然语言处理的问题。美国NEC研究院最早将深度学习引入到自然语言处理研究中，其研究院从2008年起采用将词汇映射到一维矢量空间和多层一维卷积结构去解决词性标注、分词、命名实体识别和语义角色标注四个典型的自然语言处理问题。他们构建了一个网络模型用于解决四个不同问题，都取得了相当精确的结果。总体而言，深度学习在自然语言处理上取得的成果和在图像语音识别方面相差甚远，仍有待深入研究。

## 二、人工智能、机器学习、深度学习区别联系

### 1.人工智能：从概念提出到走向繁荣

​		1956年，几个计算机科学家相聚在达特茅斯会议，提出了“人工智能”的概念，梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。其后，人工智能就一直萦绕于人们的脑海之中，并在科研实验室中慢慢孵化。之后的几十年，人工智能一直在两极反转，或被称作人类文明耀眼未来的预言，或被当成技术疯子的狂想扔到垃圾堆里。直到2012年之前，这两种声音还在同时存在。

​		2012年以后，得益于数据量的上涨、运算力的提升和机器学习新算法（深度学习）的出现，人工智能开始大爆发。据领英近日发布的《全球AI领域人才报告》显示，截至2017年一季度，基于领英平台的全球AI（人工智能）领域技术人才数量超过190万，仅国内人工智能人才缺口达到500多万。

​		人工智能的研究领域也在不断扩大，图二展示了人工智能研究的各个分支，包括专家系统、机器学习、进化计算、模糊逻辑、计算机视觉、自然语言处理、推荐系统等。

![img](https://pic4.zhimg.com/50/v2-e358e127afbe5963f5b8622e2dd5b49f_hd.jpg?source=1940ef5c)![img](https://pic4.zhimg.com/80/v2-e358e127afbe5963f5b8622e2dd5b49f_720w.jpg?source=1940ef5c)图二 人

​		但目前的科研工作都集中在弱人工智能这部分，并很有希望在近期取得重大突破，电影里的人工智能多半都是在描绘强人工智能，而这部分在目前的现实世界里难以真正实现（通常将人工智能分为弱人工智能和强人工智能，前者让机器具备观察和感知的能力，可以做到一定程度的理解和推理，而强人工智能让机器获得自适应能力，解决一些之前没有遇到过的问题）。

​		弱人工智能有希望取得突破，是如何实现的，“智能”又从何而来呢？这主要归功于一种实现人工智能的方法——机器学习。

### 2.机器学习：一种实现人工智能的方法

​		机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。

​		举个简单的例子，当我们浏览网上商城时，经常会出现商品推荐的信息。这是商城根据你往期的购物记录和冗长的收藏清单，识别出这其中哪些是你真正感兴趣，并且愿意购买的产品。这样的决策模型，可以帮助商城为客户提供建议并鼓励产品消费。

​		机器学习直接来源于早期的人工智能领域，传统的算法包括决策树、聚类、贝叶斯分类、支持向量机、EM、Adaboost等等。从学习方法上来分，机器学习算法可以分为监督学习（如分类问题）、无监督学习（如聚类问题）、半监督学习、集成学习、深度学习和强化学习。

​		传统的机器学习算法在指纹识别、基于Haar的人脸检测、基于HoG特征的物体检测等领域的应用基本达到了商业化的要求或者特定场景的商业化水平，但每前进一步都异常艰难，直到深度学习算法的出现。

### 3.深度学习：一种实现机器学习的技术

​		深度学习本来并不是一种独立的学习方法，其本身也会用到有监督和无监督的学习方法来训练深度神经网络。但由于近几年该领域发展迅猛，一些特有的学习手段相继被提出（如残差网络），因此越来越多的人将其单独看作一种学习的方法。

​		最初的深度学习是利用深度神经网络来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。其实有不少想法早年间也曾有过，但由于当时训练数据量不足、计算能力落后，因此最终的效果不尽如人意。

​		深度学习摧枯拉朽般地实现了各种任务，使得似乎所有的机器辅助功能都变为可能。无人驾驶汽车，预防性医疗保健，甚至是更好的电影推荐，都近在眼前，或者即将实现。

## 三、神经元、单层感知机、多层感知机

### 1.神经元

![img](https://img-blog.csdnimg.cn/20200220213145126.png)

​		一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。突触之间的交流通过神经递质实现。

​		把树突到细胞核的阶段简化为线性加权的过程（当然了，该过程也有可能是非线性的，但是我们可以把其**非线性过程施加到后面的非线性函数以及多层网络结构中**），其次把突触之间的信号传递简化为对求和结果的非线性变换，那么上述模型就变得清晰了：

![img](https://img-blog.csdnimg.cn/20200220213631967.png)

### 2.单层感知机


​		单层感知机目标是将被感知数据集划分为两类的分离超平面，并计算出该超平面。单层感知机是二分类的线性分类模型，输入是被感知数据集的特征向量，输出时数据集的类别{+1,-1}。感知器的模型可以简单表示为：

f ( x ) = s i g n ( w . x + b ) f(x)=sign(w.x+b)
f(x)=sign(w.x+b)

​		该函数称为单层感知机，其中w是网络的N维权重向量，b是网络的N维偏置向量, w.x是w和x的内积，w和b的N维向量取值要求在实数域。

​		sign函数是感知机的早期激活函数，后面又演化出一系列的激活函数。激活函数一般采用非线性激活函数，以增强网络的表达能力。常见的激活函数有：sign, sigmoid,tanh,ReLU等。

​	为单层感知机与逻辑回归的差别就是感知机激活函数是sign，逻辑回归的激活函数是sigmoid。sign(x)将大于0的分为1，小于0的分为-1；sigmoid将大于0.5的分为1，小于0.5的分为0。因此sign又被称为单位阶跃函数，逻辑回归也被看作是一种概率估计。

![单层感知机的模型](https://img-blog.csdn.net/20181011191519250?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYXJldmlld3M=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 3.多层感知机

 多层感知机（MLP，Multilayer Perceptron）也叫人工神经网络（ANN，Artificial Neural Network），除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构，如下图：

​                                       ![img](https://img-blog.csdnimg.cn/20190623203530221.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ZnMTM4MjEyNjc4MzY=,size_16,color_FFFFFF,t_70)

 

​           从上图可以看到，多层感知机层与层之间是全连接的。多层感知机最底层是输入层，中间是隐藏层，最后是输出层。

## 四、前向传播

​		所谓的前向传播算法就是：**将上一层的输出作为下一层的输入，并计算下一层的输出，一直到运算到输出层为止。**

![img](https://pic3.zhimg.com/80/v2-a7cb0f8caf3219039216f120466aa692_720w.jpg)

对于Layer 2的输出 ![[公式]](https://www.zhihu.com/equation?tex=a_%7B1%7D%5E%7B%282%29%7D) ，![[公式]](https://www.zhihu.com/equation?tex=a_%7B2%7D%5E%7B%282%29%7D)，![[公式]](https://www.zhihu.com/equation?tex=a_%7B3%7D%5E%7B%282%29%7D)，

![[公式]](https://www.zhihu.com/equation?tex=a_%7B1%7D%5E%7B%282%29%7D%3D%5Csigma%28z_%7B1%7D%5E%7B%282%29%7D%29%3D%5Csigma%28w_%7B11%7D%5E%7B%282%29%7Dx_%7B1%7D%2Bw_%7B12%7D%5E%7B%282%29%7Dx_%7B2%7D%2Bw_%7B13%7D%5E%7B%282%29%7Dx_%7B3%7D%2Bb_%7B1%7D%5E%7B%282%29%7D%29)

![[公式]](https://www.zhihu.com/equation?tex=a_%7B2%7D%5E%7B%282%29%7D%3D%5Csigma%28z_%7B2%7D%5E%7B%282%29%7D%29%3D%5Csigma%28w_%7B21%7D%5E%7B%282%29%7Dx_%7B1%7D%2Bw_%7B22%7D%5E%7B%282%29%7Dx_%7B2%7D%2Bw_%7B23%7D%5E%7B%282%29%7Dx_%7B3%7D%2Bb_%7B2%7D%5E%7B%282%29%7D%29)

![[公式]](https://www.zhihu.com/equation?tex=a_%7B3%7D%5E%7B%282%29%7D%3D%5Csigma%28z_%7B3%7D%5E%7B%282%29%7D%29%3D%5Csigma%28w_%7B31%7D%5E%7B%282%29%7Dx_%7B1%7D%2Bw_%7B32%7D%5E%7B%282%29%7Dx_%7B2%7D%2Bw_%7B33%7D%5E%7B%282%29%7Dx_%7B3%7D%2Bb_%7B3%7D%5E%7B%282%29%7D%29)

对于Layer 3的输出![[公式]](https://www.zhihu.com/equation?tex=a_%7B1%7D%5E%7B%283%29%7D)，

![[公式]](https://www.zhihu.com/equation?tex=a_%7B1%7D%5E%7B%283%29%7D%3D%5Csigma%28z_%7B1%7D%5E%7B%283%29%7D%29%3D%5Csigma%28w_%7B11%7D%5E%7B%283%29%7Da_%7B1%7D%5E%7B%282%29%7D%2Bw_%7B12%7D%5E%7B%283%29%7Da_%7B2%7D%5E%7B%282%29%7D%2Bw_%7B13%7D%5E%7B%283%29%7Da_%7B3%7D%5E%7B%282%29%7D%2Bb_%7B1%7D%5E%7B%283%29%7D%29)

![[公式]](https://www.zhihu.com/equation?tex=a_%7B2%7D%5E%7B%283%29%7D%3D%5Csigma%28z_%7B2%7D%5E%7B%283%29%7D%29%3D%5Csigma%28w_%7B21%7D%5E%7B%283%29%7Da_%7B1%7D%5E%7B%282%29%7D%2Bw_%7B22%7D%5E%7B%283%29%7Da_%7B2%7D%5E%7B%282%29%7D%2Bw_%7B23%7D%5E%7B%283%29%7Da_%7B3%7D%5E%7B%282%29%7D%2Bb_%7B2%7D%5E%7B%283%29%7D%29)

从上面可以看出，使用代数法一个个的表示输出比较复杂，而如果使用矩阵法则比较的简洁。将上面的例子一般化，并写成矩阵乘法的形式，

![[公式]](https://www.zhihu.com/equation?tex=z%5E%7B%28l%29%7D%3DW%5E%7B%28l%29%7Da%5E%7B%28l-1%29%7D%2Bb%5E%7B%28l%29%7D)

![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%28l%29%7D%3D%5Csigma%28z%5E%7B%28l%29%7D%29)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma) 为 sigmoid 函数。

## 五、反向传播

​		反向传播**（back propagation, **BP**）算法是 "误差反向传播" 的简称，也称为**backprop，允许来自代价函数的信息通过网络向后流动，以便计算梯度。

​		反向传播是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。

​		反向传播这个术语经常被误解为用于多层神经网络的整个学习算法。实际上，**反向传播仅指用于计算梯度的方法。**而另一种算法，例如随机梯度下降法，才是使用该梯度来进行学习。另外，反向传播还经常被误解为仅适用于多层神经网络，但是**原则上它可以计算任何函数的到导数**（对于一些函数，正确的响应是报告函数的导数是未定义的）。

​		微积分中的链式法则（为了不与概率中的链式法则相混淆）用于计复合函数的导数。反向传播是一种计算链式法则的算法，使用高效的特定运输顺序。

​		设 ![[公式]](https://www.zhihu.com/equation?tex=x) 是实数， ![[公式]](https://www.zhihu.com/equation?tex=f) 和 ![[公式]](https://www.zhihu.com/equation?tex=g) 是从实数映射到实数的函数。假设 ![[公式]](https://www.zhihu.com/equation?tex=y%3Dg%28x%29) 并且 ![[公式]](https://www.zhihu.com/equation?tex=z%3Df%28g%28x%29%29%3Df%28y%29) 。那么链式法则就是： ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7Bdz%7D%7Bdx%7D%3D%5Cfrac%7Bdz%7D%7Bdy%7D%5Cfrac%7Bdy%7D%7Bdx%7D) 。

------

**推导BP算法 ( get一下重点内容 )**

在进行反向传播算法前，我们需要选择一个损失函数，来度量训练样本计算出的输出和真实的训练样本输出之间的损失。我们使用最常见的**均方误差（MSE）来作为损失函数，**

![img](https://pic1.zhimg.com/80/v2-51fda4fae6c27a6cc8bd2150328c2b24_720w.jpg)

其中 ![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%28l%29%7D) 为训练样本计算出的输出，y为训练样本的真实值。加入系数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D) 是为了抵消微分出来的指数。

**(1) 输出层的梯度**

![img](https://pic4.zhimg.com/80/v2-73e02d5cfdc387ca23e374e95c4e0b9b_720w.jpg)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Codot) 表示Hadamard积，即两个维度相同的矩阵对应元素的乘积。

我们注意到在求解输出层梯度的时候有公共的部分，记为

![img](https://pic3.zhimg.com/80/v2-f88a0e2d3a49fd00168a4ec93fa6f632_720w.jpg)

**(2) 隐藏层的梯度**

我们把输出层 ![[公式]](https://www.zhihu.com/equation?tex=l) 的梯度算出来了 ，那么如何计算 ![[公式]](https://www.zhihu.com/equation?tex=l-1) 层的梯度， ![[公式]](https://www.zhihu.com/equation?tex=l-2) 层的梯度呢？

因为上面已经求出了输出层的误差，根据误差反向传播的原理，当前层的误差可理解为上一层所有神经元误差的复合函数，即使用上一层的误差来表示当前层误差，并依次递推。

这里我们用数学归纳法，假设第 ![[公式]](https://www.zhihu.com/equation?tex=l%2B1) 层的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%28l%2B1%29%7D) 已经求出，那么我们如何求出第 ![[公式]](https://www.zhihu.com/equation?tex=l) 层的![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%28l%29%7D)呢？

![img](https://pic3.zhimg.com/80/v2-e993d6d6bfaa012afb7beaa68fa5e95a_720w.jpg)

而![[公式]](https://www.zhihu.com/equation?tex=z%5E%7B%28l%2B1%29%7D)和![[公式]](https://www.zhihu.com/equation?tex=z%5E%7B%28l%29%7D)的关系如下：

![[公式]](https://www.zhihu.com/equation?tex=z%5E%7B%28l%2B1%29%7D%3DW%5E%7B%28l%2B1%29%7Da%5E%7B%28l%29%7D%2Bb%5E%7B%28l%2B1%29%7D%3DW%5E%7B%28l%2B1%29%7D%5Csigma%28z%5E%7B%28l%29%7D%29%2Bb%5E%7B%28l%2B1%29%7D)

这样很容易求出，

![img](https://pic4.zhimg.com/80/v2-907bb19157b2d4af27e374b46baf1ddf_720w.jpg)

所以，

![img](https://pic2.zhimg.com/80/v2-47606ddd08c7c073d4c284eb33ff7149_720w.jpg)

现在我们得到了 ![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%28l%29%7D) 的递推关系式，只要求出了某一层的![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%28l%29%7D)，求解![[公式]](https://www.zhihu.com/equation?tex=w%5E%7B%28l%29%7D)，![[公式]](https://www.zhihu.com/equation?tex=b%5E%7B%28l%29%7D)的对应梯度就很简单：

![img](https://pic3.zhimg.com/80/v2-22baef15a12f6a5ede7fd9f0bd98c896_720w.jpg)

------

**对反向传播算法的过程进行一下总结：**

**输入：**总层数L，以及各隐藏层与输出层的神经元个数，激活函数，损失函数，迭代步长 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) ，最大迭代次数MAX与停止迭代阈值 ![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon) ，输入的m个训练样本 ![[公式]](https://www.zhihu.com/equation?tex=%28%28x_%7B1%7D%2Cy_%7B1%7D%29%2C%28x_%7B2%7D%2Cy_%7B2%7D%29%2C...%28x_%7Bm%7D%2Cy_%7Bm%7D%29%29)

**1. 初始化参数W，b**

**2. 进行前向传播算法计算**，for ![[公式]](https://www.zhihu.com/equation?tex=l%3D2) to L

![[公式]](https://www.zhihu.com/equation?tex=z%5E%7B%28l%29%7D%3DW%5E%7B%28l%29%7Da%5E%7B%28l-1%29%7D%2Bb%5E%7B%28l%29%7D)

![[公式]](https://www.zhihu.com/equation?tex=a%5E%7B%28l%29%7D%3D%5Csigma%28z%5E%7B%28l%29%7D%29)

**3. 通过损失函数计算输出层的梯度**

**4. 进行反向传播算法计算**，for ![[公式]](https://www.zhihu.com/equation?tex=l%3DL-1) to 2

![img](https://pic2.zhimg.com/80/v2-47606ddd08c7c073d4c284eb33ff7149_720w.jpg)

**5. 更新W，b**

通过梯度下降算法更新权重![[公式]](https://www.zhihu.com/equation?tex=w)和偏置![[公式]](https://www.zhihu.com/equation?tex=b)的值，![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)为学习率其中![[公式]](https://www.zhihu.com/equation?tex=%5Calpha%5Cin%280%2C1%5D)。

![img](https://pic2.zhimg.com/80/v2-2efb3ca24cce35c7869943a08c1de961_720w.jpg)

**6. 如果所有W，b的变化值都小于停止迭代阈值ϵ，则跳出迭代循环**

**7. 输出各隐藏层与输出层的线性关系系数矩阵W和偏置b**。





