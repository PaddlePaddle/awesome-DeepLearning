{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\r\n",
    "import os\r\n",
    "import sys\r\n",
    "import requests\r\n",
    "from collections import OrderedDict \r\n",
    "import math\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "from paddle.nn import Embedding\r\n",
    "import paddle.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download():\r\n",
    "    corpus_url=\"https://dataset.bj.bcebos.com/word2vec/text8.txt\"\r\n",
    "    web_request=requests.get(corpus_url)\r\n",
    "    corpus=web_request.content\r\n",
    "    with open(\"./text8.txt\", \"wb\") as f:\r\n",
    "        f.write(corpus)\r\n",
    "    f.close()\r\n",
    "\r\n",
    "download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n"
     ]
    }
   ],
   "source": [
    "def load_text8():\r\n",
    "    with open(\"./text8.txt\", \"r\") as f:\r\n",
    "        corpus=f.read().strip(\"\\n\")\r\n",
    "    f.close()\r\n",
    "\r\n",
    "    return corpus\r\n",
    "\r\n",
    "corpus=load_text8()\r\n",
    "\r\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the']\n"
     ]
    }
   ],
   "source": [
    "def data_preprocess(corpus):\r\n",
    "    corpus=corpus.strip().lower()\r\n",
    "    corpus=corpus.split(\" \")\r\n",
    "    return corpus\r\n",
    "\r\n",
    "corpus=data_preprocess(corpus)\r\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totoally 253854 different words in the corpus\n",
      "word the,its id 0,its word freq 1061396\n",
      "word of,its id 1,its word freq 593677\n",
      "word and,its id 2,its word freq 416629\n",
      "word one,its id 3,its word freq 411764\n",
      "word in,its id 4,its word freq 372201\n",
      "word a,its id 5,its word freq 325873\n",
      "word to,its id 6,its word freq 316376\n",
      "word zero,its id 7,its word freq 264975\n",
      "word nine,its id 8,its word freq 250430\n",
      "word two,its id 9,its word freq 192644\n",
      "word is,its id 10,its word freq 183153\n",
      "word as,its id 11,its word freq 131815\n",
      "word eight,its id 12,its word freq 125285\n",
      "word for,its id 13,its word freq 118445\n",
      "word s,its id 14,its word freq 116710\n",
      "word five,its id 15,its word freq 115789\n",
      "word three,its id 16,its word freq 114775\n",
      "word was,its id 17,its word freq 112807\n",
      "word by,its id 18,its word freq 111831\n",
      "word that,its id 19,its word freq 109510\n",
      "word four,its id 20,its word freq 108182\n",
      "word six,its id 21,its word freq 102145\n",
      "word seven,its id 22,its word freq 99683\n",
      "word with,its id 23,its word freq 95603\n",
      "word on,its id 24,its word freq 91250\n",
      "word are,its id 25,its word freq 76527\n",
      "word it,its id 26,its word freq 73334\n",
      "word from,its id 27,its word freq 72871\n",
      "word or,its id 28,its word freq 68945\n",
      "word his,its id 29,its word freq 62603\n",
      "word an,its id 30,its word freq 61925\n",
      "word be,its id 31,its word freq 61281\n",
      "word this,its id 32,its word freq 58832\n",
      "word which,its id 33,its word freq 54788\n",
      "word at,its id 34,its word freq 54576\n",
      "word he,its id 35,its word freq 53573\n",
      "word also,its id 36,its word freq 44358\n",
      "word not,its id 37,its word freq 44033\n",
      "word have,its id 38,its word freq 39712\n",
      "word were,its id 39,its word freq 39086\n",
      "word has,its id 40,its word freq 37866\n",
      "word but,its id 41,its word freq 35358\n",
      "word other,its id 42,its word freq 32433\n",
      "word their,its id 43,its word freq 31523\n",
      "word its,its id 44,its word freq 29567\n",
      "word first,its id 45,its word freq 28810\n",
      "word they,its id 46,its word freq 28553\n",
      "word some,its id 47,its word freq 28161\n",
      "word had,its id 48,its word freq 28100\n",
      "word all,its id 49,its word freq 26229\n"
     ]
    }
   ],
   "source": [
    "def build_dict(corpus):\r\n",
    "    word_freq_dict=dict()\r\n",
    "    for word in corpus:\r\n",
    "        if word not in word_freq_dict:\r\n",
    "            word_freq_dict[word]=0\r\n",
    "        word_freq_dict[word]+=1\r\n",
    "\r\n",
    "    word_freq_dict=sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)\r\n",
    "    \r\n",
    "    word2id_dict=dict()\r\n",
    "    word2id_freq=dict()\r\n",
    "    id2word_dict=dict()\r\n",
    "\r\n",
    "    for word, freq in word_freq_dict:\r\n",
    "        curr_id=len(word2id_dict)\r\n",
    "        word2id_dict[word]=curr_id\r\n",
    "        word2id_freq[word2id_dict[word]]=freq\r\n",
    "        id2word_dict[curr_id]=word\r\n",
    "\r\n",
    "    return word2id_freq, word2id_dict, id2word_dict\r\n",
    "\r\n",
    "word2id_freq, word2id_dict, id2word_dict=build_dict(corpus)\r\n",
    "vocab_size=len(word2id_freq)\r\n",
    "print(\"there are totoally %d different words in the corpus\"%vocab_size)\r\n",
    "for _, (word,word_id) in zip(range(50),word2id_dict.items()):\r\n",
    "    print(\"word %s,its id %d,its word freq %d\" % (word,word_id,word2id_freq[word_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207 tokens in the corpus\n",
      "[5233, 3080, 11, 5, 194, 1, 3133, 45, 58, 155, 127, 741, 476, 10571, 133, 0, 27349, 1, 0, 102, 854, 2, 0, 15067, 58112, 1, 0, 150, 854, 3580, 0, 194, 10, 190, 58, 4, 5, 10712, 214, 6, 1324, 104, 454, 19, 58, 2731, 362, 6, 3672, 0]\n"
     ]
    }
   ],
   "source": [
    "def convert_corpus_to_id(corpus, word2id_dict):\r\n",
    "    corpus = [word2id_dict[word] for word in corpus]\r\n",
    "    return corpus\r\n",
    "\r\n",
    "corpus = convert_corpus_to_id(corpus, word2id_dict)\r\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\r\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8744188 tokens in the corpus\n",
      "[5233, 3080, 3133, 155, 10571, 27349, 854, 15067, 58112, 854, 3580, 190, 10712, 1324, 454, 2731, 362, 3672, 708, 40, 539, 1423, 2757, 7088, 5233, 1052, 248, 44611, 2877, 792, 186, 5233, 200, 602, 1134, 2621, 8983, 279, 4147, 141, 59, 6437, 4186, 362, 5233, 1137, 344, 1818, 4860, 6753]\n"
     ]
    }
   ],
   "source": [
    "def subsampling(corpus, word2id_freq):\r\n",
    "    \r\n",
    "    def discard(word_id):\r\n",
    "        return random.uniform(0, 1)<1-math.sqrt(\r\n",
    "            1e-4/word2id_freq[word_id]*len(corpus))\r\n",
    "\r\n",
    "    corpus=[word for word in corpus if not discard(word)]\r\n",
    "    return corpus\r\n",
    "\r\n",
    "corpus=subsampling(corpus,word2id_freq)\r\n",
    "print(\"%d tokens in the corpus\"%len(corpus))\r\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_data(corpus, word2id_dict, word2id_freq, max_window_size = 3, negative_sample_num = 4):\r\n",
    "    \r\n",
    "    dataset=[]\r\n",
    "\r\n",
    "    for center_word_idx in range(len(corpus)):\r\n",
    "        window_size=random.randint(1, max_window_size)\r\n",
    "        center_word=corpus[center_word_idx]\r\n",
    "\r\n",
    "        positive_word_range=(max(0, center_word_idx - window_size), min(len(corpus) - 1, center_word_idx + window_size))\r\n",
    "        positive_word_candidates=[corpus[idx] for idx in range(positive_word_range[0], positive_word_range[1]+1) if idx != center_word_idx]\r\n",
    "\r\n",
    "        for positive_word in positive_word_candidates:\r\n",
    "            dataset.append((center_word, positive_word,1))\r\n",
    "\r\n",
    "            i=0\r\n",
    "            while i<negative_sample_num:\r\n",
    "                negative_word_candidate=random.randint(0, vocab_size-1)\r\n",
    "\r\n",
    "                if negative_word_candidate not in positive_word_candidates:\r\n",
    "                    dataset.append((center_word, negative_word_candidate,0))\r\n",
    "                    i+=1\r\n",
    "    return dataset\r\n",
    "corpus_light=corpus[:int(len(corpus)*0.2)]\r\n",
    "dataset=build_data(corpus_light, word2id_dict,word2id_freq)\r\n",
    "for _,(center_word, target_word, label) in zip(range(50),dataset):\r\n",
    "    print(\"center_word %s, target %s, label %d\"%(id2word_dict[center_word],\r\n",
    "                                                   id2word_dict[target_word], label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_batch(dataset, batch_size, epoch_num):\r\n",
    "    \r\n",
    "    # center_word_batch缓存batch_size个中心词\r\n",
    "    center_word_batch = []\r\n",
    "    # target_word_batch缓存batch_size个目标词（可以是正样本或者负样本）\r\n",
    "    target_word_batch = []\r\n",
    "    # label_batch缓存了batch_size个0或1的标签，用于模型训练\r\n",
    "    label_batch = []\r\n",
    "\r\n",
    "    for epoch in range(epoch_num):\r\n",
    "        # 每次开启一个新epoch之前，都对数据进行一次随机打乱，提高训练效果\r\n",
    "        random.shuffle(dataset)\r\n",
    "        \r\n",
    "        for center_word, target_word, label in dataset:\r\n",
    "            # 遍历dataset中的每个样本，并将这些数据送到不同的tensor里\r\n",
    "            center_word_batch.append([center_word])\r\n",
    "            target_word_batch.append([target_word])\r\n",
    "            label_batch.append(label)\r\n",
    "\r\n",
    "            # 当样本积攒到一个batch_size后，我们把数据都返回回来\r\n",
    "            # 在这里我们使用numpy的array函数把list封装成tensor\r\n",
    "            # 并使用python的迭代器机制，将数据yield出来\r\n",
    "            # 使用迭代器的好处是可以节省内存\r\n",
    "            if len(center_word_batch) == batch_size:\r\n",
    "                yield np.array(center_word_batch).astype(\"int64\"), \\\r\n",
    "                    np.array(target_word_batch).astype(\"int64\"), \\\r\n",
    "                    np.array(label_batch).astype(\"float32\")\r\n",
    "                center_word_batch = []\r\n",
    "                target_word_batch = []\r\n",
    "                label_batch = []\r\n",
    "\r\n",
    "    if len(center_word_batch) > 0:\r\n",
    "        yield np.array(center_word_batch).astype(\"int64\"), \\\r\n",
    "            np.array(target_word_batch).astype(\"int64\"), \\\r\n",
    "            np.array(label_batch).astype(\"float32\")\r\n",
    "\r\n",
    "for _, batch in zip(range(10), build_batch(dataset, 128, 3)):\r\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SkipGram(paddle.nn.Layer):\r\n",
    "    def __init__(self, vocab_size, embedding_size, init_scale=0.1):\r\n",
    "        super(SkipGram, self).__init__()\r\n",
    "        self.vocab_size = vocab_size\r\n",
    "        self.embedding_size = embedding_size\r\n",
    "\r\n",
    "        self.embedding = Embedding( \r\n",
    "            num_embeddings = self.vocab_size,\r\n",
    "            embedding_dim = self.embedding_size,\r\n",
    "            weight_attr=paddle.ParamAttr(\r\n",
    "                initializer=paddle.nn.initializer.Uniform( \r\n",
    "                    low=-init_scale, high=init_scale)))\r\n",
    "\r\n",
    "        self.embedding_out = Embedding(\r\n",
    "            num_embeddings = self.vocab_size,\r\n",
    "            embedding_dim = self.embedding_size,\r\n",
    "            weight_attr=paddle.ParamAttr(\r\n",
    "                initializer=paddle.nn.initializer.Uniform(\r\n",
    "                    low=-init_scale, high=init_scale)))\r\n",
    "\r\n",
    "    def forward(self, center_words, target_words, label):\r\n",
    "        center_words_emb = self.embedding(center_words)\r\n",
    "        target_words_emb = self.embedding_out(target_words)\r\n",
    "\r\n",
    "        word_sim = paddle.multiply(center_words_emb, target_words_emb)\r\n",
    "        word_sim = paddle.sum(word_sim, axis=-1)\r\n",
    "        word_sim = paddle.reshape(word_sim, shape=[-1])\r\n",
    "        pred = F.sigmoid(word_sim)\r\n",
    "\r\n",
    "        loss = F.binary_cross_entropy_with_logits(word_sim, label)\r\n",
    "        loss = paddle.mean(loss)\r\n",
    "        \r\n",
    "        return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 512\r\n",
    "epoch_num = 3\r\n",
    "embedding_size = 200\r\n",
    "step = 0\r\n",
    "learning_rate = 0.001\r\n",
    "\r\n",
    "def get_similar_tokens(query_token, k, embed):\r\n",
    "    W = embed.numpy()\r\n",
    "    x = W[word2id_dict[query_token]]\r\n",
    "    cos = np.dot(W, x) / np.sqrt(np.sum(W * W, axis=1) * np.sum(x * x) + 1e-9)\r\n",
    "    flat = cos.flatten()\r\n",
    "    indices = np.argpartition(flat, -k)[-k:]\r\n",
    "    indices = indices[np.argsort(-flat[indices])]\r\n",
    "    for i in indices:\r\n",
    "        print('for word %s, the similar word is %s' % (query_token, str(id2word_dict[i])))\r\n",
    "\r\n",
    "paddle.set_device('gpu:0')\r\n",
    "\r\n",
    "skip_gram_model = SkipGram(vocab_size, embedding_size)\r\n",
    "\r\n",
    "adam = paddle.optimizer.Adam(learning_rate=learning_rate, parameters = skip_gram_model.parameters())\r\n",
    "\r\n",
    "for center_words, target_words, label in build_batch(\r\n",
    "    dataset, batch_size, epoch_num):\r\n",
    "    center_words_var = paddle.to_tensor(center_words)\r\n",
    "    target_words_var = paddle.to_tensor(target_words)\r\n",
    "    label_var = paddle.to_tensor(label)\r\n",
    "    \r\n",
    "    pred, loss = skip_gram_model(\r\n",
    "        center_words_var, target_words_var, label_var)\r\n",
    "\r\n",
    "    loss.backward()\r\n",
    "    adam.step()\r\n",
    "    adam.clear_grad()\r\n",
    "\r\n",
    "    step += 1\r\n",
    "    if step % 1000 == 0:\r\n",
    "        print(\"step %d, loss %.3f\" % (step, loss.numpy()[0]))\r\n",
    "\r\n",
    "    if step % 10000 ==0:\r\n",
    "        get_similar_tokens('movie', 5, skip_gram_model.embedding.weight)\r\n",
    "        get_similar_tokens('one', 5, skip_gram_model.embedding.weight)\r\n",
    "        get_similar_tokens('chip', 5, skip_gram_model.embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
