### 一、CNN-DSSM
以搜索引擎和搜索广告为例，最重要的也最难解决的问题是语义相似度，这里主要体现在两个方面：召回和排序。

在召回时，传统的文本相似性，无法有效发现语义类 query-Doc 结果对。

在排序时，一些细微的语言变化往往带来巨大的语义变化。

DSSM（Deep Structured Semantic Models）为计算语义相似度提供了一种思路。

DSSM（Deep Structured Semantic Models）的原理很简单，通过搜索引擎里 Query 和 Title 的海量的点击曝光日志，用 DNN 把 Query 和 Title 表达为低纬语义向量，并通过 cosine 距离来计算两个语义向量的距离，最终训练出语义相似度模型。该模型既可以用来预测两个句子的语义相似度，又可以获得某句子的低纬语义向量表达。

DSSM 从下往上可以分为三层结构：输入层、表示层、匹配层

![](https://ai-studio-static-online.cdn.bcebos.com/3e0d0c1eac014873832ea77f2a84376133dee5f0b41646998be0f8b4f09736e5)

输入层做的事情是把句子映射到一个向量空间里并输入到 DNN 中，这里英文和中文的处理方式有很大的不同。

英文的输入层处理方式是通过word hashing。

中文的输入层处理方式与英文有很大不同，首先中文分词是个让所有 NLP 从业者头疼的事情，即便业界号称能做到 95%左右的分词准确性，但分词结果极为不可控，往往会在分词阶段引入误差。所以这里我们不分词，而是仿照英文的处理方式，对应到中文的最小粒度就是单字了。（曾经有人用偏旁部首切的，感兴趣的朋友可以试试）

由于常用的单字为 1.5 万左右，而常用的双字大约到百万级别了，所以这里出于向量空间的考虑，采用字向量（one-hot）作为输入，向量空间约为 1.5 万维。

针对 DSSM 词袋模型丢失上下文信息的缺点，CLSM[2]（convolutional latent semantic model）应运而生，又叫 CNN-DSSM。CNN-DSSM 与 DSSM 的区别主要在于输入层和表示层。

CNN-DSSM就是将DNN替换为CNN，结构如下图所示。

![](https://ai-studio-static-online.cdn.bcebos.com/8c3f0474e4ce41e997d229108fcc8fcd75ba4e82cf254fd59c2ef7ea04d0c3d2)


### 二、LSTM-DSSM
LSTM[4](（Long-Short-Term Memory）是一种 RNN 特殊的类型，可以学习长期依赖信息。

![](https://ai-studio-static-online.cdn.bcebos.com/41aa222f4f464cbdae3ff49a499c6badd1869fe13f6d4e3cadc6c6ac19c1035c)

0）细胞状态

细胞状态这条线可以理解成是一条信息的传送带，只有一些少量的线性交互。在上面流动可以保持信息的不变性。

![](https://ai-studio-static-online.cdn.bcebos.com/9c941106f75846658b5c89096dd98f4313bf725420d3434c969b6deb4f8e2e8c)


（1）遗忘门

遗忘门 [5]由 Gers 提出，它用来控制细胞状态 cell 有哪些信息可以通过，继续往下传递。如下图所示，上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（遗忘门）产生一个从 0 到 1 的数值 ft，然后与细胞状态 C(t-1) 相乘，最终决定有多少细胞状态可以继续往后传递。

![](https://ai-studio-static-online.cdn.bcebos.com/b8fcacc118264c31955f6f95a09f41cb1c4beeff0f9e4d24a2b0ba8a1a075cda)


（2）输入门

输入门决定要新增什么信息到细胞状态，这里包含两部分：一个 sigmoid 输入门和一个 tanh 函数。sigmoid 决定输入的信号控制，tanh 决定输入什么内容。如下图所示，上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（输入门）产生一个从 0 到 1 的数值 it，同样的信息经过 tanh 网络做非线性变换得到结果 Ct，sigmoid 的结果和 tanh 的结果相乘，最终决定有哪些信息可以输入到细胞状态里。

![](https://ai-studio-static-online.cdn.bcebos.com/63f882492ce4479c9b4374892af827856e3c05bbf7b54d4c94a7a05f94fb3409)


（3）输出门

输出门决定从细胞状态要输出什么信息，这里也包含两部分：一个 sigmoid 输出门和一个 tanh 函数。sigmoid 决定输出的信号控制，tanh 决定输出什么内容。如下图所示，上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（输出门）产生一个从 0 到 1 的数值 Ot，细胞状态 Ct 经过 tanh 网络做非线性变换，得到结果再与 sigmoid 的结果 Ot 相乘，最终决定有哪些信息可以输出，输出的结果 ht 会作为这个细胞的输出，也会作为传递个下一个细胞。

![](https://ai-studio-static-online.cdn.bcebos.com/3692f253a13647a4b8d3f9aed25ea0b8d508e3027f17494eb6274091b366a47b)

LSTM-DSSM 其实用的是 LSTM 的一个变种——加入了peephole[6]的 LSTM。如下图所示：

![](https://ai-studio-static-online.cdn.bcebos.com/3937fe532a224c2f82ff9d36b532e3bd740ee0a09a12481d89a43e4e63203860)

 LSTM-DSSM整体网络结构如下：
 
![](https://ai-studio-static-online.cdn.bcebos.com/674708c5be10441fa98530f2a370c9d7ea8299745393468996db92346cbae8db)

### 三、MMoE
多任务模型通过学习不同任务的联系和差异，可提高每个任务的学习效率和质量。多任务学习的的框架广泛采用 shared-bottom 的结构，不同任务间共用底部的隐层。这种结构本质上可以减少过拟合的风险，但是效果上可能受到任务差异和数据分布带来的影响。也有一些其他结构，比如两个任务的参数不共用，但是通过对不同任务的参数增加 L2 范数的限制；也有一些对每个任务分别学习一套隐层然后学习所有隐层的组合。和 shared-bottom 结构相比，这些模型对增加了针对任务的特定参数，在任务差异会影响公共参数的情况下对最终效果有提升。缺点就是模型增加了参数量所以需要更大的数据量来训练模型，而且模型更复杂并不利于在真实生产环境中实际部署使用。

因此，论文中提出了一个 Multi-gate Mixture-of-Experts ( MMoE ) 的多任务学习结构。MMoE 模型刻画了任务相关性，基于共享表示来学习特定任务的函数，避免了明显增加参数的缺点。

MMoE 模型的结构 ( 下图 c ) 基于广泛使用的 Shared-Bottom 结构 ( 下图 a ) 和 MoE 结构，其中图 ( b ) 是图 ( c ) 的一种特殊情况，下面依次介绍。

![](https://ai-studio-static-online.cdn.bcebos.com/7d8978c1f9bb42728138bccad3f3213e98207ebe79114fb39cae17644dea2f92)

在MMoE提出之前，多任务模型已经有许多经典架构被提出，其中绝大多数的优化都基于share-bottom架构，即不同的任务共享相同的feature或feature_map。

然而，这种架构极大地限制了模型表达的能力，为什么这么说？因为我们在共享特征的上层直接接入了多个目标的输出，而由于多个任务各自有不同的数据分布，也就是说我们对不同任务的输出具有一定的差异性，而相同的特征输入会极大地削弱模型的多任务输出表达而在某种程度上降低了多目标模型的泛化能力。

那么，如何去降低这种架构带来的影响，作者首先提出了One-gate MoE model，这种模型架构就在一定程度上解决了上述问题，即虽然说多个模型还是共享相同的输入特征，但是每个任务都利用"gate network"来区分特征表达的权重，从而提高了模型的表达能力。但是这种模型架构的"区分"还不是很大，毕竟输入的特征还是只有一个，于是作者受集成学习（ensemble learning）思想的影响，提出了multi-experts，即我们可以把单个的共享特征看做是一个弱学习器的输入，那么，根据集成思想，若干弱学习器的组合可以作为一个强学习器来对结果进行推理，再通过"gate network"就可以极大地提高多任务模型的表达能力了。

由图可以看出，我们在定义两个任务优化模型时，在特征输入阶段，进行特征转换，分别产生若干expert，作为我们模型的基学习器，然后在每个任务对应的输入，分别使用"gate network"来表征最后的结果输出。

从某种角度上来讲，MMoE的厉害之处在于它的expert可以定义成任意一种单独的模型，可以说，MMoE是一个框架，而非简简单单的一个模型！这就使得我们在expert上可以灵活自由地实现自己想要的模型设计。
### 四、ShareBottom
多目标建模目前业内有两种模式，一种叫Shared-Bottom模式，另一种叫MOE，MOE又包含MMOE和OMOE两种。MMOE也是Google提出的一套多目标学习算法结果，被应用到了Google的内部推荐系统中。

Shared-Bottom的思路就是多个目标底层共用一套共享layer，在这之上基于不同的目标构建不同的Tower。这样的好处就是底层的layer复用，减少计算量，同时也可以防止过拟合的情况出现。

Shared-Bottom 优点：降低overfit风险，利用任务之间的关联性使模型学习效果更强

Shared-Bottom 缺点：任务之间的相关性将严重影响模型效果。假如任务之间相关性较低，模型的效果相对会较差。
### 五、YouTubeshi'p
在 Youtube 上观看视频时，页面上会展示用户可能喜欢的视频推荐列表。该论文聚焦于以下两大目标：

1）需要优化不用的目标。他们没有定义确切的目标函数，而是将目标函数分为「参与度」（点击量、花的时间）目标和「满意度」（点赞量、踩的量）目标；

2）减少系统引入的「选择偏见」：用户通常更倾向于点开排在第一位的推荐视频，尽管后面的视频可能参与度、满意度更高。如何高效地减少这些偏见是一个亟待解决的问题。

用什么方法解决？

![](https://ai-studio-static-online.cdn.bcebos.com/400c81f5febf4e258a3c0a8687c4c3db169ef8dc3d344dfda16739e0bac233c1)


图 1：模型的完整架构。

介绍的模型着眼于两个主要的目标。他们用到了一个宽度&深度模型框架。宽度模型拥有强大的记忆能力，深度神经网络拥有泛化能力，宽度&深度模型则综合了二者的优点。宽度&深度模型会为每一个定义的（参与度和满意度）目标生成一个预测。这些目标函数可以分为二分类问题（是否喜欢某个视频）和回归问题（为视频评级）。这一模型之上还有一个单独的排序模型。这只是一个输出向量的加权组合，它们是不同的预测目标。这些权重是手动调整的，以实现不同目标的最佳性能。此外，研究者还提出了结对、列表等先进的方法，以提升模型的性能，但由于计算时间的增加，这些方法没有被应用到生产中。

在宽度&深度模型的深层部分，研究者利用了一个多任务学习模型 MMoE。现有视频的特征（内容、标题、话题、上传时间等）以及正在观看的用户的信息（时间、用户配置文件等）被用作输入。MMoE 模型可以在不同的目标之间高效地共享权重。共享的底层（shared bottom layer）被分为多个专家层，用于预测不同的目标。每个目标函数都有一个门函数（gate function）。这个门函数是一个 softmax 函数，接收来自原始共享层和不同专家层的输入。该 softmax 函数将决定哪些专家层对于不同的目标函数是重要的。如下图 3 所示，不同的专家层对于不同目标的重要程度存在差别。如果与 shared-bottom 架构相比，不同的目标相关度更低，则 MMoE 模型中的训练受到的影响更小。

该模型的宽度部分致力于解决系统中由视频位置带来的选择偏见问题。研究者将该部分称为「浅塔」（shallow tower），它可以是一种简单的线性模型，使用简单的特征，如视频被点击时所处的位置、用户观看视频使用的设备等。「浅塔」的输出与 MMoE 模型的输出相结合，这也是宽度&深度模型架构的关键组成部分。

如此一来，模型将更加关注视频的位置。在训练过程中，dropout 率被定为 10%，以防止位置特征在模型中变得过于重要。如果不用宽度&深度模型，而是将位置添加为一个特征，模型可能根本就不会注意到这个特征。

结果表明，用 MMoE 替换 shared-bottom 层可以在参与度（观看推荐视频花费的时间）和满意度（调查反馈）两个目标中提升模型的性能。增加 MMoE 中的专家层数量和乘法的数量可以进一步提升模型的性能。但由于计算上的限制，现实部署中无法实现这一点。

尽管 Google 拥有强大的计算基础设施，但在训练和成本方面仍然非常谨慎；

通过使用深度&宽度模型，你可以在设计网络时预定义一些重要特征；

当你需要多目标模型时，MMoE 模型会非常有效；

即使具有强大而复杂的模型架构，大家仍在手动调整最后一层的权重，从而根据不同的客观预测确定实际排名。
