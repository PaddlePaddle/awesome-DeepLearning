# 可变形卷积（Deformable Convolution)

## 一.可变形卷积的提出背景

在计算机视觉领域，同一物体在不同场景，角度中未知的几何变换是检测/识别的一大挑战，通常来说我们有两种做法:

(1)通过充足的数据增强，扩充足够多的样本去增强模型适应尺度变换的能力。

(2)设置一些针对几何变换不变的特征或者算法，比如SIFT和sliding windows。

两种方法都有缺陷，第一种方法因为样本的局限性使得模型的泛化能力比较低，无法泛化到一般场景中，第二种方法则因为手工设计的不变特征和算法对于过于复杂的变换是很难的而无法设计。所以作者提出了可变形卷积来解决这个问题。

## 二.可变形卷积V1（DCN v1）

可变形卷积顾名思义就是卷积的位置是可变形的，并非在传统的N × N的网格上做卷积，我们知道卷积核的目的是为了提取输入物的特征。传统的卷积核通常是固定尺寸、固定大小的（例如3x3，5x5，7x7.），这种卷积核存在的最大问题就是，对于未知的变化适应性差，泛化能力不强。而可变形卷积核的好处就是能更准确地提取到我们想要的特征，通过一张图我们可以更直观地了解：

![](https://ai-studio-static-online.cdn.bcebos.com/533c93bbd7934ad38c0605602a34c7e802590a2f74124a89855b49c0eefbe27a)

在上面这张图里面，左边传统的卷积显然没有提取到完整绵羊的特征，而右边的可变形卷积则提取到了完整的不规则绵羊的特征。


**可变形卷积的结构和形式**


可变形卷积是怎么做的呢？其实就是在卷积核上的每一个元素额外增加了一个参数方向参数，如下图所示：

![](https://ai-studio-static-online.cdn.bcebos.com/d191ddd9750149f3bd65e47385c1ce8e1a9a69f4f6a545a990dd69ba76d0ff0e)

在上图中：

（a）是传统的标准卷积核，尺寸为3x3（图中绿色的点）；

（b）是可变形卷积的一般形式，通过在图（a）的基础上给每个卷积核的参数添加一个方向向量（图b中的浅绿色箭头），使的我们的卷积核可以变为任意形状；

（c）和（d）是可变形卷积的特殊形式。

![](https://ai-studio-static-online.cdn.bcebos.com/853ee67ccf2047cba9121095ab53b7ed2d5e5ec1c72e460796f9eed65c0bb43c)

如何计算这个偏移量呢？

![](https://ai-studio-static-online.cdn.bcebos.com/d11a0844ba88467585475e32ddd4e70e3e8adfe225944874bd9de70e13f4b8ad)

上图是可变形卷积的学习过程。对于输入的一张feature map，假设原来的卷积操作是3×3的，那么为了学习偏移量offset，我们定义另外一个3×3的卷积层（图中上面的那层），输出的维度其实就是原来feature map大小，channel数等于2N（分别表示x,y方向的偏移）。下面的可变形卷积可以看作先基于上面那部分生成的offset做了一个插值操作，然后再执行普通的卷积。

**应用实例**

![](https://ai-studio-static-online.cdn.bcebos.com/f8b968f0349a409c92aa415296ddf12d458e84b3dd65494b898fb5c2e5386195)

上图是作者论文中的演示用例，我们可以看到当绿色点在目标上时，红色点所在区域也集中在目标位置，并且基本能够覆盖不同尺寸的目标，因此经过可变形卷积，我们可以更好地提取出感兴趣物体的完整特征，效果是非常不错的。

## 三.可变形卷积V2（DCN v2)

DCN v1虽然比传统卷积能更好地提取特征，但他并不是没有问题的，就是我们的可变形卷积有可能引入了无用的上下文（区域）来干扰我们的特征提取，这显然会降低算法的表现。作者也做了一个实验进行对比说明：

![](https://ai-studio-static-online.cdn.bcebos.com/9da73e4ac09f4bc8a34c0cb07032d46e5371ec36825f45538512b059936394c3)

从上图我们可以看到虽然DCN v1使得提取特征的能力显著增强，空间上网络接收了更大的区域，但覆盖整个目标的同时也包含了更多的不相关的背景信息干扰。对此作者提出了三个地方的改进：

加入更多的可变形卷积层

可调节的变形模块

R-CNN特征融合


**加入更多的特征模块**

在DCN v1中只在conv 5中使用了三个可变形卷积，在DCN v2中把conv3到conv5都换成了可变形卷积，提高算法对几何形变的建模能力。

**可调节的变形模块**

为了解决引入了一些无关区域的问题，在DCN v2中我们不只添加每一个采样点的偏移，还添加了一个权重系数，来区分我们引入的区域是否为我们感兴趣的区域，假如这个采样点的区域我们不感兴趣，则把权重学习为0即可

![](https://ai-studio-static-online.cdn.bcebos.com/b8c30a30753649d99abe607301fb008eb6ea953e63764ec689f7d1ffd9c2c008)

**R-CNN特征融合**

作者发现把R-CNN和Faster RCNN的classification score结合起来可以提升performance，说明R-CNN学到的focus在物体上的feature可以解决无关上下文的问题。但是增加额外的R-CNN会使inference速度变慢很多。DCNV2里的解决方法是把R-CNN当做teacher network，让DCN V2的ROIPooling之后的feature去模拟R-CNN的feature，类似知识蒸馏的做法，下面会具体展开：

![](https://ai-studio-static-online.cdn.bcebos.com/ad2734a6115c4296952c1d0bbfd520bbdc69b284240245aba32599b59775b2f8)


左边的网络为主网络（Faster RCNN），右边的网络为子网络（RCNN）。实现上大致是用主网络训练过程中得到的RoI去裁剪原图，然后将裁剪到的图resize到224×224大小作为子网络的输入，这部分最后提取的特征和主网络输出的1024维特征作为feature mimicking loss的输入，用来约束这2个特征的差异（通过一个余弦相似度计算，如下图所示），同时子网络通过一个分类损失进行监督学习，因为并不需要回归坐标，所以没有回归损失。在inference阶段仅有主网络部分，因此这个操作不会在inference阶段增加计算成本。

![](https://ai-studio-static-online.cdn.bcebos.com/811fb10cae2a47c1ae53a1b3334131d5714bdb24ef1c4c5b9d455e5026260251)

因为RCNN这个子网络的输入就是RoI在原输入图像上裁剪出来的图像，因此不存在RoI以外区域信息的干扰，这就使得RCNN这个网络训练得到的分类结果更加可靠，以此通过一个损失函数监督主网络Faster RCNN的分类支路训练就能够使网络提取到更多RoI内部特征，而不是自己引入的外部特征。

总的loss由三部分组成：mimic loss + R-CNN classification loss + Faster-RCNN loss.

**应用实例**

![](https://ai-studio-static-online.cdn.bcebos.com/523b2f2926114d31a98444fadf7b48c4f0d6cc6cdd334bf39e976a047de6b55d)

## 四.参考文献

DCN v1论文https://arxiv.org/pdf/1703.06211.pdf

DCN v2论文https://arxiv.org/pdf/1811.11168.pdf
