\# 一、CNN-DSSM

\#\# 1、概念

DSSM （Deep Structured Semantic Models）的原理很简单，通过搜索引擎里 Query 和
Title 的海量的点击曝光日志，用 DNN 把 Query 和 Title 表达为低纬语义向量，并通过
cosine
距离来计算两个语义向量的距离，最终训练出语义相似度模型。该模型既可以用来预测两个句子的语义相似度，又可以获得某句子的低纬语义向量表达。

DSSM 从下往上可以分为三层结构：输入层、表示层、匹配层

\#\# 2、架构模型

![](https://ai-studio-static-online.cdn.bcebos.com/df3ef79c5dc740249150afa1e5cdc5bf97e48ee4b6444f51b6f87e81361fb908)

输入层做的事情是把句子映射到一个向量空间里并输入到 DNN
中，这里英文和中文的处理方式有很大的不同。

DSSM 的表示层采用 BOW（Bag of
words）的方式，相当于把字向量的位置信息抛弃了，整个句子里的词都放在一个袋子里了，不分先后顺序。

\#\# 3、优缺点

\* 优点：DSSM
用字向量作为输入既可以减少切词的依赖，又可以提高模型的范化能力，因为每个汉字所能表达的语义是可以复用的。另一方面，传统的输入层是用
Embedding 的方式（如 Word2Vec 的词向量）或者主题模型的方式（如 LDA
的主题向量）来直接做词的映射，再把各个词的向量累加或者拼接起来，由于 Word2Vec 和
LDA 都是无监督的训练，这样会给整个模型引入误差，DSSM
采用统一的有监督训练，不需要在中间过程做无监督模型的映射，因此精准度会比较高。

\* 缺点：上文提到 DSSM
采用词袋模型（BOW），因此丧失了语序信息和上下文信息。另一方面，DSSM
采用弱监督、端到端的模型，预测结果不可控。

\#\# 4、作用与场景

CNN-DSSM的结构可分为数据预处理（把文本向量化），在经过深度神经网络，压缩矩阵，最后拿压缩后的矩阵进行相似度计算。用于在NLP领域，计算语义相似度。

\# 二、LSTM-DSSM

针对 CNN-DSSM
无法捕获较远距离上下文特征的缺点，有人提出了用LSTM-DSSM[3]（Long-Short-Term
Memory）来解决该问题。

\#\# 2.1 RNN

RNN（Recurrent Neural
Networks）可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。如果我们将这个循环展开：

假设输入 xi 为一个 query 中几个连续的词，hi 为输出。那么上一个神经元的输出
h(t-1) 与当前细胞的输入 Xt 拼接后经过 tanh 函数会输出 ht，同时把 ht
传递给下一个细胞。

不幸的是，在这个间隔不断增大时，RNN 会逐渐丧失学习到远距离信息的能力。因为 RNN
随着距离的加长，会导致梯度消失。简单来说，由于求导的链式法则，直接导致梯度被表示为连乘的形式，以至梯度消失（几个小于
1 的数相乘会逐渐趋向于 0）。

\#\# 2.2 LSTM

\* （1）遗忘门

遗忘门 [5]由 Gers 提出，它用来控制细胞状态 cell
有哪些信息可以通过，继续往下传递。如下图所示，上一层的输出 h(t-1) concat
上本层的输入 xt，经过一个 sigmoid 网络（遗忘门）产生一个从 0 到 1 的数值
ft，然后与细胞状态 C(t-1) 相乘，最终决定有多少细胞状态可以继续往后传递。

\* （2）输入门

输入门决定要新增什么信息到细胞状态，这里包含两部分：一个 sigmoid 输入门和一个
tanh 函数。sigmoid 决定输入的信号控制，tanh
决定输入什么内容。如下图所示，上一层的输出 h(t-1) concat 上本层的输入
xt，经过一个 sigmoid 网络（输入门）产生一个从 0 到 1 的数值 it，同样的信息经过
tanh 网络做非线性变换得到结果 Ct，sigmoid 的结果和 tanh
的结果相乘，最终决定有哪些信息可以输入到细胞状态里。

\* （3）输出门

输出门决定从细胞状态要输出什么信息，这里也包含两部分：一个 sigmoid 输出门和一个
tanh 函数。sigmoid 决定输出的信号控制，tanh
决定输出什么内容。如下图所示，上一层的输出 h(t-1) concat 上本层的输入
xt，经过一个 sigmoid 网络（输出门）产生一个从 0 到 1 的数值 Ot，细胞状态 Ct 经过
tanh 网络做非线性变换，得到结果再与 sigmoid 的结果 Ot
相乘，最终决定有哪些信息可以输出，输出的结果 ht
会作为这个细胞的输出，也会作为传递个下一个细胞。

\#\# 2.3 LSTM-DSSM

![](https://ai-studio-static-online.cdn.bcebos.com/bd8e61c5fc5e4563bc37ab57908f3a42af430f8e61e94c5d804d588bc353199e)

\#\# 2.4优缺点

\*
对隐藏层进行了修改，改善了RNN中存在的长期依赖问题；LSTM的表现通常比时间递归神经网络及隐马尔科夫模型（HMM）更好；作为非线性模型，LSTM可作为复杂的非线性单元用于构造更大型深度神经网络。

\*
一个缺点是RNN的梯度问题在LSTM及其变种里面得到了一定程度的解决，但还是不够。它可以处理100个量级的序列，而对于1000个量级，或者更长的序列则依然会显得很棘手；另一个缺点是每一个LSTM的cell里面都意味着有4个全连接层(MLP)，如果LSTM的时间跨度很大，并且网络又很深，这个计算量会很大，很耗时。

\# 三、MMOE

实际上，多任务学习模型的表现并不总是一定会由于单任务模型：

\* 许多DNN-based multi-task learning
models会对数据分布的不同、任务之间的关系等因素特别敏感。

\*
源于不同任务之间的固有的冲突事实上会对于至少一部分任务造成不利的影响，尤其当模型的参数在各个任务中被广泛共享的时候。

现有的解决方法：

\*
基于一个特定的数据生成假设前提去探究多任务学习中任务的区别，并且依据任务区别的程度去提出建议
\-\>
真实的数据具有极为复杂的模式，很难去明确地衡量任务之间的区别并依此去应用这些建议

\*
也有不需要给出明确的任务区别程度的方法，但是为了适应不同的任务，往往带来更多的模型参数
\-\> 在资源有限的情况下，计算的成本是难以接受的

\#\# 3.1优缺点

\* MMoE对于任务之间的关系进行明确地建模，并且学习任务特定的函数去平衡共享的表达

\* MMoE能够在不添加大量新的参数的情况下让参数进行自动分配以去捕捉shared task
information以及task-specific information。

\#\# 3.2MMoE结构概述：

![](https://ai-studio-static-online.cdn.bcebos.com/84f29947269f491fb918a96341881e32d12b0467f5f14b7fa72d0a30fae609b5)

\#\# 3.3核心:

1\. 清晰地描述任务之间的关系：通过modulation和gating
networks，模型能够自动调整学习shared information以及task-specific
information之间的参数化过程。

1\. 同时提升模型的表达力以及可训练性。

\# 四、MMOE

在多任务学习模型当中，最常见的一种模型就是Shared-bottom模型。首先每个任务共享底部的network，然后再根据任务的个数在上面划分出多个tower
network来分别学习不同的目标。转换成公式是:

\$y\^k = h\^k(f(x))\$

其中f表示shared-bottom network， \$h\^k\$表示第k个tower
network，yk是最终的输出。

\#\# 优缺点

\* 优点：降低overfit风险，利用任务之间的关联性使模型学习效果更强

\*
缺点：任务之间的相关性将严重影响模型效果。假如任务之间相关性较低，模型的效果相对会较差。

![](https://ai-studio-static-online.cdn.bcebos.com/7f683151608b45dbbddb3e3a37b80c6c9ac1ea7690684be0a6d65736cf92f629)

\# 五、YouTube深度学习视屏推荐系统

\#\# 5.1 YouTube 推荐系统架构

为了对海量的视频进行快速、准确的排序，YouTube 也采用了经典的召回层 +
排序层的推荐系统架构。

![](https://ai-studio-static-online.cdn.bcebos.com/9891b43d14e84e82a34d226067fae1b8c4e434c538b84d82ad88e3750b348212)

其推荐过程可以分成二级。第一级是用候选集生成模型（Candidate Generation
Model）完成候选视频的快速筛选，在这一步，候选视频集合由百万降低到几百量级，这就相当于经典推荐系统架构中的召回层。第二级是用排序模型（Ranking
Model）完成几百个候选视频的精排，这相当于经典推荐系统架构中的排序层。

无论是候选集生成模型还是排序模型，YouTube 都采用了深度学习的解决方案。

\#\# 5.2训练和测试样本的处理

为了能够提高模型的训练效率和预测准确率，Youtube采取了诸多处理训练样本的工程措施，主要有3点：

1\. 候选集生成模型把推荐模型转换成
多分类问题，在预测下一次观看的场景中，每一个备选视频都会是一个分类，而如果采用softmax对其训练是很低效的。

1\. 在对训练集的预处理过程中，Youtube没有采用原始的用户日志，而是
对每个用户提取等数量的训练样本。

1.
在处理测试集时，Youtube没有采用经典的随机留一法，而是一定要以用户最近一次观看的行为作为测试集。

\#\# 5.3 应用场景

YouTube
推荐系统的架构是一个典型的召回层加排序层的架构，其中候选集生成模型负责从百万候选集中召回几百个候选视频，排序模型负责几百个候选视频的精排，最终选出几十个推荐给用户。

候选集生成模型是一个典型的 Embedding MLP
的架构，要注意的是它的输出层一个多分类的输出层，预测的是用户点击了“哪个”视频。在候选集生成模型的
serving 过程中，需要从输出层提取出视频 Embedding，从最后一层 ReLU 层得到用户
Embedding，然后利用 最近邻搜索快速 得到候选集。

排序模型同样是一个 Embedding MLP
的架构，不同的是，它的输入层包含了更多的用户和视频的特征，输出层采用了 Weighted
LR
作为输出层，并且使用观看时长作为正样本权重，让模型能够预测出观看时长，这更接近
YouTube 要达成的商业目标。
