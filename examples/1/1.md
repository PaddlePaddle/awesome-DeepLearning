#### 一、基于层次softmax训练词向量

​	假设有一份训练文档集，它包括了T个训练词语w1,w2,w3,⋯,wT，它们构成大小为|V|的词语集合V。语言模型通常只考虑由当前词语wi的左右n个词语组成的上下文ci。每个词语有一个d维的输入词向量vW(即embedding层的词向量)和输出词向量v’W（即softmax层的权重矩阵所表示的词语）。最后，针对模型参数θ来优化目标函数Jθ。若指定上下文c，用softmax方法计算词语w出现的概率可以用公式表示为：

​    ![](https://ai-studio-static-online.cdn.bcebos.com/b364964cbfba4c86b0579cbc89393161eacecf499ebe4fe8a98ea061022516d9)
h是网络倒数第二层的输出向量。为了简化表示，上式中用c表示上下文内容，并且省略了目标词语wt的下标t。为了得到上式的分母部分，需要计算向量h与词典V中每个词语向量之间的内积。因此，计算softmax的代价非常昂贵。
   Hierarchical softmax （H-Softmax)是由Morin和Bengio受到二叉树的启发而提出。H-Softmax本质上是用层级关系替代了扁平化的softmax层，如图1所示，每个叶子节点表示一个词语。于是，计算单个词语概率值的计算过程被拆解为一系列的概率计算，这样可以避免对所有词语进行标准化计算。用H-Softmax替换softmax层之后，词语的预测速度可以提升至少50倍，速度的提升对于低延时要求的实时系统至关重要，比如谷歌新推出的消息应用Allo。

   ![](https://ai-studio-static-online.cdn.bcebos.com/d7e3037affd043a5b4fcf95537bb132bca5e5d587dad4d1c9fdb34a36c3d5e8b)

我们可以把原来的softmax看做深度为1的树，词表V中的每一个词语表示一个叶子节点。计算一个词语的softmax概率需要对|V|个节点的概率值做标准化。如果把softmax改为二叉树结构，每个word表示叶子节点，那么只需要沿着通向该词语的叶子节点的路径搜索，而不需要考虑其它的节点。
	平衡二叉树的深度是log2(|V|)，因此，最多只需要计算log2(|V|)个节点就能得到目标词语的概率值。注意，得到的概率值已经经过了标准化，因为二叉树所有叶子节点组成一个概率分布，所有叶子节点的概率值总和等于1。我们可以简单地验证一下，在图1的根节点（Node o)处，两个分枝的概率和必须为1。之后的每个节点，它的两个子节点的概率值之和等于节点本身的概率值。因为整条搜索路径没有概率值的损失，所以最底层所有叶子节点的概率值之和必定等于1，hierarchical softmax定义了词表V中所有词语的标准化概率分布。
	具体说来，当遍历树的时候，我们需要能够计算左侧分枝或是右侧分枝的概率值。为此，给每个节点分配一个向量表示。与常规的softmax做法不同，这里不是给每个输出词语w生成词向量v’w，而是给每个节点n计算一个向量v’n。总共有|V|-1个节点，每个节点都有自己独一无二的向量表示，H-Softmax方法用到的参数与常规的softmax几乎一样。于是，在给定上下文c时，就能够计算节点n左右两个分枝的概率：

​    ![](https://ai-studio-static-online.cdn.bcebos.com/7e6934d600e04081a87e6d55382d4f07b64e7a45dded4bc493e461bf26103f4e)
上式与常规的softmax大致相同。现在需要计算h与树的每个节点的向量v’n的内积，而不是与每个输出词语的向量计算。而且，现在只需要计算一个概率值，这里就是偏向n节点右枝的概率值。相反的，偏向左枝的概率值是1−p(right|n,c)

#### 二、LSTM可以实现的NLP任务

##### 1、序列到类别

使用的案例是 UCI 项目中的人体活动识别（HAR）数据集。该数据集包含原始的时序数据和经预处理的数据（包含 561 个特征）。
神经网络架构：

![](https://ai-studio-static-online.cdn.bcebos.com/777ec293e5b04ea0a4e60f28d0e9d688228130527c014ba588b89f10bca58ceb)

为了将数据馈送到网络中，我们需要将数组分割为 128 块（序列中的每一块都会进入一个 LSTM 单元），每一块的维度为（batch_size, n_channels）。随后单层神经元将转换这些输入并馈送到 LSTM 单元中，每一个 LSTM 单元的维度为 lstm_size，一般该参数需要选定为大于通道数量。这种方式很像文本应用中的嵌入层，其中词汇从给定的词汇表中嵌入为一个向量。后面我们需要选择 LSTM 层的数量（lstm_layers)，我们可以设定为 2。

##### 2、序列到序列

第i层，t时刻
j是前一时刻的各个cell

![](https://ai-studio-static-online.cdn.bcebos.com/5fc3f5c36adc40d3806b4111ecbdddca3e17f05644b44dfabb87b8f999f4938d)
![](https://ai-studio-static-online.cdn.bcebos.com/98173e67b169403c977bd5552e8479a4d1c2f0c9d5d44586a1d9e30d162c6e3b)

