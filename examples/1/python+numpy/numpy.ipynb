{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 波士顿房价预测任务\n",
    "\n",
    "上一节我们初步认识了神经网络的基本概念（如神经元、多层连接、前向计算、计算图）和模型结构三要素（模型假设、评价函数和优化算法）。本节将以“波士顿房价预测”任务为例，向读者介绍使用Python语言和Numpy库来构建神经网络模型的思考过程和操作方法。\n",
    "\n",
    "波士顿房价预测是一个经典的机器学习任务，类似于程序员世界的“Hello World”。和大家对房价的普遍认知相同，波士顿地区的房价受诸多因素影响。该数据集统计了13种可能影响房价的因素和该类型房屋的均价，期望构建一个基于13个因素进行房价预测的模型，如 **图1** 所示。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/abce0cb2a92f4e679c6855cfa520491597171533a0b0447e8d51d904446e213e\" width=\"500\" hegiht=\"\" ></center>\n",
    "<center><br>图1：波士顿房价影响因素示意图</br></center>\n",
    "<br></br>\n",
    "\n",
    "对于预测问题，可以根据预测输出的类型是连续的实数值，还是离散的标签，区分为回归任务和分类任务。因为房价是一个连续值，所以房价预测显然是一个回归任务。下面我们尝试用最简单的线性回归模型解决这个问题，并用神经网络来实现这个模型。\n",
    "\n",
    "## 线性回归模型\n",
    "\n",
    "假设房价和各影响因素之间能够用线性关系来描述：\n",
    "\n",
    "$$y = {\\sum_{j=1}^Mx_j w_j} + b$$\n",
    "\n",
    "模型的求解即是通过数据拟合出每个$w_j$和$b$。其中，$w_j$和$b$分别表示该线性模型的权重和偏置。一维情况下，$w_j$ 和 $b$ 是直线的斜率和截距。\n",
    "\n",
    "线性回归模型使用均方误差作为损失函数（Loss），用以衡量预测房价和真实房价的差异，公式如下：\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat{Y_i} - {Y_i})^{2}$$\n",
    "\n",
    "------\n",
    "**思考：**\n",
    "\n",
    "为什么要以均方误差作为损失函数？即将模型在每个训练样本上的预测误差加和，来衡量整体样本的准确性。这是因为损失函数的设计不仅仅要考虑“合理性”，同样需要考虑“易解性”，这个问题在后面的内容中会详细阐述。\n",
    "\n",
    "------\n",
    "\n",
    "## 线性回归模型的神经网络结构\n",
    "\n",
    "神经网络的标准结构中每个神经元由加权和与非线性变换构成，然后将多个神经元分层的摆放并连接形成神经网络。线性回归模型可以认为是神经网络模型的一种极简特例，是一个只有加权和、没有非线性变换的神经元（无需形成网络），如 **图2** 所示。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/f9117a5a34d44b1eab85147e62b4e6295e485e48d79d4a03adaa14a447ffd230\" width=\"300\" hegiht=\"\" ></center>\n",
    "<center><br>图2：线性回归模型的神经网络结构</br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 构建波士顿房价预测任务的神经网络模型\n",
    "\n",
    "深度学习不仅实现了模型的端到端学习，还推动了人工智能进入工业大生产阶段，产生了标准化、自动化和模块化的通用框架。不同场景的深度学习模型具备一定的通用性，五个步骤即可完成模型的构建和训练，如 **图3** 所示。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/12fdca24a3b94166a9e8c815ef4b0e4ddfec541f3a024a4392f1fc17fa186c7b\" width=\"800\" hegiht=\"\" ></center>\n",
    "<center><br>图3：构建神经网络/深度学习模型的基本步骤</br></center>\n",
    "<br></br>\n",
    "\n",
    "正是由于深度学习的建模和训练的过程存在通用性，在构建不同的模型时，只有模型三要素不同，其它步骤基本一致，深度学习框架才有用武之地。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据处理\n",
    "\n",
    "数据处理包含五个部分：数据导入、数据形状变换、数据集划分、数据归一化处理和封装`load data`函数。数据预处理后，才能被模型调用。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "* 本教程中的代码都可以在AI Studio上直接运行，Print结果都是基于程序真实运行的结果。\n",
    "* 由于是真实案例，代码之间存在依赖关系，因此需要读者逐条、全部运行，否则会导致命令执行报错。\n",
    "\n",
    "------\n",
    "\n",
    "### 读入数据\n",
    "\n",
    "通过如下代码读入数据，了解下波士顿房价的数据集结构，数据存放在本地目录下housing.data文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, ..., 3.969e+02, 7.880e+00,\n",
       "       1.190e+01])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入需要用到的package\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "# 读入训练数据\n",
    "datafile = './work/housing.data'\n",
    "data = np.fromfile(datafile, sep=' ')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据形状变换\n",
    "\n",
    "由于读入的原始数据是1维的，所有数据都连在一起。因此需要我们将数据的形状进行变换，形成一个2维的矩阵，每行为一个数据样本（14个值），每个数据样本包含13个$X$（影响房价的特征）和一个$Y$（该类型房屋的均价）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读入之后的数据被转化成1维array，其中array的第0-13项是第一条数据，第14-27项是第二条数据，以此类推.... \n",
    "# 这里对原始数据做reshape，变成N x 14的形式\n",
    "feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS', \n",
    "                 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "feature_num = len(feature_names)\n",
    "data = data.reshape([data.shape[0] // feature_num, feature_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14,)\n",
      "[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00 2.400e+01]\n"
     ]
    }
   ],
   "source": [
    "# 查看数据\n",
    "x = data[0]\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据集划分\n",
    "\n",
    "将数据集划分成训练集和测试集，其中训练集用于确定模型的参数，测试集用于评判模型的效果。为什么要对数据集进行拆分，而不能直接应用于模型训练呢？这与学生时代的授课和考试关系比较类似，如 **图4** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/a1c845a50e28474d9aa72028edfea33f1a3deca1d54d40ec94ba366d3a18c408\" width=\"600\" hegiht=\"\" ></center>\n",
    "<center><br>图4：训练集和测试集拆分的意义</br></center>\n",
    "<br></br>\n",
    "\n",
    "上学时总有一些自作聪明的同学，平时不认真学习，考试前临阵抱佛脚，将习题死记硬背下来，但是成绩往往并不好。因为学校期望学生掌握的是知识，而不仅仅是习题本身。另出新的考题，才能鼓励学生努力去掌握习题背后的原理。同样我们期望模型学习的是任务的本质规律，而不是训练数据本身，模型训练未使用的数据，才能更真实的评估模型的效果。\n",
    "\n",
    "在本案例中，我们将80%的数据用作训练集，20%用作测试集，实现代码如下。通过打印训练集的形状，可以发现共有404个样本，每个样本含有13个特征和1个预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 14)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = 0.8\n",
    "offset = int(data.shape[0] * ratio)\n",
    "training_data = data[:offset]\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据归一化处理\n",
    "\n",
    "对每个特征进行归一化处理，使得每个特征的取值缩放到0~1之间。这样做有两个好处：一是模型训练更高效；二是特征前的权重大小可以代表该变量对预测结果的贡献度（因为每个特征值本身的范围相同）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 计算train数据集的最大值，最小值，平均值\n",
    "maximums, minimums, avgs = \\\n",
    "                     training_data.max(axis=0), \\\n",
    "                     training_data.min(axis=0), \\\n",
    "     training_data.sum(axis=0) / training_data.shape[0]\n",
    "# 对数据进行归一化处理\n",
    "for i in range(feature_num):\n",
    "    #print(maximums[i], minimums[i], avgs[i])\n",
    "    data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 封装成load data函数\n",
    "\n",
    "将上述几个数据处理操作封装成`load data`函数，以便下一步模型的调用，实现方法如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # 从文件导入数据\n",
    "    datafile = './work/housing.data'\n",
    "    data = np.fromfile(datafile, sep=' ')\n",
    "\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "    feature_num = len(feature_names)\n",
    "\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
    "\n",
    "    # 将原数据集拆分成训练集和测试集\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\n",
    "    # 测试集和训练集必须是没有交集的\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0] * ratio)\n",
    "    training_data = data[:offset]\n",
    "\n",
    "    # 计算训练集的最大值，最小值，平均值\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\n",
    "\n",
    "    # 对数据进行归一化处理\n",
    "    for i in range(feature_num):\n",
    "        #print(maximums[i], minimums[i], avgs[i])\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\n",
    "\n",
    "    # 训练集和测试集的划分比例\n",
    "    training_data = data[:offset]\n",
    "    test_data = data[offset:]\n",
    "    return training_data,test_data,minimums[13],maximums[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型设计\n",
    "\n",
    "模型设计是深度学习模型关键要素之一，也称为网络结构设计，相当于模型的假设空间，即实现模型“前向计算”（从输入到输出）的过程。\n",
    "\n",
    "如果将输入特征和输出预测值均以向量表示，输入特征$x$有13个分量，$y$有1个分量，那么参数权重的形状（shape）是$13\\times1$。假设我们以如下任意数字赋值参数做初始化：\n",
    "$$w=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, -0.1, -0.2, -0.3, -0.4, 0.0]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "完整的线性回归公式，还需要初始化偏移量$b$，同样随意赋初值-0.2。那么，线性回归模型的完整输出是$z=t+b$，这个从特征和参数计算输出值的过程称为“前向计算”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "完整的线性回归公式，还需要初始化偏移量bbb，同样随意赋初值-0.2。那么，线性回归模型的完整输出是z=t+bz=t+bz=t+b，这个从特征和参数计算输出值的过程称为“前向计算”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "将上述计算预测输出的过程以“类和对象”的方式来描述，类成员变量有参数$w$和$b$。通过写一个`forward`函数（代表“前向计算”）完成上述从特征和参数到输出预测值的计算过程，代码如下所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "基于Network类的定义，模型的计算过程如下所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练配置\n",
    "\n",
    "模型设计完成后，需要通过训练配置寻找模型的最优值，即通过损失函数来衡量模型的好坏。训练配置也是深度学习模型关键要素之一。\n",
    "\n",
    "通过模型计算$x_1$表示的影响因素所对应的房价应该是$z$, 但实际数据告诉我们房价是$y$。这时我们需要有某种指标来衡量预测值$z$跟真实值$y$之间的差距。对于回归问题，最常采用的衡量方法是使用均方误差作为评价模型好坏的指标，具体定义如下：\n",
    "\n",
    "$$Loss = (y - z)^2$$\n",
    "\n",
    "上式中的$Loss$（简记为: $L$）通常也被称作损失函数，它是衡量模型好坏的指标。在回归问题中，均方误差是一种比较常见的形式，分类问题中通常会采用交叉熵作为损失函数，在后续的章节中会更详细的介绍。对一个样本计算损失函数值的实现如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "因为计算损失函数时需要把每个样本的损失函数值都考虑到，所以我们需要对单个样本的损失函数进行求和，并除以样本总数$N$。\n",
    "$$Loss= \\frac{1}{N}\\sum_{i=1}^N{(y_i - z_i)^2}$$\n",
    "在Network类下面添加损失函数的计算过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "使用定义的Network类，可以方便的计算预测值和损失函数。需要注意的是，类中的变量$x$, $w$，$b$, $z$, $error$等均是向量。以变量$x$为例，共有两个维度，一个代表特征数量（值为13），一个代表样本数量，代码如下所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练过程\n",
    "\n",
    "上述计算过程描述了如何构建神经网络，通过神经网络完成预测值和损失函数的计算。接下来介绍如何求解参数$w$和$b$的数值，这个过程也称为模型训练过程。训练过程是深度学习模型的关键要素之一，其目标是让定义的损失函数$Loss$尽可能的小，也就是说找到一个参数解$w$和$b$，使得损失函数取得极小值。\n",
    "\n",
    "我们先做一个小测试：如 **图5** 所示，基于微积分知识，求一条曲线在某个点的斜率等于函数在该点的导数值。那么大家思考下，当处于曲线的极值点时，该点的斜率是多少？\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/94f0437e6a454a0682f3b831c96a62bdaf40898af25145ec9b5b50bc80391f5c\" width=\"300\" hegiht=\"\" ></center>\n",
    "<center><br>图5：曲线斜率等于导数值</br></center>\n",
    "<br></br>\n",
    "\n",
    "这个问题并不难回答，处于曲线极值点时的斜率为0，即函数在极值点的导数为0。那么，让损失函数取极小值的$w$和$b$应该是下述方程组的解：\n",
    "$$\\frac{\\partial{L}}{\\partial{w}}=0$$\n",
    "$$\\frac{\\partial{L}}{\\partial{b}}=0$$\n",
    "\n",
    "将样本数据$(x, y)$带入上面的方程组中即可求解出$w$和$b$的值，但是这种方法只对线性回归这样简单的任务有效。如果模型中含有非线性变换，或者损失函数不是均方差这种简单的形式，则很难通过上式求解。为了解决这个问题，下面我们将引入更加普适的数值求解方法：梯度下降法。\n",
    "\n",
    "### 梯度下降法\n",
    "\n",
    "在现实中存在大量的函数正向求解容易，但反向求解较难，被称为单向函数，这种函数在密码学中有大量的应用。密码锁的特点是可以迅速判断一个密钥是否是正确的(已知$x$，求$y$很容易)，但是即使获取到密码锁系统，无法破解出正确的密钥是什么（已知$y$，求$x$很难）。\n",
    "\n",
    "这种情况特别类似于一位想从山峰走到坡谷的盲人，他看不见坡谷在哪（无法逆向求解出$Loss$导数为0时的参数值），但可以伸脚探索身边的坡度（当前点的导数值，也称为梯度）。那么，求解Loss函数最小值可以这样实现：从当前的参数取值，一步步的按照下坡的方向下降，直到走到最低点。这种方法笔者称它为“盲人下坡法”。哦不，有个更正式的说法“梯度下降法”。\n",
    "\n",
    "训练的关键是找到一组$(w, b)$，使得损失函数$L$取极小值。我们先看一下损失函数$L$只随两个参数$w_5$、$w_9$变化时的简单情形，启发下寻解的思路。\n",
    "$$L=L(w_5, w_9)$$\n",
    "这里我们将$w_0, w_1, ..., w_{12}$中除$w_5, w_9$之外的参数和$b$都固定下来，可以用图画出$L(w_5, w_9)$的形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "对于这种简单情形，我们利用上面的程序，可以在三维空间中画出损失函数随参数变化的曲面图。从图中可以看出有些区域的函数值明显比周围的点小。\n",
    "\n",
    "需要说明的是：为什么这里我们选择$w_5$和$w_9$来画图？这是因为选择这两个参数的时候，可比较直观的从损失函数的曲面图上发现极值点的存在。其他参数组合，从图形上观测损失函数的极值点不够直观。\n",
    "\n",
    "观察上述曲线呈现出“圆滑”的坡度，这正是我们选择以均方误差作为损失函数的原因之一。**图6** 呈现了只有一个参数维度时，均方误差和绝对值误差（只将每个样本的误差累加，不做平方处理）的损失函数曲线图。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/99487dca6520441db5073d1c154b5d2fb1174b5cf4d946c29f9d80a209bc2687\" width=\"700\" hegiht=\"40\" ></center>\n",
    "<center><br>图6：均方误差和绝对值误差损失函数曲线图</br></center>\n",
    "<br></br>\n",
    "\n",
    "由此可见，均方误差表现的“圆滑”的坡度有两个好处：\n",
    "\n",
    "* 曲线的最低点是可导的。\n",
    "* 越接近最低点，曲线的坡度逐渐放缓，有助于通过当前的梯度来判断接近最低点的程度（是否逐渐减少步长，以免错过最低点）。\n",
    "\n",
    "而绝对值误差是不具备这两个特性的，这也是损失函数的设计不仅仅要考虑“合理性”，还要追求“易解性”的原因。\n",
    "\n",
    "现在我们要找出一组$[w_5, w_9]$的值，使得损失函数最小，实现梯度下降法的方案如下：\n",
    "\n",
    "- 步骤1：随机的选一组初始值，例如：$[w_5, w_9] = [-100.0, -100.0]$\n",
    "- 步骤2：选取下一个点$[w_5^{'} , w_9^{'}]$，使得$L(w_5^{'} , w_9^{'}) < L(w_5, w_9)$\n",
    "- 步骤3：重复步骤2，直到损失函数几乎不再下降。\n",
    "\n",
    "如何选择$[w_5^{'} , w_9^{'}]$是至关重要的，第一要保证$L$是下降的，第二要使得下降的趋势尽可能的快。微积分的基础知识告诉我们，沿着梯度的反方向，是函数值下降最快的方向，如 **图7** 所示。简单理解，函数在某一个点的梯度方向是曲线斜率最大的方向，但梯度方向是向上的，所以下降最快的是梯度的反方向。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/5f8322f6172542dab0f78684b70efe45d819895332af4cabb7c536217ab0bb26\" width=\"400\" hegiht=\"40\" ></center>\n",
    "<center><br>图7：梯度下降方向示意图</br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 计算梯度\n",
    "\n",
    "上面我们讲过了损失函数的计算方法，这里稍微改写，为了使梯度计算更加简洁，引入因子$\\frac{1}{2}$，定义损失函数如下：\n",
    "\n",
    "$$L= \\frac{1}{2N}\\sum_{i=1}^N{(y_i - z_i)^2}$$\n",
    "\n",
    "其中$z_i$是网络对第$i$个样本的预测值：\n",
    "\n",
    "$$z_i = \\sum_{j=0}^{12}{x_i^{j}\\cdot w_j} + b$$\n",
    "\n",
    "梯度的定义：\n",
    "\n",
    "$$𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡 = (\\frac{\\partial{L}}{\\partial{w_0}},\\frac{\\partial{L}}{\\partial{w_1}}, ... ,\\frac{\\partial{L}}{\\partial{w_{12}}} ,\\frac{\\partial{L}}{\\partial{b}})$$\n",
    "\n",
    "可以计算出$L$对$w$和$b$的偏导数：\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)\\frac{\\partial{z_i}}{\\partial{w_j}}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)x_i^{j}}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)\\frac{\\partial{z_i}}{\\partial{b}}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)}$$\n",
    "\n",
    "从导数的计算过程可以看出，因子$\\frac{1}{2}$被消掉了，这是因为二次函数求导的时候会产生因子$2$，这也是我们将损失函数改写的原因。\n",
    "\n",
    "下面我们考虑只有一个样本的情况下，计算梯度：\n",
    "\n",
    "$$L= \\frac{1}{2}{(y_i - z_i)^2}$$\n",
    "\n",
    "$$z_1 = {x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b$$\n",
    "\n",
    "可以计算出：\n",
    "\n",
    "$$L= \\frac{1}{2}{({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b - y_1)^2}$$\n",
    "\n",
    "可以计算出$L$对$w$和$b$的偏导数：\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_0}} = ({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_12} + b - y_1)\\cdot x_1^{0}=({z_1} - {y_1})\\cdot x_1^{0}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = ({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b - y_1)\\cdot 1 = ({z_1} - {y_1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "可以通过具体的程序查看每个变量的数据和维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "按上面的公式，当只有一个样本时，可以计算某个$w_j$，比如$w_0$的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "同样我们可以计算$w_1$的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "依次计算$w_2$的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "聪明的读者可能已经想到，写一个for循环即可计算从$w_0$到$w_{12}$的所有权重的梯度，该方法读者可以自行实现。\n",
    "\n",
    "### 使用Numpy进行梯度计算\n",
    "\n",
    "基于Numpy广播机制（对向量和矩阵计算如同对1个单一变量计算一样），可以更快速的实现梯度计算。计算梯度的代码中直接用$(z_1 - y_1) \\cdot x_1$，得到的是一个13维的向量，每个分量分别代表该维度的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "输入数据中有多个样本，每个样本都对梯度有贡献。如上代码计算了只有样本1时的梯度值，同样的计算方法也可以计算样本2和样本3对梯度的贡献。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可能有的读者再次想到可以使用for循环把每个样本对梯度的贡献都计算出来，然后再作平均。但是我们不需要这么做，仍然可以使用Numpy的矩阵操作来简化运算，如3个样本的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "上面的x3samples, y3samples, z3samples的第一维大小均为3，表示有3个样本。下面计算这3个样本对梯度的贡献。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "此处可见，计算梯度`gradient_w`的维度是$3 \\times 13$，并且其第1行与上面第1个样本计算的梯度gradient_w_by_sample1一致，第2行与上面第2个样本计算的梯度gradient_w_by_sample2一致，第3行与上面第3个样本计算的梯度gradient_w_by_sample3一致。这里使用矩阵操作，可以更加方便的对3个样本分别计算各自对梯度的贡献。\n",
    "\n",
    "那么对于有N个样本的情形，我们可以直接使用如下方式计算出所有样本对梯度的贡献，这就是使用Numpy库广播功能带来的便捷。\n",
    "小结一下这里使用Numpy库的广播功能：\n",
    "- 一方面可以扩展参数的维度，代替for循环来计算1个样本对从$w_0$到$w_12$的所有参数的梯度。\n",
    "- 另一方面可以扩展样本的维度，代替for循环来计算样本0到样本403对参数的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "上面gradient_w的每一行代表了一个样本对梯度的贡献。根据梯度的计算公式，总梯度是对每个样本对梯度贡献的平均值。\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)\\frac{\\partial{z_i}}{\\partial{w_j}}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)x_i^{j}}$$\n",
    "\n",
    "我们也可以使用Numpy的均值函数来完成此过程："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "我们使用Numpy的矩阵操作方便地完成了gradient的计算，但引入了一个问题，`gradient_w`的形状是(13,)，而$w$的维度是(13, 1)。导致该问题的原因是使用`np.mean`函数时消除了第0维。为了加减乘除等计算方便，`gradient_w`和$w$必须保持一致的形状。因此我们将`gradient_w`的维度也设置为(13,1)，代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "综合上面的剖析，计算梯度的代码如下所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "上述代码非常简洁地完成了$w$的梯度计算。同样，计算$b$的梯度的代码也是类似的原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "将上面计算$w$和$b$的梯度的过程，写成Network类的`gradient`函数，实现方法如下所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 确定损失函数更小的点\n",
    "\n",
    "下面我们开始研究更新梯度的方法。首先沿着梯度的反方向移动一小步，找到下一个点P1，观察损失函数的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "运行上面的代码，可以发现沿着梯度反方向走一小步，下一个点的损失函数的确减少了。感兴趣的话，大家可以尝试不停的点击上面的代码块，观察损失函数是否一直在变小。\n",
    "\n",
    "在上述代码中，每次更新参数使用的语句：\n",
    "`net.w[5] = net.w[5] - eta * gradient_w5`\n",
    "\n",
    "* 相减：参数需要向梯度的反方向移动。\n",
    "* eta：控制每次参数值沿着梯度反方向变动的大小，即每次移动的步长，又称为学习率。\n",
    "\n",
    "大家可以思考下，为什么之前我们要做输入特征的归一化，保持尺度一致？这是为了让统一的步长更加合适。\n",
    "\n",
    "如 **图8** 所示，特征输入归一化后，不同参数输出的Loss是一个比较规整的曲线，学习率可以设置成统一的值 ；特征输入未归一化时，不同特征对应的参数所需的步长不一致，尺度较大的参数需要大步长，尺寸较小的参数需要小步长，导致无法设置统一的学习率。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/903f552bc55b4a5eba71caa7dd86fd2d7b71b8ebb6cb4500a5f5711f465707f3\" width=\"300\" hegiht=\"40\" ></center>\n",
    "<center><br>图8：未归一化的特征，会导致不同特征维度的理想步长不同</br></center>\n",
    "<br></br>\n",
    "\n",
    "###  代码封装Train函数\n",
    "\n",
    "将上面的循环计算过程封装在`train`和`update`函数中，实现方法如下所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 训练扩展到全部参数\n",
    "\n",
    "为了能给读者直观的感受，上面演示的梯度下降的过程仅包含$w_5$和$w_9$两个参数，但房价预测的完整模型，必须要对所有参数$w$和$b$进行求解。这需要将Network中的`update`和`train`函数进行修改。由于不再限定参与计算的参数（所有参数均参与计算），修改之后的代码反而更加简洁。实现逻辑：“前向计算输出、根据输出和真实值计算Loss、基于Loss和输入计算梯度、根据梯度更新参数值”四个部分反复执行，直到到损失函数最小。具体代码如下所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 随机梯度下降法（ Stochastic Gradient Descent）\n",
    "\n",
    "在上述程序中，每次损失函数和梯度计算都是基于数据集中的全量数据。对于波士顿房价预测任务数据集而言，样本数比较少，只有404个。但在实际问题中，数据集往往非常大，如果每次都使用全量数据进行计算，效率非常低，通俗地说就是“杀鸡焉用牛刀”。由于参数每次只沿着梯度反方向更新一点点，因此方向并不需要那么精确。一个合理的解决方案是每次从总的数据集中随机抽取出小部分数据来代表整体，基于这部分数据计算梯度和损失来更新参数，这种方法被称作随机梯度下降法（Stochastic Gradient Descent，SGD），核心概念如下：\n",
    "\n",
    "* mini-batch：每次迭代时抽取出来的一批数据被称为一个mini-batch。\n",
    "* batch_size：一个mini-batch所包含的样本数目称为batch_size。\n",
    "* epoch：当程序迭代的时候，按mini-batch逐渐抽取出样本，当把整个数据集都遍历到了的时候，则完成了一轮训练，也叫一个epoch。启动训练时，可以将训练的轮数num_epochs和batch_size作为参数传入。\n",
    "\n",
    "下面结合程序介绍具体的实现过程，涉及到数据处理和训练过程两部分代码的修改。\n",
    "\n",
    "#### **数据处理代码修改**\n",
    "\n",
    "数据处理需要实现拆分数据批次和样本乱序（为了实现随机抽样的效果）两个功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "train_data中一共包含404条数据，如果batch_size=10，即取前0-9号样本作为第一个mini-batch，命名train_data1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "使用train_data1的数据（0-9号样本）计算梯度并更新网络参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "再取出10-19号样本作为第二个mini-batch，计算梯度并更新网络参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "按此方法不断的取出新的mini-batch，并逐渐更新网络参数。\n",
    "\n",
    "接下来，将train_data分成大小为batch_size的多个mini_batch，如下代码所示：将train_data分成 $\\frac{404}{10} + 1 = 41$ 个 mini_batch，其中前40个mini_batch，每个均含有10个样本，最后一个mini_batch只含有4个样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "另外，这里是按顺序读取mini_batch，而SGD里面是随机抽取一部分样本代表总体。为了实现随机抽样的效果，我们先将train_data里面的样本顺序随机打乱，然后再抽取mini_batch。随机打乱样本顺序，需要用到`np.random.shuffle`函数，下面先介绍它的用法。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "通过大量实验发现，模型对最后出现的数据印象更加深刻。训练数据导入后，越接近模型训练结束，最后几个批次数据对模型参数的影响越大。为了避免模型记忆影响训练效果，需要进行样本乱序操作。\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "多次运行上面的代码，可以发现每次执行shuffle函数后的数字顺序均不同。\n",
    "上面举的是一个1维数组乱序的案例，我们再观察下2维数组乱序后的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "观察运行结果可发现，数组的元素在第0维被随机打乱，但第1维的顺序保持不变。例如数字2仍然紧挨在数字1的后面，数字8仍然紧挨在数字7的后面，而第二维的[3, 4]并不排在[1, 2]的后面。将这部分实现SGD算法的代码集成到Network类中的`train`函数中，最终的完整代码如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### **训练过程代码修改**\n",
    "\n",
    "将每个随机抽取的mini-batch数据输入到模型中用于参数训练。训练过程的核心是两层循环：\n",
    "\n",
    "1. 第一层循环，代表样本集合要被训练遍历几次，称为“epoch”，代码如下：\n",
    "\n",
    "`for epoch_id in range(num_epochs):`\n",
    "\n",
    "2. 第二层循环，代表每次遍历时，样本集合被拆分成的多个批次，需要全部执行训练，称为“iter (iteration)”，代码如下：\n",
    "\n",
    "`for iter_id,mini_batch in emumerate(mini_batches):`\n",
    "\n",
    "在两层循环的内部是经典的四步训练流程：前向计算->计算损失->计算梯度->更新参数，这与大家之前所学是一致的，代码如下：\n",
    "\n",
    "                x = mini_batch[:, :-1]\n",
    "                y = mini_batch[:, -1:]\n",
    "                a = self.forward(x)  #前向计算\n",
    "                loss = self.loss(a, y)  #计算损失\n",
    "                gradient_w, gradient_b = self.gradient(x, y)  #计算梯度\n",
    "                self.update(gradient_w, gradient_b, eta)  #更新参数\n",
    "\n",
    "\n",
    "将两部分改写的代码集成到Network类中的`train`函数中，最终的实现如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / iter   0, loss = 29.3223\n",
      "Epoch   0 / iter   1, loss = 5.5165\n",
      "Epoch   0 / iter   2, loss = 3.2071\n",
      "Epoch   0 / iter   3, loss = 1.9857\n",
      "Epoch   0 / iter   4, loss = 3.5202\n",
      "Epoch   1 / iter   0, loss = 1.1147\n",
      "Epoch   1 / iter   1, loss = 1.4117\n",
      "Epoch   1 / iter   2, loss = 1.8714\n",
      "Epoch   1 / iter   3, loss = 1.5079\n",
      "Epoch   1 / iter   4, loss = 2.4143\n",
      "Epoch   2 / iter   0, loss = 2.3491\n",
      "Epoch   2 / iter   1, loss = 0.6355\n",
      "Epoch   2 / iter   2, loss = 1.5726\n",
      "Epoch   2 / iter   3, loss = 1.3482\n",
      "Epoch   2 / iter   4, loss = 3.8860\n",
      "Epoch   3 / iter   0, loss = 2.7768\n",
      "Epoch   3 / iter   1, loss = 2.0430\n",
      "Epoch   3 / iter   2, loss = 0.8047\n",
      "Epoch   3 / iter   3, loss = 0.9028\n",
      "Epoch   3 / iter   4, loss = 2.6762\n",
      "Epoch   4 / iter   0, loss = 0.5181\n",
      "Epoch   4 / iter   1, loss = 0.6813\n",
      "Epoch   4 / iter   2, loss = 0.5433\n",
      "Epoch   4 / iter   3, loss = 0.4883\n",
      "Epoch   4 / iter   4, loss = 0.1427\n",
      "Epoch   5 / iter   0, loss = 0.8846\n",
      "Epoch   5 / iter   1, loss = 0.6426\n",
      "Epoch   5 / iter   2, loss = 0.3902\n",
      "Epoch   5 / iter   3, loss = 0.3947\n",
      "Epoch   5 / iter   4, loss = 0.6258\n",
      "Epoch   6 / iter   0, loss = 0.5720\n",
      "Epoch   6 / iter   1, loss = 0.4884\n",
      "Epoch   6 / iter   2, loss = 0.5587\n",
      "Epoch   6 / iter   3, loss = 0.3885\n",
      "Epoch   6 / iter   4, loss = 0.3111\n",
      "Epoch   7 / iter   0, loss = 0.6367\n",
      "Epoch   7 / iter   1, loss = 0.8045\n",
      "Epoch   7 / iter   2, loss = 0.6341\n",
      "Epoch   7 / iter   3, loss = 0.4193\n",
      "Epoch   7 / iter   4, loss = 0.1341\n",
      "Epoch   8 / iter   0, loss = 0.6051\n",
      "Epoch   8 / iter   1, loss = 0.4116\n",
      "Epoch   8 / iter   2, loss = 0.7340\n",
      "Epoch   8 / iter   3, loss = 0.6172\n",
      "Epoch   8 / iter   4, loss = 0.0883\n",
      "Epoch   9 / iter   0, loss = 0.3985\n",
      "Epoch   9 / iter   1, loss = 0.3745\n",
      "Epoch   9 / iter   2, loss = 0.4531\n",
      "Epoch   9 / iter   3, loss = 0.3886\n",
      "Epoch   9 / iter   4, loss = 0.1227\n",
      "Epoch  10 / iter   0, loss = 0.7094\n",
      "Epoch  10 / iter   1, loss = 0.5139\n",
      "Epoch  10 / iter   2, loss = 0.2716\n",
      "Epoch  10 / iter   3, loss = 0.5875\n",
      "Epoch  10 / iter   4, loss = 0.2862\n",
      "Epoch  11 / iter   0, loss = 0.4679\n",
      "Epoch  11 / iter   1, loss = 0.3857\n",
      "Epoch  11 / iter   2, loss = 0.4182\n",
      "Epoch  11 / iter   3, loss = 0.3417\n",
      "Epoch  11 / iter   4, loss = 0.1157\n",
      "Epoch  12 / iter   0, loss = 0.4876\n",
      "Epoch  12 / iter   1, loss = 0.3873\n",
      "Epoch  12 / iter   2, loss = 0.3550\n",
      "Epoch  12 / iter   3, loss = 0.3901\n",
      "Epoch  12 / iter   4, loss = 0.4542\n",
      "Epoch  13 / iter   0, loss = 1.0860\n",
      "Epoch  13 / iter   1, loss = 0.5112\n",
      "Epoch  13 / iter   2, loss = 0.4436\n",
      "Epoch  13 / iter   3, loss = 0.4763\n",
      "Epoch  13 / iter   4, loss = 0.2538\n",
      "Epoch  14 / iter   0, loss = 0.3269\n",
      "Epoch  14 / iter   1, loss = 0.3171\n",
      "Epoch  14 / iter   2, loss = 0.3426\n",
      "Epoch  14 / iter   3, loss = 0.3202\n",
      "Epoch  14 / iter   4, loss = 0.7143\n",
      "Epoch  15 / iter   0, loss = 0.3985\n",
      "Epoch  15 / iter   1, loss = 0.2647\n",
      "Epoch  15 / iter   2, loss = 0.3412\n",
      "Epoch  15 / iter   3, loss = 0.1958\n",
      "Epoch  15 / iter   4, loss = 0.0867\n",
      "Epoch  16 / iter   0, loss = 0.2726\n",
      "Epoch  16 / iter   1, loss = 0.3872\n",
      "Epoch  16 / iter   2, loss = 0.2647\n",
      "Epoch  16 / iter   3, loss = 0.1763\n",
      "Epoch  16 / iter   4, loss = 0.2262\n",
      "Epoch  17 / iter   0, loss = 0.2592\n",
      "Epoch  17 / iter   1, loss = 0.2298\n",
      "Epoch  17 / iter   2, loss = 0.1906\n",
      "Epoch  17 / iter   3, loss = 0.3376\n",
      "Epoch  17 / iter   4, loss = 0.2521\n",
      "Epoch  18 / iter   0, loss = 0.5433\n",
      "Epoch  18 / iter   1, loss = 0.4082\n",
      "Epoch  18 / iter   2, loss = 0.3131\n",
      "Epoch  18 / iter   3, loss = 0.2581\n",
      "Epoch  18 / iter   4, loss = 0.0496\n",
      "Epoch  19 / iter   0, loss = 0.2650\n",
      "Epoch  19 / iter   1, loss = 0.2917\n",
      "Epoch  19 / iter   2, loss = 0.1672\n",
      "Epoch  19 / iter   3, loss = 0.1858\n",
      "Epoch  19 / iter   4, loss = 0.0788\n",
      "Epoch  20 / iter   0, loss = 0.3086\n",
      "Epoch  20 / iter   1, loss = 0.4975\n",
      "Epoch  20 / iter   2, loss = 0.2015\n",
      "Epoch  20 / iter   3, loss = 0.1917\n",
      "Epoch  20 / iter   4, loss = 0.3318\n",
      "Epoch  21 / iter   0, loss = 0.1909\n",
      "Epoch  21 / iter   1, loss = 0.1582\n",
      "Epoch  21 / iter   2, loss = 0.1610\n",
      "Epoch  21 / iter   3, loss = 0.2179\n",
      "Epoch  21 / iter   4, loss = 0.0741\n",
      "Epoch  22 / iter   0, loss = 0.2156\n",
      "Epoch  22 / iter   1, loss = 0.2192\n",
      "Epoch  22 / iter   2, loss = 0.1142\n",
      "Epoch  22 / iter   3, loss = 0.1413\n",
      "Epoch  22 / iter   4, loss = 0.0269\n",
      "Epoch  23 / iter   0, loss = 0.1530\n",
      "Epoch  23 / iter   1, loss = 0.1546\n",
      "Epoch  23 / iter   2, loss = 0.2284\n",
      "Epoch  23 / iter   3, loss = 0.1614\n",
      "Epoch  23 / iter   4, loss = 0.2863\n",
      "Epoch  24 / iter   0, loss = 0.1819\n",
      "Epoch  24 / iter   1, loss = 0.1907\n",
      "Epoch  24 / iter   2, loss = 0.1194\n",
      "Epoch  24 / iter   3, loss = 0.1579\n",
      "Epoch  24 / iter   4, loss = 0.3537\n",
      "Epoch  25 / iter   0, loss = 0.4621\n",
      "Epoch  25 / iter   1, loss = 0.1571\n",
      "Epoch  25 / iter   2, loss = 0.1669\n",
      "Epoch  25 / iter   3, loss = 0.1197\n",
      "Epoch  25 / iter   4, loss = 0.1356\n",
      "Epoch  26 / iter   0, loss = 0.2653\n",
      "Epoch  26 / iter   1, loss = 0.2590\n",
      "Epoch  26 / iter   2, loss = 0.1953\n",
      "Epoch  26 / iter   3, loss = 0.1685\n",
      "Epoch  26 / iter   4, loss = 0.1026\n",
      "Epoch  27 / iter   0, loss = 0.1205\n",
      "Epoch  27 / iter   1, loss = 0.1994\n",
      "Epoch  27 / iter   2, loss = 0.1253\n",
      "Epoch  27 / iter   3, loss = 0.1217\n",
      "Epoch  27 / iter   4, loss = 0.1854\n",
      "Epoch  28 / iter   0, loss = 0.1366\n",
      "Epoch  28 / iter   1, loss = 0.1235\n",
      "Epoch  28 / iter   2, loss = 0.2096\n",
      "Epoch  28 / iter   3, loss = 0.1363\n",
      "Epoch  28 / iter   4, loss = 0.4803\n",
      "Epoch  29 / iter   0, loss = 0.1351\n",
      "Epoch  29 / iter   1, loss = 0.1827\n",
      "Epoch  29 / iter   2, loss = 0.1458\n",
      "Epoch  29 / iter   3, loss = 0.2210\n",
      "Epoch  29 / iter   4, loss = 0.2581\n",
      "Epoch  30 / iter   0, loss = 0.4465\n",
      "Epoch  30 / iter   1, loss = 0.3929\n",
      "Epoch  30 / iter   2, loss = 0.2920\n",
      "Epoch  30 / iter   3, loss = 0.2161\n",
      "Epoch  30 / iter   4, loss = 0.3525\n",
      "Epoch  31 / iter   0, loss = 0.1493\n",
      "Epoch  31 / iter   1, loss = 0.1151\n",
      "Epoch  31 / iter   2, loss = 0.1622\n",
      "Epoch  31 / iter   3, loss = 0.1364\n",
      "Epoch  31 / iter   4, loss = 0.1627\n",
      "Epoch  32 / iter   0, loss = 0.5252\n",
      "Epoch  32 / iter   1, loss = 0.3260\n",
      "Epoch  32 / iter   2, loss = 0.2147\n",
      "Epoch  32 / iter   3, loss = 0.1605\n",
      "Epoch  32 / iter   4, loss = 0.0830\n",
      "Epoch  33 / iter   0, loss = 0.1704\n",
      "Epoch  33 / iter   1, loss = 0.1438\n",
      "Epoch  33 / iter   2, loss = 0.1165\n",
      "Epoch  33 / iter   3, loss = 0.1253\n",
      "Epoch  33 / iter   4, loss = 0.0719\n",
      "Epoch  34 / iter   0, loss = 0.1903\n",
      "Epoch  34 / iter   1, loss = 0.1027\n",
      "Epoch  34 / iter   2, loss = 0.1688\n",
      "Epoch  34 / iter   3, loss = 0.1471\n",
      "Epoch  34 / iter   4, loss = 0.0805\n",
      "Epoch  35 / iter   0, loss = 0.1933\n",
      "Epoch  35 / iter   1, loss = 0.1794\n",
      "Epoch  35 / iter   2, loss = 0.1394\n",
      "Epoch  35 / iter   3, loss = 0.1677\n",
      "Epoch  35 / iter   4, loss = 0.0625\n",
      "Epoch  36 / iter   0, loss = 0.1789\n",
      "Epoch  36 / iter   1, loss = 0.1667\n",
      "Epoch  36 / iter   2, loss = 0.2818\n",
      "Epoch  36 / iter   3, loss = 0.2933\n",
      "Epoch  36 / iter   4, loss = 0.1278\n",
      "Epoch  37 / iter   0, loss = 0.1574\n",
      "Epoch  37 / iter   1, loss = 0.1117\n",
      "Epoch  37 / iter   2, loss = 0.1501\n",
      "Epoch  37 / iter   3, loss = 0.1638\n",
      "Epoch  37 / iter   4, loss = 0.0979\n",
      "Epoch  38 / iter   0, loss = 0.1392\n",
      "Epoch  38 / iter   1, loss = 0.1074\n",
      "Epoch  38 / iter   2, loss = 0.0772\n",
      "Epoch  38 / iter   3, loss = 0.1751\n",
      "Epoch  38 / iter   4, loss = 0.1255\n",
      "Epoch  39 / iter   0, loss = 0.5037\n",
      "Epoch  39 / iter   1, loss = 0.5653\n",
      "Epoch  39 / iter   2, loss = 0.5047\n",
      "Epoch  39 / iter   3, loss = 0.3235\n",
      "Epoch  39 / iter   4, loss = 0.2484\n",
      "Epoch  40 / iter   0, loss = 0.1606\n",
      "Epoch  40 / iter   1, loss = 0.1180\n",
      "Epoch  40 / iter   2, loss = 0.0945\n",
      "Epoch  40 / iter   3, loss = 0.1172\n",
      "Epoch  40 / iter   4, loss = 0.0791\n",
      "Epoch  41 / iter   0, loss = 0.1935\n",
      "Epoch  41 / iter   1, loss = 0.1284\n",
      "Epoch  41 / iter   2, loss = 0.1398\n",
      "Epoch  41 / iter   3, loss = 0.0754\n",
      "Epoch  41 / iter   4, loss = 0.0769\n",
      "Epoch  42 / iter   0, loss = 0.1374\n",
      "Epoch  42 / iter   1, loss = 0.1346\n",
      "Epoch  42 / iter   2, loss = 0.0926\n",
      "Epoch  42 / iter   3, loss = 0.0865\n",
      "Epoch  42 / iter   4, loss = 0.1846\n",
      "Epoch  43 / iter   0, loss = 0.5332\n",
      "Epoch  43 / iter   1, loss = 0.3765\n",
      "Epoch  43 / iter   2, loss = 0.3266\n",
      "Epoch  43 / iter   3, loss = 0.3930\n",
      "Epoch  43 / iter   4, loss = 0.2343\n",
      "Epoch  44 / iter   0, loss = 0.2857\n",
      "Epoch  44 / iter   1, loss = 0.2720\n",
      "Epoch  44 / iter   2, loss = 0.3051\n",
      "Epoch  44 / iter   3, loss = 0.2971\n",
      "Epoch  44 / iter   4, loss = 0.3953\n",
      "Epoch  45 / iter   0, loss = 0.2027\n",
      "Epoch  45 / iter   1, loss = 0.1314\n",
      "Epoch  45 / iter   2, loss = 0.0538\n",
      "Epoch  45 / iter   3, loss = 0.0762\n",
      "Epoch  45 / iter   4, loss = 0.1873\n",
      "Epoch  46 / iter   0, loss = 0.4173\n",
      "Epoch  46 / iter   1, loss = 0.3177\n",
      "Epoch  46 / iter   2, loss = 0.3160\n",
      "Epoch  46 / iter   3, loss = 0.2396\n",
      "Epoch  46 / iter   4, loss = 0.3560\n",
      "Epoch  47 / iter   0, loss = 0.3933\n",
      "Epoch  47 / iter   1, loss = 0.3114\n",
      "Epoch  47 / iter   2, loss = 0.2985\n",
      "Epoch  47 / iter   3, loss = 0.2313\n",
      "Epoch  47 / iter   4, loss = 0.0354\n",
      "Epoch  48 / iter   0, loss = 0.0652\n",
      "Epoch  48 / iter   1, loss = 0.1261\n",
      "Epoch  48 / iter   2, loss = 0.1535\n",
      "Epoch  48 / iter   3, loss = 0.0803\n",
      "Epoch  48 / iter   4, loss = 0.0185\n",
      "Epoch  49 / iter   0, loss = 0.1274\n",
      "Epoch  49 / iter   1, loss = 0.0964\n",
      "Epoch  49 / iter   2, loss = 0.0750\n",
      "Epoch  49 / iter   3, loss = 0.1060\n",
      "Epoch  49 / iter   4, loss = 0.1201\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xd4XNW97vHvb9R7sWRZlm1kgzEYA7YxpiaBAIlDcmNKkhNICDeNFEi5yUk/91xIzoEkJ+XkXFKAQCAJISGUCxyqIfRiLPdecJWsZvWuKev+sUdCtjUzsorlLd7P8+iZ0Z6tmbU88qs1v73X2uacQ0RE/C8w3g0QEZHRoUAXEZkgFOgiIhOEAl1EZIJQoIuITBAKdBGRCUKBLiIyQSjQRUQmCAW6iMgEkXw0X6yoqMiVl5cfzZcUEfG9lStXHnDOFSfa76gGenl5ORUVFUfzJUVEfM/M9gxlv4QlFzNLN7M3zWytmW00s5ui22ea2XIz22FmfzOz1JE2WkREhm8oNfQe4L3OudOB+cASMzsb+AnwS+fcCUAT8Nmxa6aIiCSSMNCdpz36bUr0ywHvBR6Ibr8HuGxMWigiIkMypLNczCzJzNYAdcAy4C2g2TkXiu5SCZSNTRNFRGQohhTozrmwc24+MA1YDJw01Bcws+vMrMLMKurr64fZTBERSeSIzkN3zjUDzwPnAPlm1neWzDSgKsbP3O6cW+ScW1RcnPCsGxERGaahnOVSbGb50fsZwCXAZrxg/0h0t2uBR8aqkSIikthQRuilwPNmtg5YASxzzv038B3gG2a2A5gE3DlWjXxucy2/eWHHWD29iMiEkHBikXNuHbBgkO078erpY+6FrfU8vr6aL19wwtF4ORERX/LFWi4Bg4guZi0iEpcvAt3MiEQU6CIi8fgi0ANmaIAuIhKfTwJdJRcRkUT8EegBQxUXEZH4fBHophG6iEhCvgh01dBFRBLzSaBrhC4ikohPAt0U6CIiCfgi0M10UFREJBFfBHrAvFunUbqISEw+CXQv0TVKFxGJzSeB7t2qji4iEpsvAt36R+gKdBGRWHwR6H0lF+W5iEhsPgl071YjdBGR2HwS6DooKiKSiC8C3TRCFxFJyCeBrhq6iEgivgh0TSwSEUnMJ4GuGrqISCI+CXTvVjV0EZHYfBHomlgkIpKYLwJdE4tERBLzSaB7txqhi4jEljDQzWy6mT1vZpvMbKOZfS26/UYzqzKzNdGvS8eskTooKiKSUPIQ9gkB33TOrTKzHGClmS2LPvZL59zPxq55nv6JRUp0EZGYEga6c64aqI7ebzOzzUDZWDdsINXQRUQSO6IaupmVAwuA5dFNN5jZOjO7y8wKYvzMdWZWYWYV9fX1w2tktJWqoYuIxDbkQDezbOBB4OvOuVbgt8DxwHy8EfzPB/s559ztzrlFzrlFxcXFw2ukTlsUEUloSIFuZil4YX6vc+4hAOdcrXMu7JyLAHcAi8eqkaaDoiIiCQ3lLBcD7gQ2O+d+MWB76YDdLgc2jH7zPFrLRUQksaGc5XIecA2w3szWRLd9H7jKzOYDDtgNfGFMWohOWxQRGYqhnOXyCmCDPPTE6DdncJpYJCKSmC9mimotFxGRxPwR6NFb5bmISGy+CHSdtigikpg/Aj3aSuW5iEhsvgh01dBFRBLzRaDrtEURkcR8EujerSYWiYjE5pNA1whdRCQRXwS6aWKRiEhCvgh0nbYoIpKYrwJdeS4iEptPAt271QhdRCQ2XwS61kMXEUnMF4GuEbqISGI+CfS+GroCXUQkFl8FeiQyzg0RETmG+SLQdR66iEhivgh0zRQVEUnMH4Hev3yuEl1EJBZ/BLpG6CIiCfki0PsuQacauohIbP4IdK3lIiKSkC8CvW9ikYiIxOaTQNcIXUQkkYSBbmbTzex5M9tkZhvN7GvR7YVmtszMtkdvC8askZpYJCKS0FBG6CHgm865ucDZwPVmNhf4LvCcc2428Fz0+zGhiUUiIoklDHTnXLVzblX0fhuwGSgDlgL3RHe7B7hszBoZ0HroIiKJHFEN3czKgQXAcqDEOVcdfagGKBnVlg2g1RZFRBIbcqCbWTbwIPB151zrwMecN4Vz0LQ1s+vMrMLMKurr64fXSE0sEhFJaEiBbmYpeGF+r3PuoejmWjMrjT5eCtQN9rPOududc4ucc4uKi4uH1UjV0EVEEhvKWS4G3Alsds79YsBDjwLXRu9fCzwy+s3zaD10EZHEkoewz3nANcB6M1sT3fZ94MfA/Wb2WWAP8LGxaaJKLiIiQ5Ew0J1zr/D2ciqHumh0mzM4HRQVEUnMFzNFdZFoEZHEfBHofSN01dBFRGLzSaBrLRcRkUR8Fujj3BARkWOYLwJd56GLiCTmq0BXnouIxOaLQH97+VwluohILL4KdMW5iEhsPgl071Y1dBGR2HwR6JpYJCKSmC8CHbxRuiYWiYjE5qNAN5VcRETi8Fmgj3crRESOXb4JdDMdFBURicc3gR4w08QiEZE4fBTomlgkIhKPjwJdNXQRkXh8E+iqoYuIxOebQA8ETOehi4jE4Z9AV8lFRCQuHwW6Si4iIvH4JtBNI3QRkbh8E+hay0VEJD4fBbrWchERicc3gW5o+VwRkXgSBrqZ3WVmdWa2YcC2G82syszWRL8uHdtm9tXQlegiIrEMZYR+N7BkkO2/dM7Nj349MbrNOlwggK5BJyISR8JAd869BDQehbbEpRq6iEh8I6mh32Bm66IlmYJRa1EMmlgkIhLfcAP9t8DxwHygGvh5rB3N7DozqzCzivr6+mG+nNZyERFJZFiB7pyrdc6FnXMR4A5gcZx9b3fOLXLOLSouLh5uO7UeuohIAsMKdDMrHfDt5cCGWPuOFk39FxGJLznRDmZ2H3ABUGRmlcD/AS4ws/l4553sBr4whm0EdFBURCSRhIHunLtqkM13jkFb4tJaLiIi8flmpqjWchERic9Hga4RuohIPD4KdB0UFRGJxzeBrhq6iEh8vgl01dBFROLzUaDrtEURkXj8FeiR8W6FiMixyzeBrrVcRETi81WgK89FRGLzTaCrhi4iEp8CXURkgvBNoJvpCnQiIvH4JtA19V9EJD4fBbomFomIxOOjQFcNXUQkHt8EumlikYhIXL4JdK22KCISn48CXReJFhGJxz+BHtAIXUQkHt8EuumgqIhIXL4JdJVcRETi81Ggq+QiIhKPjwJdM0VFROLxTaBrPXQRkfh8E+iqoYuIxJcw0M3sLjOrM7MNA7YVmtkyM9sevS0Y22aqhi4ikshQRuh3A0sO2fZd4Dnn3Gzguej3Y0pruYiIxJcw0J1zLwGNh2xeCtwTvX8PcNkot+swXg19rF9FRMS/hltDL3HOVUfv1wAlo9SemMxMy+eKiMQx4oOizkvZmElrZteZWYWZVdTX1w/7dQK6SLSISFzDDfRaMysFiN7WxdrROXe7c26Rc25RcXHxMF9ONXQRkUSGG+iPAtdG718LPDI6zYlNE4tEROIbymmL9wGvA3PMrNLMPgv8GLjEzLYDF0e/H1OaWCQiEl9yoh2cc1fFeOiiUW5LXJpYJCISn49mimqELiISj48CXQdFRUTi8U2gmw6KiojE5ZtA985DV6KLiMTio0DXCF1EJB4fBboOioqIxOObQLfoaYsqu4iIDM43gR4wA7Sei4hILD4KdO9WZRcRkcH5J9Cjia4DoyIig/NNoJtG6CIicfkn0FENXUQkHt8EumroIiLx+SjQ+2roCnQRkcH4JtD7auiKcxGRwfkm0PvPQ4+Mc0NERI5RPgp071YlFxGRwfkn0AOqoYuIxOObQDfTxCIRkXh8E+h9JRctziUiMjgfBbpG6CIi8fgo0L1b1dBFRAbnm0A3TSwSEYnLN4Gu9dBFROLzUaB7t2EV0UVEBpU8kh82s91AGxAGQs65RaPRqMFkpXlNbe8JjdVLiIj42ogCPepC59yBUXieuAqzUgFo7Ogd65cSEfEl35RcCjK9QG/qVKCLiAxmpIHugGfMbKWZXTfYDmZ2nZlVmFlFfX39sF9okkboIiJxjTTQz3fOLQQ+AFxvZu8+dAfn3O3OuUXOuUXFxcXDfqHcjBQCBk0KdBGRQY0o0J1zVdHbOuBhYPFoNGowSQEjPzOVRpVcREQGNexAN7MsM8vpuw+8D9gwWg0bTEFmCk0dwbF8CRER3xrJWS4lwMPRGZzJwF+cc0+NSqtiKMxKVQ1dRCSGYQe6c24ncPootiWhgsxU9jZ2Hs2XFBHxDd+ctgjeCL1BI3QRkUH5KtALslJp6ugd1proL26r56rb3yAU1kVJRWRi8lWgF2amEoo42oYx/X/5zgZe39mgko2ITFi+CvSC6OSipo5edtS1s76yZcg/29zlnR2zva59TNomIjLefBXohVkpgDdb9N8e38R3Hlw35J9t6fQCfYcCXUQmqNFYnOuoKcxKA7z1XPY2dtLePfTSS98aMAp0EZmofBXofeu51LX2sL+5i0jEu2h039WM4mnu7Cu5tI1pG0VExouvSi5T8zNISTJW722mOxihNxyhozc8pJ9t6Xq75BLRRTJEZALyVaAnBYzjJmXxyo63l18f6mJdzZ295KQn0x2MUNXcNVZNFBEZN74KdICZRVkHBXJfKSWe3pA3kl9cXgjAhqq3z47Z19jJN+9fS09oaCN9EZFjle8CfVZR1kHfD2X1xeYub5/zTigiIyWJ5bsa+x97YWsdD66q1MFSEfE93wX6zEMCvfmQQL/rlV2cc8tz3PzE5v4LSvedsliUk8YZxxXwxs6G/v1rW3sAaGjXkgIi4m++DfSCTO+c9ENr6M9sqqG6pZvbX9rZf0ZL36SigswUzppZyNbatv4/BLWt3YCuhCQi/ue/QC/2Av3k0lwAGgfU0CMRx4aq1v7Halq8sO6rs+dnpHLWrEk4B29Gyy410UA/0N5zdDogIjJGfBfoxdlp5GWkMKs4i7yMlP6R9vrKFjbXtNLeE+KSuSXA24HeN6koPzOF06fnkRQw1lY2A9457aARuoj4n68mFgGYGfd+7ixKctN5ZfsBmjqD7GvsZOmvX6EkNx2Ai06azH89t71/9N1XQ8/LTCEtOYnpBRnsPuAt0lXb5u2jGrqI+J3vAh1gXlkeAPmZqTR39nJ/xT4iDqpbuklNDjB3ai5F2anUtnazo66N3Q0dJAWMnDSvuzOLsth5oIPuYLi/HKN11kXE73wZ6H0Ks1LZ39zF/RX7OH16Ppv3e/XzlKQAJbnpVLd089HfvU5TZ5CMlKT+JQLKi7J4Y2djf7kFoKFDNXQR8TdfB3p+Zgr/2FIHwA+XzqMnFCE/wzv7ZUpuOit2N9IaXcCrK/j2xKFZRVl0BcOsq2rufx7V0EXE73wd6IHoiHvBjHwuObmEQODtRbpK8tL7w/xz589kVnF2/2Pl0VMf+85Hn1uay7ojWFtdRORY5LuzXAaaXpAJwI+WzjsozMEboQMkB4xvLZnD1WfN6H9sZn+ge6cuzi3Npb0nRHdw4k3/r2vr5owfLePFbfXj3RQRGWO+DvQvvGcWL37rgv6DpAP1BfrskhzSkpMOemxqXgapyQF21LWTmhzoH70fWnZxzvHTp7awZl/zEbUrEnF09h75ZfLGwj8219HQ0cvfK/aNd1NEZIz5OtDTU5I4blLWoI+V5HmBfsrU3MMeCwSME4qzMfPKMZOyvXXW39zVeNDM050HOvjNC2/xhT9VDLnG3tod5ON3vME5t/yDVXubDnu8qaOXHz62iU37W+M+T1NHL4+t3T+sC2IP9MLW+v7bRAuQhSOOP76+mwZNshLxJV8HejxTo4E+b5BAB/jNJxby7Dfew7eXnNR/4Yyv/20N7/vPl3h6Yw09oTDLoyWZA+29fOvva3HOsbO+nYt+/gKfvXsFj63dz4rdjTy0qhLnHM45Pn9PBav2NJGZmsQnf7+8/7qn96/Yxy1PbuanT2/lrld38eFbX+HZTbUx2/+fz27jK/et5sdPbQHg7xX7eHpjzRH9GwTDEV7dcYDjJmXS3hPitbca4u7/1IYa/vWRjdz02KYjep1YGtp7WLnn8D9qiYS1Xn1c1S1dBMOR8W7GkOxp6OD3L+/s/z812mpauqmLzjeRER4UNbMlwK+AJOD3zrkfj0qrRsEJk7P52UdP5wPzpgz6ePmARb5K8zO827x0MlKT+MKfVlKWn8GJJdkUZadx/YXHc9Njm/i3xzfz8vZ6DrT30h2M8JX7Vvc/xxs7G1g8cxLLdzXy75fP4+KTS7jiN6/xuT+u4IEvnsvNT27uP+f9I2dM481djfzhtV2cUpbLm7sa6Q1FeGZTLXNLc7nmnONYtqmWzNQkbntxJ87B71/eSWpygB9cejIPrKxkybxSIs5xfHE2F588mS01bXz67hX8j9OmcvmCMnrDYR5fV0NbT4ibrziV7z+8nq/dt5oZkzJJS07it59YyORoWcr7YwS/e/EtzODRtfv55NnHsXhm4WH/bgfae6ht7eaUqYeXuQaKRByf+2MFq/c2c83Zx/GtJXPITU9J+L69tK2e6/+yips+fApXLJyWcP/RUNXcxc+e3soJk7O5/sITRv35f//yTp7ZWMu9nz+LlKSDx1D7Gjv56dNbmVuay5ULy/rfk2A4cti+AJVNnVz08xdZVF7AXf/zzMPKiceSlXsa+dhtb/T/gT5pSg5/+fzZFEYHUCPVEwpz5W9fo607yK+uWsCZ5YWsq2xm/vR8MlMPj7b1lS1kpiVx/IATJPpUNXexbGMNV5wxrf/3NBJxbKpupaalm/NnF5Gecuz+W/ex4X6kN7MkYBtwCVAJrACucs7FHN4tWrTIVVRUDOv1xtqGqhZOLMnB4XhmYy1f/etqnIMPnlrKrVcv4Et/XsVTG2tITQrwh0+fyTmzJvGPLXW09QR5q66DW5/fAXi/tI9/9V0kBYwtNa1c9utXKchMpbqlm4tPnsxb9R08+KVzufvVXdz6/A5OLMlhS423iNjknDTq23uYmpdBVXMXP7nyVB5bW80rOw6Qm55MMOzoCobJz0w5aB3406fnk5eRwvKdDQTDEfoGuGZw0Ukl3Hr1ArbXtvOHV3dR395Dxe4mphVkEHGO7mCEtu4gnb1hQhHHv3zwZO56ZRfNXUFOmZpLbWsPN19+KufPLuLh1ZX86yMbaesO8YmzZvC/LjmR7mCYlXua6A6GOTl6cPnbD6wjNz2FTdWtvGt2ES9vP0BeRgofOq2Uz5w/k4yUJP74+h4a2nv49HkzmRv9FLV6bxNX3fEG3cEI+ZkpPPm1d3HPa3t4dnMtP77iVBZF17Pv7A0RcZCdlkxNSzdNnb3MLMoa9D9cc2cvjR29ZKUlc/tLO7no5Mmce3xR/+PrK1u4+o43aOsJETB49IbzDzoms3xnA8t3NVKUncbFcyczOSd9yL9TXb1h2ntCXPAfz9PRG+bmy0/tPzjf0hXkzV2N/PC/N1Lb0kNvOEJmahLXnltOc2eQh1ZV8rtrzuDCOZMPes7vPbSev1fsIxRxnHv8JP798lMPW4H0SMS7hGNdq7fIXXVLNx84dQofOm3qET33p+56k41VLTz85fNYV9XMN+9fy5S8dK6/8ASWzp864j9Gd7+6ixsf20RpnjfnxAyc805yuHhuCXWt3rZdBzroDkZYs6+ZnPRk/nbdOf2/cwAdPSGu+M1rbK1tIyctmcm5aZw1axIbqlr6z34ryEzhxJIc5k/P5+qzZsQs9Y4VM1vpnFuUcL8RBPo5wI3OufdHv/8egHPullg/cywH+qG+et9qHl27nx8uPYVPnVNOJOLY09hJfkYKBYOMMCp2N/Lgqio+fuZ0Tp+e37/9thff4pYnt1CWn8HL374QM2/5gm21bbzvly8B8C8fPJnFMwuZNzWPJzZUc8NfVhMwWPGDi4k4+MzdK/j0eeWEIo7nNtfy04+cTlt3kJz0FJ7eWMN3HlyHc/CNS07kioVlrK9sITkpwPzp+RTnpB3W1ifXV/OV+1ZzZnkhpfnpZKUmk5WWTCgc4Z/fP4fGjl6+8+A6Kpu6MIPdBzq46OQSlm2qZXF5IXOn5nLP67sxYLDqyLSCDDp7w5xalsfdnz6Tjftb+e0Lb/H81jqy0pLJSU9mX2MnqUkBesMRctJTOLEkm10HOkhNDvCTK0/jU3e+SSj65AWZKbR0BZlZlEVWWjJba9oImPHB00p5cFUlznkHwd9/infq6mXzy/jDq7vY39zNmspmekMRUpKMYNh7vtSkAJNz0zhn1iSe3VxLZmoyt11zBtfe9SZmUJqXwWnT8mjtDvHY2v0H9W1Kbjrnzy7ihMnZvLL9AMt3NfDekyYzvSCTN3Y10NoVYuGMfEIRx1MbakhJCtATCjOrOJva1m4yU5OYnJPO9ro2uoMR8jJS+ONnFpObkcLPntnKk+uriUT709IV5GOLplHT2k1zZ5C8DG/exdVnzeCUqbn88LFNdPSGOW1aHpeeWspL2+rpDoaZWZTNlLw0Ig5OKM7mz8v3EAo7TpuWx6q9zRTnpHHu8ZNYtqmWNfuaKclJY3ZJDk2dvbxrdhEN7b28sLWe1u4gobAjPzOFurYeLpxTTNh5g5Y9DR3UtPZQlp/OKVPz2NvQyYxJmcwsyuLxddXsaexgQ1Ur314yhy9f4H3qef2tBm56bCNbatooyk7jvScV908E7OgJkZYc4LaXdlKck8ZJU3LITE3mvSdNJmDGi9u8g/vF2WnMnZpLS1eQnz+zjZNLc7jz2jN5emMN2+vaKcvP4JYnNtMVDDMpO41wxDGzKAsDzj1+Eg+srKSpM8hlC6byT2fOIByJcMsTW1i1t4kbP3wKG6taaejo5eXt9eSkJ/PN981hSl46j6yuYl9TF2v3NZORksSfP3cWSQFj2aZattS0UpiVxpULy5hZlMW9y/eSlhxgZlEWATN2N3TQ1NnLR8+YflBl4EgcjUD/CLDEOfe56PfXAGc5526I9TN+CvR9jZ18/+H1/MdHTmdK3tBHZYcKhSN8+d5VXHjSZK5aPOOgxz70f18mNSnAA18896DTLr/30Ho6e0P86uMLhvQad7y0k7+u2MtDXz6PvIzEZQ3wruKUmpz4EEpHT4ibn9jMX97cy3tOLOa2a84gLTmJt+rbeWBlJaV56SycUUB2WjKbqlupbe3migXTyErzZuYmDejXtto2rvzNa3SHwvz5s2dxYkkOtz6/g46eEM9sqqWzN8RDXzqPuVO9MtTKPU2cWJLNovJC/vDqLjbtb6U3HKF8UhYbqlqo2NPE0vlTuXDOZP7w6i621bYTjrj+0e68sjzmlOQwsyiL7XVtXHN2Oct3NVDT2s3GqlbWV7VwalkeP7psHjOLsnh2Uy1/Xr6HYDjCun0tpKUE+NBpU/nn98+hurmLZZtr2VzdxvNb6mjvCTE1L53zTijixW1e+M2ZkktJThobqlroCoZZMq+UyqZOFswo4MI5xXzvofWcWJLDgfYejpuUxeULyji1LI+M1LdHqlXNXXT2hMjLTOGb969l5Z4mCrNSKYkG/JySHG5aegpF2WnUtnbz4KpKHl2zny01bZTlZzC90FunqK6tm4AZoYhjck4a2enJVDZ1sbi8kPq2HrbWtjEpK5UrFpZR19bDttp2slKTqNjTRFLAWDJvCrnpyXz+XbMoK8jgf/+/Dbyxs5HM1CS21bZRPimLqfkZbK72ArAwK7X/xIHstGROmpJDe0+Iv3/xHHIGlNqcc7yy4wB/fmMPy3c1HnbFsdOn5+Oco7a1m5auIN1B71hBalKA4pw06tu8TzMA86fn84uPnX7QHBPwPv1EIm7Qgde+xk5u/ccOHllb1f/cuenJ3LT0FC5f8HaJr70nRHLADvvUt6+xk3+67XX2Rxf+C5h3GvT+5m66gmGSAjboMaCkgHHntYu44JBPXEN1zAS6mV0HXAcwY8aMM/bs2TOs15uIWruDJAds0HrfkYr30Xk0VLd0UZydRvIgdd0jsbWmjbbuYH/5pE9rd5DWriDTonMLEgmGI2ypbmNeWW5/v51z7Gno5L4Ve7nqzBnDHg0l0hMKEww7stOOjXl5zjl2N3QyrSCjv+7unPeHbeP+Vk6YnE12ajKhiOv/I76vsZOCrNTD+rChqoW05ACzS3Jivl4k4voHIMFwhM6eMHmZKbR1B9lZ38GMwsxBw3Swdjd09BIKO9KSA1Q1dzG3NLf/uTt6Qqze650yfNr0PHLTU+gOhtnT4C2sd2JJ9rB/51u6gjy3uZb0lCTOnjXpiOr6exs6eWRNFdMKM3j37GImZafR3hPiyfXVbKhq4ZNnH8fk3HR2H+ggFHEcH10ZdiT/P1VyERGZIIYa6CMZbq0AZpvZTDNLBT4OPDqC5xMRkREY9mdG51zIzG4AnsY7bfEu59zGUWuZiIgckREVAZ1zTwBPjFJbRERkBCbsTFERkXcaBbqIyAShQBcRmSAU6CIiE4QCXURkghj2xKJhvZhZPTDcqaJFwIFRbI4fqM/vHO/EfqvPQ3ecc6440U5HNdBHwswqhjJTaiJRn9853on9Vp9Hn0ouIiIThAJdRGSC8FOg3z7eDRgH6vM7xzux3+rzKPNNDV1EROLz0whdRETi8EWgm9kSM9tqZjvM7Lvj3Z6xYma7zWy9ma0xs4rotkIzW2Zm26O3BePdzpEws7vMrM7MNgzYNmgfzfNf0fd9nZktHL+WD1+MPt9oZlXR93qNmV064LHvRfu81czePz6tHhkzm25mz5vZJjPbaGZfi26fsO91nD4fvffau+L7sfuFtzTvW8AsIBVYC8wd73aNUV93A0WHbPsp8N3o/e8CPxnvdo6wj+8GFgIbEvURuBR4EjDgbGD5eLd/FPt8I/DPg+w7N/o7ngbMjP7uJ413H4bR51JgYfR+Dt4F5edO5Pc6Tp+P2nvthxH6YmCHc26nc64X+CuwdJzbdDQtBe6J3r8HuGwc2zJizrmXgMZDNsfq41Lgj87zBpBvZqVHp6WjJ0afY1kK/NU51+Oc2wXswPs/4CvOuWrn3Kro/TZgM1DGBH6v4/Q5llF/r/0Q6GXAvgHfVxL/H8nPHPCMma2MXosVoMQ5Vx29XwOUjE/TxlSsPk709/6GaHnhrgGltAnR9/AOAAABsElEQVTXZzMrBxYAy3mHvNeH9BmO0nvth0B/JznfObcQ+ABwvZm9e+CDzvucNqFPS3on9DHqt8DxwHygGvj5+DZnbJhZNvAg8HXnXOvAxybqez1In4/ae+2HQK8Cpg/4flp024TjnKuK3tYBD+N9/Krt++gZva0bvxaOmVh9nLDvvXOu1jkXds5FgDt4+6P2hOmzmaXgBdu9zrmHopsn9Hs9WJ+P5nvth0B/R1yM2syyzCyn7z7wPmADXl+vje52LfDI+LRwTMXq46PAp6JnQJwNtAz4uO5rh9SHL8d7r8Hr88fNLM3MZgKzgTePdvtGyswMuBPY7Jz7xYCHJux7HavPR/W9Hu8jw0M8enwp3hHjt4AfjHd7xqiPs/COeK8FNvb1E5gEPAdsB54FCse7rSPs5314HzuDeDXDz8bqI94ZD7+Ovu/rgUXj3f5R7POfon1aF/2PXTpg/x9E+7wV+MB4t3+YfT4fr5yyDlgT/bp0Ir/Xcfp81N5rzRQVEZkg/FByERGRIVCgi4hMEAp0EZEJQoEuIjJBKNBFRCYIBbqIyAShQBcRmSAU6CIiE8T/B7e5KIhkMi0EAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        # np.random.seed(0)\n",
    "        self.w1 = np.random.randn(num_of_weights, num_of_weights)\n",
    "        self.b1= 0.\n",
    "        self.w2 = np.random.randn(num_of_weights, 1)\n",
    "        self.b2 = 0.\n",
    "\n",
    "    def forward(self, x):\n",
    "        z1 = np.dot(x, self.w1) + self.b1\n",
    "        z1relu=np.maximum(z1,0)\n",
    "        z =np.dot(z1relu,self.w2) + self.b2\n",
    "        return z,z1relu,z1\n",
    "\n",
    "    def loss(self, z, y):\n",
    "\n",
    "        error = z - y\n",
    "\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "\n",
    "    def gradient(self, x, y):\n",
    "        z,z1relu,z1= self.forward(x)\n",
    "        N = x.shape[0]\n",
    "        gradient_w2 = 1. / N * np.sum((z - y) * z1relu, axis=0)\n",
    "        gradient_w2 = gradient_w2[:, np.newaxis]\n",
    "        gradient_b2 = 1. / N * np.sum(z - y)\n",
    "        gradient_w1 = 1. / N * np.sum((z - y) * z1relu * x, axis=0)\n",
    "        gradient_w1 = gradient_w1[:, np.newaxis]\n",
    "        gradient_b1 = 1. / N * np.sum((z - y)*(y-z1relu))\n",
    "        \n",
    "        return gradient_w1, gradient_b1,gradient_w2,gradient_b2\n",
    "\n",
    "    def update(self, gradient_w1, gradient_b1,gradient_w2 ,gradient_b2,eta=0.01):\n",
    "        self.w1 = self.w1 - eta * gradient_w1\n",
    "        self.b1 = self.b1 - eta * gradient_b1\n",
    "        self.w2 = self.w2 - eta * gradient_w2\n",
    "        self.b2 = self.b2 - eta * gradient_b2\n",
    "    def train(self, training_data, num_epochs, batch_size=10, eta=0.01):\n",
    "        n = len(training_data)\n",
    "        losses = []\n",
    "        for epoch_id in range(num_epochs):\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\n",
    "            # 然后再按每次取batch_size条数据的方式取出\n",
    "            np.random.shuffle(training_data)\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\n",
    "            mini_batches = [training_data[k:k + batch_size] for k in range(0, n, batch_size)]\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\n",
    "                # print(self.w.shape)\n",
    "                # print(self.b)\n",
    "                x = mini_batch[:, :-1]\n",
    "                y = mini_batch[:, -1:]\n",
    "                a ,a1,a2= self.forward(x)\n",
    "                loss = self.loss(a, y)\n",
    "                gradient_w1, gradient_b1,gradient_w2,gradient_b2 = self.gradient(x, y)\n",
    "                self.update(gradient_w1, gradient_b1,gradient_w2,gradient_b2, eta)\n",
    "                losses.append(loss)\n",
    "                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\n",
    "                      format(epoch_id, iter_id, loss))\n",
    "\n",
    "        return losses\n",
    "\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data,min,max= load_data()\n",
    "\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "# 启动训练\n",
    "losses = net.train(train_data, num_epochs=50, batch_size=100, eta=0.1)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(len(losses))\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "观察上述Loss的变化，随机梯度下降加快了训练过程，但由于每次仅基于少量样本更新参数和计算损失，所以损失下降曲线会出现震荡。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "由于房价预测的数据量过少，所以难以感受到随机梯度下降带来的性能提升。\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取倒数第十行数据测试实际值为 19.7 预测值为 [1.80989304]\n"
     ]
    }
   ],
   "source": [
    "x=test_data[-10,-1]\r\n",
    "pred,pred1,pred2=net.forward(test_data[-10,:-1])\r\n",
    "repred=(pred*max)-(min*(pred-1))\r\n",
    "retest=(x*max)-(min*(x-1))\r\n",
    "print(\"取倒数第十行数据测试实际值为\",retest,\"预测值为\",repred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "取一行数据进行测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 总结\n",
    "\n",
    "本节我们详细介绍了如何使用Numpy实现梯度下降算法，构建并训练了一个简单的线性模型实现波士顿房价预测，可以总结出，使用神经网络建模房价预测有三个要点：\n",
    "\n",
    "* 构建网络，初始化参数$w$和$b$，定义预测和损失函数的计算方法。\n",
    "* 随机选择初始点，建立梯度的计算方法和参数更新方式。\n",
    "* 从总的数据集中抽取部分数据作为一个mini_batch，计算梯度并更新参数，不断迭代直到损失函数几乎不再下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 作业1-2\n",
    "\n",
    "1. 样本归一化：预测时的样本数据同样也需要归一化，但使用训练样本的均值和极值计算，这是为什么？\n",
    "\n",
    "2. 当部分参数的梯度计算为0（接近0）时，可能是什么情况？是否意味着完成训练？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 作业 1-3\n",
    "\n",
    "1. 随机梯度下降的batchsize设置成多少合适？过小有什么问题？过大有什么问题？提示：过大以整个样本集合为例，过小以单个样本为例来思考。\n",
    "1. 一次训练使用的配置：5个epoch，1000个样本，batchsize=20，最内层循环执行多少轮？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 作业1-4\n",
    "\n",
    "#### 基本知识\n",
    "\n",
    "**1. 求导的链式法则**\n",
    "\n",
    "链式法则是微积分中的求导法则，用于求一个复合函数的导数，是在微积分的求导运算中一种常用的方法。复合函数的导数将是构成复合这有限个函数在相应点的导数的乘积，就像锁链一样一环套一环，故称链式法则。如 **图9** 所示，如果求最终输出对内层输入（第一层）的梯度，等于外层梯度（第二层）乘以本层函数的梯度。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/2beffa3f3d7c402685671b0825561a91c17216fe8b924f64b9f29a96f45cbc85\" width=\"200\" hegiht=\"\" ></center>\n",
    "<center><br>图9：求导的链式法则</br></center>\n",
    "<br></br>\n",
    "\n",
    "**2. 计算图的概念**\n",
    "\n",
    "（1）为何是反向计算梯度？即梯度是由网络后端向前端计算。当前层的梯度要依据处于网络中后一层的梯度来计算，所以只有先算后一层的梯度才能计算本层的梯度。     \n",
    "\n",
    "（2）案例：购买苹果产生消费的计算图。假设一家商店9折促销苹果，每个的单价100元。计算一个顾客总消费的结构如 **图10** 所示。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/46c43ead4fa942f5be87f25538a046ff9456516816274cbcb5f6df3768c0fd34\" width=\"400\" hegiht=\"40\" ></center>\n",
    "<center><br>图10：购买苹果所产生的消费计算图</br></center>\n",
    "<br></br>\n",
    "\n",
    "*  前向计算过程：以黑色箭头表示，顾客购买了2个苹果，再加上九折的折扣，一共消费100\\*2\\*0.9=180元。\n",
    "*  后向传播过程：以红色箭头表示，根据链式法则，本层的梯度计算 * 后一层传递过来的梯度，所以需从后向前计算。\n",
    " \n",
    "最后一层的输出对自身的求导为1。导数第二层根据 **图11** 所示的乘法求导的公式，分别为0.9\\*1和200\\*1。同样的，第三层为100 * 0.9=90，2 * 0.9=1.8。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/c251a2c290e946f99ce3a3381396c392b50e5a4243c346509bd91177b7f2da90\" width=\"200\"  ></center>\n",
    "<center><br>图11：乘法求导的公式</br></center>\n",
    "<br></br>\n",
    "\n",
    "#### 作业题\n",
    "\n",
    "1. 根据 **图12** 所示的乘法和加法的导数公式，完成 **图13** 购买苹果和橘子的梯度传播的题目。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/4ce8715f03f9477699707056544b1e6363f78aa09fda411d972878abb6d1d26f\" width=\"300\"  ></center>\n",
    "<center><br>图12：乘法和加法的导数公式</br></center>\n",
    "<br></br>\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/2fc6665e10f34f9e863172bb399862319f0914467d72457d9e7328616bdbe6df\" width=\"500\"  ></center>\n",
    "<center><br>图13：购买苹果和橘子产生消费的计算图</br></center>\n",
    "<br></br>  \n",
    "\n",
    "2. 挑战题：用代码实现两层的神经网络的梯度传播，中间层的尺寸为13【房价预测案例】（教案当前的版本为一层的神经网络），如 **图14** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/580f2553aa4643809006f5a8d3deb2aa8dd4e1aa69d94cf6a35ead5fe7cf469e\" width=\"300\"  ></center>\n",
    "<center><br>图14：两层的神经网络</br></center>\n",
    "<br></br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
