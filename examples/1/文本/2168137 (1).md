**深度学习基础知识**（文中图片均已放置在image文件夹下）

**第一题：归一化方法详解——群组归一化**
**1.概念：**
群组归一化方法2018年由吴育昕和何恺明提出，目的是解决批量归一化方法在小批量环境下图像分类准确度低的问题。如图1(b)所示群组归一化固定特征图像批量轴大小不变，将特征图像通道轴分成G个群组，G是预先设定的超参数，图1中选择G=2，即将特征图像通道分为两个群组，通过调整G的数值调整特征图像通道，再结合宽高轴共同求取特征图像均值和方差，通过群组归一化求取特征图像的均值与方差如图2所示。

其中⎣·」表示下截断取整操作，群组归一化沿着H,W轴和C/G通道方向计算特征图像的均值和方差，当G=C时群组归一化就变成了实例归一化，但是实例归一化仅依靠空间维度(H，W)计算均值和方差，并没有探索改变通道对于归一化的影响。群组归一化方法同层次归一化方法类似，都进行了通道维度的探索，不同于层次归一化方法用于生成文本，群组归一化在视觉任务中的表现更好。

**2.算法流程：**
我们通过以下四种归一化方法对比来进行算法流程展示，BN，LN，IN，GN从学术化上解释差异如下：
BatchNorm：batch方向做归一化，算N*H*W的均值
LayerNorm：channel方向做归一化，算C*H*W的均值
InstanceNorm：一个channel内做归一化，算H*W的均值
GroupNorm：将channel方向分group，然后每个group内做归一化，算(C//G)*H*W的均值
GN具体的代码实现过程如图3。

**3.作用：**
**（1）归一化的作用：**
不同特征往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，为了消除指标之间的量纲影响，需要进行数据归一化处理，以解决数据指标之间的可比性。原始数据经过数据归一化处理后，各指标处于同一数量级，适合进行综合对比评价。
归一化的作用就是使得预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]），从而消除奇异样本数据导致的不良影响。
**（2）GN的作用：**
LN 和 IN 在视觉识别上的成功率都是很有限的，对于训练序列模型（RNN/LSTM）或生成模型（GAN）很有效。
所以，在视觉领域，BN用的比较多，GN就是为了改善BN的不足而来的。
GN 把通道分为组，并计算每一组之内的均值和方差，以进行归一化。
GN 的计算与批量大小无关，其精度也在各种批量大小下保持稳定，可以看到，GN和LN很像。

**4.应用场景：**
归一化经常用于确保数据点没有因为特征的基本性质而产生较大差异，即确保数据处于同一数量级（同一量纲），提高不同特征数据的可比性。
概率模型（树形模型）往往不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。而像Adaboost、SVM、LR、Knn、KMeans之类的最优化问题就需要归一化。


