## 深度学习发展历史
1943年由神经科学家麦卡洛克(W.S.McCilloch) 和数学家皮兹（W.Pitts）在《数学生物物理学公告》上发表论文《神经活动中内在思想的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity）。建立了神经网络和数学模型，称为MCP模型。所谓MCP模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，也就诞生了所谓的“模拟大脑”，人工神经网络的大门由此开启。 
1969年，美国数学家及人工智能先驱 Marvin Minsky 在其著作中证明了感知器本质上是一种线性模型（linear model），只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了将近20年的停滞。
1986年由神经网络之父 Geoffrey Hinton 在1986年发明了适用于多层感知器（MLP）的BP（Backpropagation）算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。
1991年BP算法被指出存在梯度消失问题，也就是说在误差梯度后项传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该问题直接阻碍了深度学习的进一步发展。
2006年，加拿大多伦多大学教授、机器学习领域泰斗、神经网络之父—— Geoffrey Hinton 和他的学生 Ruslan Salakhutdinov 在顶尖学术刊物《科学》上发表了一篇文章，该文章提出了深层网络训练中梯度消失问题的解决方案：无监督预训练对权值进行初始化+有监督训练微调。
2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，AlexNet的创新点在于:
(1)首次采用ReLU激活函数，极大增大收敛速度且从根本上解决了梯度消失问题。
(2)由于ReLU方法可以很好抑制梯度消失问题，AlexNet抛弃了“预训练+微调”的方法，完全采用有监督训练。也正因为如此，DL的主流学习方法也因此变为了纯粹的有监督学习。
(3)扩展了LeNet5结构，添加Dropout层减小过拟合，LRN层增强泛化能力减小过拟合。
(4)第一次使用GPU加速模型计算。

## 人工智能、机器学习、深度学习有什么区别和联系？

![](https://ai-studio-static-online.cdn.bcebos.com/43038d2ada5444e18e0ca6355da297a9a78cb9ecebe4491a991a903b3030bfe7)
 
人工智能指机器获得人类一样的智能来解决各类问题，而机器学习和深度学习都是实现人工智能的算法。机器学习现在一般指传统的机器学习方法，包括决策树、聚类、贝叶斯分类、支持向量机、EM、Adaboost等等。深度学习指利用深度神经网络来训练模型使机器完成任务。
神经元、单层感知机、多层感知机。
神经元：神经元是一种处理单元，是对人脑组织的神经元的某种抽象、简化和模拟。是人工神经网络的关键部分。通过神经元，人工神经网络可以以数学模型模拟人脑神经元活动，继而进行高效的计算以及其他处理。

## 神经元、单层感知机、多层感知机。
**神经元**：神经元是一种处理单元，是对人脑组织的神经元的某种抽象、简化和模拟。是人工神经网络的关键部分。通过神经元，人工神经网络可以以数学模型模拟人脑神经元活动，继而进行高效的计算以及其他处理。

![](https://ai-studio-static-online.cdn.bcebos.com/60e156d46a884fc4971e0c49246d9c3b0f50d4dc37cd4535a1139f352954e0c1)

**单层感知机**：单层感知机是二分类的线性分类模型，输入是被感知数据集的特征向量，输出时数据集的类别{+1,-1}。单层感知机的函数近似非常有限，其决策边界必须是一个超平面，严格要求数据是线性可分的。

![](https://ai-studio-static-online.cdn.bcebos.com/390616723bd948278f3ebbcf3b0319bf89ada8169b05456fab81a6728724071f)
 
**多层感知机**：多层感知机在单层神经网络的基础上引入了一到多个隐藏层 (hidden layer)。隐藏层位于输入层和输出层之间，以下图为例，它含有一个隐藏层，该层包含5个隐藏单元 (hidden unit)：
 
 ![](https://ai-studio-static-online.cdn.bcebos.com/e1866315c7ea4ad9bfcdfba02c3d74fede6fe99bdc01427eb835750935de9e37)
 
## 什么是前向传播？
假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项，最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。

![](https://ai-studio-static-online.cdn.bcebos.com/9b6eaf3e587c409490ebf2b4d1855ae5703529d42d094c0399e67b1aaafa2627)

![](https://ai-studio-static-online.cdn.bcebos.com/053c17776834461988b88f9afa372bd9c7170bf765d44fefa22f5505cc7f1218)
 
 
## 什么是反向传播算法？
先进行神经网络的前向传播，最后输出的y就是本次前向传播神经网络计算出来的结果（预测结果），但这个预测结果不一定是正确的，要和真实的标签（z）相比较，计算预测结果和真实标签的误差（ δ ），然后通过误差反向传播修改各神经元的权重。
 
 
 ![](https://ai-studio-static-online.cdn.bcebos.com/d90608046dfe4e29ad23dee12cb1a572f5377d2bcd314ee88a8bcd459cc7153a)

![](https://ai-studio-static-online.cdn.bcebos.com/957a3950b182457aad9b5eb1d4d1b8c2114a60b2bf354d64be5e371a9705f402)
 



