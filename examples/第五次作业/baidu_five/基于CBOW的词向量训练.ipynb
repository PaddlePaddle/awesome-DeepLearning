{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data98805\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW_word2vec.pdmodel\r\n"
     ]
    }
   ],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "# !mkdir /home/aistudio/external-libraries\n",
    "# !pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "# import sys \n",
    "# sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import io\r\n",
    "import sys\r\n",
    "import requests\r\n",
    "from collections import OrderedDict\r\n",
    "import math\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "from paddle.nn import Embedding\r\n",
    "import paddle.nn.functional as F\r\n",
    "import paddle.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def download():\r\n",
    "#     corpus_url='https://dataset.bj.bcebos.com/work2vec/text8.txt'\r\n",
    "#     web_request=requests.get(corpus_url)\r\n",
    "#     print(web_request)\r\n",
    "#     corpus=web_request.content\r\n",
    "#     with open(\"./text8.txt\",\"wb\") as f:\r\n",
    "#         f.write(corpus)\r\n",
    "#     f.close()\r\n",
    "# download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**加载数据集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "def load_text8():\r\n",
    "    with open(\"./data/data98805/text8.txt\",'r') as f:\r\n",
    "        corpus=f.read().strip(\"\\n\")\r\n",
    "    f.close()\r\n",
    "    return corpus\r\n",
    "corpus=load_text8()\r\n",
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**转换小写，切分单词**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n"
     ]
    }
   ],
   "source": [
    "def data_preprocess(corpus):\r\n",
    "    corpus=corpus.strip().lower()\r\n",
    "    corpus=corpus.split(\" \")\r\n",
    "    return corpus\r\n",
    "    \r\n",
    "corpus=data_preprocess(corpus)\r\n",
    "print(len(corpus))\r\n",
    "print(corpus[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "corpus=corpus[:1000000]\r\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**构造词典，内含单词、编号、频次**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totally 52754 different words in the corpus\n",
      "word the, its id 0, its word freq 62827\n",
      "word of, its id 1, its word freq 36789\n",
      "word and, its id 2, its word freq 25238\n",
      "word one, its id 3, its word freq 24679\n",
      "word in, its id 4, its word freq 22502\n",
      "word a, its id 5, its word freq 18620\n",
      "word to, its id 6, its word freq 18504\n",
      "word zero, its id 7, its word freq 14349\n",
      "word nine, its id 8, its word freq 14056\n",
      "word is, its id 9, its word freq 11094\n",
      "word two, its id 10, its word freq 10968\n",
      "word as, its id 11, its word freq 7737\n",
      "word eight, its id 12, its word freq 7708\n",
      "word three, its id 13, its word freq 7049\n",
      "word was, its id 14, its word freq 6892\n",
      "word by, its id 15, its word freq 6796\n",
      "word five, its id 16, its word freq 6647\n",
      "word s, its id 17, its word freq 6606\n",
      "word that, its id 18, its word freq 6541\n",
      "word for, its id 19, its word freq 6447\n"
     ]
    }
   ],
   "source": [
    "def build_dict(corpus):\r\n",
    "    word_freq_dict=dict()\r\n",
    "    for word in corpus:\r\n",
    "        if word not in word_freq_dict:\r\n",
    "            word_freq_dict[word]=0\r\n",
    "        word_freq_dict[word]+=1\r\n",
    "    \r\n",
    "    word_freq_dict=sorted(word_freq_dict.items(),key=lambda x:x[1],reverse=True)\r\n",
    "\r\n",
    "    word2id_dict=dict()\r\n",
    "    word2id_freq=dict()\r\n",
    "    id2word_dict=dict()\r\n",
    "\r\n",
    "    for word,freq in word_freq_dict:\r\n",
    "        curr_id=len(word2id_dict)\r\n",
    "        word2id_dict[word]=curr_id\r\n",
    "        word2id_freq[curr_id]=freq\r\n",
    "        id2word_dict[curr_id]=word\r\n",
    "    return word2id_dict,word2id_freq,id2word_dict\r\n",
    "\r\n",
    "word2id_dict,word2id_freq,id2word_dict=build_dict(corpus)\r\n",
    "vocab_size=len(word2id_freq)\r\n",
    "print('there are totally {} different words in the corpus'.format(vocab_size))\r\n",
    "for _,(word,word_id)in zip(range(20),word2id_dict.items()):\r\n",
    "    print(f\"word {word}, its id {word_id}, its word freq {word2id_freq[word_id]}\")\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**语料库的id表示**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 tokens in the corpus\n",
      "[631, 3667, 11, 5, 172, 1, 4287, 46, 62, 149, 123, 869, 665, 6236, 160, 0, 16524, 1, 0, 110, 948, 2, 0, 16525, 28013, 1, 0, 137, 948, 3999, 0, 172, 9, 178, 62, 4, 5, 9948, 211, 6, 1229, 104, 449, 18, 62, 2301, 364, 6, 3105, 0]\n"
     ]
    }
   ],
   "source": [
    "def convert_corpus_to_id(corpus,word2id_dict):\r\n",
    "    corpus=[word2id_dict[word] for word in corpus]\r\n",
    "    return corpus\r\n",
    "corpus=convert_corpus_to_id(corpus,word2id_dict)\r\n",
    "print(f\"{len(corpus)} tokens in the corpus\")\r\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**二次下采样**  \n",
    "公式：  \n",
    "$$\\ random_{num} < 1-\\sqrt{\\frac{10^{-4}} { wordid_{freq}} * wordall_{freq} }$$\n",
    "其中$random_{num}$表示一个0-1之间的随机数，$wordid_{freq}$表示单词对应的频次，$wordall_{freq}$表示所有单词的频次之和   \n",
    "\n",
    "如果上式成立则丢弃，如果不成立则保留"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506826 tokens in the corpus\n",
      "[631, 3667, 5, 172, 4287, 46, 869, 665, 6236, 160, 16524, 948, 16525, 28013, 137, 948, 3999, 9, 9948, 1229, 2301, 3105, 1048, 366, 29, 450, 1087, 3768, 674, 712, 1105, 214, 631, 878, 209, 229, 20461, 2428, 666, 9, 2793, 6872, 4000, 7328, 3769, 140, 364, 631, 1112, 485]\n"
     ]
    }
   ],
   "source": [
    "def subsampling(corpus,word2id_freq):\r\n",
    "    # 函数discard判断一个词是否会被遗弃，如果TRUE则替换\r\n",
    "    # 频次越大，被遗弃概率越大\r\n",
    "    def discard(word_id):\r\n",
    "        return random.uniform(0,1)<1-math.sqrt(\r\n",
    "            1e-4/word2id_freq[word_id]*len(corpus)\r\n",
    "        )\r\n",
    "    corpus=[word for word in corpus if not discard(word)]\r\n",
    "    return corpus\r\n",
    "corpus=subsampling(corpus,word2id_freq)\r\n",
    "print(f\"{len(corpus)} tokens in the corpus\")\r\n",
    "print(corpus[:50])\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**构造数据集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_window_size代表了最大的window_size的大小，程序会根据max_window_size从左到右扫描整个语料库\r\n",
    "# negative_sample_num代表了对于每个正样本，我们需要随机采样的负样本用于训练\r\n",
    "# 一般来说，negative_sample_num的值越大，训练效果越稳定，但训练效果越慢\r\n",
    "def build_data(corpus,max_window_size=3,negative_sample_num=4):\r\n",
    "    # 使用一个list来存储处理好的数据\r\n",
    "    dataset=[]\r\n",
    "    # 从左到右，开始枚举每个中心点的位置\r\n",
    "    for center_word_idx in range(len(corpus)):\r\n",
    "        # 以max_window_size为上限，随机采样一个window_size\r\n",
    "        window_size=random.randint(1,max_window_size)\r\n",
    "        # 当前的window_size就是center_word_idx所指向的词\r\n",
    "        center_word = corpus[center_word_idx]\r\n",
    "\r\n",
    "        # 以当前中心词为中心，左右两侧在window_size内的词都可以看做是正样本\r\n",
    "        positive_word_range=(max(0,center_word_idx-window_size),\r\n",
    "        min(len(corpus)-1,center_word_idx + window_size))\r\n",
    "        positive_word_candidates = [corpus[idx] for idx in range(positive_word_range[0],\r\n",
    "        positive_word_range[1]+1) if idx !=center_word_idx]\r\n",
    "    \r\n",
    "        # 对每个正样本来说，随机采样negative_sample_num个负样本，用于训练\r\n",
    "        for positive_word in positive_word_candidates:\r\n",
    "            # 首先把（正样本，中心词，label=1）的三元组数据放入dataset中，\r\n",
    "            # 这里label=1表示这个样本是个正样本\r\n",
    "            dataset.append((center_word,positive_word,1))\r\n",
    "            \r\n",
    "            # 开始负采样\r\n",
    "            i=0\r\n",
    "            while i<negative_sample_num:\r\n",
    "                negative_word_candidate=random.randint(0,vocab_size-1)\r\n",
    "\r\n",
    "                if negative_word_candidate not in positive_word_candidates:\r\n",
    "                    # 把（负样本，中心词，label=0）的三元组放入dataset中，\r\n",
    "                    # 这里label=0表示这个样本是个负样本\r\n",
    "                    dataset.append((center_word,negative_word_candidate,0))\r\n",
    "                    i+=1\r\n",
    "    return dataset\r\n",
    "# dataset=build_data(corpus,max_window_size=3,negative_sample_num=4)\r\n",
    "# print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_window_size代表了最大的window_size的大小，程序会根据max_window_size从左到右扫描整个语料库\r\n",
    "# negative_sample_num代表了对于每个正样本，我们需要随机采样的负样本用于训练\r\n",
    "# 一般来说，negative_sample_num的值越大，训练效果越稳定，但训练效果越慢\r\n",
    "def build_data_1(corpus,max_window_size=3,negative_sample_num=4):\r\n",
    "    # 使用一个list来存储处理好的数据\r\n",
    "    dataset=[]\r\n",
    "    # 从左到右，开始枚举每个中心点的位置\r\n",
    "    for center_word_idx in range(len(corpus)):\r\n",
    "        # 以max_window_size为上限，随机采样一个window_size\r\n",
    "        window_size=random.randint(1,max_window_size)\r\n",
    "        # 当前的window_size就是center_word_idx所指向的词\r\n",
    "        center_word = corpus[center_word_idx]\r\n",
    "\r\n",
    "        # 以当前中心词为中心，左右两侧在window_size内的词都可以看做是正样本\r\n",
    "        positive_word_range=(max(0,center_word_idx-window_size),\r\n",
    "        min(len(corpus)-1,center_word_idx + window_size))\r\n",
    "        positive_word_candidates = [corpus[idx] for idx in range(positive_word_range[0],\r\n",
    "        positive_word_range[1]+1) if idx !=center_word_idx]\r\n",
    "    \r\n",
    "        # 对每个正样本来说，随机采样negative_sample_num个负样本，用于训练\r\n",
    "        for positive_word in positive_word_candidates:\r\n",
    "            # 首先把（正样本，中心词，label=1）的三元组数据放入dataset中，\r\n",
    "            # 这里label=1表示这个样本是个正样本\r\n",
    "            dataset.append((positive_word,center_word,1))\r\n",
    "            \r\n",
    "            # 开始负采样\r\n",
    "            i=0\r\n",
    "            while i<negative_sample_num:\r\n",
    "                negative_word_candidata=random.randint(0,vocab_size-1)\r\n",
    "\r\n",
    "                if negative_word_candidata not in positive_word_candidates:\r\n",
    "                    # 把（负样本，中心词，label=0）的三元组放入dataset中，\r\n",
    "                    # 这里label=0表示这个样本是个负样本\r\n",
    "                    dataset.append((negative_word_candidata,center_word,0))\r\n",
    "                    i+=1\r\n",
    "        # print(len(dataset))\r\n",
    "    return dataset\r\n",
    "# dataset=build_data_1(corpus,max_window_size=3,negative_sample_num=4)\r\n",
    "# print(dataset[0])\r\n",
    "# print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_window_size代表了最大的window_size的大小，程序会根据max_window_size从左到右扫描整个语料库\r\n",
    "# negative_sample_num代表了对于每个正样本，我们需要随机采样的负样本用于训练\r\n",
    "# 一般来说，negative_sample_num的值越大，训练效果越稳定，但训练效果越慢\r\n",
    "def build_data_2(corpus,window_size=3):\r\n",
    "    # 使用一个list来存储处理好的数据\r\n",
    "    dataset=[]\r\n",
    "    # 从左到右，开始枚举每个中心点的位置\r\n",
    "    for center_word_idx in range(window_size,len(corpus)-window_size):\r\n",
    "        center_word = corpus[center_word_idx]\r\n",
    "\r\n",
    "        # 以当前中心词为中心，左右两侧在window_size内的词都可以看做是正样本\r\n",
    "        positive_word_range=(center_word_idx-window_size,center_word_idx + window_size)\r\n",
    "        positive_word_candidates = [corpus[idx] for idx in range(positive_word_range[0],\r\n",
    "        positive_word_range[1]+1) if idx !=center_word_idx]\r\n",
    "        dataset.append([positive_word_candidates,[center_word]])\r\n",
    "    \r\n",
    "        \r\n",
    "    return dataset\r\n",
    "# dataset=build_data_2(corpus,window_size=2)\r\n",
    "# # print(dataset.shape)\r\n",
    "# print(len(corpus))\r\n",
    "# print(dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_batch(dataset,batch_size,epoch_num):\r\n",
    "    \r\n",
    "    center_word_batch=[]\r\n",
    "    target_word_batch=[]\r\n",
    "    label_batch=[]\r\n",
    "\r\n",
    "    for epoch in range(epoch_num):\r\n",
    "        random.shuffle(dataset)\r\n",
    "        \r\n",
    "        for center_word,target_word,label in dataset:\r\n",
    "            center_word_batch.append([center_word])\r\n",
    "            target_word_batch.append([target_word])\r\n",
    "            label_batch.append(label)\r\n",
    "\r\n",
    "            if len(center_word_batch)==batch_size:\r\n",
    "                yield np.array(center_word_batch).astype('int64'),\\\r\n",
    "                    np.array(target_word_batch).astype('int64'),\\\r\n",
    "                    np.array(label_batch).astype('float32')\r\n",
    "                center_word_batch=[]\r\n",
    "                target_word_batch=[]\r\n",
    "                label_batch=[]\r\n",
    "    if len(center_word_batch)>0:\r\n",
    "        yield np.array(center_word_batch).astype('int64'),\\\r\n",
    "            np.array(target_word_batch).astype('int64'),\\\r\n",
    "            np.array(label_batch).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_batch_1(dataset,batch_size):\r\n",
    "    \r\n",
    "    def reader():\r\n",
    "        random.shuffle(dataset)\r\n",
    "        center_word_batch=[]\r\n",
    "        target_word_batch=[]\r\n",
    "        label_batch=[]\r\n",
    "        for target_word,center_word,label in dataset:\r\n",
    "            center_word_batch.append([center_word])\r\n",
    "            target_word_batch.append([target_word])\r\n",
    "            label_batch.append(label)\r\n",
    "\r\n",
    "            if len(center_word_batch)==batch_size:\r\n",
    "                yield np.array(center_word_batch).astype('int64'),\\\r\n",
    "                    np.array(target_word_batch).astype('int64'),\\\r\n",
    "                    np.array(label_batch).astype('float32')\r\n",
    "                center_word_batch=[]\r\n",
    "                target_word_batch=[]\r\n",
    "                label_batch=[]\r\n",
    "        if len(center_word_batch)>0:\r\n",
    "            yield np.array(center_word_batch).astype('int64'),\\\r\n",
    "                np.array(target_word_batch).astype('int64'),\\\r\n",
    "                np.array(label_batch).astype('float32')\r\n",
    "    return reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_batch_2(dataset,batch_size):\r\n",
    "    def reader():\r\n",
    "        random.shuffle(dataset)\r\n",
    "        positive_word_batch=[]\r\n",
    "        center_word_batch=[]\r\n",
    "        for positive_word,center_word in dataset:\r\n",
    "            positive_word_batch.append(positive_word)\r\n",
    "            center_word_batch.append(center_word)\r\n",
    "\r\n",
    "            if len(positive_word_batch)==batch_size:\r\n",
    "                yield np.array(positive_word_batch).astype('int64'),\\\r\n",
    "                    np.array(center_word_batch).astype('int64')\r\n",
    "                positive_word_batch=[]\r\n",
    "                center_word_batch=[]\r\n",
    "        if len(positive_word_batch)>0:\r\n",
    "            yield np.array(positive_word_batch).astype('int64'),\\\r\n",
    "                np.array(center_word_batch).astype('int64')\r\n",
    "    return reader\r\n",
    "# data_reader=build_batch_2(dataset,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Layer):\r\n",
    "    def __init__(self,vocab_size,embed_size,window_size=5):\r\n",
    "        super(CBOW,self).__init__()\r\n",
    "        self.positive_num=2*window_size\r\n",
    "        self.embed=nn.Embedding(vocab_size,embed_size)\r\n",
    "        self.l1=nn.Linear(embed_size,128)\r\n",
    "        self.l2=nn.Linear(128,vocab_size)\r\n",
    "    def forward(self,x_sample):\r\n",
    "        sum_hidden=0\r\n",
    "        for x in x_sample:\r\n",
    "            inputs=self.embed(x)\r\n",
    "            inputs=nn.Flatten()(inputs)\r\n",
    "            inputs=nn.ReLU()(inputs)\r\n",
    "            sum_hidden+=self.l1(inputs)\r\n",
    "        avg_hidden=sum_hidden/self.positive_num\r\n",
    "        out=self.l2(avg_hidden)\r\n",
    "        out=F.log_softmax(out,axis=-1)\r\n",
    "        return out\r\n",
    "\r\n",
    "# model=CBOW(vocab_size,32,4)\r\n",
    "# paddle.summary(model,(10,))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CBOW_1(nn.Layer):\r\n",
    "    def __init__(self,vocab_size,embed_size):\r\n",
    "        super(CBOW_1,self).__init__()\r\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_size)\r\n",
    "        self.embedding_out=nn.Embedding(vocab_size,embed_size)\r\n",
    "    def forward(self,target_words,center_words,labels):\r\n",
    "        \r\n",
    "        target_words_emb=self.embedding(target_words)\r\n",
    "        center_words_emb=self.embedding_out(center_words)\r\n",
    "        \r\n",
    "        word_sim=paddle.multiply(target_words_emb,center_words_emb)\r\n",
    "        word_sim=paddle.sum(word_sim,axis=-1)\r\n",
    "        word_sim=paddle.reshape(word_sim,shape=[-1])\r\n",
    "        pred=F.sigmoid(word_sim)\r\n",
    "\r\n",
    "        loss=F.binary_cross_entropy_with_logits(word_sim,labels)\r\n",
    "        loss=paddle.mean(loss)\r\n",
    "\r\n",
    "        return pred,loss\r\n",
    "\r\n",
    "# model=CBOW(vocab_size,32,4)\r\n",
    "# paddle.summary(model,(10,))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[52754], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [-10.87341976, -10.87340355, -10.87387276, ..., -10.87437057, -10.87340641, -10.87414455])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/tensor/creation.py:125: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if data.dtype == np.object:\n"
     ]
    }
   ],
   "source": [
    "class CBOW_2(nn.Layer):\r\n",
    "    def __init__(self,vocab_size,embed_size,window_size=5):\r\n",
    "        super(CBOW_2,self).__init__()\r\n",
    "        self.positive_num=2*window_size\r\n",
    "        self.embed=nn.Embedding(vocab_size,embed_size)\r\n",
    "        self.l1=nn.Linear(2*window_size*embed_size,128)\r\n",
    "        self.l2=nn.Linear(128,vocab_size)\r\n",
    "\r\n",
    "    def forward(self,x_sample):\r\n",
    "        inputs=self.embed(x_sample)\r\n",
    "        inputs=paddle.flatten(inputs)\r\n",
    "        out=self.l1(inputs)\r\n",
    "        out=F.relu(out)\r\n",
    "        out=self.l2(out)\r\n",
    "        out=F.log_softmax(out,axis=-1)\r\n",
    "        return out\r\n",
    "\r\n",
    "model=CBOW_2(vocab_size,32,2)\r\n",
    "print(model(paddle.to_tensor([2,3,4,5])))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token,k,embed):\r\n",
    "    W=embed.numpy()\r\n",
    "    x=W[word2id_dict[query_token]]\r\n",
    "    cos=np.dot(W,x)/np.sqrt(np.sum(W*W,axis=1)*np.sum(x*x)+1e-9)\r\n",
    "    flat=cos.flatten()\r\n",
    "    indices=np.argpartition(flat,-k)[-k:]\r\n",
    "    indices=indices[np.argsort(-flat[indices])]\r\n",
    "    similar_words=''\r\n",
    "    for i in indices:\r\n",
    "        if similar_words=='':\r\n",
    "            similar_words=str(id2word_dict[i])\r\n",
    "        else:\r\n",
    "            similar_words+=','+str(id2word_dict[i])\r\n",
    "    print(\"for word %s, the similar word is %s\"%(query_token,similar_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "data import \n",
      "***************\n",
      "corpus: 17005207\n",
      "corpus: 170052\n",
      "vocab_size: 16902\n",
      "corpus: 170052\n",
      "corpus: 82735\n",
      "dataset: 82729 [[6292, 0, 267, 8928, 185, 651], [8927]]\n",
      "start training \n",
      "***************\n",
      "epoch:0\n",
      "***************\n",
      "epoch:0:batch_id:0===loss:12.493160\n",
      "epoch:0:batch_id:100===loss:12.053917\n",
      "epoch:0:batch_id:200===loss:12.393070\n",
      "epoch:0:batch_id:300===loss:10.783705\n",
      "epoch:0:batch_id:400===loss:12.021126\n",
      "epoch:0:batch_id:500===loss:11.931007\n",
      "epoch:0:batch_id:600===loss:10.980579\n",
      "***************\n",
      "epoch:1\n",
      "***************\n",
      "epoch:1:batch_id:0===loss:11.769585\n",
      "epoch:1:batch_id:100===loss:11.462132\n",
      "epoch:1:batch_id:200===loss:11.931100\n",
      "epoch:1:batch_id:300===loss:12.234723\n",
      "epoch:1:batch_id:400===loss:11.328584\n",
      "epoch:1:batch_id:500===loss:11.910265\n",
      "epoch:1:batch_id:600===loss:12.472374\n",
      "***************\n",
      "epoch:2\n",
      "***************\n",
      "epoch:2:batch_id:0===loss:11.900136\n",
      "epoch:2:batch_id:100===loss:12.888954\n",
      "epoch:2:batch_id:200===loss:11.080948\n",
      "epoch:2:batch_id:300===loss:10.703289\n",
      "epoch:2:batch_id:400===loss:11.096130\n",
      "epoch:2:batch_id:500===loss:12.320399\n",
      "epoch:2:batch_id:600===loss:11.231253\n",
      "***************\n",
      "epoch:3\n",
      "***************\n",
      "epoch:3:batch_id:0===loss:12.284019\n",
      "epoch:3:batch_id:100===loss:11.603594\n",
      "epoch:3:batch_id:200===loss:12.034416\n",
      "epoch:3:batch_id:300===loss:11.507963\n",
      "epoch:3:batch_id:400===loss:11.875523\n",
      "epoch:3:batch_id:500===loss:12.098063\n",
      "epoch:3:batch_id:600===loss:11.588954\n",
      "***************\n",
      "epoch:4\n",
      "***************\n",
      "epoch:4:batch_id:0===loss:11.713251\n",
      "epoch:4:batch_id:100===loss:11.801576\n",
      "epoch:4:batch_id:200===loss:11.116392\n",
      "epoch:4:batch_id:300===loss:11.525600\n",
      "epoch:4:batch_id:400===loss:11.471210\n",
      "epoch:4:batch_id:500===loss:11.783238\n",
      "epoch:4:batch_id:600===loss:10.815613\n",
      "***************\n",
      "epoch:5\n",
      "***************\n",
      "epoch:5:batch_id:0===loss:12.256611\n",
      "epoch:5:batch_id:100===loss:11.076797\n",
      "epoch:5:batch_id:200===loss:12.971076\n",
      "epoch:5:batch_id:300===loss:12.587025\n",
      "epoch:5:batch_id:400===loss:11.601940\n",
      "epoch:5:batch_id:500===loss:10.820404\n",
      "epoch:5:batch_id:600===loss:11.917503\n",
      "***************\n",
      "epoch:6\n",
      "***************\n",
      "epoch:6:batch_id:0===loss:11.857412\n",
      "epoch:6:batch_id:100===loss:11.601630\n",
      "epoch:6:batch_id:200===loss:11.645493\n",
      "epoch:6:batch_id:300===loss:11.148414\n",
      "epoch:6:batch_id:400===loss:11.328070\n",
      "epoch:6:batch_id:500===loss:11.989471\n",
      "epoch:6:batch_id:600===loss:12.271553\n",
      "***************\n",
      "epoch:7\n",
      "***************\n",
      "epoch:7:batch_id:0===loss:11.939745\n",
      "epoch:7:batch_id:100===loss:12.503462\n",
      "epoch:7:batch_id:200===loss:12.489688\n",
      "epoch:7:batch_id:300===loss:11.272988\n",
      "epoch:7:batch_id:400===loss:11.839647\n",
      "epoch:7:batch_id:500===loss:11.463551\n",
      "epoch:7:batch_id:600===loss:11.786565\n",
      "***************\n",
      "epoch:8\n",
      "***************\n",
      "epoch:8:batch_id:0===loss:10.389240\n",
      "epoch:8:batch_id:100===loss:11.965543\n",
      "epoch:8:batch_id:200===loss:12.033482\n",
      "epoch:8:batch_id:300===loss:11.674007\n",
      "epoch:8:batch_id:400===loss:11.284816\n",
      "epoch:8:batch_id:500===loss:11.027262\n",
      "epoch:8:batch_id:600===loss:11.404648\n",
      "***************\n",
      "epoch:9\n",
      "***************\n",
      "epoch:9:batch_id:0===loss:11.725920\n",
      "epoch:9:batch_id:100===loss:11.332370\n",
      "epoch:9:batch_id:200===loss:11.055222\n",
      "epoch:9:batch_id:300===loss:11.895058\n",
      "epoch:9:batch_id:400===loss:11.329630\n",
      "epoch:9:batch_id:500===loss:11.221594\n",
      "epoch:9:batch_id:600===loss:12.549686\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "def train_2():\r\n",
    "    # 超参数\r\n",
    "    batch_size=128\r\n",
    "    epoch_num=10\r\n",
    "    embedding_size=200\r\n",
    "    learning_rate=0.01\r\n",
    "    window_size=3\r\n",
    "    split_rate=0.01\r\n",
    "    model_path='./work/CBOW_word2vec.pdmodel'\r\n",
    "    load_model=True\r\n",
    "    if paddle.is_compiled_with_cuda():\r\n",
    "        paddle.set_device('gpu:0')\r\n",
    "    print(\"*\"*15)\r\n",
    "    print(\"data import \")\r\n",
    "    print(\"*\"*15)\r\n",
    "    # 加载数据集\r\n",
    "    corpus=load_text8()\r\n",
    "    # 转换小写，切分单词\r\n",
    "    corpus=data_preprocess(corpus)\r\n",
    "    print('corpus:',len(corpus))\r\n",
    "    # 切分数据集\r\n",
    "    corpus=corpus[:int(len(corpus)*split_rate)]\r\n",
    "    print('corpus:',len(corpus))\r\n",
    "    # 构造词典\r\n",
    "    word2id_dict,word2id_freq,id2word_dict=build_dict(corpus)\r\n",
    "    vocab_size=len(word2id_dict)\r\n",
    "    print('vocab_size:',vocab_size)\r\n",
    "    # 语料库id表示\r\n",
    "    corpus=convert_corpus_to_id(corpus,word2id_dict)\r\n",
    "    print('corpus:',len(corpus))\r\n",
    "    # 语料库下采样\r\n",
    "    corpus=subsampling(corpus,word2id_freq)\r\n",
    "    print('corpus:',len(corpus))\r\n",
    "    # 构造数据集\r\n",
    "    dataset=build_data_2(corpus,window_size=window_size)\r\n",
    "    print('dataset:',len(dataset),dataset[10])\r\n",
    "    # 设置或者加载模型\r\n",
    "    if os.path.exists(model_path) and load_model:\r\n",
    "        CBOW_model=paddle.load(model_path)\r\n",
    "    else:\r\n",
    "        CBOW_model=CBOW_2(vocab_size,embed_size=embedding_size,window_size=window_size)\r\n",
    "    # 设置优化器\r\n",
    "    opt=paddle.optimizer.Adam(learning_rate=learning_rate,parameters=CBOW_model.parameters())\r\n",
    "    # 选择模型模式\r\n",
    "    CBOW_model.train()\r\n",
    "    print(\"start training \")\r\n",
    "    print('*'*15)\r\n",
    "    for epoch in range(epoch_num):\r\n",
    "            # 加载数据生成器\r\n",
    "        data_reader=build_batch_2(dataset,batch_size=batch_size)\r\n",
    "        print(f'epoch:{epoch}')\r\n",
    "        print('*'*15)\r\n",
    "        for batch_id,(positive_samples,center_samples) in enumerate(data_reader()):\r\n",
    "            positive_samples_vec=paddle.to_tensor(positive_samples)\r\n",
    "            center_samples_vec=paddle.to_tensor(center_samples)\r\n",
    "            loss_list=[]\r\n",
    "            for positive_sample_vec,center_sample_vec in zip(positive_samples_vec,center_samples_vec):\r\n",
    "                out=CBOW_model(positive_sample_vec)\r\n",
    "                # print('out:',out.shape,out)\r\n",
    "                loss=nn.functional.cross_entropy(out,center_sample_vec)\r\n",
    "                # print('loss:',loss)\r\n",
    "                loss=paddle.mean(loss)\r\n",
    "                opt.clear_grad()\r\n",
    "                loss.backward()\r\n",
    "                opt.step()\r\n",
    "                loss_list.append(loss.numpy())\r\n",
    "            if batch_id %100==0:\r\n",
    "                print(\"epoch:{}:batch_id:{}===loss:{:.6f}\"\\\r\n",
    "                .format(epoch,batch_id,np.mean(loss_list)))\r\n",
    "        print('*'*15)\r\n",
    "    paddle.save(CBOW_model,model_path)\r\n",
    "            \r\n",
    "\r\n",
    "train_2()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for word one, the similar word is one,solves,bantu,fastest,popularity\n",
      "for word year, the similar word is year,hilferding,twerms,castigo,dementia\n",
      "for word what, the similar word is what,deirdre,straightest,thaana,alzheimer\n",
      "for word in, the similar word is in,conforms,carelessness,mount,microscopic\n",
      "for word if, the similar word is if,nieuwenhuys,cruising,resorts,prince\n",
      "for word soft, the similar word is soft,systematized,nba,fianna,nucleon\n"
     ]
    }
   ],
   "source": [
    "model_path='./work/CBOW_word2vec.pdmodel'\r\n",
    "CBOW_model=paddle.load(model_path)\r\n",
    "get_similar_tokens('one',5,model.embed.weight)\r\n",
    "get_similar_tokens('year',5,model.embed.weight)\r\n",
    "get_similar_tokens('what',5,model.embed.weight)\r\n",
    "get_similar_tokens('in',5,model.embed.weight)\r\n",
    "get_similar_tokens('if',5,model.embed.weight)\r\n",
    "get_similar_tokens('soft',5,model.embed.weight)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
