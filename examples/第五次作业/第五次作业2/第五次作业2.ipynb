{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "from collections import OrderedDict \n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "\n",
    "from paddle.fluid.dygraph.nn import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\paddle2\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#下载语料用来训练word2vec\n",
    "def download():\n",
    "    #可以从百度云服务器下载一些开源数据集（dataset.bj.bcebos.com）\n",
    "    corpus_url = \"https://dataset.bj.bcebos.com/word2vec/text8.txt\"\n",
    "    #使用python的requests包下载数据集到本地\n",
    "    web_request = requests.get(corpus_url)\n",
    "    corpus = web_request.content\n",
    "    #把下载后的文件存储在当前目录的text8.txt文件内\n",
    "    with open(\"./text8.txt\", \"wb\") as f:\n",
    "        f.write(corpus)\n",
    "    f.close()\n",
    "\n",
    "download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n"
     ]
    }
   ],
   "source": [
    "#读取text8数据\n",
    "def load_text8():\n",
    "    with open(\"./text8.txt\", \"r\") as f:\n",
    "        corpus = f.read().strip(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    return corpus\n",
    "\n",
    "corpus = load_text8()\n",
    "\n",
    "#打印前500个字符，简要看一下这个语料的样子\n",
    "print(corpus[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the']\n"
     ]
    }
   ],
   "source": [
    "#对语料进行预处理（分词）\n",
    "def data_preprocess(corpus):\n",
    "    #由于英文单词出现在句首的时候经常要大写，所以我们把所有英文字符都转换为小写，\n",
    "    #以便对语料进行归一化处理（Apple vs apple等）\n",
    "    corpus = corpus.strip().lower()\n",
    "    corpus = corpus.split(\" \")\n",
    "\n",
    "    return corpus\n",
    "\n",
    "corpus = data_preprocess(corpus)\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totoally 253854 different words in the corpus\n",
      "word the, its id 0, its word freq 1061396\n",
      "word of, its id 1, its word freq 593677\n",
      "word and, its id 2, its word freq 416629\n",
      "word one, its id 3, its word freq 411764\n",
      "word in, its id 4, its word freq 372201\n",
      "word a, its id 5, its word freq 325873\n",
      "word to, its id 6, its word freq 316376\n",
      "word zero, its id 7, its word freq 264975\n",
      "word nine, its id 8, its word freq 250430\n",
      "word two, its id 9, its word freq 192644\n",
      "word is, its id 10, its word freq 183153\n",
      "word as, its id 11, its word freq 131815\n",
      "word eight, its id 12, its word freq 125285\n",
      "word for, its id 13, its word freq 118445\n",
      "word s, its id 14, its word freq 116710\n",
      "word five, its id 15, its word freq 115789\n",
      "word three, its id 16, its word freq 114775\n",
      "word was, its id 17, its word freq 112807\n",
      "word by, its id 18, its word freq 111831\n",
      "word that, its id 19, its word freq 109510\n",
      "word four, its id 20, its word freq 108182\n",
      "word six, its id 21, its word freq 102145\n",
      "word seven, its id 22, its word freq 99683\n",
      "word with, its id 23, its word freq 95603\n",
      "word on, its id 24, its word freq 91250\n",
      "word are, its id 25, its word freq 76527\n",
      "word it, its id 26, its word freq 73334\n",
      "word from, its id 27, its word freq 72871\n",
      "word or, its id 28, its word freq 68945\n",
      "word his, its id 29, its word freq 62603\n",
      "word an, its id 30, its word freq 61925\n",
      "word be, its id 31, its word freq 61281\n",
      "word this, its id 32, its word freq 58832\n",
      "word which, its id 33, its word freq 54788\n",
      "word at, its id 34, its word freq 54576\n",
      "word he, its id 35, its word freq 53573\n",
      "word also, its id 36, its word freq 44358\n",
      "word not, its id 37, its word freq 44033\n",
      "word have, its id 38, its word freq 39712\n",
      "word were, its id 39, its word freq 39086\n",
      "word has, its id 40, its word freq 37866\n",
      "word but, its id 41, its word freq 35358\n",
      "word other, its id 42, its word freq 32433\n",
      "word their, its id 43, its word freq 31523\n",
      "word its, its id 44, its word freq 29567\n",
      "word first, its id 45, its word freq 28810\n",
      "word they, its id 46, its word freq 28553\n",
      "word some, its id 47, its word freq 28161\n",
      "word had, its id 48, its word freq 28100\n",
      "word all, its id 49, its word freq 26229\n"
     ]
    }
   ],
   "source": [
    "#构造词典，统计每个词的频率，并根据频率将每个词转换为一个整数id\n",
    "def build_dict(corpus):\n",
    "    #首先统计每个不同词的频率（出现的次数），使用一个词典记录\n",
    "    word_freq_dict = dict()\n",
    "    for word in corpus:\n",
    "        if word not in word_freq_dict:\n",
    "            word_freq_dict[word] = 0\n",
    "        word_freq_dict[word] += 1\n",
    "\n",
    "    #将这个词典中的词，按照出现次数排序，出现次数越高，排序越靠前\n",
    "    #一般来说，出现频率高的高频词往往是：I，the，you这种代词，而出现频率低的词，往往是一些名词，如：nlp\n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "    #构造3个不同的词典，分别存储，\n",
    "    #每个词到id的映射关系：word2id_dict\n",
    "    #每个id出现的频率：word2id_freq\n",
    "    #每个id到词典映射关系：id2word_dict\n",
    "    word2id_dict = dict()\n",
    "    word2id_freq = dict()\n",
    "    id2word_dict = dict()\n",
    "\n",
    "    #按照频率，从高到低，开始遍历每个单词，并为这个单词构造一个独一无二的id\n",
    "    for word, freq in word_freq_dict:\n",
    "        curr_id = len(word2id_dict)\n",
    "        word2id_dict[word] = curr_id\n",
    "        word2id_freq[word2id_dict[word]] = freq\n",
    "        id2word_dict[curr_id] = word\n",
    "\n",
    "    return word2id_freq, word2id_dict, id2word_dict\n",
    "\n",
    "word2id_freq, word2id_dict, id2word_dict = build_dict(corpus)\n",
    "vocab_size = len(word2id_freq)\n",
    "print(\"there are totoally %d different words in the corpus\" % vocab_size)\n",
    "for _, (word, word_id) in zip(range(50), word2id_dict.items()):\n",
    "    print(\"word %s, its id %d, its word freq %d\" % (word, word_id, word2id_freq[word_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207 tokens in the corpus\n",
      "[5233, 3080, 11, 5, 194, 1, 3133, 45, 58, 155, 127, 741, 476, 10571, 133, 0, 27349, 1, 0, 102, 854, 2, 0, 15067, 58112, 1, 0, 150, 854, 3580, 0, 194, 10, 190, 58, 4, 5, 10712, 214, 6, 1324, 104, 454, 19, 58, 2731, 362, 6, 3672, 0]\n"
     ]
    }
   ],
   "source": [
    "#把语料转换为id序列\n",
    "def convert_corpus_to_id(corpus, word2id_dict):\n",
    "    #使用一个循环，将语料中的每个词替换成对应的id，以便于神经网络进行处理\n",
    "    corpus = [word2id_dict[word] for word in corpus]\n",
    "    return corpus\n",
    "\n",
    "corpus = convert_corpus_to_id(corpus, word2id_dict)\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8745310 tokens in the corpus\n",
      "[5233, 3080, 11, 3133, 58, 741, 476, 10571, 133, 27349, 15067, 58112, 150, 854, 3580, 190, 58, 5, 10712, 1324, 454, 2731, 362, 3672, 708, 371, 1423, 2757, 567, 686, 7088, 247, 5233, 1052, 44611, 2877, 792, 5233, 11, 200, 10, 1134, 2621, 8983, 279, 4147, 6437, 4186, 32, 362]\n"
     ]
    }
   ],
   "source": [
    "#使用二次采样算法（subsampling）处理语料，强化训练效果\n",
    "def subsampling(corpus, word2id_freq):\n",
    "    \n",
    "    #这个discard函数决定了一个词会不会被替换，这个函数是具有随机性的，每次调用结果不同\n",
    "    #如果一个词的频率很大，那么它被遗弃的概率就很大\n",
    "    def discard(word_id):\n",
    "        return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "            1e-4 / word2id_freq[word_id] * len(corpus))\n",
    "\n",
    "    corpus = [word for word in corpus if not discard(word)]\n",
    "    return corpus\n",
    "\n",
    "corpus = subsampling(corpus, word2id_freq)\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\n",
    "print(corpus[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "500000\n",
      "600000\n",
      "900000\n",
      "1100000\n",
      "1200000\n",
      "1400000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "2000000\n",
      "2300000\n",
      "2500000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3300000\n",
      "4000000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "5300000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5900000\n",
      "6000000\n",
      "6100000\n",
      "6600000\n",
      "6700000\n",
      "6800000\n",
      "7000000\n",
      "7300000\n",
      "7400000\n",
      "7500000\n",
      "8200000\n",
      "8500000\n",
      "8600000\n",
      "8700000\n",
      "center_word originated, target anarchism, label 1\n",
      "center_word originated, target ernoul, label 0\n",
      "center_word originated, target gynaecologists, label 0\n",
      "center_word originated, target sprachprofi, label 0\n",
      "center_word originated, target nomismata, label 0\n",
      "center_word anarchism, target originated, label 1\n",
      "center_word anarchism, target geertruid, label 0\n",
      "center_word anarchism, target jonietz, label 0\n",
      "center_word anarchism, target expertise, label 0\n",
      "center_word anarchism, target incipient, label 0\n",
      "center_word as, target originated, label 1\n",
      "center_word as, target tranvserse, label 0\n",
      "center_word as, target capitialization, label 0\n",
      "center_word as, target pitk, label 0\n",
      "center_word as, target poulet, label 0\n",
      "center_word anarchism, target as, label 1\n",
      "center_word anarchism, target ehirleraras, label 0\n",
      "center_word anarchism, target dispute, label 0\n",
      "center_word anarchism, target ossl, label 0\n",
      "center_word anarchism, target assassins, label 0\n",
      "center_word originated, target as, label 1\n",
      "center_word originated, target mituku, label 0\n",
      "center_word originated, target systeme, label 0\n",
      "center_word originated, target synonymized, label 0\n",
      "center_word originated, target keanu, label 0\n",
      "center_word abuse, target as, label 1\n",
      "center_word abuse, target eckhart, label 0\n",
      "center_word abuse, target kimmie, label 0\n",
      "center_word abuse, target mali, label 0\n",
      "center_word abuse, target sikhwomen, label 0\n",
      "center_word used, target as, label 1\n",
      "center_word used, target kusuba, label 0\n",
      "center_word used, target poullada, label 0\n",
      "center_word used, target moivre, label 0\n",
      "center_word used, target aksel, label 0\n",
      "center_word working, target as, label 1\n",
      "center_word working, target ecologos, label 0\n",
      "center_word working, target gorge, label 0\n",
      "center_word working, target atitl, label 0\n",
      "center_word working, target nrg, label 0\n",
      "center_word used, target working, label 1\n",
      "center_word used, target remington, label 0\n",
      "center_word used, target melbury, label 0\n",
      "center_word used, target apa, label 0\n",
      "center_word used, target sobrarbe, label 0\n",
      "center_word class, target working, label 1\n",
      "center_word class, target reviewer, label 0\n",
      "center_word class, target microphallus, label 0\n",
      "center_word class, target foolishness, label 0\n",
      "center_word class, target sinfonias, label 0\n"
     ]
    }
   ],
   "source": [
    "#构造数据，准备模型训练\n",
    "#max_window_size代表了最大的window_size的大小，程序会根据max_window_size从左到右扫描整个语料\n",
    "#negative_sample_num代表了对于每个正样本，我们需要随机采样多少负样本用于训练，\n",
    "#一般来说，negative_sample_num的值越大，训练效果越稳定，但是训练速度越慢。 \n",
    "def build_data(corpus, word2id_dict, word2id_freq, max_window_size = 3, \n",
    "               negative_sample_num = 4):\n",
    "    \n",
    "    #使用一个list存储处理好的数据\n",
    "    dataset = []\n",
    "    center_word_idx=0\n",
    "\n",
    "    #从左到右，开始枚举每个中心点的位置\n",
    "    while center_word_idx < len(corpus):\n",
    "        #以max_window_size为上限，随机采样一个window_size，这样会使得训练更加稳定\n",
    "        window_size = random.randint(1, max_window_size)\n",
    "        #当前的中心词就是center_word_idx所指向的词，可以当作正样本\n",
    "        positive_word = corpus[center_word_idx]\n",
    "\n",
    "        #以当前中心词为中心，左右两侧在window_size内的词就是上下文\n",
    "        context_word_range = (max(0, center_word_idx - window_size), min(len(corpus) - 1, center_word_idx + window_size))\n",
    "        context_word_candidates = [corpus[idx] for idx in range(context_word_range[0], context_word_range[1]+1) if idx != center_word_idx]\n",
    "\n",
    "        #对于每个正样本来说，随机采样negative_sample_num个负样本，用于训练\n",
    "        for context_word in context_word_candidates:\n",
    "            #首先把（上下文，正样本，label=1）的三元组数据放入dataset中，\n",
    "            #这里label=1表示这个样本是个正样本\n",
    "            dataset.append((context_word, positive_word, 1))\n",
    "\n",
    "            #开始负采样\n",
    "            i = 0\n",
    "            while i < negative_sample_num:\n",
    "                negative_word_candidate = random.randint(0, vocab_size-1)\n",
    "\n",
    "                if negative_word_candidate is not positive_word:\n",
    "                    #把（上下文，负样本，label=0）的三元组数据放入dataset中，\n",
    "                    #这里label=0表示这个样本是个负样本\n",
    "                    dataset.append((context_word, negative_word_candidate, 0))\n",
    "                    i += 1\n",
    "        \n",
    "        center_word_idx = min(len(corpus) - 1, center_word_idx + window_size)\n",
    "        if center_word_idx == (len(corpus) - 1):\n",
    "            center_word_idx += 1\n",
    "        if center_word_idx % 100000 == 0:\n",
    "            print(center_word_idx)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = build_data(corpus, word2id_dict, word2id_freq)\n",
    "for _, (context_word, target_word, label) in zip(range(50), dataset):\n",
    "    print(\"center_word %s, target %s, label %d\" % (id2word_dict[context_word],\n",
    "                                                   id2word_dict[target_word], label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       [ 56]]))\r"
     ]
    }
   ],
   "source": [
    "#构造mini-batch，准备对模型进行训练\n",
    "#我们将不同类型的数据放到不同的tensor里，便于神经网络进行处理\n",
    "#并通过numpy的array函数，构造出不同的tensor来，并把这些tensor送入神经网络中进行训练\n",
    "def build_batch(dataset, batch_size, epoch_num):\n",
    "    \n",
    "    #context_word_batch缓存batch_size个中心词\n",
    "    context_word_batch = []\n",
    "    #target_word_batch缓存batch_size个目标词（可以是正样本或者负样本）\n",
    "    target_word_batch = []\n",
    "    #label_batch缓存了batch_size个0或1的标签，用于模型训练\n",
    "    label_batch = []\n",
    "    #eval_word_batch每次随机生成几个样例，用于在运行阶段对模型做评估，以便更好地可视化训练效果。\n",
    "    eval_word_batch = []\n",
    "    \n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        #每次开启一个新epoch之前，都对数据进行一次随机打乱，提高训练效果\n",
    "        random.shuffle(dataset)\n",
    "        \n",
    "        for context_word, target_word, label in dataset:\n",
    "            #遍历dataset中的每个样本，并将这些数据送到不同的tensor里\n",
    "            context_word_batch.append([context_word])\n",
    "            target_word_batch.append([target_word])\n",
    "            label_batch.append(label)\n",
    "            \n",
    "            #构造训练中评估的样本，这里我们生成'one','king','chip'三个词的同义词，\n",
    "            #看模型认为的同义词有哪些\n",
    "            if len(eval_word_batch) == 0:\n",
    "                eval_word_batch.append([word2id_dict['one']])\n",
    "            elif len(eval_word_batch) == 1:\n",
    "                eval_word_batch.append([word2id_dict['king']])\n",
    "            elif len(eval_word_batch) ==2:\n",
    "                eval_word_batch.append([word2id_dict['who']])\n",
    "            #     eval_word_batch.append([random.randint(0, 99)])\n",
    "            # elif len(eval_word_batch) < 10:\n",
    "            #     eval_word_batch.append([random.randint(0, vocab_size-1)])\n",
    "\n",
    "            #当样本积攒到一个batch_size后，我们把数据都返回回来\n",
    "            #在这里我们使用numpy的array函数把list封装成tensor\n",
    "            #并使用python的迭代器机制，将数据yield出来\n",
    "            #使用迭代器的好处是可以节省内存\n",
    "            if len(context_word_batch) == batch_size:\n",
    "                yield epoch,\\\n",
    "                    np.array(context_word_batch).astype(\"int64\"),\\\n",
    "                    np.array(target_word_batch).astype(\"int64\"),\\\n",
    "                    np.array(label_batch).astype(\"float32\"),\\\n",
    "                    np.array(eval_word_batch).astype(\"int64\")\n",
    "                context_word_batch = []\n",
    "                target_word_batch = []\n",
    "                label_batch = []\n",
    "                eval_word_batch = []\n",
    "        \n",
    "    if len(context_word_batch) > 0:\n",
    "        yield epoch,\\\n",
    "            np.array(context_word_batch).astype(\"int64\"),\\\n",
    "            np.array(target_word_batch).astype(\"int64\"),\\\n",
    "            np.array(label_batch).astype(\"float32\"),\\\n",
    "            np.array(eval_word_batch).astype(\"int64\")\n",
    "\n",
    "for _, batch in zip(range(10), build_batch(dataset, 128, 3)):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义skip-gram训练网络结构\n",
    "#这里我们使用的是paddlepaddle的1.6.1版本\n",
    "#一般来说，在使用fluid训练的时候，我们需要通过一个类来定义网络结构，这个类继承了fluid.dygraph.Layer\n",
    "class SkipGram(fluid.dygraph.Layer):\n",
    "    def __init__(self, name_scope, vocab_size, embedding_size, init_scale=0.1):\n",
    "        #name_scope定义了这个类某个具体实例的名字，以便于区分不同的实例（模型）\n",
    "        #vocab_size定义了这个skipgram这个模型的词表大小\n",
    "        #embedding_size定义了词向量的维度是多少\n",
    "        #init_scale定义了词向量初始化的范围，一般来说，比较小的初始化范围有助于模型训练\n",
    "        super(SkipGram, self).__init__(name_scope)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        #使用paddle.fluid.dygraph提供的Embedding函数，构造一个词向量参数\n",
    "        #这个参数的大小为：[self.vocab_size, self.embedding_size]\n",
    "        #数据类型为：float32\n",
    "        #这个参数的名称为：embedding_para\n",
    "        #这个参数的初始化方式为在[-init_scale, init_scale]区间进行均匀采样\n",
    "        self.embedding = Embedding(\n",
    "            self.full_name(),\n",
    "            size=[self.vocab_size, self.embedding_size],\n",
    "            dtype='float32',\n",
    "            param_attr=fluid.ParamAttr(\n",
    "                name='embedding_para',\n",
    "                initializer=fluid.initializer.UniformInitializer(\n",
    "                    low=-0.5/embedding_size, high=0.5/embedding_size)))\n",
    "\n",
    "        #使用paddle.fluid.dygraph提供的Embedding函数，构造另外一个词向量参数\n",
    "        #这个参数的大小为：[self.vocab_size, self.embedding_size]\n",
    "        #数据类型为：float32\n",
    "        #这个参数的名称为：embedding_para\n",
    "        #这个参数的初始化方式为在[-init_scale, init_scale]区间进行均匀采样\n",
    "        #跟上面不同的是，这个参数的名称跟上面不同，因此，\n",
    "        #embedding_out_para和embedding_para虽然有相同的shape，但是权重不共享\n",
    "        self.embedding_out = Embedding(\n",
    "            self.full_name(),\n",
    "            size=[self.vocab_size, self.embedding_size],\n",
    "            dtype='float32',\n",
    "            param_attr=fluid.ParamAttr(\n",
    "                name='embedding_out_para',\n",
    "                initializer=fluid.initializer.UniformInitializer(\n",
    "                    low=-0.5/embedding_size, high=0.5/embedding_size)))\n",
    "\n",
    "    #定义网络的前向计算逻辑\n",
    "    #context_words是一个tensor（mini-batch），表示中心词\n",
    "    #target_words是一个tensor（mini-batch），表示目标词\n",
    "    #label是一个tensor（mini-batch），表示这个词是正样本还是负样本（用0或1表示）\n",
    "    #eval_words是一个tensor（mini-batch），\n",
    "    #用于在训练中计算这个tensor中对应词的同义词，用于观察模型的训练效果\n",
    "    def forward(self, context_words, target_words, label, eval_words):\n",
    "        #首先，通过embedding_para（self.embedding）参数，将mini-batch中的词转换为词向量\n",
    "        #这里context_words和eval_words_emb查询的是一个相同的参数\n",
    "        #而target_words_emb查询的是另一个参数\n",
    "        context_words_emb = self.embedding(context_words)\n",
    "        target_words_emb = self.embedding_out(target_words)\n",
    "        eval_words_emb = self.embedding(eval_words)\n",
    "        \n",
    "        #context_words_emb = [batch_size, embedding_size]\n",
    "        #target_words_emb = [batch_size, embedding_size]\n",
    "        #我们通过点乘的方式计算中心词到目标词的输出概率，并通过sigmoid函数估计这个词是正样本还是负样本的概率。\n",
    "        word_sim = fluid.layers.elementwise_mul(context_words_emb, target_words_emb)\n",
    "        word_sim = fluid.layers.reduce_sum(word_sim, dim = -1)\n",
    "        pred = fluid.layers.sigmoid(word_sim)\n",
    "\n",
    "        #通过估计的输出概率定义损失函数\n",
    "        loss = fluid.layers.sigmoid_cross_entropy_with_logits(word_sim, label)\n",
    "        loss = fluid.layers.reduce_mean(loss)\n",
    "        \n",
    "        #我们通过一个矩阵乘法，来对每个词计算他的同义词\n",
    "        #on_fly在机器学习或深度学习中往往指在在线计算中做什么，\n",
    "        #比如我们需要在训练中做评估，就可以说evaluation_on_fly\n",
    "        word_sim_on_fly = fluid.layers.matmul(eval_words_emb, \n",
    "            self.embedding._w, transpose_y = True)\n",
    "\n",
    "        #返回前向计算的结果，飞桨会通过backward函数自动计算出反向结果。\n",
    "        return pred, loss, word_sim_on_fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch num:0, step 100, loss 0.693\n",
      "epoch num:0, step 200, loss 0.693\n",
      "epoch num:0, step 300, loss 0.693\n",
      "epoch num:0, step 400, loss 0.693\n",
      "epoch num:0, step 500, loss 0.691\n",
      "epoch num:0, step 600, loss 0.687\n",
      "epoch num:0, step 700, loss 0.680\n",
      "epoch num:0, step 800, loss 0.666\n",
      "epoch num:0, step 900, loss 0.661\n",
      "epoch num:0, step 1000, loss 0.635\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is zero\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is eight\n",
      "for word one, the similar word is of\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is one\n",
      "for word who, the similar word is the\n",
      "for word who, the similar word is a\n",
      "for word who, the similar word is of\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is six\n",
      "for word king, the similar word is the\n",
      "for word king, the similar word is of\n",
      "for word king, the similar word is zero\n",
      "epoch num:0, step 1100, loss 0.622\n",
      "epoch num:0, step 1200, loss 0.601\n",
      "epoch num:0, step 1300, loss 0.595\n",
      "epoch num:0, step 1400, loss 0.561\n",
      "epoch num:0, step 1500, loss 0.539\n",
      "epoch num:0, step 1600, loss 0.567\n",
      "epoch num:0, step 1700, loss 0.507\n",
      "epoch num:0, step 1800, loss 0.459\n",
      "epoch num:0, step 1900, loss 0.471\n",
      "epoch num:0, step 2000, loss 0.457\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is eight\n",
      "for word one, the similar word is zero\n",
      "for word one, the similar word is was\n",
      "for word one, the similar word is nine\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is was\n",
      "for word who, the similar word is be\n",
      "for word who, the similar word is its\n",
      "for word who, the similar word is a\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is was\n",
      "for word king, the similar word is before\n",
      "for word king, the similar word is u\n",
      "for word king, the similar word is movie\n",
      "epoch num:0, step 2100, loss 0.464\n",
      "epoch num:0, step 2200, loss 0.417\n",
      "epoch num:0, step 2300, loss 0.391\n",
      "epoch num:0, step 2400, loss 0.444\n",
      "epoch num:0, step 2500, loss 0.353\n",
      "epoch num:0, step 2600, loss 0.402\n",
      "epoch num:0, step 2700, loss 0.397\n",
      "epoch num:0, step 2800, loss 0.287\n",
      "epoch num:0, step 2900, loss 0.337\n",
      "epoch num:0, step 3000, loss 0.329\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is zero\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is was\n",
      "for word one, the similar word is four\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is on\n",
      "for word who, the similar word is however\n",
      "for word who, the similar word is took\n",
      "for word who, the similar word is some\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is turn\n",
      "for word king, the similar word is will\n",
      "for word king, the similar word is before\n",
      "for word king, the similar word is operation\n",
      "epoch num:0, step 3100, loss 0.312\n",
      "epoch num:0, step 3200, loss 0.368\n",
      "epoch num:0, step 3300, loss 0.362\n",
      "epoch num:0, step 3400, loss 0.284\n",
      "epoch num:0, step 3500, loss 0.308\n",
      "epoch num:0, step 3600, loss 0.318\n",
      "epoch num:0, step 3700, loss 0.286\n",
      "epoch num:0, step 3800, loss 0.285\n",
      "epoch num:0, step 3900, loss 0.272\n",
      "epoch num:0, step 4000, loss 0.263\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is four\n",
      "for word one, the similar word is zero\n",
      "for word one, the similar word is eight\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is have\n",
      "for word who, the similar word is may\n",
      "for word who, the similar word is is\n",
      "for word who, the similar word is be\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is oxford\n",
      "for word king, the similar word is will\n",
      "for word king, the similar word is what\n",
      "for word king, the similar word is be\n",
      "epoch num:0, step 4100, loss 0.255\n",
      "epoch num:0, step 4200, loss 0.352\n",
      "epoch num:0, step 4300, loss 0.273\n",
      "epoch num:0, step 4400, loss 0.257\n",
      "epoch num:0, step 4500, loss 0.272\n",
      "epoch num:0, step 4600, loss 0.261\n",
      "epoch num:0, step 4700, loss 0.199\n",
      "epoch num:0, step 4800, loss 0.313\n",
      "epoch num:0, step 4900, loss 0.298\n",
      "epoch num:0, step 5000, loss 0.246\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is was\n",
      "for word one, the similar word is four\n",
      "for word one, the similar word is three\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is may\n",
      "for word who, the similar word is rock\n",
      "for word who, the similar word is conversion\n",
      "for word who, the similar word is large\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is will\n",
      "for word king, the similar word is what\n",
      "for word king, the similar word is exchange\n",
      "for word king, the similar word is about\n",
      "epoch num:0, step 5100, loss 0.231\n",
      "epoch num:0, step 5200, loss 0.281\n",
      "epoch num:0, step 5300, loss 0.251\n",
      "epoch num:0, step 5400, loss 0.243\n",
      "epoch num:0, step 5500, loss 0.224\n",
      "epoch num:0, step 5600, loss 0.271\n",
      "epoch num:0, step 5700, loss 0.210\n",
      "epoch num:0, step 5800, loss 0.246\n",
      "epoch num:0, step 5900, loss 0.251\n",
      "epoch num:0, step 6000, loss 0.300\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is four\n",
      "for word one, the similar word is two\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is may\n",
      "for word who, the similar word is simply\n",
      "for word who, the similar word is response\n",
      "for word who, the similar word is depends\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is what\n",
      "for word king, the similar word is museum\n",
      "for word king, the similar word is could\n",
      "for word king, the similar word is germany\n",
      "epoch num:0, step 6100, loss 0.280\n",
      "epoch num:0, step 6200, loss 0.236\n",
      "epoch num:0, step 6300, loss 0.218\n",
      "epoch num:0, step 6400, loss 0.239\n",
      "epoch num:0, step 6500, loss 0.203\n",
      "epoch num:0, step 6600, loss 0.183\n",
      "epoch num:0, step 6700, loss 0.220\n",
      "epoch num:0, step 6800, loss 0.219\n",
      "epoch num:0, step 6900, loss 0.216\n",
      "epoch num:0, step 7000, loss 0.220\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is two\n",
      "for word one, the similar word is is\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is council\n",
      "for word who, the similar word is may\n",
      "for word who, the similar word is on\n",
      "for word who, the similar word is free\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is forms\n",
      "for word king, the similar word is what\n",
      "for word king, the similar word is inches\n",
      "for word king, the similar word is end\n",
      "epoch num:0, step 7100, loss 0.225\n",
      "epoch num:0, step 7200, loss 0.244\n",
      "epoch num:0, step 7300, loss 0.254\n",
      "epoch num:0, step 7400, loss 0.248\n",
      "epoch num:0, step 7500, loss 0.233\n",
      "epoch num:0, step 7600, loss 0.260\n",
      "epoch num:0, step 7700, loss 0.210\n",
      "epoch num:0, step 7800, loss 0.199\n",
      "epoch num:0, step 7900, loss 0.176\n",
      "epoch num:0, step 8000, loss 0.229\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is two\n",
      "for word one, the similar word is won\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is may\n",
      "for word who, the similar word is structures\n",
      "for word who, the similar word is exchange\n",
      "for word who, the similar word is only\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is museum\n",
      "for word king, the similar word is forms\n",
      "for word king, the similar word is sky\n",
      "for word king, the similar word is stability\n",
      "epoch num:0, step 8100, loss 0.222\n",
      "epoch num:0, step 8200, loss 0.242\n",
      "epoch num:0, step 8300, loss 0.205\n",
      "epoch num:0, step 8400, loss 0.238\n",
      "epoch num:0, step 8500, loss 0.294\n",
      "epoch num:0, step 8600, loss 0.169\n",
      "epoch num:0, step 8700, loss 0.278\n",
      "epoch num:0, step 8800, loss 0.205\n",
      "epoch num:0, step 8900, loss 0.202\n",
      "epoch num:0, step 9000, loss 0.230\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is five\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is is\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is may\n",
      "for word who, the similar word is hitler\n",
      "for word who, the similar word is issues\n",
      "for word who, the similar word is simply\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is going\n",
      "for word king, the similar word is peoples\n",
      "for word king, the similar word is bolder\n",
      "for word king, the similar word is sat\n",
      "epoch num:0, step 9100, loss 0.271\n",
      "epoch num:0, step 9200, loss 0.270\n",
      "epoch num:0, step 9300, loss 0.300\n",
      "epoch num:0, step 9400, loss 0.214\n",
      "epoch num:0, step 9500, loss 0.251\n",
      "epoch num:0, step 9600, loss 0.224\n",
      "epoch num:0, step 9700, loss 0.202\n",
      "epoch num:0, step 9800, loss 0.196\n",
      "epoch num:0, step 9900, loss 0.214\n",
      "epoch num:0, step 10000, loss 0.263\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is two\n",
      "for word one, the similar word is eight\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is sat\n",
      "for word who, the similar word is may\n",
      "for word who, the similar word is corner\n",
      "for word who, the similar word is sometimes\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is sat\n",
      "for word king, the similar word is lewinsky\n",
      "for word king, the similar word is peoples\n",
      "for word king, the similar word is going\n",
      "epoch num:0, step 10100, loss 0.273\n",
      "epoch num:0, step 10200, loss 0.265\n",
      "epoch num:0, step 10300, loss 0.252\n",
      "epoch num:0, step 10400, loss 0.154\n",
      "epoch num:0, step 10500, loss 0.194\n",
      "epoch num:0, step 10600, loss 0.191\n",
      "epoch num:0, step 10700, loss 0.233\n",
      "epoch num:0, step 10800, loss 0.178\n",
      "epoch num:0, step 10900, loss 0.206\n",
      "epoch num:0, step 11000, loss 0.183\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is five\n",
      "for word one, the similar word is two\n",
      "for word one, the similar word is three\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is sat\n",
      "for word who, the similar word is personal\n",
      "for word who, the similar word is day\n",
      "for word who, the similar word is corner\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is museum\n",
      "for word king, the similar word is sat\n",
      "for word king, the similar word is forms\n",
      "for word king, the similar word is continue\n",
      "epoch num:0, step 11100, loss 0.203\n",
      "epoch num:0, step 11200, loss 0.261\n",
      "epoch num:0, step 11300, loss 0.238\n",
      "epoch num:0, step 11400, loss 0.186\n",
      "epoch num:0, step 11500, loss 0.267\n",
      "epoch num:0, step 11600, loss 0.200\n",
      "epoch num:0, step 11700, loss 0.230\n",
      "epoch num:0, step 11800, loss 0.202\n",
      "epoch num:0, step 11900, loss 0.219\n",
      "epoch num:0, step 12000, loss 0.174\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is five\n",
      "for word one, the similar word is eight\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is parliament\n",
      "for word who, the similar word is army\n",
      "for word who, the similar word is errors\n",
      "for word who, the similar word is day\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is museum\n",
      "for word king, the similar word is will\n",
      "for word king, the similar word is forms\n",
      "for word king, the similar word is programming\n",
      "epoch num:0, step 12100, loss 0.245\n",
      "epoch num:0, step 12200, loss 0.179\n",
      "epoch num:0, step 12300, loss 0.307\n",
      "epoch num:0, step 12400, loss 0.222\n",
      "epoch num:0, step 12500, loss 0.223\n",
      "epoch num:0, step 12600, loss 0.187\n",
      "epoch num:0, step 12700, loss 0.224\n",
      "epoch num:0, step 12800, loss 0.185\n",
      "epoch num:0, step 12900, loss 0.199\n",
      "epoch num:0, step 13000, loss 0.243\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is five\n",
      "for word one, the similar word is zero\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is errors\n",
      "for word who, the similar word is personal\n",
      "for word who, the similar word is rock\n",
      "for word who, the similar word is will\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is will\n",
      "for word king, the similar word is forms\n",
      "for word king, the similar word is museum\n",
      "for word king, the similar word is determined\n",
      "epoch num:0, step 13100, loss 0.293\n",
      "epoch num:0, step 13200, loss 0.228\n",
      "epoch num:0, step 13300, loss 0.217\n",
      "epoch num:0, step 13400, loss 0.214\n",
      "epoch num:0, step 13500, loss 0.224\n",
      "epoch num:0, step 13600, loss 0.215\n",
      "epoch num:0, step 13700, loss 0.210\n",
      "epoch num:0, step 13800, loss 0.195\n",
      "epoch num:0, step 13900, loss 0.202\n",
      "epoch num:0, step 14000, loss 0.227\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is zero\n",
      "for word one, the similar word is five\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is fighting\n",
      "for word who, the similar word is errors\n",
      "for word who, the similar word is personal\n",
      "for word who, the similar word is young\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is gains\n",
      "for word king, the similar word is forms\n",
      "for word king, the similar word is continue\n",
      "for word king, the similar word is lewinsky\n",
      "epoch num:0, step 14100, loss 0.214\n",
      "epoch num:0, step 14200, loss 0.230\n",
      "epoch num:0, step 14300, loss 0.194\n",
      "epoch num:0, step 14400, loss 0.199\n",
      "epoch num:0, step 14500, loss 0.194\n",
      "epoch num:0, step 14600, loss 0.198\n",
      "epoch num:0, step 14700, loss 0.248\n",
      "epoch num:0, step 14800, loss 0.235\n",
      "epoch num:0, step 14900, loss 0.191\n",
      "epoch num:0, step 15000, loss 0.193\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is five\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is two\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is errors\n",
      "for word who, the similar word is fighting\n",
      "for word who, the similar word is sat\n",
      "for word who, the similar word is landings\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is gains\n",
      "for word king, the similar word is chronicle\n",
      "for word king, the similar word is seventh\n",
      "for word king, the similar word is lewinsky\n",
      "epoch num:0, step 15100, loss 0.260\n",
      "epoch num:0, step 15200, loss 0.201\n",
      "epoch num:0, step 15300, loss 0.280\n",
      "epoch num:0, step 15400, loss 0.181\n",
      "epoch num:0, step 15500, loss 0.247\n",
      "epoch num:0, step 15600, loss 0.179\n",
      "epoch num:0, step 15700, loss 0.238\n",
      "epoch num:0, step 15800, loss 0.200\n",
      "epoch num:0, step 15900, loss 0.261\n",
      "epoch num:0, step 16000, loss 0.230\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is five\n",
      "for word one, the similar word is eight\n",
      "for word one, the similar word is three\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is temporal\n",
      "for word who, the similar word is solitary\n",
      "for word who, the similar word is basis\n",
      "for word who, the similar word is dams\n",
      "for word king, the similar word is king\n",
      "for word king, the similar word is asterix\n",
      "for word king, the similar word is chronicle\n",
      "for word king, the similar word is gains\n",
      "for word king, the similar word is largly\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a511493fac51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#使用build_batch函数，以mini-batch为单位，遍历训练数据，并训练网络\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     for epoch_num, context_words, target_words, label, eval_words in build_batch(\n\u001b[0;32m---> 27\u001b[0;31m         dataset, batch_size, epoch_num):\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# print(eval_words.shape[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#使用fluid.dygraph.to_variable函数，将一个numpy的tensor，转换为飞桨可计算的tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7a58d4a73277>\u001b[0m in \u001b[0;36mbuild_batch\u001b[0;34m(dataset, batch_size, epoch_num)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m#遍历dataset中的每个样本，并将这些数据送到不同的tensor里\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mcontext_word_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mtarget_word_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mlabel_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#开始训练，定义一些训练过程中需要使用的超参数\n",
    "batch_size = 512\n",
    "epoch_num = 3\n",
    "embedding_size = 200\n",
    "step = 0\n",
    "learning_rate = 0.001\n",
    "\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.numpy()\n",
    "    x = W[word2id_dict[query_token]]\n",
    "    cos = np.dot(W, x) / np.sqrt(np.sum(W * W, axis=1) * np.sum(x * x) + 1e-9)\n",
    "    flat = cos.flatten()\n",
    "    indices = np.argpartition(flat, -k)[-k:]\n",
    "    indices = indices[np.argsort(-flat[indices])]\n",
    "    for i in indices:\n",
    "        print('for word %s, the similar word is %s' % (query_token, str(id2word_dict[i])))\n",
    "\n",
    "#将模型放到GPU上训练（fluid.CUDAPlace(0)），如果需要指定CPU，则需要改为fluid.CPUPlace()\n",
    "with fluid.dygraph.guard(fluid.CUDAPlace(0)):\n",
    "    #通过我们定义的SkipGram类，来构造一个Skip-gram模型网络\n",
    "    skip_gram_model = SkipGram(\"skip_gram_model\", vocab_size, embedding_size)\n",
    "    #构造训练这个网络的优化器\n",
    "    adam = fluid.optimizer.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    #使用build_batch函数，以mini-batch为单位，遍历训练数据，并训练网络\n",
    "    for epoch_num, context_words, target_words, label, eval_words in build_batch(\n",
    "        dataset, batch_size, epoch_num):\n",
    "        # print(eval_words.shape[0])\n",
    "        #使用fluid.dygraph.to_variable函数，将一个numpy的tensor，转换为飞桨可计算的tensor\n",
    "        context_words_var = fluid.dygraph.to_variable(context_words)\n",
    "        target_words_var = fluid.dygraph.to_variable(target_words)\n",
    "        label_var = fluid.dygraph.to_variable(label)\n",
    "        eval_words_var = fluid.dygraph.to_variable(eval_words)\n",
    "        \n",
    "        #将转换后的tensor送入飞桨中，进行一次前向计算，并得到计算结果\n",
    "        pred, loss, word_sim_on_fly = skip_gram_model(\n",
    "            context_words_var, target_words_var, label_var, eval_words_var)\n",
    "\n",
    "        #通过backward函数，让程序自动完成反向计算\n",
    "        loss.backward()\n",
    "        #通过minimize函数，让程序根据loss，完成一步对参数的优化更新\n",
    "        adam.minimize(loss)\n",
    "        #使用clear_gradients函数清空模型中的梯度，以便于下一个mini-batch进行更新\n",
    "        skip_gram_model.clear_gradients()\n",
    "\n",
    "        #每经过100个mini-batch，打印一次当前的loss，看看loss是否在稳定下降\n",
    "        step += 1\n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch num:%d, step %d, loss %.3f\" % (epoch_num, step, loss.numpy()[0]))\n",
    "\n",
    "        #没经过1000个mini-batch，打印一次模型对eval_words中的10个词计算的同义词\n",
    "        #这里我们使用词和词之间的向量点积作为衡量相似度的方法\n",
    "        #我们只打印了5个最相似的词\n",
    "        if step % 1000 == 0:\n",
    "            # word_sim_on_fly = word_sim_on_fly.numpy()\n",
    "            # word_sim_on_fly = np.argsort(word_sim_on_fly)\n",
    "\n",
    "            # for _id in range(len(eval_words)):\n",
    "            #     curr_eval_word = id2word_dict[eval_words[_id][0]]\n",
    "            #     top_n_sim_words = []\n",
    "            #     for j in range(1, 6):\n",
    "            #         top_n_sim_words.append(id2word_dict[word_sim_on_fly[_id][-1 * j]])\n",
    "            #     print(\"for word %s, the most similar word is: %s\" % \n",
    "            #           (curr_eval_word, \", \".join(top_n_sim_words)))\n",
    "            get_similar_tokens('one', 5, skip_gram_model.embedding._w)\n",
    "            get_similar_tokens('who', 5, skip_gram_model.embedding._w)\n",
    "            get_similar_tokens('king', 5, skip_gram_model.embedding._w)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
