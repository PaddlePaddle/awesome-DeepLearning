{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>#广州#【广州游行打砸抢罪犯资料公布！居然是日本间谍！】 ​</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>【政协委员提议恢复大清王朝】康熙十世孙、广州政协委员金复新表示，他准备走遍中国收集100万人...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>有木有人和我一样。睡觉时头总爱靠在枕头的一角。据说这样的孩纸，都没安全感。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>据说，看到这张图的人，许个愿，在十秒内转发的，就能美梦成真！！我们也试试！！！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>【老小子走了！李登辉今天凌晨心脏病复发身亡】台北消息：原国民党、台联党主席，有“台独教父”之...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0                     #广州#【广州游行打砸抢罪犯资料公布！居然是日本间谍！】 ​\n",
       "1      0  【政协委员提议恢复大清王朝】康熙十世孙、广州政协委员金复新表示，他准备走遍中国收集100万人...\n",
       "2      1              有木有人和我一样。睡觉时头总爱靠在枕头的一角。据说这样的孩纸，都没安全感。\n",
       "3      1            据说，看到这张图的人，许个愿，在十秒内转发的，就能美梦成真！！我们也试试！！！\n",
       "4      0  【老小子走了！李登辉今天凌晨心脏病复发身亡】台北消息：原国民党、台联党主席，有“台独教父”之..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.read_csv(\"data/data69671/all_data.tsv\", sep=\"\\t\")\r\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 生成词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "all_str = all_data[\"text\"].values.tolist()\r\n",
    "dict_set = set() # 保证每个字符只有唯一的对应数字\r\n",
    "for content in all_str:\r\n",
    "    for s in content:\r\n",
    "        dict_set.add(s)\r\n",
    "# 添加未知字符\r\n",
    "dict_set.add(\"<unk>\")\r\n",
    "# 把元组转换成字典，一个字对应一个数字\r\n",
    "dict_list = []\r\n",
    "i = 0\r\n",
    "for s in dict_set:\r\n",
    "    dict_list.append([s, i])\r\n",
    "    i += 1\r\n",
    "dict_txt = dict(dict_list)\r\n",
    "# 字典保存到本地\r\n",
    "with open(\"dict.txt\", 'w', encoding='utf-8') as f:\r\n",
    "    f.write(str(dict_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 获取字典的长度\r\n",
    "def get_dict_len(dict_path):\r\n",
    "    with open(dict_path, 'r', encoding='utf-8') as f:\r\n",
    "        line = eval(f.readlines()[0])\r\n",
    "    return len(line.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4410\n"
     ]
    }
   ],
   "source": [
    "print(get_dict_len(\"dict.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 划分训练集、验证集以及测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2366\n",
      "676\n",
      "345\n"
     ]
    }
   ],
   "source": [
    "all_data_list = all_data.values.tolist()\r\n",
    "train_length = len(all_data) // 10 * 7\r\n",
    "dev_length = len(all_data) // 10 * 2\r\n",
    "\r\n",
    "train_data = []\r\n",
    "dev_data = []\r\n",
    "test_data = []\r\n",
    "for i in range(train_length):\r\n",
    "    text = \"\"\r\n",
    "    for s in all_data_list[i][1]:\r\n",
    "        text = text + str(dict_txt[s]) + \",\"\r\n",
    "    text = text[:-1]\r\n",
    "    train_data.append([text, all_data_list[i][0]])\r\n",
    "\r\n",
    "for i in range(train_length, train_length+dev_length):\r\n",
    "    text = \"\"\r\n",
    "    for s in all_data_list[i][1]:\r\n",
    "        text = text + str(dict_txt[s]) + \",\"\r\n",
    "    text = text[:-1]\r\n",
    "    dev_data.append([text, all_data_list[i][0]])\r\n",
    "\r\n",
    "for i in range(train_length+dev_length, len(all_data)):\r\n",
    "    text = \"\"\r\n",
    "    for s in all_data_list[i][1]:\r\n",
    "        text = text + str(dict_txt[s]) + \",\"\r\n",
    "    text = text[:-1]\r\n",
    "    test_data.append([text, all_data_list[i][0]])\r\n",
    "\r\n",
    "print(len(train_data))\r\n",
    "print(len(dev_data))\r\n",
    "print(len(test_data))\r\n",
    "df_train = pd.DataFrame(columns=[\"text\", \"label\"], data=train_data)\r\n",
    "df_dev = pd.DataFrame(columns=[\"text\", \"label\"], data=dev_data)\r\n",
    "df_test = pd.DataFrame(columns=[\"text\", \"label\"], data=test_data)\r\n",
    "df_train.to_csv(\"train_data.csv\", index=False)\r\n",
    "df_dev.to_csv(\"dev_data.csv\", index=False)\r\n",
    "df_test.to_csv(\"test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\r\n",
    "import paddle\r\n",
    "from paddle.io import Dataset, DataLoader\r\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\r\n",
    "    \"\"\"\r\n",
    "    步骤一：继承paddle.io.Dataset类\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, mode='train'):\r\n",
    "        \"\"\"\r\n",
    "        步骤二：实现构造函数，定义数据读取方式，划分训练和测试数据集\r\n",
    "        \"\"\"\r\n",
    "        super(MyDataset, self).__init__()\r\n",
    "        self.label = True\r\n",
    "        if mode == 'train':\r\n",
    "            text = pd.read_csv(\"train_data.csv\")[\"text\"].values.tolist()\r\n",
    "            label = pd.read_csv(\"train_data.csv\")[\"label\"].values.tolist()\r\n",
    "            self.data = []\r\n",
    "            for i in range(len(text)):\r\n",
    "                self.data.append([])\r\n",
    "                self.data[-1].append(np.array([int(i) for i in text[i].split(\",\")]))\r\n",
    "                self.data[-1][0] = self.data[-1][0][:256].astype('int64')if len(self.data[-1][0])>=256 else np.concatenate([self.data[-1][0], np.array([dict_txt[\"<unk>\"]]*(256-len(self.data[-1][0])))]).astype('int64')\r\n",
    "                self.data[-1].append(np.array(int(label[i])).astype('int64'))\r\n",
    "        elif mode == 'dev':\r\n",
    "            text = pd.read_csv(\"dev_data.csv\")[\"text\"].values.tolist()\r\n",
    "            label = pd.read_csv(\"dev_data.csv\")[\"label\"].values.tolist()\r\n",
    "            self.data = []\r\n",
    "            for i in range(len(text)):\r\n",
    "                self.data.append([])\r\n",
    "                self.data[-1].append(np.array([int(i) for i in text[i].split(\",\")]))\r\n",
    "                self.data[-1][0] = self.data[-1][0][:256].astype('int64')if len(self.data[-1][0])>=256 else np.concatenate([self.data[-1][0], np.array([dict_txt[\"<unk>\"]]*(256-len(self.data[-1][0])))]).astype('int64')\r\n",
    "                self.data[-1].append(np.array(int(label[i])).astype('int64'))\r\n",
    "        else:\r\n",
    "            text = pd.read_csv(\"test_data.csv\")[\"text\"].values.tolist()\r\n",
    "            label = pd.read_csv(\"test_data.csv\")[\"label\"].values.tolist()\r\n",
    "            self.data = []\r\n",
    "            for i in range(len(text)):\r\n",
    "                self.data.append([])\r\n",
    "                self.data[-1].append(np.array([int(i) for i in text[i].split(\",\")]))\r\n",
    "                self.data[-1][0] = self.data[-1][0][:256].astype('int64')if len(self.data[-1][0])>=256 else np.concatenate([self.data[-1][0], np.array([dict_txt[\"<unk>\"]]*(256-len(self.data[-1][0])))]).astype('int64')\r\n",
    "                self.data[-1].append(np.array(int(label[i])).astype('int64'))\r\n",
    "            self.label = False\r\n",
    "    def __getitem__(self, index):\r\n",
    "        \"\"\"\r\n",
    "        步骤三：实现__getitem__方法，定义指定index时如何获取数据，并返回单条数据（训练数据，对应的标签）\r\n",
    "        \"\"\"\r\n",
    "        text_ =  self.data[index][0]\r\n",
    "        label_ = self.data[index][1]\r\n",
    "        \r\n",
    "        if self.label:\r\n",
    "            return text_, label_\r\n",
    "        else:\r\n",
    "            return text_\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        \"\"\"\r\n",
    "        步骤四：实现__len__方法，返回数据集总数目\r\n",
    "        \"\"\"\r\n",
    "        return len(self.data)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = MyDataset(mode=\"train\")\r\n",
    "dev_data = MyDataset(mode=\"dev\")\r\n",
    "test_data = MyDataset(mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\r\n",
    "dev_loader = DataLoader(dev_data, batch_size=BATCH_SIZE, shuffle=True)\r\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs_dim = get_dict_len(\"dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class myLSTM(nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(myLSTM, self).__init__()\r\n",
    "\r\n",
    "        # num_embeddings (int) - 嵌入字典的大小， input中的id必须满足 0 =< id < num_embeddings 。 。\r\n",
    "        # embedding_dim (int) - 每个嵌入向量的维度。\r\n",
    "        # padding_idx (int|long|None) - padding_idx的配置区间为 [-weight.shape[0], weight.shape[0]，如果配置了padding_idx，那么在训练过程中遇到此id时会被用\r\n",
    "        # sparse (bool) - 是否使用稀疏更新，在词嵌入权重较大的情况下，使用稀疏更新能够获得更快的训练速度及更小的内存/显存占用。\r\n",
    "        # weight_attr (ParamAttr|None) - 指定嵌入向量的配置，包括初始化方法，具体用法请参见 ParamAttr ，一般无需设置，默认值为None。\r\n",
    "        self.embedding = nn.Embedding(inputs_dim, 256)\r\n",
    "\r\n",
    "        # input_size (int) - 输入的大小。\r\n",
    "        # hidden_size (int) - 隐藏状态大小。\r\n",
    "        # num_layers (int，可选) - 网络层数。默认为1。\r\n",
    "        # direction (str，可选) - 网络迭代方向，可设置为forward或bidirect（或bidirectional）。默认为forward。\r\n",
    "        # time_major (bool，可选) - 指定input的第一个维度是否是time steps。默认为False。\r\n",
    "        # dropout (float，可选) - dropout概率，指的是出第一层外每层输入时的dropout概率。默认为0。\r\n",
    "        # weight_ih_attr (ParamAttr，可选) - weight_ih的参数。默认为None。\r\n",
    "        # weight_hh_attr (ParamAttr，可选) - weight_hh的参数。默认为None。\r\n",
    "        # bias_ih_attr (ParamAttr，可选) - bias_ih的参数。默认为None。\r\n",
    "        # bias_hh_attr (ParamAttr，可选) - bias_hh的参数。默认为None。\r\n",
    "        self.lstm = nn.LSTM(256, 256, num_layers=2, direction='bidirectional',dropout=0.5)\r\n",
    "\r\n",
    "        # in_features (int) – 线性变换层输入单元的数目。\r\n",
    "        # out_features (int) – 线性变换层输出单元的数目。\r\n",
    "        # weight_attr (ParamAttr, 可选) – 指定权重参数的属性。默认值为None，表示使用默认的权重参数属性，将权重参数初始化为0。具体用法请参见 ParamAttr 。\r\n",
    "        # bias_attr (ParamAttr|bool, 可选) – 指定偏置参数的属性。 bias_attr 为bool类型且设置为False时，表示不会为该层添加偏置。 bias_attr 如果设置为True或者None，则表示使用默认的偏置参数属性，将偏置参数初始化为0。具体用法请参见 ParamAttr 。默认值为None。\r\n",
    "        # name (str，可选) – 具体用法请参见 Name ，一般无需设置，默认值为None。\r\n",
    "        self.linear = nn.Linear(in_features=256*2, out_features=2)\r\n",
    "\r\n",
    "        self.dropout = nn.Dropout(0.5)\r\n",
    "    \r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        \r\n",
    "        emb = self.dropout(self.embedding(inputs))\r\n",
    "        \r\n",
    "        output, (hidden, _) = self.lstm(emb)\r\n",
    "        #output形状大小为[batch_size,seq_len,num_directions * hidden_size]\r\n",
    "        #hidden形状大小为[num_layers * num_directions, batch_size, hidden_size]\r\n",
    "        #把前向的hidden与后向的hidden合并在一起\r\n",
    "        hidden = paddle.concat((hidden[-2,:,:], hidden[-1,:,:]), axis = 1)\r\n",
    "        hidden = self.dropout(hidden)\r\n",
    "        #hidden形状大小为[batch_size, hidden_size * num_directions]\r\n",
    "        return self.linear(hidden) \r\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 封装模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_model = paddle.Model(myLSTM())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 配置优化器等参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm_model.prepare(paddle.optimizer.Adam(learning_rate=0.001, parameters=lstm_model.parameters()),\r\n",
    "              paddle.nn.CrossEntropyLoss(),\r\n",
    "              paddle.metric.Accuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous step.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 19/19 [==============================] - loss: 0.6986 - acc: 0.5207 - 115ms/step         \n",
      "save checkpoint at /home/aistudio/work/lstm/0\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 6/6 [==============================] - loss: 0.7058 - acc: 0.5311 - 35ms/step\n",
      "Eval samples: 676\n",
      "Epoch 2/10\n",
      "step 19/19 [==============================] - loss: 0.5084 - acc: 0.6167 - 106ms/step         \n",
      "save checkpoint at /home/aistudio/work/lstm/1\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 6/6 [==============================] - loss: 0.6028 - acc: 0.7352 - 33ms/step\n",
      "Eval samples: 676\n",
      "Epoch 3/10\n",
      "step 19/19 [==============================] - loss: 0.2600 - acc: 0.8588 - 105ms/step         \n",
      "save checkpoint at /home/aistudio/work/lstm/2\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 6/6 [==============================] - loss: 0.3325 - acc: 0.8299 - 34ms/step\n",
      "Eval samples: 676\n",
      "Epoch 4/10\n",
      "step 19/19 [==============================] - loss: 0.2010 - acc: 0.9193 - 105ms/step         \n",
      "save checkpoint at /home/aistudio/work/lstm/3\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 6/6 [==============================] - loss: 0.5653 - acc: 0.8151 - 34ms/step\n",
      "Eval samples: 676\n",
      "Epoch 5/10\n",
      "step 19/19 [==============================] - loss: 0.0431 - acc: 0.9607 - 106ms/step         \n",
      "save checkpoint at /home/aistudio/work/lstm/4\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 6/6 [==============================] - loss: 0.4175 - acc: 0.8166 - 35ms/step\n",
      "Eval samples: 676\n",
      "Epoch 6/10\n",
      "step 19/19 [==============================] - loss: 0.0508 - acc: 0.9755 - 106ms/step         \n",
      "save checkpoint at /home/aistudio/work/lstm/5\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 6/6 [==============================] - loss: 0.5019 - acc: 0.8343 - 36ms/step\n",
      "Eval samples: 676\n",
      "Epoch 7/10\n",
      "step 19/19 [==============================] - loss: 0.0300 - acc: 0.9839 - 106ms/step         \n",
      "save checkpoint at /home/aistudio/work/lstm/6\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 6/6 [==============================] - loss: 0.9145 - acc: 0.8151 - 35ms/step\n",
      "Eval samples: 676\n",
      "Epoch 8/10\n",
      "step 19/19 [==============================] - loss: 0.0025 - acc: 0.9911 - 107ms/step         \n",
      "save checkpoint at /home/aistudio/work/lstm/7\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 6/6 [==============================] - loss: 0.8443 - acc: 0.8210 - 36ms/step\n",
      "Eval samples: 676\n",
      "Epoch 9/10\n",
      "step 19/19 [==============================] - loss: 0.0145 - acc: 0.9958 - 106ms/step         \n",
      "save checkpoint at /home/aistudio/work/lstm/8\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 6/6 [==============================] - loss: 0.4440 - acc: 0.8358 - 36ms/step\n",
      "Eval samples: 676\n",
      "Epoch 10/10\n",
      "step 19/19 [==============================] - loss: 0.0272 - acc: 0.9958 - 106ms/step         \n",
      "save checkpoint at /home/aistudio/work/lstm/9\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 6/6 [==============================] - loss: 1.4123 - acc: 0.8314 - 34ms/step\n",
      "Eval samples: 676\n",
      "save checkpoint at /home/aistudio/work/lstm/final\n"
     ]
    }
   ],
   "source": [
    "lstm_model.fit(train_loader,\r\n",
    "          dev_loader,\r\n",
    "          epochs=10,\r\n",
    "          batch_size=BATCH_SIZE,\r\n",
    "          verbose=1,\r\n",
    "          save_dir=\"work/lstm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict begin...\n",
      "step 3/3 [==============================] - 34ms/step         \n",
      "Predict samples: 345\n"
     ]
    }
   ],
   "source": [
    "result = lstm_model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class myGRU(nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(myGRU, self).__init__()\r\n",
    "\r\n",
    "        # num_embeddings (int) - 嵌入字典的大小， input中的id必须满足 0 =< id < num_embeddings 。 。\r\n",
    "        # embedding_dim (int) - 每个嵌入向量的维度。\r\n",
    "        # padding_idx (int|long|None) - padding_idx的配置区间为 [-weight.shape[0], weight.shape[0]，如果配置了padding_idx，那么在训练过程中遇到此id时会被用\r\n",
    "        # sparse (bool) - 是否使用稀疏更新，在词嵌入权重较大的情况下，使用稀疏更新能够获得更快的训练速度及更小的内存/显存占用。\r\n",
    "        # weight_attr (ParamAttr|None) - 指定嵌入向量的配置，包括初始化方法，具体用法请参见 ParamAttr ，一般无需设置，默认值为None。\r\n",
    "        self.embedding = nn.Embedding(inputs_dim, 256)\r\n",
    "\r\n",
    "        # input_size (int) - 输入的大小。\r\n",
    "        # hidden_size (int) - 隐藏状态大小。\r\n",
    "        # num_layers (int，可选) - 网络层数。默认为1。\r\n",
    "        # direction (str，可选) - 网络迭代方向，可设置为forward或bidirect（或bidirectional）。默认为forward。\r\n",
    "        # time_major (bool，可选) - 指定input的第一个维度是否是time steps。默认为False。\r\n",
    "        # dropout (float，可选) - dropout概率，指的是出第一层外每层输入时的dropout概率。默认为0。\r\n",
    "        # weight_ih_attr (ParamAttr，可选) - weight_ih的参数。默认为None。\r\n",
    "        # weight_hh_attr (ParamAttr，可选) - weight_hh的参数。默认为None。\r\n",
    "        # bias_ih_attr (ParamAttr，可选) - bias_ih的参数。默认为None。\r\n",
    "        # bias_hh_attr (ParamAttr，可选) - bias_hh的参数。默认为None。\r\n",
    "        self.gru = nn.GRU(256, 256, num_layers=2, direction='bidirectional',dropout=0.5)\r\n",
    "\r\n",
    "        # in_features (int) – 线性变换层输入单元的数目。\r\n",
    "        # out_features (int) – 线性变换层输出单元的数目。\r\n",
    "        # weight_attr (ParamAttr, 可选) – 指定权重参数的属性。默认值为None，表示使用默认的权重参数属性，将权重参数初始化为0。具体用法请参见 ParamAttr 。\r\n",
    "        # bias_attr (ParamAttr|bool, 可选) – 指定偏置参数的属性。 bias_attr 为bool类型且设置为False时，表示不会为该层添加偏置。 bias_attr 如果设置为True或者None，则表示使用默认的偏置参数属性，将偏置参数初始化为0。具体用法请参见 ParamAttr 。默认值为None。\r\n",
    "        # name (str，可选) – 具体用法请参见 Name ，一般无需设置，默认值为None。\r\n",
    "        self.linear = nn.Linear(in_features=256*2, out_features=2)\r\n",
    "\r\n",
    "        self.dropout = nn.Dropout(0.5)\r\n",
    "    \r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        \r\n",
    "        emb = self.dropout(self.embedding(inputs))\r\n",
    "        \r\n",
    "        output, hidden = self.gru(emb)\r\n",
    "        #output形状大小为[batch_size,seq_len,num_directions * hidden_size]\r\n",
    "        #hidden形状大小为[num_layers * num_directions, batch_size, hidden_size]\r\n",
    "        #把前向的hidden与后向的hidden合并在一起\r\n",
    "        hidden = paddle.concat((hidden[-2,:,:], hidden[-1,:,:]), axis = 1)\r\n",
    "        hidden = self.dropout(hidden)\r\n",
    "        #hidden形状大小为[batch_size, hidden_size * num_directions]\r\n",
    "        return self.linear(hidden) \r\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 封装模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GRU_model = paddle.Model(myGRU())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 配置优化器等参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GRU_model.prepare(paddle.optimizer.Adam(learning_rate=0.001, parameters=GRU_model.parameters()),\r\n",
    "              paddle.nn.CrossEntropyLoss(),\r\n",
    "              paddle.metric.Accuracy())\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous step.\n",
      "Epoch 1/10\n",
      "step 19/19 [==============================] - loss: 0.6643 - acc: 0.5347 - 93ms/step        \n",
      "save checkpoint at /home/aistudio/work/GRU/0\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 6/6 [==============================] - loss: 0.7029 - acc: 0.5311 - 33ms/step\n",
      "Eval samples: 676\n",
      "Epoch 2/10\n",
      "step 10/19 [==============>...............] - loss: 0.5497 - acc: 0.6516 - ETA: 0s - 92ms/step"
     ]
    }
   ],
   "source": [
    "GRU_model.fit(train_loader,\r\n",
    "          dev_loader,\r\n",
    "          epochs=10,\r\n",
    "          batch_size=BATCH_SIZE,\r\n",
    "          verbose=1,\r\n",
    "          save_dir=\"work/GRU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = GRU_model.predict(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
