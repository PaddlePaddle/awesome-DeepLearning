LSTM-DSSM

概念

RNN（Recurrent Neural Networks）可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。LSTM(（Long-Short-Term Memory）是一种 RNN 特殊的类型，可以学习长期依赖信息。而LSTM-DSSM 其实用的是 LSTM 的一个变种，即加入了peephole的 LSTM。

LSTM-DSSM模型通过采用LSTM进行编码的方式，同样将文本编码为一段向量，LSTM需要对文本长度进行预先的处理，对于query的长尾词处理非常的友好。

LSTM-DSSM整体网络结构如图所示：

![1501556241446_432_1501556242436](C:\Users\apple\Desktop\image\1501556241446_432_1501556242436.png)

模型

LSTM-DSSM模型如图所示：

![1501556197309_9865_1501556198338](C:\Users\apple\Desktop\image\1501556197309_9865_1501556198338.png)

![1501556209287_3423_1501556210288](C:\Users\apple\Desktop\image\1501556209287_3423_1501556210288.png)

这里三条黑线就是所谓的 peephole，传统的 LSTM 中遗忘门、输入门和输出门只用了 h(t-1) 和 xt 来控制门缝的大小，peephole 的意思是说不但要考虑 h(t-1) 和 xt，也要考虑 Ct-1 和 Ct，其中遗忘门和输入门考虑了 Ct-1，而输出门考虑了 Ct。总体来说需要考虑的信息更丰富了。

作用

细胞零状态：

![1501555993000_6630_1501555993959](C:\Users\apple\Desktop\image\1501555993000_6630_1501555993959.png)

细胞状态这条线可以理解成是一条信息的传送带，只有一些少量的线性交互。在上面流动可以保持信息的不变性。

遗忘门：

![1501556106681_5306_1501556107676](C:\Users\apple\Desktop\image\1501556106681_5306_1501556107676.png)

遗忘门 由 Gers 提出，它用来控制细胞状态 cell 有哪些信息可以通过，继续往下传递。如下图所示，上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（遗忘门）产生一个从 0 到 1 的数值 ft，然后与细胞状态 C(t-1) 相乘，最终决定有多少细胞状态可以继续往后传递。

输入门：

![1501556121547_3898_1501556122806](C:\Users\apple\Desktop\image\1501556121547_3898_1501556122806.png)

输入门决定要新增什么信息到细胞状态，这里包含两部分：一个 sigmoid 输入门和一个 tanh 函数。sigmoid 决定输入的信号控制，tanh 决定输入什么内容。如下图所示，上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（输入门）产生一个从 0 到 1 的数值 it，同样的信息经过 tanh 网络做非线性变换得到结果 Ct，sigmoid 的结果和 tanh 的结果相乘，最终决定有哪些信息可以输入到细胞状态里。

输入门：

![1501556106681_5306_1501556107676](C:\Users\apple\Desktop\image\1501556106681_5306_1501556107676.png)

输出门决定从细胞状态要输出什么信息，这里也包含两部分：一个 sigmoid 输出门和一个 tanh 函数。sigmoid 决定输出的信号控制，tanh 决定输出什么内容。如下图所示，上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（输出门）产生一个从 0 到 1 的数值 Ot，细胞状态 Ct 经过 tanh 网络做非线性变换，得到结果再与 sigmoid 的结果 Ot 相乘，最终决定有哪些信息可以输出，输出的结果 ht 会作为这个细胞的输出，也会作为传递个下一个细胞。

场景

LSTM-DSSM模型既可以用来预测两个句子的语义相似度，又可以获得某句子的低纬语义向量表达。

优缺点

- 优点：解决了CNN-DSSM对于间隔较远的上下文信息难以有效保留的问题。
- 缺点：LSTM-DSSM采用弱监督、端到端的模型，预测结果不可控。
