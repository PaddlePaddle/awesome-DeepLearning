## 目标检测知识
**第一问**
Focal loss：专注难样本
当前一阶的物体检测算法，如SSD和YOLO等虽然实现了实时的速度，但精度始终无法与两阶的Faster RCNN相比。是什么阻碍了一阶算法的高精度呢？何凯明等人将其归咎于正、负样本的不均衡，并基于此提出了新的损失函数Focal Loss及网络结构RetinaNet，在与同期一阶网络速度相同的前提下，其检测精度比同期最优的二阶网络还要高。

对于SSD等一阶网络，由于其需要直接从所有的预选框中进行筛选，即使使用了固定正、负样本比例的方法，仍然效率低下，简单的负样本仍然占据主要地位，导致其精度不如两阶网络。为了解决一阶网络中样本的不均衡问题，何凯明等人首先改善了分类过程中的交叉熵函数，提出了可以动态调整权重的Focal Loss。

GHM（损失函数梯度均衡化机制）
对于一个样本，如果它能很容易地被正确分类，那么这个样本对模型来说就是一个简单样本，模型很难从这个样本中得到更多的信息，从梯度的角度来说，这个样本产生的梯度幅值相对较小。而对于一个分错的样本来说，它产生的梯度信息则会更丰富，它更能指导模型优化的方向。

对于单阶段分类器来说，简单样本的数量非常大，他们产生的累计贡献在模型更新中占主导作用，而这部分样本本身就能被模型很好地分类，所以这部分的参数更新并不会改善模型的判断能力，这会导致整个训练变得低效。因此单阶段目标检测中样本不均衡性的本质是简单困难样本的不均衡性。

受 Focal loss 启发，本文通过深入分析 one-stage 目标检测框架中样本分布不平衡（正负样本不均衡和难易样本不均衡）的本质影响，提出了基于梯度分布的角度来缓解 one-stage 中的训练样本不平衡（简称 GHM，Gradient Harmonizing Mechanism，发表于 AAAI 2019，并获得了 Oral），从而改善 one-stage 的目标检测的检测精度。

本文提出了梯度密度，并将梯度密度的倒数作为损失函数的权重分别引入到分类损失函数（GHM-C）和边框损失函数（GHM-R）。

**第二问**
YOLO v1虽然相较于RCNN类的检测算法快很多，但是还是有问题, 比如: 预测的框不准确，很多目标找不到：YOLO v2提出一种并非直接预测bbox坐标的方法，而是改为预测基于grid的偏移量和基于anchor的偏移量。
YOLO v1会miss掉很多目标，即漏检现象明显
这个是因为在多目标多类别检测中，目标物体的大小，高宽比都不同。比如行人就是窄长型的bbox，而汽车则是偏向正方形的bbox。
YOLO v2的作者根据这一点，从数据集中预先准备几个出现几率比较大的bounding box，再以它们为基准进行预测呢，这就是Anchor的初衷。
到YOLO v2这里，对小目标检测的效果还是不够好.
到了YOLO v3，这里主要的改进是增加了多尺度预测, 并把YOLO v2的Backbone从19层的Darknet变为了53层的Darknet, 多尺度预测YOLO v3检测头分叉了，分成了3部分, 每个维度都有3个anchor.
Yolo v4在v3的基础上加了一些特点，主要是3个特点:1. Using multi-anchors for single ground truth
2.Eliminate_grid sensitivity 3. CIoU Loss
YOLO v5基本上在YOLO v3的结构上进行修改,对于YOLO V5，无论是V5s，V5m，V5l还是V5x其Backbone，Neck和Head一致。唯一的区别在与模型的深度和宽度设置，只需要修改这两个参数就可以调整模型的网络结构。V5l 的参数是默认参数。

**第三问**
在2016年，Shaoqing Ren等人在Fast R-CNN[6]的基础上继续做出了创新，提出了Faster R-CNN模型。Faster R-CNN在提取候选口的时候，取消了使用SS[10]算法，提出了一种RPN[6]网络。Faster R-CNN的结构是，首先将图片放入一个全卷积网络，在输出feature map的时候，将feature map送入RNP网络，RNP网络输出一批候选框，这些候选框与之前的feature map进行相应的映射，然后将其送入POI Pooling，生成固定的feature map,然后再对其进行特征提取，最后进行分类与回归。因此Faster R-CNN也可以理解为RPN+Fast R-CNN的结合。
RPN是一种全卷积网络，该网络是输入一张任意大小的图像，输出的则是一组候选框。生成的候选框送入Fast R-CNN的训练网络进行目标检测与精确定位。RNP在输入一张feature map之后，使用一个n×n的滑动窗口（文中n取3，即为3x3的滑动窗口），每一个滑动窗口映射到一个低维的特征（文中采用的是ZF网络，特征为256-d），该特征被输入两个全连接层，一个用作边框回归，另一个用作分类。每一个滑动窗口的位置，同时预测多个区域建议。每个区域建议的最大数目为k（文中k取9），因此边框回归层有4k个输出，分类层有2k个输出。分类层输出的数值表示这个候选框是前景或者背景，边框回归层则是输出该候选框的坐标与偏移量。
经过实验对比，Faster R-CNN在VOC 2007数据集上Fast R-CNN提取一张图片的候选框需要2秒，而Faster R-CNN仅需0.2秒，而且mAP值与Fast R-CNN保存一致。Faster R-CNN提出了RPN网络，使Faster R-CNN整个网络结构实现了全局权值共享，实现了两阶段目标检测的端对端的训练，这是两阶段目标检测的一个大的突破。

**第四问**
  (1）输入端：Mosaic数据增强、自适应锚框计算
（2）Backbone：Focus结构，CSP结构
（3）Neck：FPN+PAN结构
（4）Prediction：GIOU_Loss