{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['next', 'word', 'prediction', 'is', 'also', 'called', 'language', 'modeling', 'it', 'is', 'the', 'task', 'of', 'predicting', 'what', 'word', 'comes', 'next', 'it', 'is', 'one', 'of', 'the', 'fundamental', 'tasks', 'of', 'nlp', 'and', 'has', 'many', 'applications', 'you', 'might', 'be', 'using', 'it', 'daily', 'when', 'you', 'write', 'texts', 'or', 'emails', 'without', 'realizing', 'it', 'the', 'people', 's', 'republic', 'of', 'china', 'or', 'china', 'for', 'short', 'was', 'founded', 'on', 'october', '1', '1949', 'it', 'is', 'located', 'in', 'the', 'east', 'of', 'asia', 'and', 'the', 'west', 'coast', 'of', 'the', 'pacific', 'ocean', 'it', 'is', 'a', 'socialist', 'country', 'under', 'the', 'people', 's', 'democratic', 'dictatorship', 'led', 'by', 'the', 'working', 'class', 'and', 'based', 'on', 'the', 'alliance', 'of', 'workers', 'and', 'peasants']\n",
      "69\n",
      "[[5, 6, 13], [6, 13, 3], [13, 3, 14], [3, 14, 15], [14, 15, 16], [15, 16, 17], [16, 17, 2], [17, 2, 3], [2, 3, 0], [3, 0, 18], [0, 18, 1], [18, 1, 19], [1, 19, 20], [19, 20, 6], [20, 6, 21], [6, 21, 5], [21, 5, 2], [5, 2, 3], [2, 3, 22], [3, 22, 1], [22, 1, 0], [1, 0, 23], [0, 23, 24], [23, 24, 1], [24, 1, 25], [1, 25, 4], [25, 4, 26], [4, 26, 27], [26, 27, 28], [27, 28, 7], [28, 7, 29], [7, 29, 30], [29, 30, 31], [30, 31, 2], [31, 2, 32], [2, 32, 33], [32, 33, 7], [33, 7, 34], [7, 34, 35], [34, 35, 8], [35, 8, 36], [8, 36, 37], [36, 37, 38], [37, 38, 2], [38, 2, 0], [2, 0, 9], [0, 9, 10], [9, 10, 39], [10, 39, 1], [39, 1, 11], [1, 11, 8], [11, 8, 11], [8, 11, 40], [11, 40, 41], [40, 41, 42], [41, 42, 43], [42, 43, 12], [43, 12, 44], [12, 44, 45], [44, 45, 46], [45, 46, 2], [46, 2, 3], [2, 3, 47], [3, 47, 48], [47, 48, 0], [48, 0, 49], [0, 49, 1], [49, 1, 50], [1, 50, 4], [50, 4, 0], [4, 0, 51], [0, 51, 52], [51, 52, 1], [52, 1, 0], [1, 0, 53], [0, 53, 54], [53, 54, 2], [54, 2, 3], [2, 3, 55], [3, 55, 56], [55, 56, 57], [56, 57, 58], [57, 58, 0], [58, 0, 9], [0, 9, 10], [9, 10, 59], [10, 59, 60], [59, 60, 61], [60, 61, 62], [61, 62, 0], [62, 0, 63], [0, 63, 64], [63, 64, 4], [64, 4, 65], [4, 65, 12], [65, 12, 0], [12, 0, 66], [0, 66, 1], [66, 1, 67], [1, 67, 4], [67, 4, 68]]\n"
     ]
    }
   ],
   "source": [
    "import re\r\n",
    "import nltk\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# nltk.download('punkt')\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "\r\n",
    "corpus=open(\"data/data100652/corpus.txt\").read() #文本数据集\r\n",
    "# print(corpus)\r\n",
    "corpus=corpus.lower()\r\n",
    "clean_corpus=re.sub('[^a-z0-9]+',' ', corpus) #去除标点符号 空格\r\n",
    "\r\n",
    "tokens = word_tokenize(clean_corpus)\r\n",
    "print(tokens) ###分词\r\n",
    "\r\n",
    "train_len = 3 #句子长度\r\n",
    "text_sequences = []\r\n",
    "for i in range(train_len,len(tokens)+1):\r\n",
    "  seq = tokens[i-train_len:i]\r\n",
    "  text_sequences.append(seq)\r\n",
    "# print(text_sequences)\r\n",
    "\r\n",
    "# #将文本转变为标号\r\n",
    "# tokenizer = Tokenizer()\r\n",
    "# tokenizer.fit_on_texts(text_sequences)\r\n",
    "# sequences = tokenizer.texts_to_sequences(text_sequences)\r\n",
    "# print(sequences)\r\n",
    "\r\n",
    "#构造词典，统计每一个词的频率并根据词频转换为整数id\r\n",
    "def build_dict(tokens):\r\n",
    "    word_freq_dict = dict()\r\n",
    "    for s in tokens:\r\n",
    "        if s not in word_freq_dict:\r\n",
    "            word_freq_dict[s]=0\r\n",
    "        word_freq_dict[s]+=1\r\n",
    "    word_freq_dict = sorted(word_freq_dict.items(),key=lambda x:x[1],reverse=True)\r\n",
    "    word2id_dict=dict()\r\n",
    "    word2id_freq=dict()\r\n",
    "    for word,freq in word_freq_dict:\r\n",
    "        word2id_dict[word] = len(word2id_dict)\r\n",
    "        word2id_freq[word2id_dict[word]]=freq\r\n",
    "\r\n",
    "    return word2id_freq,word2id_dict\r\n",
    "\r\n",
    "def convert_to_id(text_sequences,word2id_dict):\r\n",
    "    data_set = []\r\n",
    "    for s in text_sequences:\r\n",
    "        s =[word2id_dict[word] for word in s]\r\n",
    "        data_set.append(s)\r\n",
    "    return data_set\r\n",
    "\r\n",
    "\r\n",
    "word2id_freq,word2id_dict = build_dict(tokens)\r\n",
    "vocab_size = len(word2id_freq)\r\n",
    "print(vocab_size)\r\n",
    "\r\n",
    "train_set = convert_to_id(text_sequences,word2id_dict)\r\n",
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "sequences=np.asarray(train_set)\r\n",
    "train_inputs=sequences[:,:-1] #最终的训练输入\r\n",
    "\r\n",
    "seq_length=train_inputs.shape[1]\r\n",
    "\r\n",
    "train_label=sequences[:,-1]  #最终的训练标签\r\n",
    "\r\n",
    "#将标签转换为独热编码\r\n",
    "onehot_label = np.zeros([train_inputs.shape[0],vocab_size])\r\n",
    "for i in range(train_inputs.shape[0]):\r\n",
    "    onehot_label[i,train_label[i]]=1\r\n",
    "print(type(onehot_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,loss:0.693\n",
      "epoch:1,loss:0.641\n",
      "epoch:2,loss:0.539\n",
      "epoch:3,loss:0.344\n",
      "epoch:4,loss:0.121\n",
      "epoch:5,loss:0.085\n",
      "epoch:6,loss:0.120\n",
      "epoch:7,loss:0.141\n",
      "epoch:8,loss:0.149\n",
      "epoch:9,loss:0.148\n",
      "epoch:10,loss:0.141\n",
      "epoch:11,loss:0.130\n",
      "epoch:12,loss:0.121\n",
      "epoch:13,loss:0.114\n",
      "epoch:14,loss:0.107\n",
      "epoch:15,loss:0.099\n",
      "epoch:16,loss:0.091\n",
      "epoch:17,loss:0.083\n",
      "epoch:18,loss:0.078\n",
      "epoch:19,loss:0.076\n",
      "epoch:20,loss:0.076\n",
      "epoch:21,loss:0.076\n",
      "epoch:22,loss:0.076\n",
      "epoch:23,loss:0.076\n",
      "epoch:24,loss:0.075\n",
      "epoch:25,loss:0.075\n",
      "epoch:26,loss:0.074\n",
      "epoch:27,loss:0.074\n",
      "epoch:28,loss:0.075\n",
      "epoch:29,loss:0.075\n",
      "epoch:30,loss:0.075\n",
      "epoch:31,loss:0.075\n",
      "epoch:32,loss:0.075\n",
      "epoch:33,loss:0.075\n",
      "epoch:34,loss:0.074\n",
      "epoch:35,loss:0.074\n",
      "epoch:36,loss:0.073\n",
      "epoch:37,loss:0.073\n",
      "epoch:38,loss:0.072\n",
      "epoch:39,loss:0.072\n",
      "epoch:40,loss:0.072\n",
      "epoch:41,loss:0.072\n",
      "epoch:42,loss:0.072\n",
      "epoch:43,loss:0.072\n",
      "epoch:44,loss:0.072\n",
      "epoch:45,loss:0.072\n",
      "epoch:46,loss:0.071\n",
      "epoch:47,loss:0.071\n",
      "epoch:48,loss:0.071\n",
      "epoch:49,loss:0.071\n",
      "epoch:50,loss:0.071\n",
      "epoch:51,loss:0.071\n",
      "epoch:52,loss:0.071\n",
      "epoch:53,loss:0.071\n",
      "epoch:54,loss:0.071\n",
      "epoch:55,loss:0.070\n",
      "epoch:56,loss:0.070\n",
      "epoch:57,loss:0.070\n",
      "epoch:58,loss:0.070\n",
      "epoch:59,loss:0.070\n",
      "epoch:60,loss:0.069\n",
      "epoch:61,loss:0.069\n",
      "epoch:62,loss:0.068\n",
      "epoch:63,loss:0.068\n",
      "epoch:64,loss:0.067\n",
      "epoch:65,loss:0.067\n",
      "epoch:66,loss:0.066\n",
      "epoch:67,loss:0.065\n",
      "epoch:68,loss:0.064\n",
      "epoch:69,loss:0.063\n",
      "epoch:70,loss:0.062\n",
      "epoch:71,loss:0.061\n",
      "epoch:72,loss:0.060\n",
      "epoch:73,loss:0.059\n",
      "epoch:74,loss:0.058\n",
      "epoch:75,loss:0.057\n",
      "epoch:76,loss:0.056\n",
      "epoch:77,loss:0.055\n",
      "epoch:78,loss:0.053\n",
      "epoch:79,loss:0.052\n",
      "epoch:80,loss:0.051\n",
      "epoch:81,loss:0.049\n",
      "epoch:82,loss:0.048\n",
      "epoch:83,loss:0.047\n",
      "epoch:84,loss:0.045\n",
      "epoch:85,loss:0.044\n",
      "epoch:86,loss:0.042\n",
      "epoch:87,loss:0.041\n",
      "epoch:88,loss:0.039\n",
      "epoch:89,loss:0.038\n",
      "epoch:90,loss:0.036\n",
      "epoch:91,loss:0.035\n",
      "epoch:92,loss:0.033\n",
      "epoch:93,loss:0.032\n",
      "epoch:94,loss:0.030\n",
      "epoch:95,loss:0.029\n",
      "epoch:96,loss:0.027\n",
      "epoch:97,loss:0.026\n",
      "epoch:98,loss:0.025\n",
      "epoch:99,loss:0.023\n",
      "epoch:100,loss:0.022\n",
      "epoch:101,loss:0.021\n",
      "epoch:102,loss:0.020\n",
      "epoch:103,loss:0.019\n",
      "epoch:104,loss:0.018\n",
      "epoch:105,loss:0.017\n",
      "epoch:106,loss:0.016\n",
      "epoch:107,loss:0.015\n",
      "epoch:108,loss:0.015\n",
      "epoch:109,loss:0.014\n",
      "epoch:110,loss:0.013\n",
      "epoch:111,loss:0.013\n",
      "epoch:112,loss:0.012\n",
      "epoch:113,loss:0.011\n",
      "epoch:114,loss:0.011\n",
      "epoch:115,loss:0.010\n",
      "epoch:116,loss:0.010\n",
      "epoch:117,loss:0.010\n",
      "epoch:118,loss:0.009\n",
      "epoch:119,loss:0.009\n",
      "epoch:120,loss:0.009\n",
      "epoch:121,loss:0.008\n",
      "epoch:122,loss:0.008\n",
      "epoch:123,loss:0.008\n",
      "epoch:124,loss:0.007\n",
      "epoch:125,loss:0.007\n",
      "epoch:126,loss:0.007\n",
      "epoch:127,loss:0.007\n",
      "epoch:128,loss:0.006\n",
      "epoch:129,loss:0.006\n",
      "epoch:130,loss:0.006\n",
      "epoch:131,loss:0.006\n",
      "epoch:132,loss:0.006\n",
      "epoch:133,loss:0.005\n",
      "epoch:134,loss:0.005\n",
      "epoch:135,loss:0.005\n",
      "epoch:136,loss:0.005\n",
      "epoch:137,loss:0.005\n",
      "epoch:138,loss:0.005\n",
      "epoch:139,loss:0.005\n",
      "epoch:140,loss:0.004\n",
      "epoch:141,loss:0.004\n",
      "epoch:142,loss:0.004\n",
      "epoch:143,loss:0.004\n",
      "epoch:144,loss:0.004\n",
      "epoch:145,loss:0.004\n",
      "epoch:146,loss:0.004\n",
      "epoch:147,loss:0.004\n",
      "epoch:148,loss:0.004\n",
      "epoch:149,loss:0.004\n",
      "epoch:150,loss:0.004\n",
      "epoch:151,loss:0.003\n",
      "epoch:152,loss:0.003\n",
      "epoch:153,loss:0.003\n",
      "epoch:154,loss:0.003\n",
      "epoch:155,loss:0.003\n",
      "epoch:156,loss:0.003\n",
      "epoch:157,loss:0.003\n",
      "epoch:158,loss:0.003\n",
      "epoch:159,loss:0.003\n",
      "epoch:160,loss:0.003\n",
      "epoch:161,loss:0.003\n",
      "epoch:162,loss:0.003\n",
      "epoch:163,loss:0.003\n",
      "epoch:164,loss:0.003\n",
      "epoch:165,loss:0.003\n",
      "epoch:166,loss:0.003\n",
      "epoch:167,loss:0.003\n",
      "epoch:168,loss:0.003\n",
      "epoch:169,loss:0.003\n",
      "epoch:170,loss:0.003\n",
      "epoch:171,loss:0.003\n",
      "epoch:172,loss:0.003\n",
      "epoch:173,loss:0.003\n",
      "epoch:174,loss:0.002\n",
      "epoch:175,loss:0.002\n",
      "epoch:176,loss:0.002\n",
      "epoch:177,loss:0.002\n",
      "epoch:178,loss:0.002\n",
      "epoch:179,loss:0.002\n",
      "epoch:180,loss:0.002\n",
      "epoch:181,loss:0.002\n",
      "epoch:182,loss:0.002\n",
      "epoch:183,loss:0.002\n",
      "epoch:184,loss:0.002\n",
      "epoch:185,loss:0.002\n",
      "epoch:186,loss:0.002\n",
      "epoch:187,loss:0.002\n",
      "epoch:188,loss:0.002\n",
      "epoch:189,loss:0.002\n",
      "epoch:190,loss:0.002\n",
      "epoch:191,loss:0.002\n",
      "epoch:192,loss:0.002\n",
      "epoch:193,loss:0.002\n",
      "epoch:194,loss:0.002\n",
      "epoch:195,loss:0.002\n",
      "epoch:196,loss:0.002\n",
      "epoch:197,loss:0.002\n",
      "epoch:198,loss:0.002\n",
      "epoch:199,loss:0.002\n"
     ]
    }
   ],
   "source": [
    "#构建LSTM模型\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "import paddle.fluid as fluid\r\n",
    "import numpy as np\r\n",
    "from paddle.fluid.dygraph.nn import Embedding\r\n",
    "import random\r\n",
    "#参数设置\r\n",
    "embedding_size = 128\r\n",
    "hidden_size = 256\r\n",
    "num_encoder_lstm_layers = 3\r\n",
    "input_vocab_size = vocab_size\r\n",
    "epochs = 200\r\n",
    "batch_size = 7\r\n",
    "class LSTM(paddle.nn.Layer):\r\n",
    "    def __init__(self,input_vocab_size,embedding_size,hidden_size):\r\n",
    "        super(LSTM, self).__init__()\r\n",
    "        self.emb = paddle.nn.Embedding(input_vocab_size, embedding_size)\r\n",
    "        self.lstm = paddle.nn.LSTM(input_size=embedding_size, \r\n",
    "                                   hidden_size=hidden_size, \r\n",
    "                                   num_layers=num_encoder_lstm_layers)\r\n",
    "        self.linear = paddle.nn.Linear(in_features=2*hidden_size,out_features=input_vocab_size,weight_attr=None)\r\n",
    "        \r\n",
    "    def forward(self,input_word):\r\n",
    "        input_word = paddle.to_tensor(input_word)\r\n",
    "        embedded = self.emb(input_word)\r\n",
    "        \r\n",
    "        # embedded = paddle.reshape(embedded,shape=[-1,input_vocab_size, embedding_size])\r\n",
    "        #passing the embedding to lstm model\r\n",
    "        output, hidden = self.lstm(embedded)\r\n",
    "        \r\n",
    "        #reshaping\r\n",
    "        logits=paddle.reshape(output,shape=[-1,2*hidden_size])\r\n",
    "        # print(logits)\r\n",
    "        #fully connected layer\r\n",
    "        logits = self.linear(logits)\r\n",
    "        return logits,hidden\r\n",
    "\r\n",
    "#训练\r\n",
    "#检测是否可以使用GPU，如果可以优先使用GPu\r\n",
    "use_gpu= True if paddle.get_device().startswith(\"gpu\") else False\r\n",
    "if use_gpu:\r\n",
    "    paddle.set_device('gpu：0')\r\n",
    "model = LSTM(input_vocab_size,embedding_size,hidden_size)\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=0.01,beta1=0.9,beta2=0.999,parameters= model.parameters())\r\n",
    "losses=[]\r\n",
    "steps=[]\r\n",
    "# seq_set = sequences\r\n",
    "# for i in range(seq_set.shape[0]):\r\n",
    "#     seq_set[i,-1]=i\r\n",
    "def train(model,train_set,onehot_label,batch_size,epochs):\r\n",
    "    model.train()\r\n",
    "    onehot_label=paddle.to_tensor(onehot_label).astype('float32')\r\n",
    "    batch_num = int(train_set.shape[0]/batch_size)\r\n",
    "    #生成mini-batch\r\n",
    "    # label_batch=onehot_label[0:batch_num]\r\n",
    "    # def get_batch(batch_size,seq_set,train_label,i):\r\n",
    "    #     if True:\r\n",
    "    #         random.shuffle(seq_set)\r\n",
    "        \r\n",
    "    #     # for num in range(batch_num):\r\n",
    "    #     seq_batch = seq_set[i*batch_num:(i+1)*batch_num,:-1]\r\n",
    "    #     id_select = list(seq_set[i*batch_num:(i+1)*batch_num,-1])\r\n",
    "    #     for i in range(len(id_select)):\r\n",
    "    #         label_batch[i] = onehot_label[id_select[i]]\r\n",
    "    #     return seq_batch,label_batch\r\n",
    "    for e in range(epochs):\r\n",
    "        # for i in range(batch_num):\r\n",
    "        #     seq_batch,label_batch = get_batch(batch_size,seq_set,train_label,i)\r\n",
    "        #     seq_batch = paddle.to_tensor(seq_batch)\r\n",
    "        #     label_batch = paddle.to_tensor(label_batch)\r\n",
    "        #     logits,_ = model(seq_batch)\r\n",
    "        #     print(logits)\r\n",
    "        #     loss = F.cross_entropy(input=logits,label=label_batch,soft_label=False)\r\n",
    "        #     loss = paddle.mean(loss)\r\n",
    "\r\n",
    "        #     #后向传播\r\n",
    "        #     loss.backward()\r\n",
    "        #     optimizer.step()\r\n",
    "        #     optimizer.clear_grad()\r\n",
    "        logits,_=model(train_set)\r\n",
    "        # print(logits)\r\n",
    "        loss = F.binary_cross_entropy_with_logits(logit=logits,label=onehot_label)\r\n",
    "        loss = paddle.mean(loss)\r\n",
    "        #后向传播\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "\r\n",
    "        losses.append(loss.numpy()[0])\r\n",
    "        steps.append(e)\r\n",
    "        print(\"epoch:%d,loss:%.3f\"% (e,loss.numpy()[0]))\r\n",
    "\r\n",
    "train(model,train_inputs,onehot_label,batch_size,epochs)\r\n",
    "    \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "plt.plot(steps,losses,label='Training loss')\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next word is:  prediction\n",
      "0.6831683168316832\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "def predict_next_word(model,text,word2id_dict):\r\n",
    "    model.eval()\r\n",
    "    text = text.lower().strip()\r\n",
    "    input_tokens = word_tokenize(text)\r\n",
    "    word_list = []\r\n",
    "    for w in input_tokens:\r\n",
    "        word_list.append(word2id_dict[w])\r\n",
    "    sequences=np.array(word_list)\r\n",
    "    sequences=sequences.reshape([2,1])\r\n",
    "    sequences=paddle.to_tensor(sequences)\r\n",
    " \r\n",
    "    next_word,_ = model(sequences)\r\n",
    "    softmax = paddle.exp(next_word)\r\n",
    "    prob = list(softmax.numpy())\r\n",
    "    predictions = np.argmax(prob)\r\n",
    "    next_word=[key for key, value in word2id_dict.items() if value == predictions]\r\n",
    "    return next_word\r\n",
    "#计算正确率 test_set 测试集 \r\n",
    "def get_accuracy(model,test_set,test_label):\r\n",
    "    model.eval()\r\n",
    "    correct =0\r\n",
    "    for i in range(test_set.shape[0]):\r\n",
    "        input_seq = paddle.to_tensor(test_set[i,:])\r\n",
    "        input_seq = paddle.reshape(input_seq,shape=[2,1])\r\n",
    "        next_word,_ = model(input_seq)\r\n",
    "        softmax = paddle.exp(next_word)\r\n",
    "        prob = list(softmax.numpy())\r\n",
    "        predictions = np.argmax(prob)\r\n",
    "        if predictions==test_label[i]:\r\n",
    "            correct +=1\r\n",
    "    return float(correct/test_set.shape[0])\r\n",
    "text =\"next word\"\r\n",
    "next_word = predict_next_word(model,text,word2id_dict)\r\n",
    "print(\"next word is:  {}\".format(next_word[0]))\r\n",
    "\r\n",
    "acc = get_accuracy(model,train_inputs,train_label)\r\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
