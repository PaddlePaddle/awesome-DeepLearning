# 深度学习基础知识

## 1 深度学习发展历史

**深度学习起源阶段**

1943年

由神经科学家**麦卡洛克(W.S.McCilloch)** 和**数学家皮兹（W.Pitts）**在《数学生物物理学公告》上发表论文1943年，心里学家麦卡洛克和数学逻辑学家皮兹发表论文《神经活动中内在思想的逻辑演算》，提出了MP模型。MP模型是模仿神经元的结构和工作原理，构成出的一个基于神经网络的数学模型，本质上是一种“模拟人类大脑”的神经元模型。MP模型作为人工神经网络的起源，开创了人工神经网络的新时代，也奠定了神经网络模型的基础。

1949年

1949年，加拿大著名心理学家唐纳德·赫布在《行为的组织》中提出了一种基于无监督学习的规则——海布学习规则(Hebb Rule)。海布规则模仿人类认知世界的过程建立一种“网络模型”，该网络模型针对训练集进行大量的训练并提取训练集的统计特征，然后按照样本的相似程度进行分类，把相互之间联系密切的样本分为一类，这样就把样本分成了若干类。海布学习规则与“条件反射”机理一致，为以后的神经网络学习算法奠定了基础，具有重大的历史意义。

20实际50年代末

20世纪50年代末，在MP模型和海布学习规则的研究基础上，美国科学家罗森布拉特发现了一种类似于人类学习过程的学习算法——感知机学习。并于1958年，正式提出了由两层神经元组成的神经网络，称之为“感知器”。感知器本质上是一种线性模型，可以对输入的训练集数据进行二分类，且能够在训练集中自动更新权值。感知器的提出吸引了大量科学家对人工神经网络研究的兴趣，对神经网络的发展具有里程碑式的意义。

1969年

1969年，“AI之父”马文·明斯基和LOGO语言的创始人西蒙·派珀特共同编写了一本书籍《感知器》，在书中他们证明了单层感知器无法解决线性不可分问题（例如：异或问题）。由于这个致命的缺陷以及没有及时推广感知器到多层神经网络中，在20世纪70年代，人工神经网络进入了第一个寒冬期，人们对神经网络的研究也停滞了将近20年。

**深度学习的发展阶段**

1982年

1982年，著名物理学家约翰·霍普菲尔德发明了Hopfield神经网络。Hopfield神经网络是一种结合存储系统和二元系统的循环神经网络。Hopfield网络也可以模拟人类的记忆，根据激活函数的选取不同，有连续型和离散型两种类型，分别用于优化计算和联想记忆。但由于容易陷入局部最小值的缺陷，该算法并未在当时引起很大的轰动。

1986年

1986年，深度学习之父杰弗里·辛顿提出了一种适用于多层感知器的反向传播算法——BP算法。BP算法在传统神经网络正向传播的基础上，增加了误差的反向传播过程。反向传播过程不断地调整神经元之间的权值和阈值，直到输出的误差达到减小到允许的范围之内，或达到预先设定的训练次数为止。BP算法完美的解决了非线性分类问题，让人工神经网络再次的引起了人们广泛的关注。但是由于八十年代计算机的硬件水平有限，如：运算能力跟不上，这就导致当神经网络的规模增大时，再使用BP算法会出现“梯度消失”的问题。这使得BP算法的发展受到了很大的限制。再加上90年代中期，以SVM为代表的其它浅层机器学习算法被提出，并在分类、回归问题上均取得了很好的效果，其原理又明显不同于神经网络模型，所以人工神经网络的发展再次进入了瓶颈期。

**深度学习的爆发阶段**

2006年

2006年，杰弗里·辛顿以及他的学生鲁斯兰·萨拉赫丁诺夫正式提出了深度学习的概念。他们在世界顶级学术期刊《科学》发表的一篇文章中详细的给出了“梯度消失”问题的解决方案——通过无监督的学习方法逐层训练算法，再使用有监督的反向传播算法进行调优。该深度学习方法的提出，立即在学术圈引起了巨大的反响，以斯坦福大学、多伦多大学为代表的众多世界知名高校纷纷投入巨大的人力、财力进行深度学习领域的相关研究。而后又在迅速蔓延到工业界中。

2012年

2012年，在著名的ImageNet图像识别大赛中，杰弗里·辛顿领导的小组采用深度学习模型AlexNet一举夺冠。AlexNet采用ReLU激活函数，从根本上解决了梯度消失问题，并采用GPU极大的提高了模型的运算速度。同年，由斯坦福大学著名的吴恩达教授和世界顶尖计算机专家Jeff Dean共同主导的深度神经网络——DNN技术在图像识别领域取得了惊人的成绩，在ImageNet评测中成功的把错误率从26％降低到了15％。深度学习算法在世界大赛的脱颖而出，也再一次吸引了学术界和工业界对于深度学习领域的关注。

2014年

2014年，Facebook基于深度学习技术的DeepFace项目，在人脸识别方面的准确率已经能达到97%以上，跟人类识别的准确率几乎没有差别。这样的结果也再一次证明了深度学习算法在图像识别方面的一骑绝尘。

2016年

2016年，随着谷歌公司基于深度学习开发的AlphaGo以4:1的比分战胜了国际顶尖围棋高手李世石，深度学习的热度一时无两。后来，AlphaGo又接连和众多世界级围棋高手过招，均取得了完胜。这也证明了在围棋界，基于深度学习技术的机器人已经超越了人类。

2017年

2017年，基于强化学习算法的AlphaGo升级版AlphaGo Zero横空出世。其采用“从零开始”、“无师自通”的学习模式，以100:0的比分轻而易举打败了之前的AlphaGo。除了围棋，它还精通国际象棋等其它棋类游戏，可以说是真正的棋类“天才”。此外在这一年，深度学习的相关算法在医疗、金融、艺术、无人驾驶等多个领域均取得了显著的成果。所以，也有专家把2017年看作是深度学习甚至是人工智能发展最为突飞猛进的一年。

深度学习正在不断的发展





## 2 人工智能、机器学习、深度学习有什么区别和联系

**人工智能（Artificial Intelligence）**

**人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门技术科学。**“人工智能”是“一门技术科学”，它研究与开发的对象是“理论、技术及应用系统”，研究的目的是为了“模拟、延伸和扩展人的智能”。我们现在看到的貌似很高端的技术，如图像识别、NLP，其实依然没有脱离这个范围，就是“模拟人在看图方面的智能”和“模拟人在听话方面的智能”，本质上和“模拟人在计算方面的智能”没啥两样，虽然难度有高低，但目的是一样的——模拟、延伸和扩展人的智能。另外，人工智能在50年代就提出了。

**机器学习**

   随着人对计算机科学的期望越来越高，要求它解决的问题越来越复杂，已经远远不能满足人们的诉求了。于是有人提出了一个新的思路——能否不为难码农，让机器自己去学习呢？

**机器学习就是用算法解析数据，不断学习，对世界中发生的事做出判断和预测的一项技术。**研究人员不会亲手编写软件、确定特殊指令集、然后让程序完成特殊任务；相反，研究人员会用大量数据和算法“训练”机器，让机器学会如何执行任务。这里有三个重要的信息：1、“机器学习”是“模拟、延伸和扩展人的智能”的一条路径，所以**是人工智能的一个子集**；2、“机器学习”是要基于大量数据的，也就是说它的“智能”是用大量数据喂出来的；3、正是因为要处理海量数据，所以大数据技术尤为重要；“机器学习”只是大数据技术上的一个应用。常用的10大机器学习算法有：决策树、随机森林、逻辑回归、SVM、朴素贝叶斯、K最近邻算法、K均值算法、Adaboost算法、神经网络、马尔科夫。

**深度学习**

  相较而言，深度学习是一个比较新的概念，严格地说是2006年提出的。**深度学习是用于建立、模拟人脑进行分析学习的神经网络，并模仿人脑的机制来解释数据的一种机器学习技术。**它的基本特点，是试图模仿大脑的神经元之间传递，处理信息的模式。最显著的应用是计算机视觉和自然语言处理(NLP)领域。显然，**“深度学习”是与机器学习中的“神经网络”是强相关**，“神经网络”也是其主要的算法和手段；或者我们可以将“深度学习”称之为“改良版的神经网络”算法。深度学习又分为卷积神经网络（Convolutional neural networks，简称CNN）和深度置信网（Deep Belief Nets，简称DBN）。其主要的思想就是模拟人的神经元，每个神经元接受到信息，处理完后传递给与之相邻的所有神经元即可。

**人工智能是一种概念，机器学习是人工智能的一个子集，深度学习又是机器学习的一个子集。机器学习与深度学习都是需要大量数据来进行的的，是大数据技术上的一个应用，同时深度学习还需要更高的运算能力支撑**

## 3 神经元、单层感知机、多层感知机

### 3.1 神经元

正如其名字所表明，神经网络的灵感来源于人类大脑的神经结构，像在一个人类大脑中，最基本的构件就叫做神经元。它的功能和人的神经元很相似，换句话说，它有一些输入，然后给一个输出。在数学上，在机器学习中的神经元就是一个数学函数的占位符，它仅有的工作就是对输入使用一个函数，然后给一个输出。

![理解神经网络：从神经元到RNN、CNN、深度学习](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tcHQuMTM1ZWRpdG9yLmNvbS9tbWJpel9wbmcvYmljZE1MekltbGliUVVQZkpFaWE4eDVpYkc0NWlhZGhZU1hHc2liQmZvQXVxTndPaWEzMkZlQ3VvVnZoa09OSlo0OG0xMUFnN0dIaFREaWFpYmxLbGI2aWNHdDA4UUNnLzY0MD93eF9mbXQ9cG5nJnd4X2NvPTE?x-oss-process=image/format,png)

这种神经元中使用的函数，在术语上通常叫做激活函数。主要的激活函数有5种，date,step,sigmoid,tanh和ReLU。

### 3.2 单层感知机

单层感知器属于单层前向网络，即除输入层和输出层之外，只拥有一层神经元节点。

　　特点：输入数据从输入层经过隐藏层向输出层逐层传播，相邻两层的神经元之间相互连接，同一层的神经元之间没有连接。

　　感知器（perception）是由美国学者F.Rosenblatt提出的。与最早提出的MP模型不同，神经元突触权值可变，因此可以通过一定规则进行学习。可以快速、可靠地解决线性可分的问题。

 

#### 3.2.1 单层感知器的结构

　　单层感知器由一个线性组合器和一个二值阈值元件组成。输入向量的各个分量先与权值相乘，然后在线性组合器中进行叠加，得到一个标量结果，其输出是线性组合结果经过一个二值阈值函数。二值阈值元件通常是一个上升函数，典型功能是非负数映射为1，负数映射为0或负一。

　　输入是一个N维向量 x=[x1,x2,...,xn]，其中每一个分量对应一个权值wi，隐含层输出叠加为一个标量值：

![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115104935967-821769030.png)

随后在二值阈值元件中对得到的v值进行判断，产生二值输出：

　　　　　　　　　　　　　　　      ![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115105450576-1393524587.gif)

可以将数据分为两类。实际应用中，还加入偏置，值恒为1，权值为b。这时，y输出为：

　　　　　　　　　　　　　　　　　　![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115110119170-381381331.gif)

把偏置值当作特殊权值：

 

 

![img](https://upload-images.jianshu.io/upload_images/10865154-b5afb460887fdf1c.png)

　单层感知器结构图：

　　　　　　　　　　　　　　　　![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115110155326-596049826.png)

　单层感知器进行模式识别的超平面由下式决定：

　　　　　　　　　　　　　　　　　　　　![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115110437763-1231637778.gif)

当维数N=2时，输入向量可以表示为平面直角坐标系中的一个点。此时分类超平面是一条直线:

　　　　　　　　　　　　　　　　　　　　![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115110540217-26972109.gif)

这样就可以将点沿直线划分成两类。

 

#### 3.2.2 单层感知器的学习算法

(1)定义变量和参数，这里的n是迭代次数。N是N维输入，将其中的偏置也作为输入，不过其值恒为1,。

　　x(n)=N+1维输入向量=[+1,x1(n),x2(n),...,xN(n)]T

　　w(n)=N+1维权值向量=[b(n),w1(n),w2(n),...,wN(n)]T

　　b(n)=偏置

　　y(n)=实际输出

　　d(n)=期望输出

　　η(n)=学习率参数，是一个比1小的正常数

所以线性组合器的输出为：v(n)=wT(n)x(n)

(2)初始化。n=0,将权值向量w设置为随机值或全零值。

(3)激活。输入训练样本，对每个训练样本x(n)=[+1,x1(n),x2(n),...,xN(n)]T,指定其期望输出d。即若x属于l1，则d=1，若x属于l2，则d属于-1（或者0）。

(4)计算实际输出。　　　　　　　　y(n)=sgn(wT(n)x(n))

(5)更新权值向量　　　　　　　　　w(n+1)=w(n)+η[d(n)-y(n)]x(n)

感知器的学习规则：学习信号等于神经元期望输出与实际输出之差:

![img](https://upload-images.jianshu.io/upload_images/10865154-628e471716bb213a.png)

dj为期望的输出，oj为实际的输出。
W代表特征向量或者是矩阵。T代表矩阵的转置，X为输入信号或者是训练数据集中的一个m维样本



 

![img](https://upload-images.jianshu.io/upload_images/10865154-ce2d8179fc872cb8.png)

权值调整公式为：

 

![img](https://upload-images.jianshu.io/upload_images/10865154-6ff05aba5ed0186c.png)

公式右侧前面的符号η为学习率。权值调整值为学习率乘以误差乘以输入信号(X)。
当实际输出与期望值相同时，权值不需要调整，在有误差存在的情况下，并且期望值与实际输出值为1或者-1，此时权值调整公式可以简化为：

![img](https://upload-images.jianshu.io/upload_images/10865154-307dd2fc055cf393.png)

**每一次带入一个数据进行迭代，而不是带入所有数据（为什么呢，感觉带入所有数据进行运算会好很多）其实也能一批数据全部直接带入计算，只不过现在的我还不太懂为什么可以这样。此问题留待日后回答。**

这里

　　　　　　　　　　　　　　　　![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115131952982-500157079.gif)

　　　　　　　　　　　　　　　　0<η<1

**为什么权值更新会是这样呢：我认为可以这样理解，当y与d相同时，分类正确，权值不用更新。**

**当y与d不同时 : w(n+1)\*x（n）=w(n)\*x(n)+η[d(n)-y(n)]x(n)\*x(n) 即当d为1而y为-1时，整体的w(n+1)\*x(n)增加的，当d为-1而y为1时，整体是往-1的方向变化的，所以w(n+1)权值会使得整体往正确的分类走。但是这个权值更新公式怎么来的就还不知道了。**

(6)判断。若满足收敛条件，则算法结束，若不满足，n++，转到第(3)步。

收敛条件：当权值向量w已经能正确实现分类时，算法就收敛了，此时网络误差为零。收敛条件通常可以是：

　　　　误差小于某个预先设定的较小的值ε。即

　　　　　　　　　　　　　　　　　　|d(n)-y(n)|<ε

　　　　两次迭代之间的权值变化已经很小，即

　　　　　　　　　　　　　　　　　　|w(n+1)-w(n)|<ε

为防止偶然因素导致的提取收敛，前两个条件还可以改进为连续若干次误差或者权值变化小于某个值。

　　　　设定最大迭代次数M，当迭代了M次就停止迭代。

　　　　需事先通过经验设定学习率η,不应该过大，以便为输入向量提供一个比较稳定的权值估计。不应过小，以便使权值能根据输入的向量x实时变化，体现误差对权值的修正作用。

- 学习率大于零，小于等于１
- 学习率太大，容易造成权值调整不稳定。
- 学习率太小，权值调整太慢，迭代次数太多。
- 学习率一般选择可变的，类似于调节显微镜一样，开始时步长比较大，后面步长比较小。

　　　　它只对线性可分的问题收敛，通过学习调整权值，最终找到合适的决策面，实现正确分类。

单层感知器在研究线性不可分的问题中：可以追求尽量正确的分类，定义一个误差准则，在不同的超平面中选择一个最优超平面，，使得误差最小，实现近似分类。

 

#### 3.2.3 感知器的局限性

- 感知器的激活函数使用阈值函数，使得输出只能取两个值（-1/1或0/1）
- 只对线性可分的问题收敛
- 如果输入样本存在奇异样本，则网络需要花费很长时间。(奇异样本是数值上远远偏离其他样本的数据)
- 感知器的学习算法只对单层有效，因此无法套用其规则设计多层感知器。
-  感知器算法的另一个缺陷是，一旦所有样本均被正确分类，它就会停止更新权值，这看起来有些矛盾。直觉告诉我们，具有大间隔的决策面比感知器的决策面具有更好的分类误差。但是诸如“Support Vector Machines”之类的大间隔分类器不在本次讨论范围。

### 3.3 多层感知机

多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络(DNN: Deep Neural Networks)。

多层感知机的一个重要特点就是多层，我们将第一层称之为输入层，最后一层称之有输出层，中间的层称之为隐层。MLP并没有规定隐层的数量，因此可以根据各自的需求选择合适的隐层层数。且对于输出层神经元的个数也没有限制。
MLP神经网络结构模型如下,本文中只涉及了一个隐层，输入只有三个变量[x1,x2,x3][x1,x2,x3]和一个偏置量bb，输出层有三个神经元。相比于感知机算法中的神经元模型对其进行了集成。

![img](https://img-blog.csdn.net/20171107195647865?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveGhvbGVz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## 4 什么是前向转播

假设神经网络结构如下图所示：有2个输入单元；隐含层为2个神经元；输出层也是2个神经元，隐含层和输出层各有1个偏置。

![img](https://img-blog.csdn.net/20170919162604453?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

为了直观，这里初始化权重和偏置量，得到如下效果：

![img](https://img-blog.csdn.net/20170919163150250?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

**----前向传播----**

隐含层神经元***h1\***的输入：

![img](http://www.zhihu.com/equation?tex=net_%7Bh_%7B1%7D+%7D+%3Dw_%7B1%7D%5Cast+i_%7B1%7D++%2Bw_%7B2%7D%5Cast+i_%7B2%7D%2Bb_%7B1%7D%5Cast+1)

代入数据可得：

![img](http://www.zhihu.com/equation?tex=net_%7Bh_%7B1%7D+%7D+%3D0.15%5Cast+0.05++%2B0.2%5Cast+0.1%2B0.35%5Cast+1%3D0.3775)

假设激励函数用***logistic\***函数，计算得隐含层神经元***h1\***的输出：

![img](http://www.zhihu.com/equation?tex=out_%7Bh_%7B1%7D+%7D%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-net_%7Bh_%7B1%7D+%7D+%7D+%7D++%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-0.3775%7D++%7D%3D0.593269992)

同样的方法，可以得到隐含层神经元***h2\***的输出为：

![img](http://www.zhihu.com/equation?tex=out_%7Bh_%7B2%7D+%7D%3D0.596884378)

对输出层神经元重复这个过程，使用隐藏层神经元的输出作为输入。这样输出层神经元**O***1*的输出为：

![img](http://www.zhihu.com/equation?tex=net_%7Bo_%7B1%7D+%7D+%3Dw_%7B5%7D%5Cast+out_%7Bh_%7B1%7D+%7D++%2Bw_%7B6%7D%5Cast+out_%7Bh_%7B2%7D+%7D%2Bb_%7B2%7D%5Cast+1)

代入数据：

![img](http://www.zhihu.com/equation?tex=net_%7Bo_%7B1%7D+%7D+%3D0.4%5Cast+0.593269992++%2B0.45%5Cast+0.596884378%2B0.6%3D1.105905967)

输出层神经元**O1**的输出：

![img](http://www.zhihu.com/equation?tex=out_%7Bo_%7B1%7D+%7D%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-net_%7Bo_%7B1%7D+%7D+%7D+%7D++%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-1.105905967%7D++%7D%3D0.75136507)

同样的方法，可以得到输出层神经元**O2**的输出为：

![img](http://www.zhihu.com/equation?tex=out_%7Bo_%7B2%7D+%7D%3D0.772928465)

**----统计误差----**

假如误差公式为：

![img](http://www.zhihu.com/equation?tex=E_%7Btotal%7D+%3D%5Csum_%7B%7D%5E%7B%7D%7B%5Cfrac%7B1%7D%7B2%7D%28target+-+output%29%5E%7B2%7D++%7D+)

如上图，**O1**的原始输出为0.01，而神经网络的输出为0.75136507，则其误差为：

![img](http://www.zhihu.com/equation?tex=E_%7Bo_%7B1%7D+%7D+%3D%5Csum_%7B%7D%5E%7B%7D%7B%5Cfrac%7B1%7D%7B2%7D%280.01+-+0.75136507%29%5E%7B2%7D++%7D%3D0.298371109+)

同理可得，**O2**的误差为：

![img](http://www.zhihu.com/equation?tex=E_%7Bo_%7B2%7D+%7D+%3D0.023560026)

这样，总的误差为：

![img](http://www.zhihu.com/equation?tex=E_%7Btotal%7D+%3DE_%7Bo_%7B1%7D+%7D%2B++E_%7Bo_%7B2%7D+%7D%3D0.298371109)

## 5 什么是反向传播

对于***w5\***，想知道其改变对总误差有多少影响，得求偏导：

![img](http://www.zhihu.com/equation?tex=%5Cfrac%7Bd+E_%7Btotal%7D+%7D%7Bd+w_%7B5%7D+%7D+)

根据链式法则：

![img](http://www.zhihu.com/equation?tex=%5Cfrac%7Bd+E_%7Btotal%7D+%7D%7Bd+w_%7B5%7D+%7D%3D%5Cfrac%7Bd+E_%7Btotal%7D+%7D%7Bd+out_%7Bo_%7B1%7D+%7D+%7D%5Cast%5Cfrac%7Bd+out_%7Bo_%7B1%7D+%7D+%7D%7Bd+net_%7Bo_%7B1%7D+%7D+%7D%5Cast+%5Cfrac%7Bd+net_%7Bo_%7B1%7D+%7D+%7D%7Bd+w_%7B5%7D+%7D+++)

在这个过程中，需要弄清楚每一个部分。

首先：

![img](https://pic2.zhimg.com/v2-5414eaa222162dfb50acd23cb3de78ed_b.png)

其次：

![img](https://pic1.zhimg.com/v2-4b4f1d78ed0cf45eae5176e7d0183718_b.png)

最后：

![img](https://pic3.zhimg.com/v2-c9e6e0d82026c81c894aaf8427387636_b.png)

把以上三部分相乘，得到：

![img](https://pic4.zhimg.com/v2-efedc9a244680df235402145a9f99dfb_b.png)

根据梯度下降原理，从当前的权重减去这个值（假设学习率为0.5），得：

![img](https://pic1.zhimg.com/v2-ddb478df679c272e70a1d040bc32ed7c_b.png)

同理，可以得到：

![img](https://pic3.zhimg.com/v2-0d8aecf78d509a20b0ee8ed3dc04b7e2_b.png)

这样，输出层的所以权值就都更新了（先不管偏置），接下来看**隐含层**：

对***w1\***求偏导：

![img](https://img-blog.csdn.net/20170919165824421?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

用图表来描述上述链式法则求导的路径：

![img](https://img-blog.csdn.net/20170919165942656?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZ3VveXVuZmVpMjA=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

最后，更新了所有的权重！ 当最初前馈传播时输入为0.05和0.1，网络上的误差是0.298371109。 在第一轮反向传播之后，总误差现在下降到0.291027924。 它可能看起来不太多，但是在重复此过程10,000次之后。例如，错误倾斜到0.000035085。

