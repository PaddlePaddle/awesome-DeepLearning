# 深度学习基础知识

## 1 深度学习发展历史

- **1943年**

由神经科学家**麦卡洛克(W.S.McCilloch)** 和**数学家皮兹（W.Pitts）**在《数学生物物理学公告》上发表论文《神经活动中内在思想的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity）。建立了神经网络和数学模型，称为MCP模型。所谓**MCP**模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，也就诞生了所谓的“模拟大脑”，人工神经网络的大门由此开启。



![img](https://pic2.zhimg.com/80/v2-83aaa4536666794e0dd62fdd3e926ad9_720w.jpg)

**麦卡洛克(W.S.McCilloch)**

![img](https://pic2.zhimg.com/80/v2-e3ad45ef4a0e6d14c203b7c0ea1184e1_720w.jpg)

**皮兹（W.Pitts）**

MCP当时是希望能够用计算机来模拟人的神经元反应的过程，该模型将神经元简化为了三个过程：输入信号线性加权，求和，非线性激活（阈值法）。如下图所示

![img](https://pic1.zhimg.com/80/v2-3c42edad1e322afa103dca26b6b3a2ac_720w.png)

- **1958年**

计算机科学家**罗森布拉特（ Rosenblatt）**提出了两层神经元组成的神经网络，称之为**“感知器”(Perceptrons)**。第一次将MCP用于**机器学习（machine learning）分类(classification)**。“感知器”算法算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年,该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。

- **1969年**

纵观科学发展史，无疑都是充满曲折的，深度学习也毫不例外。 1969年，美国数学家及人工智能先驱 **Marvin Minsky** 在其著作中证明了感知器本质上是一种**线性模型（linear model）**，只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了将近20年的停滞。

- **1986年**

由神经网络之父 **Geoffrey Hinton** 在1986年发明了适用于多层感知器（MLP）的**BP（Backpropagation）**算法，并采用**Sigmoid**进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。

注：**Sigmoid** 函数是一个在生物学中常见的S型的函数，也称为S型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间。



![img](https://pic1.zhimg.com/80/v2-4b01991e1f9450bd8ecfb90d1a2bdc68_720w.png)

- **90年代时期**

1991年BP算法被指出存在梯度消失问题，也就是说在误差梯度后项传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该问题直接阻碍了深度学习的进一步发展。

此外90年代中期，支持向量机算法诞生（SVM算法）等各种浅层机器学习模型被提出，SVM也是一种有监督的学习模型，应用于模式识别，分类以及回归分析等。支持向量机以统计学为基础，和神经网络有明显的差异，支持向量机等算法的提出再次阻碍了深度学习的发展。

- **发展期 2006年 - 2012年**

2006年，加拿大多伦多大学教授、机器学习领域泰斗、神经网络之父—— Geoffrey Hinton 和他的学生 Ruslan Salakhutdinov 在顶尖学术刊物《科学》上发表了一篇文章，该文章提出了深层网络训练中梯度消失问题的解决方案：**无监督预训练对权值进行初始化+有监督训练微调**。斯坦福大学、纽约大学、加拿大蒙特利尔大学等成为研究深度学习的重镇，至此开启了深度学习在学术界和工业界的浪潮。

![img](https://pic4.zhimg.com/80/v2-96716b57d6339a5beb0930a58533bf27_720w.jpg)

Geoffrey Hinton



2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。2011年以来，微软首次将DL应用在语音识别上，取得了重大突破。微软研究院和Google的语音识别研究人员先后采用DNN技术降低语音识别错误率20％~30％，是语音识别领域十多年来最大的突破性进展。2012年，DNN技术在图像识别领域取得惊人的效果，在ImageNet评测上将错误率从26％降低到15％。在这一年，DNN还被应用于制药公司的DrugeActivity预测问题，并获得世界最好成绩。

- **爆发期 2012 - 2017**

2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。

![img](https://pic4.zhimg.com/80/v2-b99ff7ee8e6785de6da16eba1d7d8cab_720w.jpg)

**AlexNet**的创新点在于:

**(1)首次采用ReLU激活函数，极大增大收敛速度且从根本上解决了梯度消失问题。**

**(2)由于ReLU方法可以很好抑制梯度消失问题，AlexNet抛弃了“预训练+微调”的方法，完全采用有监督训练。也正因为如此，DL的主流学习方法也因此变为了纯粹的有监督学习。**

**(3)扩展了LeNet5结构，添加Dropout层减小过拟合，LRN层增强泛化能力/减小过拟合。**

**(4)第一次使用GPU加速模型计算。**

2013、2014、2015、2016年，通过ImageNet图像识别比赛，DL的网络结构，训练方法，GPU硬件的不断进步，促使其在其他领域也在不断的征服战场。

2016年3月，由谷歌（Google）旗下DeepMind公司开发的AlphaGo(基于深度学习)与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜；2016年末2017年初，该程序在中国棋类网站上以“大师”（Master）为注册帐号与中日韩数十位围棋高手进行快棋对决，连续60局无一败绩；2017年5月，在中国乌镇围棋峰会上，它与排名世界第一的世界围棋冠军柯洁对战，以3比0的总比分获胜。围棋界公认阿尔法围棋的棋力已经超过人类职业围棋顶尖水平。

## 2 人工智能、机器学习、深度学习有什么区别和联系

我们就用最简单的方法——同心圆，可视化地展现出它们三者的关系和应用。

![人工智能、机器学习和深度学习之间的区别和联系](https://static.leiphone.com/uploads/new/article/740_740/201609/57ce9265aae3f.png?imageMogr2/format/jpg/quality/90)

如上图，人工智能是最早出现的，也是最大、最外侧的同心圆；其次是机器学习，稍晚一点；最内侧，是深度学习，当今人工智能大爆炸的核心驱动。

五十年代，人工智能曾一度被极为看好。之后，人工智能的一些较小的子集发展了起来。先是机器学习，然后是深度学习。深度学习又是机器学习的子集。深度学习造成了前所未有的巨大的影响。

**|** **从概念的提出到走向繁荣**

1956年，几个计算机科学家相聚在达特茅斯会议（Dartmouth Conferences），提出了“人工智能”的概念。其后，人工智能就一直萦绕于人们的脑海之中，并在科研实验室中慢慢孵化。之后的几十年，人工智能一直在两极反转，或被称作人类文明耀眼未来的预言；或者被当成技术疯子的狂想扔到垃圾堆里。坦白说，直到2012年之前，这两种声音还在同时存在。

过去几年，尤其是2015年以来，人工智能开始大爆发。很大一部分是由于GPU的广泛应用，使得并行计算变得更快、更便宜、更有效。当然，无限拓展的存储能力和骤然爆发的数据洪流（大数据）的组合拳，也使得图像数据、文本数据、交易数据、映射数据全面海量爆发。

让我们慢慢梳理一下计算机科学家们是如何将人工智能从最早的一点点苗头，发展到能够支撑那些每天被数亿用户使用的应用的。

**|** **人工智能（Artificial Intelligence）——为机器赋予人的智能**

**![人工智能、机器学习和深度学习之间的区别和联系](https://static.leiphone.com/uploads/new/article/740_740/201609/57ce951069278.png?imageMogr2/format/jpg/quality/90)**



早在1956年夏天那次会议，人工智能的先驱们就梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。这就是我们现在所说的“**强人工智能**”（General AI）。这个无所不能的机器，它有着我们所有的感知（甚至比人更多），我们所有的理性，可以像我们一样思考。

人们在电影里也总是看到这样的机器：友好的，像星球大战中的C-3PO；邪恶的，如终结者。强人工智能现在还只存在于电影和科幻小说中，原因不难理解，我们还没法实现它们，至少目前还不行。

我们目前能实现的，一般被称为“**弱人工智能**”（Narrow AI）。**弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。**例如，Pinterest上的图像分类；或者Facebook的人脸识别。

这些是弱人工智能在实践中的例子。这些技术实现的是人类智能的一些具体的局部。但它们是如何实现的？这种智能是从何而来？这就带我们来到同心圆的里面一层，机器学习。

**|** **机器学习—— 一种实现人工智能的方法**

**![人工智能、机器学习和深度学习之间的区别和联系](https://static.leiphone.com/uploads/new/article/740_740/201609/57ce956b0a1d9.png?imageMogr2/format/jpg/quality/90)**



**机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。**与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。

机器学习直接来源于早期的人工智能领域。传统算法包括决策树学习、推导逻辑规划、聚类、强化学习和贝叶斯网络等等。众所周知，我们还没有实现强人工智能。早期机器学习方法甚至都无法实现弱人工智能。

**机器学习最成功的应用领域是计算机视觉**，虽然也还是需要大量的手工编码来完成工作。人们需要手工编写分类器、边缘检测滤波器，以便让程序能识别物体从哪里开始，到哪里结束；写形状检测程序来判断检测对象是不是有八条边；写分类器来识别字母“ST-O-P”。使用以上这些手工编写的分类器，人们总算可以开发算法来感知图像，判断图像是不是一个停止标志牌。

这个结果还算不错，但并不是那种能让人为之一振的成功。特别是遇到云雾天，标志牌变得不是那么清晰可见，又或者被树遮挡一部分，算法就难以成功了。这就是为什么前一段时间，计算机视觉的性能一直无法接近到人的能力。它太僵化，太容易受环境条件的干扰。

随着时间的推进，学习算法的发展改变了一切。

**|** **深度学习——一种实现机器学习的技术**

**![人工智能、机器学习和深度学习之间的区别和联系](https://static.leiphone.com/uploads/new/article/740_740/201609/57ce95bd0798d.png?imageMogr2/format/jpg/quality/90)**



人工神经网络（Artificial Neural Networks）是早期机器学习中的一个重要的算法，历经数十年风风雨雨。神经网络的原理是受我们大脑的生理结构——互相交叉相连的神经元启发。但与大脑中一个神经元可以连接一定距离内的任意神经元不同，**人工神经网络具有离散的层、连接和数据传播的方向**。

例如，我们可以把一幅图像切分成图像块，输入到神经网络的第一层。在第一层的每一个神经元都把数据传递到第二层。第二层的神经元也是完成类似的工作，把数据传递到第三层，以此类推，直到最后一层，然后生成结果。

每一个神经元都为它的输入分配权重，这个权重的正确与否与其执行的任务直接相关。最终的输出由这些权重加总来决定。

我们仍以停止（Stop）标志牌为例。将一个停止标志牌图像的所有元素都打碎，然后用神经元进行“检查”：八边形的外形、救火车般的红颜色、鲜明突出的字母、交通标志的典型尺寸和静止不动运动特性等等。神经网络的任务就是给出结论，它到底是不是一个停止标志牌。神经网络会根据所有权重，给出一个经过深思熟虑的猜测——“概率向量”。

这个例子里，系统可能会给出这样的结果：86%可能是一个停止标志牌；7%的可能是一个限速标志牌；5%的可能是一个风筝挂在树上等等。然后网络结构告知神经网络，它的结论是否正确。

即使是这个例子，也算是比较超前了。直到前不久，神经网络也还是为人工智能圈所淡忘。其实在人工智能出现的早期，神经网络就已经存在了，但神经网络对于“智能”的贡献微乎其微。主要问题是，即使是最基本的神经网络，也需要大量的运算。神经网络算法的运算需求难以得到满足。

不过，还是有一些虔诚的研究团队，以多伦多大学的Geoffrey Hinton为代表，坚持研究，实现了以超算为目标的并行算法的运行与概念证明。但也直到GPU得到广泛应用，这些努力才见到成效。

我们回过头来看这个停止标志识别的例子。**神经网络**是调制、训练出来的，时不时还是很容易出错的。它**最需要的，就是训练**。需要成百上千甚至几百万张图像来训练，直到神经元的输入的权值都被调制得十分精确，无论是否有雾，晴天还是雨天，每次都能得到正确的结果。

只有这个时候，我们才可以说神经网络成功地自学习到一个停止标志的样子；或者在Facebook的应用里，神经网络自学习了你妈妈的脸；又或者是2012年吴恩达（Andrew Ng）教授在Google实现了神经网络学习到猫的样子等等。

吴教授的突破在于，把这些神经网络从基础上显著地增大了。层数非常多，神经元也非常多，然后给系统输入海量的数据，来训练网络。在吴教授这里，数据是一千万YouTube视频中的图像。吴教授为深度学习（deep learning）加入了“深度”（deep）。这里的**“深度”就是说神经网络中众多的层**。

现在，经过深度学习训练的图像识别，在一些场景中甚至可以比人做得更好：从识别猫，到辨别血液中癌症的早期成分，到识别核磁共振成像中的肿瘤。Google的AlphaGo先是学会了如何下围棋，然后与它自己下棋训练。它训练自己神经网络的方法，就是不断地与自己下棋，反复地下，永不停歇。

## 3 神经元、单层感知机、多层感知机

### 3.1 神经元

正如其名字所表明，神经网络的灵感来源于人类大脑的神经结构，像在一个人类大脑中，最基本的构件就叫做神经元。它的功能和人的神经元很相似，换句话说，它有一些输入，然后给一个输出。在数学上，在机器学习中的神经元就是一个数学函数的占位符，它仅有的工作就是对输入使用一个函数，然后给一个输出。

![理解神经网络：从神经元到RNN、CNN、深度学习](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tcHQuMTM1ZWRpdG9yLmNvbS9tbWJpel9wbmcvYmljZE1MekltbGliUVVQZkpFaWE4eDVpYkc0NWlhZGhZU1hHc2liQmZvQXVxTndPaWEzMkZlQ3VvVnZoa09OSlo0OG0xMUFnN0dIaFREaWFpYmxLbGI2aWNHdDA4UUNnLzY0MD93eF9mbXQ9cG5nJnd4X2NvPTE?x-oss-process=image/format,png)

这种神经元中使用的函数，在术语上通常叫做激活函数。主要的激活函数有5种，date,step,sigmoid,tanh和ReLU。

### 3.2 单层感知机

单层感知器属于单层前向网络，即除输入层和输出层之外，只拥有一层神经元节点。

　　特点：输入数据从输入层经过隐藏层向输出层逐层传播，相邻两层的神经元之间相互连接，同一层的神经元之间没有连接。

　　感知器（perception）是由美国学者F.Rosenblatt提出的。与最早提出的MP模型不同，神经元突触权值可变，因此可以通过一定规则进行学习。可以快速、可靠地解决线性可分的问题。



#### 3.2.1 单层感知器的结构

　　单层感知器由一个线性组合器和一个二值阈值元件组成。输入向量的各个分量先与权值相乘，然后在线性组合器中进行叠加，得到一个标量结果，其输出是线性组合结果经过一个二值阈值函数。二值阈值元件通常是一个上升函数，典型功能是非负数映射为1，负数映射为0或负一。

　　输入是一个N维向量 x=[x1,x2,...,xn]，其中每一个分量对应一个权值wi，隐含层输出叠加为一个标量值：

![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115104935967-821769030.png)

随后在二值阈值元件中对得到的v值进行判断，产生二值输出：

　　　　　　　　　　　　　　　      ![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115105450576-1393524587.gif)

可以将数据分为两类。实际应用中，还加入偏置，值恒为1，权值为b。这时，y输出为：

　　　　　　　　　　　　　　　　　　![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115110119170-381381331.gif)

把偏置值当作特殊权值：





![img](https://upload-images.jianshu.io/upload_images/10865154-b5afb460887fdf1c.png)

　单层感知器结构图：

　　　　　　　　　　　　　　　　![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115110155326-596049826.png)

　单层感知器进行模式识别的超平面由下式决定：

　　　　　　　　　　　　　　　　　　　　![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115110437763-1231637778.gif)

当维数N=2时，输入向量可以表示为平面直角坐标系中的一个点。此时分类超平面是一条直线:

　　　　　　　　　　　　　　　　　　　　![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115110540217-26972109.gif)

这样就可以将点沿直线划分成两类。



#### 3.2.2 单层感知器的学习算法

(1)定义变量和参数，这里的n是迭代次数。N是N维输入，将其中的偏置也作为输入，不过其值恒为1,。

　　x(n)=N+1维输入向量=[+1,x1(n),x2(n),...,xN(n)]T

　　w(n)=N+1维权值向量=[b(n),w1(n),w2(n),...,wN(n)]T

　　b(n)=偏置

　　y(n)=实际输出

　　d(n)=期望输出

　　η(n)=学习率参数，是一个比1小的正常数

所以线性组合器的输出为：v(n)=wT(n)x(n)

(2)初始化。n=0,将权值向量w设置为随机值或全零值。

(3)激活。输入训练样本，对每个训练样本x(n)=[+1,x1(n),x2(n),...,xN(n)]T,指定其期望输出d。即若x属于l1，则d=1，若x属于l2，则d属于-1（或者0）。

(4)计算实际输出。　　　　　　　　y(n)=sgn(wT(n)x(n))

(5)更新权值向量　　　　　　　　　w(n+1)=w(n)+η[d(n)-y(n)]x(n)

感知器的学习规则：学习信号等于神经元期望输出与实际输出之差:

![img](https://upload-images.jianshu.io/upload_images/10865154-628e471716bb213a.png)

dj为期望的输出，oj为实际的输出。
W代表特征向量或者是矩阵。T代表矩阵的转置，X为输入信号或者是训练数据集中的一个m维样本





![img](https://upload-images.jianshu.io/upload_images/10865154-ce2d8179fc872cb8.png)

权值调整公式为：



![img](https://upload-images.jianshu.io/upload_images/10865154-6ff05aba5ed0186c.png)

公式右侧前面的符号η为学习率。权值调整值为学习率乘以误差乘以输入信号(X)。
当实际输出与期望值相同时，权值不需要调整，在有误差存在的情况下，并且期望值与实际输出值为1或者-1，此时权值调整公式可以简化为：

![img](https://upload-images.jianshu.io/upload_images/10865154-307dd2fc055cf393.png)

**每一次带入一个数据进行迭代，而不是带入所有数据（为什么呢，感觉带入所有数据进行运算会好很多）其实也能一批数据全部直接带入计算，只不过现在的我还不太懂为什么可以这样。此问题留待日后回答。**

这里

　　　　　　　　　　　　　　　　![img](https://images2015.cnblogs.com/blog/997392/201611/997392-20161115131952982-500157079.gif)

　　　　　　　　　　　　　　　　0<η<1

**为什么权值更新会是这样呢：我认为可以这样理解，当y与d相同时，分类正确，权值不用更新。**

**当y与d不同时 : w(n+1)\*x（n）=w(n)\*x(n)+η[d(n)-y(n)]x(n)\*x(n) 即当d为1而y为-1时，整体的w(n+1)\*x(n)增加的，当d为-1而y为1时，整体是往-1的方向变化的，所以w(n+1)权值会使得整体往正确的分类走。但是这个权值更新公式怎么来的就还不知道了。**

(6)判断。若满足收敛条件，则算法结束，若不满足，n++，转到第(3)步。

收敛条件：当权值向量w已经能正确实现分类时，算法就收敛了，此时网络误差为零。收敛条件通常可以是：

　　　　误差小于某个预先设定的较小的值ε。即

　　　　　　　　　　　　　　　　　　|d(n)-y(n)|<ε

　　　　两次迭代之间的权值变化已经很小，即

　　　　　　　　　　　　　　　　　　|w(n+1)-w(n)|<ε

为防止偶然因素导致的提取收敛，前两个条件还可以改进为连续若干次误差或者权值变化小于某个值。

　　　　设定最大迭代次数M，当迭代了M次就停止迭代。

　　　　需事先通过经验设定学习率η,不应该过大，以便为输入向量提供一个比较稳定的权值估计。不应过小，以便使权值能根据输入的向量x实时变化，体现误差对权值的修正作用。

- 学习率大于零，小于等于１
- 学习率太大，容易造成权值调整不稳定。
- 学习率太小，权值调整太慢，迭代次数太多。
- 学习率一般选择可变的，类似于调节显微镜一样，开始时步长比较大，后面步长比较小。

　　　　它只对线性可分的问题收敛，通过学习调整权值，最终找到合适的决策面，实现正确分类。

单层感知器在研究线性不可分的问题中：可以追求尽量正确的分类，定义一个误差准则，在不同的超平面中选择一个最优超平面，，使得误差最小，实现近似分类。



#### 3.2.3 感知器的局限性

- 感知器的激活函数使用阈值函数，使得输出只能取两个值（-1/1或0/1）
- 只对线性可分的问题收敛
- 如果输入样本存在奇异样本，则网络需要花费很长时间。(奇异样本是数值上远远偏离其他样本的数据)
- 感知器的学习算法只对单层有效，因此无法套用其规则设计多层感知器。
-  感知器算法的另一个缺陷是，一旦所有样本均被正确分类，它就会停止更新权值，这看起来有些矛盾。直觉告诉我们，具有大间隔的决策面比感知器的决策面具有更好的分类误差。但是诸如“Support Vector Machines”之类的大间隔分类器不在本次讨论范围。

### 3.3 多层感知机

多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络(DNN: Deep Neural Networks)。

多层感知机的一个重要特点就是多层，我们将第一层称之为输入层，最后一层称之有输出层，中间的层称之为隐层。MLP并没有规定隐层的数量，因此可以根据各自的需求选择合适的隐层层数。且对于输出层神经元的个数也没有限制。
MLP神经网络结构模型如下,本文中只涉及了一个隐层，输入只有三个变量[x1,x2,x3][x1,x2,x3]和一个偏置量bb，输出层有三个神经元。相比于感知机算法中的神经元模型对其进行了集成。

![img](https://img-blog.csdn.net/20171107195647865?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveGhvbGVz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## 4 什么是前向转播

![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/07e9efff73ad4e8cf528305497236cdf.png)

如图所示，这里讲得已经很清楚了，前向传播的思想比较简单。
举个例子，假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。
最终不断的通过这种方法一层层的运算，得到输出层结果。

对于前向传播来说，不管维度多高，其过程都可以用如下公式表示：
$$
a 2 = σ ( z 2 ) = σ ( a 1 ∗ W 2 + b 2 ) a^2 = \sigma(z^2) = \sigma(a^1 * W^2 + b^2)*a*2=*σ*(*z*2)=*σ*(*a*1∗*W*2+*b*2)
$$
a 2 = σ ( z 2 ) = σ ( a 1 ∗ W 2 + b 2 ) a^2 = \sigma(z^2) = \sigma(a^1 * W^2 + b^2)*a*2=*σ*(*z*2)=*σ*(*a*1∗*W*2+*b*2)

$$

其中，上标代表层数，星号表示卷积，b表示偏置项bias，σ \sigma*σ*表示激活函数。

## 5 什么是反向传播

BackPropagation算法是多层神经网络的训练中举足轻重的算法。简单的理解，它的确就是复合函数的链式法则，但其在实际运算中的意义比链式法则要大的多。要回答题主这个问题“如何直观的解释back propagation算法？” 需要先直观理解多层神经网络的训练。

机器学习可以看做是数理统计的一个应用，在数理统计中一个常见的任务就是拟合，也就是给定一些样本点，用合适的曲线揭示这些样本点随着自变量的变化关系.

深度学习同样也是为了这个目的，只不过此时，样本点不再限定为(x, y)点对，而可以是由向量、矩阵等等组成的广义点对(X,Y)。而此时，(X,Y)之间的关系也变得十分复杂，不太可能用一个简单函数表示。然而，人们发现可以用多层神经网络来表示这样的关系，而多层神经网络的本质就是一个多层复合的函数。借用网上找到的一幅图[1]，来直观描绘一下这种复合关系。

![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/ef43e6a4e39749579a50bbb04bd0f867.png)

其对应的表达式如下：
![这里写图片描述](https://img-blog.csdnimg.cn/img_convert/07d8c80d5512aa9fd6c3a0f18e94e91d.png)

上面式中的Wij就是相邻两层神经元之间的权值，它们就是深度学习需要学习的参数，也就相当于直线拟合y=k*x+b中的待求参数k和b。

和直线拟合一样，深度学习的训练也有一个目标函数，这个目标函数定义了什么样的参数才算一组“好参数”，不过在机器学习中，一般是采用成本函数（cost function），然后，训练目标就是通过调整每一个权值Wij来使得cost达到最小。cost函数也可以看成是由所有待求权值Wij为自变量的复合函数，而且基本上是非凸的，即含有许多局部最小值。但实际中发现，采用我们常用的梯度下降法就可以有效的求解最小化cost函数的问题。
