### 一、CNN-DSSM知识点补充

​	针对 DSSM 词袋模型丢失上下文信息的缺点，CLSM（convolutional latent semantic model）应运而生，又叫 CNN-DSSM。CNN-DSSM 与 DSSM 的区别主要在于输入层和表示层。

#### 1、输入层

##### (1)英文

英文的处理方式，除了letter-trigram，CNN-DSSM 还在输入层增加了word-trigram

![](https://ai-studio-static-online.cdn.bcebos.com/7049c685828f4ac1b8eefedbef57c5204961852b152a4689bd4816f2c9f8c89d)

如上图所示，word-trigram其实就是一个包含了上下文信息的滑动窗口。举个例子：把 online auto body ... 这句话提取出前三个词 online auto，之后再分别对这三个词进行letter-trigram映射到一个 3 万维的向量空间里，然后把三个向量 concat 起来，最终映射到一个 9 万维的向量空间里。

##### (2)中文

英文的处理方式（word-trigram letter-trigram）在中文中并不可取，因为英文中虽然用了 word-ngram 把样本空间拉成了百万级，但是经过 letter-trigram 又把向量空间降到可控级别，只有 3*30K（9 万）。而中文如果用 word-trigram，那向量空间就是百万级的了，显然还是字向量（1.5 万维）比较可控。

#### 2、表示层

​	CNN-DSSM 的表示层由一个卷积神经网络组成，如下图所示：

![](https://ai-studio-static-online.cdn.bcebos.com/f1130709e03b4d55817afab05e7961302448a0ef9ba54547bfa56ecff03c8321)

#### 3、匹配层

CNN-DSSM 的匹配层和 DSSM 的一样

#### 4、优缺点

优点：CNN-DSSM 通过卷积层提取了滑动窗口下的上下文信息，又通过池化层提取了全局的上下文信息，上下文信息得到较为有效的保留。
缺点：对于间隔较远的上下文信息，难以有效保留。举个例子，I grew up in France... I speak fluent French，显然 France 和 French 是具有上下文依赖关系的，但是由于 CNN-DSSM 滑动窗口（卷积核)大小的限制，导致无法捕获该上下文信息。

### 二、LSTM-DSSM知识点补充

​	针对 CNN-DSSM 无法捕获较远距离上下文特征的缺点，有人提出了用LSTM-DSSM（Long-Short-Term Memory）来解决该问题。LSTM-DSSM 其实用的是 LSTM 的一个变种——加入了peephole的 LSTM。如下图所示：

![](https://ai-studio-static-online.cdn.bcebos.com/d7347babef7941afa05260ed0ab104ee5bd6af287eb44d1ba6e29df72350bdc6)
![](https://ai-studio-static-online.cdn.bcebos.com/1b74ceb3252343399525e64aa0153499ac588ee862284d6699cf8b7b21396e96)

​	这里三条黑线就是所谓的 peephole，传统的 LSTM 中遗忘门、输入门和输出门只用了 h(t-1) 和 xt 来控制门缝的大小，peephole 的意思是说不但要考虑 h(t-1) 和 xt，也要考虑 Ct-1 和 Ct，其中遗忘门和输入门考虑了 Ct-1，而输出门考虑了 Ct。总体来说需要考虑的信息更丰富了。、
LSTM-DSSM 整体的网络结构：

![](https://ai-studio-static-online.cdn.bcebos.com/343bfe0d57d84521890cefd04dca09104c843a5185b640ddbee913968183c6c7)  



### 三、MMoE多任务学习知识点补充

​	为了进行不相关任务的多任务学习，很多人做了很多工作都见效甚微，然后后来就有了Google的这个相当新颖的模型MMoE。

![](https://ai-studio-static-online.cdn.bcebos.com/c61d80c403ff4970af9b39cb09778d64c3dca7521d6e4f85a1f04e387349bbe9)

​	它的脑洞大开之处在于跳出了Shared Bottom那种将整个隐藏层一股脑的共享的思维定式，而是将共享层有意识的（按照数据领域之类的）划分成了多个Expert，并引入了gate机制，得以个性化组合使用共享层。
观察一下上面Shared Bottom的模型结构图和MMoE的图，不难发现，MMoE实际上就是把Shared Bottom层替换成了一个双Gate的MoE层：

![](https://ai-studio-static-online.cdn.bcebos.com/c6dffab4976d44b99bec443a40f7d190bc3dc303ca7e4b51b7a6b8773516c73e)

​	MoE共享层将这个大的Shared Bottom网络拆分成了多个小的Expert网络（如图所示，拆成了三个，并且保持参数个数不变，显然分成多少个Expert、每个多少参数，都是可以根据实际情况自己设定的）。我们把第i个Expert网络的运算记为fi(x),然后Gate操作记为g(x)，他是一个n元的softmax值（是Expert的个数，有几个Expert，就有几元），之后就是常见的每个Expert输出的加权求和，假设MoE的输出为y,那么可以表示为：

![](https://ai-studio-static-online.cdn.bcebos.com/221eeace65ac4f478bca375ce6a57d5a62331101ebfe4b4d89cb2ff78edd1a85)

Gate
把输入通过一个线性变换映射到nums_expert维，再算个softmax得到每个Expert的权重
Expert
简单的基层全连接网络，relu激活，每个Expert独立权重
多任务模型训练效果对比：

![](https://ai-studio-static-online.cdn.bcebos.com/4f790121d4424c218c8baef8205b5ae8135b5aca21ab4c21b2d34a270bb5c326)

### 四、ShareBottom多任务学习知识点补充

ShareBottom多任务学习思路就是共享底部最抽象的表示层，然后在上层分化出不同的任务：

![](https://ai-studio-static-online.cdn.bcebos.com/6977aae0da1d4ad6a077e24d5acaa438d704201fcbcf4bb58d75fc1eccae88bb)

​	这实际跟迁移学习有点类似，在图像领域甚是常见，因为图像识别的底层特征往往代表一些像素纹理之类的抽象特征，而跟具体的任务不是特别相关，因此这种低冲突的表示层共享是比较容易出效果的，并且可以减少多任务的计算量。
猫狗识别：

![](https://ai-studio-static-online.cdn.bcebos.com/bd8663b7ca0a4b7ea854c259e930ddcc3ef30599c1d84d79aeec3547856a4414)

猫狗识别共享底层：

![](https://ai-studio-static-online.cdn.bcebos.com/98a899447d7140a38b274a482d0382f3abc496ea96fd4fc0a10113edde28881f)

​	比如说我们可以很轻松的合并一个识别猫的任务和一个识别狗的任务，因为这两个任务所需要学习的表示很相似，因此同时学好这两个任务是可能的。

### 五、YouTube深度学习视频推荐系统知识点补充

#### 1、推荐系统的应用场景

​	作为全球最大的视频分享网站，YouTube 平台中几乎所有的视频都来自 UGC（User Generated Content，用户原创内容），这样的内容产生模式有两个特点：
​	一是其商业模式不同于 Netflix，以及国内的腾讯视频、爱奇艺这样的流媒体，这些流媒体的大部分内容都是采购或自制的电影、剧集等头部内容，而 YouTube 的内容都是用户上传的自制视频，种类风格繁多，头部效应没那么明显；
​	二是由于 YouTube 的视频基数巨大，用户难以发现喜欢的内容。

#### 2、YouTube 推荐系统架构

为了对海量的视频进行快速、准确的排序，YouTube 也采用了经典的召回层 + 排序层的推荐系统架构。

![](https://ai-studio-static-online.cdn.bcebos.com/a04049c718704535ba92f019f506689261b34a5c0a8d438281fbb89cd5166670)

其推荐过程可以分成二级。第一级是用候选集生成模型（Candidate Generation Model）完成候选视频的快速筛选，在这一步，候选视频集合由百万降低到几百量级，这就相当于经典推荐系统架构中的召回层。第二级是用排序模型（Ranking Model)完成几百个候选视频的精排，这相当于经典推荐系统架构中的排序层。

#### 3、候选集生成模型

用于视频召回的候选集生成模型，架构如下图所示

![](https://ai-studio-static-online.cdn.bcebos.com/e39f6b1f072741c99798b42c24a564a2dd9fb9493e0c485c8cb4b0a992f312b6)

​	最底层是它的输入层，输入的特征包括用户历史观看视频的 Embedding 向量，以及搜索词的 Embedding 向量。对于这些 Embedding 特征，YouTube 是利用用户的观看序列和搜索序列，采用了类似 Item2vec 的预训练方式生成的。
​	除了视频和搜索词 Embedding 向量，特征向量中还包括用户的地理位置 Embedding、年龄、性别等特征。这里我们需要注意的是，对于样本年龄这个特征，YouTube 不仅使用了原始特征值，还把经过平方处理的特征值也作为一个新的特征输入模型。这个操作其实是为了挖掘特征非线性的特性。
​	确定好了特征，这些特征会在 concat 层中连接起来，输入到上层的 ReLU 神经网络进行训练。
​	三层 ReLU 神经网络过后，YouTube 又使用了 softmax 函数作为输出层。值得一提的是，这里的输出层不是要预测用户会不会点击这个视频，而是要预测用户会点击哪个视频，这就跟一般深度推荐模型不一样。
​	总的来讲，YouTube 推荐系统的候选集生成模型，是一个标准的利用了 Embedding 预训练特征的深度推荐模型，它遵循Embedding MLP 模型的架构，只是在最后的输出层有所区别。

#### 4、排序模型



![](https://ai-studio-static-online.cdn.bcebos.com/809205ba5ce24c84bf73637a52c40a77e00dcb1aeaf245628fee57c33d24de1f)

输入层，相比于候选集生成模型需要对几百万候选集进行粗筛，排序模型只需对几百个候选视频进行排序，因此可以引入更多特征进行精排。具体来说，YouTube 的输入层从左至右引入的特征依次是：
(1)impression video ID embedding：当前候选视频的 Embedding；
(2)watched video IDs average embedding：用户观看过的最后 N 个视频 Embedding 的平均值；
(3)language embedding：用户语言的 Embedding 和当前候选视频语言的 Embedding；
(4)time since last watch：表示用户上次观看同频道视频距今的时间；
(5)#previous impressions：该视频已经被曝光给该用户的次数；
	这 5 类特征连接起来之后，需要再经过三层 ReLU 网络进行充分的特征交叉，然后就到了输出层。这里重点注意，排序模型的输出层与候选集生成模型又有所不同。不同主要有两点：一是候选集生成模型选择了 softmax 作为其输出层，而排序模型选择了 weighted logistic regression（加权逻辑回归）作为模型输出层；二是候选集生成模型预测的是用户会点击“哪个视频”，排序模型预测的是用户“要不要点击当前视频”。
	排序模型采用不同输出层的根本原因就在于，YouTube 想要更精确地预测用户的观看时长，因为观看时长才是 YouTube 最看中的商业指标，而使用 Weighted LR 作为输出层，就可以实现这样的目标。
	在 Weighted LR 的训练中，我们需要为每个样本设置一个权重，权重的大小，代表了这个样本的重要程度。为了能够预估观看时长，YouTube 将正样本的权重设置为用户观看这个视频的时长，然后再用 Weighted LR 进行训练，就可以让模型学到用户观看时长的信息。
	对于排序模型，必须使用 TensorFlow Serving 等模型服务平台，来进行模型的线上推断。

#### 5、训练和测试样本的处理

为了能够提高模型的训练效率和预测准确率，Youtube采取了诸多处理训练样本的工程措施，主要有3点：

+ 候选集生成模型把推荐模型转换成 多分类问题，在预测下一次观看的场景中，每一个备选视频都会是一个分类，而如果采用softmax对其训练是很低效的。Youtube采用word2vec中常用的 负采样训练方法减 少每次预测的分类数量，从而加快整个模型的收敛速度。
+ 在对训练集的预处理过程中，Youtube没有采用原始的用户日志，而是 对每个用户提取等数量的训练样本。YouTube这样做的目的是减少高度活跃用户对模型损失的过度影响，使模型过于偏向活跃用户的行为模式，忽略数量更广大的长尾用户体验。
+ 在处理测试集时，Youtube没有采用经典的随机留一法，而是一定要以用户最近一次观看的行为作为测试集。只留最后一次观看行为做测试集主要是为了避免引入未来信息(future information)，产生于事实不符的数据穿越问题。

