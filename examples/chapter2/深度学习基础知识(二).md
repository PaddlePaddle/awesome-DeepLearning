# 深度学习基础知识(二)

## 损失函数方法补充

### 均方损失

均方损失也叫最小二乘法，是线性回归的一种，它假设样本和噪声都服从高斯分布，其基本原则是最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小，这个距离就是常用的欧氏距离，因其简单计算方便，而且是一种很好的相似性度量标准，在不同的表示域变换后特征性质不变，因此得到了广泛的应用。用公式表示为
$$
L(Y,f(X))=(Y-f(X))^2
$$
当样本数为n的时候，损失函数变为：
$$
L(Y,f(X))=\sum_{i=1}^n(Y-f(X))^2
$$
这个公式表示了残差的平方和，目的最小化这个函数值，即最小化残差的平方和。

### 交叉熵损失

交叉熵（cross entropy）是信息论中的重要概念，主要用来度量两个概率分布间的差异。假定p和 q是数据 x的两个概率分布，通过 q 来表示 p 的交叉熵可如下计算：
$$
H(p,q)=-\sum_xp(x)\log q(x)
$$
它刻画了两个概率分布的距离，交叉熵越小，两个概率分布越接近。一个良好的神经网络需要尽量保证对于每一个输入数据，神经网络所预测类别分布概率与实际类别分布概率之间的差距越小越好，因此交叉熵可以作为损失函数来训练神经网络。

### 指数损失

Adaboost算法是前向分步算法是特例，是一个加和模型，损失函数就是指数函数，经过m次迭代后可以得到
$$
f_m(x)=f_{m-1}(x)+\alpha_mG_m(x)
$$
Adaboost每次迭代时的目的是为了找到最小化下列式子时的参数α 和G：
$$
\sum_{i=1}^Nexp[-y_i(f_{m-1}(x_i)+\alpha G(x_i))]
$$
指数函数的标准形式为：
$$
L(y,f(x))=\exp[-yf(x)]
$$
因此，Adaboost的目标式子就是指数损失。



### Hinge损失

在机器学习算法中，hinge损失函数和SVM是息息相关的。在线性支持向量机中，最优化问题可以等价于下列式子：
$$
\mathop{\min}\limits_{w,b}\sum_{i}^N[1-y_i(w\cdot x_i+b)]_++\lambda||w||^2
$$
经过变换可以得到
$$
\frac{1}{m}\sum_{i=1}^ml(w\cdot x_i+b,y_i)+||w||^2
$$
前半部分的l就是hinge损失函数，后面的就是L2正则项。

其标准形式为
$$
L(y)=max(0,1-y\tilde{y} ),y=\pm1
$$


### 0-1损失

表达式为
$$
L(y,f(X))=\left\{
\begin{matrix}
1,Y\ne f(X) \\
0, Y=f(X)
\end{matrix}
\right.
$$

### 绝对值损失

表达式为
$$
L(Y,f(X))=|Y-f(X)|
$$






## 损失函数python代码补充

均方损失实现

```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)
```

交叉熵损失实现：

```python
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```

Hinge损失实现：

```python
def update_weights_Hinge(m1, m2, b, X1, X2, Y, learning_rate):

    m1_deriv = 0
    m2_deriv = 0
    b_deriv = 0
    N = len(X1)
    for i in range(N):
        # 计算偏导数
        if Y[i]*(m1*X1[i] + m2*X2[i] + b) <= 1:
            m1_deriv += -X1[i] * Y[i]
            m2_deriv += -X2[i] * Y[i]
            b_deriv += -Y[i]
        # 否则偏导数为0
    # 我们减去它，因为导数指向最陡的上升方向
    m1 -= (m1_deriv / float(N)) * learning_rate
    m2 -= (m2_deriv / float(N)) * learning_rate
    b -= (b_deriv / float(N)) * learning_rate
return m1, m2, b
```



## 池化方法补充

**随机池化**：Stochastic pooling是一种简单有效的正则化CNN的方法，能够降低max pooling的过拟合现象，提高泛化能力。对于pooling层的输入，根据输入的多项式分布随机选择一个值作为输出。训练阶段和测试阶段的操作略有不同。

训练阶段：

1）前向传播：先将池化窗口中的元素全部除以它们的和，得到概率矩阵；再按照概率随机选中的方格的值，作为该区域池化后的值。

2）反向传播：求导时，只需保留前向传播中已经被选中节点的位置的值，其它值都为0，类似max-pooling的反向传播。

测试阶段：

在测试时也使用Stochastic Pooling会对预测值引入噪音，降低性能。取而代之的是使用概率矩阵加权平均。比使用Average Pooling表现要好一些。在平均意义上，与Average Pooling近似，在局部意义上，服从Max Pooling准则。

**重叠池化**：重叠池化，即相邻池化窗口之间会有重叠区域。如果定义池化窗口的大小为sizeX，定义两个相邻池化窗口的水平位移 / 竖直位移为stride，此时sizeX>stride。

## 数据增强方法修改及补充

### 翻转

对图片进行水平和垂直翻转。一些框架不提供垂直翻转功能。但是，一个垂直反转的图片等同于图片的180度旋转，然后再执行水平翻转。

### 旋转

一个关键性的问题是当旋转之后图像的维数可能并不能保持跟原来一样。

### 缩放比例

图像可以向外或向内缩放。向外缩放时，最终图像尺寸将大于原始图像尺寸。大多数图像框架从新图像中剪切出一个部分，其大小等于原始图像。

### 裁剪

只是从原始图像中随机抽样一个部分。然后，将此部分的大小调整为原始图像大小。这种方法通常称为随机裁剪。

### 移位

移位只涉及沿X或Y方向（或两者）移动图像。

### 高斯噪声

当神经网络试图学习可能无用的高频特征（大量出现的模式）时，通常会发生过度拟合。具有零均值的高斯噪声基本上在所有频率中具有数据点，从而有效地扭曲高频特征。这也意味着较低频率的组件（通常是您的预期数据）也会失真，但神经网络可以学会超越它。添加适量的噪音可以增强学习能力。

### 生成对抗网络

在某些情况下，添加生成的或合成的数据(称为数据增强的过程)也可以提高性能。最常见的方法是对现有数据应用一些转换。在图像分类的例子中，我们知道，例如，在移动或翻转一个猫的图像后，它仍然是一只猫的图像。因此，图像分类数据集通常会增加移位、翻转、旋转或颜色变化，以获得可能的最佳结果。

通过生成器不断的学习生成出新的样本，从而使得样本总量增加，使用GANs进行简单的数据增强有时可以提高分类器的性能，特别是在非常小或有限的数据集的情况下，但使用GANs进行增强的最有希望的情况似乎包括迁移学习或少量学习。

## 图像分类方法综述

### 传统图像分类算法

通常完整建立图像识别模型一般包括底层特征学习、特征编码、空间约束、分类器设计、模型融合等几个阶段。

- 底层特征提取: 通常从图像中按照固定步长、尺度提取大量局部特征描述。常用的局部特征包括SIFT(Scale-Invariant Feature Transform, 尺度不变特征转换) 、HOG(Histogram of Oriented Gradient, 方向梯度直方图) 、LBP(Local Bianray Pattern, 局部二值模式)等，一般也采用多种特征描述，防止丢失过多的有用信息。

- 特征编码: 底层特征中包含了大量冗余与噪声，为了提高特征表达的鲁棒性，需要使用一种特征变换算法对底层特征进行编码，称作特征编码。常用的特征编码方法包括向量量化编码、稀疏编码、局部线性约束编码、Fisher向量编码等。

- 空间特征约束: 特征编码之后一般会经过空间特征约束，也称作特征汇聚。特征汇聚是指在一个空间范围内，对每一维特征取最大值或者平均值，可以获得一定特征不变形的特征表达。金字塔特征匹配是一种常用的特征汇聚方法，这种方法提出将图像均匀分块，在分块内做特征汇聚。

- 通过分类器分类: 经过前面步骤之后一张图像可以用一个固定维度的向量进行描述，接下来就是经过分类器对图像进行分类。通常使用的分类器包括SVM(Support Vector Machine, 支持向量机)、随机森林等。而使用核方法的SVM是最为广泛的分类器，在传统图像分类任务上性能很好。

这种传统的图像分类方法在PASCAL VOC竞赛中的图像分类算法中被广泛使用 。

### 深度学习算法
Alex Krizhevsky在2012年ILSVRC提出的CNN模型取得了历史性的突破，效果大幅度超越传统方法，获得了ILSVRC2012冠军，该模型被称作AlexNet。这也是首次将深度学习用于大规模图像分类中。

经典的深度学习算法主要有以下几个方面：

**CNN**：传统CNN包含卷积层、全连接层等组件，并采用softmax多类别分类器和多类交叉熵损失函数。

**VGG**：该模型相比以往模型进一步加宽和加深了网络结构，它的核心是五组卷积操作，每两组之间做Max-Pooling空间降维。同一组内采用多次连续的3X3卷积，卷积核的数目由较浅组的64增多到最深组的512，同一组内的卷积核数目是一样的。卷积之后接两层全连接层，之后是分类层。

**GoogLeNet**：GoogLeNet由多组Inception模块堆积而成。另外，在网络最后也没有采用传统的多层全连接层，而是像NIN网络一样采用了均值池化层；但与NIN不同的是，GoogLeNet在池化层后加了一个全连接层来映射类别数。

除了这两个特点之外，由于网络中间层特征也很有判别性，GoogLeNet在中间层添加了两个辅助分类器，在后向传播中增强梯度并且增强正则化，而整个网络的损失函数是这个三个分类器的损失加权求和。
**ResNet**：在已有设计思路(BN, 小卷积核，全卷积网络)的基础上，引入了残差模块。每个残差模块包含两条路径，其中一条路径是输入特征的直连通路，另一条路径对该特征做两到三次卷积操作得到该特征的残差，最后再将两条路径上的特征相加。



