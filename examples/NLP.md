##**用LSTM实现NLP任务**

循环神经网络在不同类型的机器学习任务中有不同的模式：序列到类别模式、同步的序列到序列模式、异步的序列到序列模式。

**1、序列到类别的模式**

序列到类别模式主要用于序列数据的分类问题：输入为序列（T个数据），输出为类别（一个数据）。典型的就是文本分类任务，输入数据为单词的序列（构成一篇文档），输出为该文本的类别。

假设有一个样本x1:T = (x1, x2, ..., xT)为一个长度为T的序列，输出为一个类别y∈{1, 2, ..., C}。将样本x按不同的时刻输入到循环神经网络中去，可以得到不同时刻的隐状态h1, h2, ..., hT，然后将hT看做整个序列的最终表示，输入给分类器g(•)做分类。

![img](https://img2018.cnblogs.com/blog/1630478/201904/1630478-20190414143242124-596455325.png)

当然除了采用最后时刻的隐状态hT作为序列的表示之外，还可以对整个序列的所有状态进行平均，用平均隐状态来作为整个序列的表示。

![img](https://img2018.cnblogs.com/blog/1630478/201904/1630478-20190414143423322-177267787.png)

这两种序列到类别模式的图示如下：

![img](https://img2018.cnblogs.com/blog/1630478/201904/1630478-20190414143507265-894209702.png)

**2、同步的序列到序列模式**

同步的序列到序列模式主要用于序列标注任务，即每一时刻都有输入和输出，输入序列和输出序列的长度相同。比如词性标注（Pos Tagging），每个单词都需要标注它的词性。命名实体识别（Name Entity Recognition，NER）也可以看做是序列标注问题，与词性标注的做法类似，特点在于对于命名实体，输出它的命名实体标签来代替词性。

假设有一个样本x1:T = (x1, x2, ..., xT)为一个长度为T的序列，输出序列为y1:T = (y1, y2, ..., yT)。将样本x按不同的时刻输入到循环神经网络中去，可以得到不同时刻的隐状态h1, h2, ..., hT，然后把每个时刻的隐状态输入给分类器g(•)，得到当前时刻的标签。

![img](https://img2018.cnblogs.com/blog/1630478/201904/1630478-20190414144450745-655677717.png)

![img](https://img2018.cnblogs.com/blog/1630478/201904/1630478-20190414144511400-1379419670.png)

 **3、异步的序列到序列模式**

异步的序列到序列模式也称为编码器-解码器（Encoder-Decoder）模型，即输入序列和输出序列不需要有严格的对应关系，也不用保持相同的长度。比如机器翻译中，输入为源语言的单词序列，输出为目标语言的单词序列。

在异步的序列到序列模式中，输入为一个长度为T的序列：x1:T = (x1, x2, ..., xT)，输出一个长度为M的序列：y1:M = (y1, y2, ..., yM)，通过先编码后解码的方式实现。

先将样本x按不同时刻输入到一个循环神经网络（编码器）中，得到其编码hT，然后在另一个循环神经网络（解码器）中得到输出序列ý1:M。为了建立输出序列之间的依赖关系，在解码器中通常使用非线性的自回归模型。

![img](https://img2018.cnblogs.com/blog/1630478/201904/1630478-20190414150605725-535756165.png)

其中f1(•)和f2(•)分别表示用作编码器和解码器的循环神经网络，g(•)为分类器。编码器和解码器的工作过程如下图所示：

![img](https://img2018.cnblogs.com/blog/1630478/201904/1630478-20190414150837758-1378223876.png)