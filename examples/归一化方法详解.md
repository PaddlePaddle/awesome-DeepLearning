## 归一化方法详解 （BN  GN LN)

### Batch Normalization

批量归一化(BN)是深度学习发展中的里程碑技术，可以实现各种网络的训练。然而，沿着Batch维度进行归一化会带来一些问题–当批次规模变小时，BN的误差会迅速增加，这是Batch统计估计不准确造成的。这限制了BN在训练大型模型和将特征转移到计算机视觉任务（包括检测、分割和视频）中的使用，这些任务需要小批量的内存消耗限制。

批归一化（Batch Normalization）的详细解释
 以前在神经网络训练中，只是对输入层数据进行归一化处理，却没有在中间层进行归一化处理。要知道，虽然我们对输入数据进行了归一化处理，但是输入数据经过矩阵乘法以及非线性运算之后，其数据分布很可能被改变，而随着深度网络的多层运算之后，数据分布的变化将越来越大。如果我们能在网络的中间也进行归一化处理，是否对网络的训练起到改进作用呢？答案是肯定的。

 这种在神经网络中间层也进行归一化处理，使训练效果更好的方法，就是批归一化Batch Normalization。

####  1.批归一化（BN）算法的优点

下面总结一下BN算法的优点：

减少了人为选择参数。在某些情况下可以取消 dropout 和 L2 正则项参数,或者采取更小的 L2 正则项约束参数；
减少了对学习率的要求。现在我们可以使用初始很大的学习率或者选择了较小的学习率，算法也能够快速训练收敛；
可以不再使用局部响应归一化。BN 本身就是归一化网络(局部响应归一化在 AlexNet 网络中存在)
破坏原来的数据分布，一定程度上缓解过拟合（防止每批训练中某一个样本经常被挑选到，文献说这个可以提高 1% 的精度）。
减少梯度消失，加快收敛速度，提高训练精度。

#### 2.批归一化（BN）算法流程

下面给出 BN 算法在训练时的过程:

![在这里插入图片描述](https://github.com/BLINDHANZO/interesting-codes/raw/master/BN.png)


BN的作用及应用场景： 

1. 改善流经网络的梯度；

2. 允许更大的学习率，大幅提高训练速度；

3. 减少对初始化的强烈依赖；

4. 改善正则化策略：作为正则化的一种形式，轻微减少了对dropout的需要；

5. 再也不需要使用使用局部响应归一化层了，因为BN本身就是一个归一化网络层。

   BN可以应用于网络中任意的activation set。BN应作用在非线性映射前，即对![x=Wu+b](https://www.zhihu.com/equation?tex=x%3DWu%2Bb)做规范化。另外对CNN的“权值共享”策略，BN还有其对应的做法

### Group Normalization(组归一化)

#### 1.简介

Group Normalization 作为BN的简单替代方法。GN将通道分成若干组，并在每组内计算均值和方差进行归一化。GN的计算不受batch sizes的影响，在很大的batch sizes范围内，GN的精度都很稳定。在ImageNet中训练的ResNet-50上，当使用batch sizes大小为2时，GN的误差比BN低10.6%；当使用典型的batch sizes时，GN与BN相当，并优于其他归一化变种。此外，GN可以自然地从预训练转移到微调。在COCO中，GN在目标检测和分割方面的表现优于BN，在Kinetics中，GN在视频分类方面的表现也优于BN，这表明GN可以在各种任务中有效地替代强大的BN。GN在现代库中只需几行代码就能轻松实现。

![在这里插入图片描述](https://github.com/BLINDHANZO/interesting-codes/raw/master/GN.JPG)



批量归一化(Batch Norm或BN)[26]已被确立为深度学习中一个非常有效的组件，在很大程度上帮助推动了计算机视觉[59，20]及其他领域的前沿[54]。BN通过在一个(迷你)批次内计算的均值和方差对特征进行标准化。这已经被许多实践证明，可以简化优化，使很深的网络能够收敛。批量统计的随机不确定性也可以作为正则化器，有利于泛化。BN已经成为许多统计学计算机视觉算法的基础。

ImageNet分类误差与batch sizes的关系——ImageNet分类误差与批次大小。这是在ImageNet训练集中使用8个worker（GPU）训练的ResNet-50模型，在验证集中进行评估。

尽管BN取得了巨大的成功，但它也有缺点，这些缺点也是由它沿batch维度进行归一化的独特行为引起的。特别是，BN需要在足够大的batch size下工作（例如，每个工人32个2[26，59，20]）。小批量会导致批量统计的估计不准确，而减小BN的批量大小会使模型误差急剧增加。因此，最近的许多模型[59，20，57，24，63]都是用非平凡的批量大小来训练的，这些模型都很耗费内存。严重依赖BN的有效性来训练模型，又使人们无法探索更高容量的模型，而这些模型会受到内存的限制。

在计算机视觉任务中，包括检测[12，47，18]、分割[38，18]、视频识别[60，6]以及其他建立在其上的高级系统中，对批量大小的限制要求更高。例如，Fast/er和Mask R-CNN框架[12，47，18]由于分辨率较高，使用1或2张图像的批量大小，BN通过转化为线性层来 “冻结”[20]；在3D卷积的视频分类[60，6]中，空间-时间特征的存在引入了时间长度和批量大小之间的权衡。BN的使用往往要求这些系统在模型设计和批量大小之间进行折衷。

![在这里插入图片描述](https://github.com/BLINDHANZO/interesting-codes/raw/master/NM.JPG)

图2。标准化方法。每个子图显示一个特征映射张量，N为batch轴，C为通道轴，（H，W）为空间轴。蓝色的像素用相同的平均值和方差进行标准化，计算方法是aggregating这些像素的值。

组归一化（GN）作为BN的一个简单替代方案.我们注意到，许多经典特征如SIFT[39]和HOG[9]都是group-wise特征，涉及group-wise归一化。例如，一个HOG向量是几个空间单元的结果，其中每个单元由一个归一化的方向直方图表示。类似地，我们提出GN作为一个层，将通道划分为组，并对每个组内的特征进行归一化（图2）。GN不利用batch维度，其计算与batch大小无关。

**GN在广泛的batch sizes范围内表现得非常稳定（图1）。在批量大小为2个样本的情况下，GN的误差比ImageNet[50]中ResNet-50[20]的BN同类产品低10.6%**。在常规批次大小的情况下，GN与BN相当好（差距为0.5%），并优于其他归一化变体[3，61，51]。此外，虽然批次大小可能会发生变化，但GN可以自然地从预训练转移到微调。GN在用于COCO目标检测和分割的Mask R-CNN上[37]，以及在用于Kinetics视频分类的3D卷积网络上[30]，都显示出比其BN同类产品更好的结果。GN在ImageNet、COCO和Kinetics中的有效性表明，GN是在这些任务中占主导地位的BN的一个有竞争力的替代方案。

已有的方法，如层归一化(LN)[3]和实例归一化(IN)[61(图2)，也避免了沿批次维度的归一化。这些方法对于训练顺序模型（RNN/LSTM[49，22]）或生成模型（GANs[15，27]）是有效的。但正如我们将通过实验表明的那样，LN和IN在视觉识别中的成功率都很有限，对于视觉识别，GN呈现出更好的结果。相反，GN可以用来代替LN和IN，因此适用于顺序模型或生成模型。

众所周知，对输入数据进行归一化可以使训练速度更快[33]。为了对hidden features进行归一化，人们基于特征分布的强假设推导出了初始化方法[33，14，19]，这些方法在训练演化时可能变得无效。

在BN发展之前，深度网络中的归一化层已经被广泛使用。 **Local Response Normalization** (LRN)[40，28，32]是AlexNet[32]和后续模型中的一个组成部分。与最近的方法不同的是，LRN计算每个像素在一个小的邻域中的统计数据。批量归一化[26]沿着batch维度执行更多的全局归一化(同样重要的是，它建议对所有层都这样做)。但是 "batch"的概念并不是一直存在的，或者说它可能会时常变化。例如，批维归一化在推理时是不合法的，所以均值和方差是从训练集中预先计算出来的[26]，通常是通过运行平均来计算的；因此，测试时没有进行归一化。当目标数据分布发生变化时，预计算的统计数据也可能发生变化[45]。这些问题导致训练、转移和测试时的不一致性。此外，如前所述，减少批次大小会对估计的batch统计量产生巨大影响。

为了避免利用批次维度，已经提出了几种归一化方法[3，61，51，2，46]。层归一化(LN)[3]沿着通道维度操作，实例归一化(IN)[61]执行BN-likecomputation，但只针对每个样本(图2)。权重归一化（WN）[51]提出对滤波器权重进行归一化，而不是对特征进行操作。这些方法不会受到批维造成的问题的影响，但在许多视觉识别任务中，它们还无法接近BN的精度。我们将在其余章节中提供与这些方法的比较。

**Addressing small batches.** Ioffe[25]提出了**Batch Renormalization（BR）**，缓解了BN涉及小批量的问题。BR引入了两个额外的参数，将BN的估计均值和方差约束在一定范围内，减少了小批量时的漂移。在小批量制度下，BR比BN有更好的精度。但BR也是依赖于批次的，当批次规模减小时，其精度仍然会降低[25]。

**也有人尝试避免使用小批量。[43]中的目标检测器执行同步BN，其均值和方差是在多个GPU上计算的**。然而，这种方法并不能解决小批量的问题，相反，它将算法问题迁移到工程和硬件需求上，使用的GPU数量与BN的要求成正比。**此外，同步BN计算可以防止使用异步求解器（asynchronous solvers ASGD[10]），这是工业界广泛使用的大规模训练的实用解决方案。这些问题会限制同步BN的使用范围**。

我们的归一化方法没有解决批量统计计算（如[25，43]），而是从本质上避免了这种计算。

**Group-wise computation.** AlexNet[32]提出了用于将一个模型分配到两个GPU中的分组卷积。组作为模型设计的一个维度的概念最近得到了更广泛的研究。ResNeXt[63]的工作研究了深度、宽度和群组之间的权衡，它认为在计算成本相近的情况下，更大数量的群组可以提高精度。MobileNet[23]和Xception[7]利用了信道卷积（也称为 “深度卷积”），即组数等于信道数的组卷积。ShuffleNet[65]提出了一种通道洗牌操作，对分组特征的轴进行换算。这些方法都涉及到将通道维度划分为组。尽管与这些方法有关系，但GN不需要分组卷积。GN是一个通用层，正如在标准ResNets[20]中评估的那样。

#### 2. 计算公式

我们首先描述了特征归一化的一般表述，然后介绍了这个表述中的GN。一系列的特征归一化方法，包括BN、LN、IN和GN，进行如下计算。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200817170134162.jpg#pic_center)

这里x是一层计算的特征，i是一个索引。在二维图像的情况下，i = ( i N , i C , i H , i W ) i=(i_N,i_C,i_H,i_W)*i*=(*i**N*,*i**C*,*i**H*,*i**W*)是一个4D向量，按照(N,C,H,W)的顺序对特征进行索引，其中N是batch轴，C是通道轴，H和W是空间高度和宽度轴。

μ 和σ*为均值和方差，计算公式为:

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200817170142859.jpg#pic_center)

其中ϵ 是一个小的常数。S*i*是计算平均数和标准数的像素集，m是这个集合的大小。许多类型的特征归一化方法主要是在如何定义集Si方面有所不同（图2），讨论如下。

在Batch Norm[26]中，集合S*i*定义为:

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200817170152134.jpg#pic_center)。

这意味着共享相同通道索引的像素被归一化，即对于每个通道，BN沿(N,H,W)轴计算 μ 和σ  。在层规范[3]中，集合是。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200817170159924.jpg#pic_center)

意味着LN为每个样本计算 μ \mu*μ* 和σ \sigma*σ* 沿（C,H,W）轴。在Instance Norm[61]中，集合为：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200817170207888.jpg#pic_center)

意味着IN为每个样本和每个通道计算和沿(H,W)轴。BN、LN、IN之间的关系如图2所示。

与[26]一样，BN、LN和IN的所有方法都会学习一个per-channel线性变换，以补偿可能丧失的表征能力。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200817170215356.jpg#pic_center)

其中γ 和β*为可训练参数和偏置(在所有情况下都以Ci为索引，为了简化记号，我们省略了)

**Group Norm.** 形式上，Group Norm layer在集后S*i*里计算μ*和*σ*：

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020081717022480.jpg#pic_center)



给定公式(7)中的Si，由Eqn.(1)、(2)和(6)定义一个GN层。具体来说，同一组中的像素由相同的μ  和σ 归一化，GN也学习每通道的γ和β*。

**Relation to Prior Work.** LN、IN和GN都是沿batch轴进行独立计算。GN的两种极端情况相当于LN和IN（图4）。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200817170235459.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNzk1ODI3Mg==,size_16,color_FFFFFF,t_70#pic_center)

**Relation to Layer Normalization [3].** 如果我们设置组数为G=1，GN就变成了LN。LN假设一个层中的所有通道都做出 “类似的贡献”[3]。与[3]中研究的完全连接层的情况不同，这个假设在存在卷积的情况下可能不那么有效，这在[3]中讨论过。GN比LN的限制更少，因为每组通道（而不是所有通道）都被假定为受共享的均值和方差的影响；模型仍然可以灵活地学习每组的不同分布。这导致GN的表示能力比LN有所提高，这从实验中较低的训练和验证误差可以看出（图4）。

**Relation to Instance Normalization [61].** 如果我们将组数设置为G=C（即每组一个通道），GN就变成了IN。但IN只能依靠空间维度来计算均值和方差，它错过了利用通道依赖性的机会。

#### 3. 应用Experiments

GN对batch_size 不敏感，BN的误差随着小批量的大小而变得相当高。GN的行为比较稳定，对批量大小不敏感。实际上，GN在32到2的大批量范围内有非常相似的曲线（受随机变化的影响），在批量大小为2的情况下，GN的误差率比BN的误差率低

这些结果表明，批均值和方差估计可能是过度随机和不准确的，特别是当它们是在4或2张图像上计算的。然而，如果从1张图像中计算统计数据，这种随机性就会消失，在这种情况下，BN在训练时变得与IN相似。我们看到，在批次大小为2的情况下，IN的结果比BN更好批均值和方差估计可能是过度随机和不准确的，特别是当它们是在4或2张图像上计算的。然而，如果从1张图像中计算统计数据，这种随机性就会消失，在这种情况下，BN在训练时变得与IN相似。我们看到，在批次大小为2的情况下，IN的结果比BN更好。GN的稳健结果显示了GN的优势。它允许消除BN所施加的批量大小的约束，可以给出相当多的内存（如16个或更多）。这将使训练更大容量的模型成为可能，否则就会受到内存限制的瓶颈。

### Layer Normalization(层归一化)

![img](https://pic1.zhimg.com/v2-2f1ad5749e4432d11e777cf24b655da8_b.jpg)

层规范化就是针对 BN 的上述不足而提出的。与 BN 不同，LN 是一种横向的规范化，如图所示。它综合考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入。

![[公式]](https://www.zhihu.com/equation?tex=%5Cmu+%3D+%5Csum_i%7Bx_i%7D%2C+%5Cquad+%5Csigma%3D+%5Csqrt%7B%5Csum_i%7B%28x_i-%5Cmu%29%5E2%7D%2B%5Cepsilon+%7D%5C%5C) 

其中 ![[公式]](https://www.zhihu.com/equation?tex=i) 枚举了该层所有的输入神经元。对应到标准公式中，四大参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Cmu%7D), ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7B%5Csigma%7D), ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bg%7D), ![[公式]](https://www.zhihu.com/equation?tex=%5Cbold%7Bb%7D) 均为标量（BN中是向量），所有输入共享一个规范化变换。LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。但是，BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。

