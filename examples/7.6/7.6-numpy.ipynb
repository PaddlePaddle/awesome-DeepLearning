{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "housing.data\r\n"
     ]
    }
   ],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/external-libraries’: File exists\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 18.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve-2.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/beautifulsoup4-4.9.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/bs4 already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import json\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import paddle.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    datafile='./work/housing.data'\r\n",
    "    data=np.fromfile(datafile,sep=' ',dtype=np.float32)\r\n",
    "    \r\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\r\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\r\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\r\n",
    "    feature_num = len(feature_names)\r\n",
    "\r\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\r\n",
    "    #将一维的数据转成二维\r\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\r\n",
    "    \r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio=0.8\r\n",
    "    offset=int(data.shape[0]*ratio)\r\n",
    "    training_data=data[:offset]\r\n",
    "\r\n",
    "    # 计算train数据集的最大值，最小值，平均值\r\n",
    "    maximums,minimums,avgs=training_data.max(axis=0),training_data.min(axis=0),training_data.sum(axis=0)/training_data.shape[0]\r\n",
    "\r\n",
    "    # 记录数据的归一化参数，在预测时对数据做归一化\r\n",
    "    global max_values\r\n",
    "    global min_values\r\n",
    "    global avg_values\r\n",
    "    max_values=maximums\r\n",
    "    min_values=minimums\r\n",
    "    avg_values=avgs\r\n",
    "\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        data[:,i]=(data[:,i]-avgs[i])/(maximums[i]-minimums[i])\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data=data[:offset]\r\n",
    "    test_data=data[offset:]\r\n",
    "    \r\n",
    "    return training_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义隐藏层使用的激活函数\r\n",
    "def Sigmoid(x):\r\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建一个两层神经网络的类\r\n",
    "\r\n",
    "class Network(object):\r\n",
    "    def __init__(self, num_of_weights):\r\n",
    "        # 随机产生w的初始值\r\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\r\n",
    "        np.random.seed(0)\r\n",
    "        #初始化参数\r\n",
    "        self.w1 = np.random.randn(num_of_weights, 128)\r\n",
    "        self.b1 = 0.\r\n",
    "        self.w2=np.random.randn(128,1)\r\n",
    "        self.b2=0.\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        #正向传播\r\n",
    "        z1 = np.dot(x, self.w1) + self.b1\r\n",
    "        o1=Sigmoid(z1)\r\n",
    "        o2= np.dot(o1,self.w2)+self.b2\r\n",
    "        return z1,o1,o2\r\n",
    "    \r\n",
    "    #计算损失\r\n",
    "    def loss(self, z, y):\r\n",
    "        error = z - y\r\n",
    "        num_samples = error.shape[0]\r\n",
    "        cost = error * error\r\n",
    "        cost = np.sum(cost) / num_samples\r\n",
    "        return cost\r\n",
    "    \r\n",
    "    def gradient(self, x, y, z1,o1,o2):\r\n",
    "\r\n",
    "        #此处略复杂，经过手工计算梯度反向传播公式，然后分别计算相应的梯度\r\n",
    "        N = o1.shape[0]\r\n",
    "        gradient_w2 = 1. / N * np.sum((o2-y) * o1, axis=0)\r\n",
    "        gradient_w2 = gradient_w2[:, np.newaxis]\r\n",
    "        gradient_b2 = 1. / N * np.sum(o2-y)\r\n",
    "\r\n",
    "        M=x.shape[0]\r\n",
    "        gradient_w1=1. /M*np.sum(np.dot(self.w2.T,np.dot(o1.T,(o2-y)))*np.dot(x.T,(1-o1)),axis=0)\r\n",
    "        gradient_w1 = gradient_w1[:, np.newaxis]\r\n",
    "        gradient_b1=1. /M*np.sum(np.dot(self.w2.T,np.dot(o1.T,(o2-y)))*(1-o1),axis=0)\r\n",
    "\r\n",
    "        return gradient_w1, gradient_b1,gradient_w2, gradient_b2\r\n",
    "    \r\n",
    "    #根据计算出的梯度分别对两层的参数进行更新\r\n",
    "    def update(self, gradient_w1, gradient_b1, gradient_w2, gradient_b2, eta = 0.01):\r\n",
    "        self.w2 = self.w2 - eta * gradient_w2\r\n",
    "        self.b2 = self.b2 - eta * gradient_b2\r\n",
    "        self.w1 = self.w1 - eta * gradient_w1.T\r\n",
    "        self.b1 = self.b1 - eta * gradient_b1\r\n",
    "            \r\n",
    "    # 训练过程\r\n",
    "    def train(self, training_data, num_epochs, batch_size=10, eta=0.01):\r\n",
    "        n = len(training_data)\r\n",
    "        losses = []\r\n",
    "        for epoch_id in range(num_epochs):\r\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\r\n",
    "            # 然后再按每次取batch_size条数据的方式取出\r\n",
    "            np.random.shuffle(training_data)\r\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\r\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\r\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "                #print(self.w.shape)\r\n",
    "                #print(self.b)\r\n",
    "                x = mini_batch[:, :-1]\r\n",
    "                y = mini_batch[:, -1:]\r\n",
    "                z1,o1,o2 = self.forward(x)\r\n",
    "                loss = self.loss(o2, y)\r\n",
    "                gradient_w1, gradient_b1, gradient_w2, gradient_b2= self.gradient(x, y, z1,o1,o2)\r\n",
    "                self.update(gradient_w1, gradient_b1, gradient_w2, gradient_b2, eta)\r\n",
    "                losses.append(loss)\r\n",
    "                print(\"epoch: {}, iter: {}, loss is: {}\".format(epoch_id, iter_id, loss))\r\n",
    "                \r\n",
    "        \r\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss is: 18.111580444945503\n",
      "epoch: 0, iter: 1, loss is: 0.14498051234691686\n",
      "epoch: 0, iter: 2, loss is: 0.22971707607135677\n",
      "epoch: 0, iter: 3, loss is: 0.07232001053003789\n",
      "epoch: 0, iter: 4, loss is: 0.029914688750341607\n",
      "epoch: 1, iter: 0, loss is: 0.24111990165123642\n",
      "epoch: 1, iter: 1, loss is: 0.07742667132226934\n",
      "epoch: 1, iter: 2, loss is: 0.09302604676552281\n",
      "epoch: 1, iter: 3, loss is: 0.10965454558606962\n",
      "epoch: 1, iter: 4, loss is: 0.05191885949193933\n",
      "epoch: 2, iter: 0, loss is: 0.04994551658991393\n",
      "epoch: 2, iter: 1, loss is: 0.10725665151341875\n",
      "epoch: 2, iter: 2, loss is: 0.17276675056196267\n",
      "epoch: 2, iter: 3, loss is: 0.09059992532500637\n",
      "epoch: 2, iter: 4, loss is: 0.012884720474931369\n",
      "epoch: 3, iter: 0, loss is: 0.11011143578588704\n",
      "epoch: 3, iter: 1, loss is: 0.06171771282502803\n",
      "epoch: 3, iter: 2, loss is: 0.08117285817363078\n",
      "epoch: 3, iter: 3, loss is: 0.11623498722816131\n",
      "epoch: 3, iter: 4, loss is: 0.04588253623783141\n",
      "epoch: 4, iter: 0, loss is: 0.10698675141785666\n",
      "epoch: 4, iter: 1, loss is: 0.06880285668338383\n",
      "epoch: 4, iter: 2, loss is: 0.10397021852332439\n",
      "epoch: 4, iter: 3, loss is: 0.07013156245749542\n",
      "epoch: 4, iter: 4, loss is: 0.01274888461303904\n",
      "epoch: 5, iter: 0, loss is: 0.14335545273202574\n",
      "epoch: 5, iter: 1, loss is: 0.07724784599399767\n",
      "epoch: 5, iter: 2, loss is: 0.05066707300236785\n",
      "epoch: 5, iter: 3, loss is: 0.06642371403202964\n",
      "epoch: 5, iter: 4, loss is: 0.21333659071368405\n",
      "epoch: 6, iter: 0, loss is: 0.13045288773153818\n",
      "epoch: 6, iter: 1, loss is: 0.0640415439225635\n",
      "epoch: 6, iter: 2, loss is: 0.0842581773093156\n",
      "epoch: 6, iter: 3, loss is: 0.05611461093273783\n",
      "epoch: 6, iter: 4, loss is: 0.02083528894128587\n",
      "epoch: 7, iter: 0, loss is: 0.043619977049036925\n",
      "epoch: 7, iter: 1, loss is: 0.10637064017283158\n",
      "epoch: 7, iter: 2, loss is: 0.06398029998168604\n",
      "epoch: 7, iter: 3, loss is: 0.1331033012423533\n",
      "epoch: 7, iter: 4, loss is: 0.029876794538556087\n",
      "epoch: 8, iter: 0, loss is: 0.0804170401065119\n",
      "epoch: 8, iter: 1, loss is: 0.09964850034790693\n",
      "epoch: 8, iter: 2, loss is: 0.07310538762471477\n",
      "epoch: 8, iter: 3, loss is: 0.09612403270510722\n",
      "epoch: 8, iter: 4, loss is: 0.13684060601821696\n",
      "epoch: 9, iter: 0, loss is: 0.0898604632225894\n",
      "epoch: 9, iter: 1, loss is: 0.06942623509109853\n",
      "epoch: 9, iter: 2, loss is: 0.07679772595651675\n",
      "epoch: 9, iter: 3, loss is: 0.12694871104390656\n",
      "epoch: 9, iter: 4, loss is: 0.09107193866347553\n",
      "epoch: 10, iter: 0, loss is: 0.12928498266268648\n",
      "epoch: 10, iter: 1, loss is: 0.05031759005664642\n",
      "epoch: 10, iter: 2, loss is: 0.12781060069197556\n",
      "epoch: 10, iter: 3, loss is: 0.058176911512947314\n",
      "epoch: 10, iter: 4, loss is: 0.19871005540300457\n",
      "epoch: 11, iter: 0, loss is: 0.15209246145023742\n",
      "epoch: 11, iter: 1, loss is: 0.09064777856023895\n",
      "epoch: 11, iter: 2, loss is: 0.043231013651695405\n",
      "epoch: 11, iter: 3, loss is: 0.10366670765824045\n",
      "epoch: 11, iter: 4, loss is: 0.08820461243782526\n",
      "epoch: 12, iter: 0, loss is: 0.06757735494739482\n",
      "epoch: 12, iter: 1, loss is: 0.06781309711624325\n",
      "epoch: 12, iter: 2, loss is: 0.13335779799038527\n",
      "epoch: 12, iter: 3, loss is: 0.14077445886788362\n",
      "epoch: 12, iter: 4, loss is: 0.009823848146343541\n",
      "epoch: 13, iter: 0, loss is: 0.10225527414926862\n",
      "epoch: 13, iter: 1, loss is: 0.06754276888715113\n",
      "epoch: 13, iter: 2, loss is: 0.13183985560290473\n",
      "epoch: 13, iter: 3, loss is: 0.13168977171596263\n",
      "epoch: 13, iter: 4, loss is: 0.022966909952081973\n",
      "epoch: 14, iter: 0, loss is: 0.11075201110466577\n",
      "epoch: 14, iter: 1, loss is: 0.07112479843592148\n",
      "epoch: 14, iter: 2, loss is: 0.22133860469923228\n",
      "epoch: 14, iter: 3, loss is: 0.04513729911769411\n",
      "epoch: 14, iter: 4, loss is: 0.005268595386767051\n",
      "epoch: 15, iter: 0, loss is: 0.18021419879724906\n",
      "epoch: 15, iter: 1, loss is: 0.09838965378904067\n",
      "epoch: 15, iter: 2, loss is: 0.1284062196599715\n",
      "epoch: 15, iter: 3, loss is: 0.06106372454338269\n",
      "epoch: 15, iter: 4, loss is: 0.07267624974091416\n",
      "epoch: 16, iter: 0, loss is: 0.17016288788398984\n",
      "epoch: 16, iter: 1, loss is: 0.1287929146448269\n",
      "epoch: 16, iter: 2, loss is: 0.04543068593914543\n",
      "epoch: 16, iter: 3, loss is: 0.1410372528860592\n",
      "epoch: 16, iter: 4, loss is: 0.009198507079153562\n",
      "epoch: 17, iter: 0, loss is: 0.17150674103911048\n",
      "epoch: 17, iter: 1, loss is: 0.07438088600456977\n",
      "epoch: 17, iter: 2, loss is: 0.07199390122538281\n",
      "epoch: 17, iter: 3, loss is: 0.17978937655622984\n",
      "epoch: 17, iter: 4, loss is: 0.12565968466201766\n",
      "epoch: 18, iter: 0, loss is: 0.15215079656899178\n",
      "epoch: 18, iter: 1, loss is: 0.18767526670753562\n",
      "epoch: 18, iter: 2, loss is: 0.11041011292380823\n",
      "epoch: 18, iter: 3, loss is: 0.06831488772037247\n",
      "epoch: 18, iter: 4, loss is: 0.01309639858625513\n",
      "epoch: 19, iter: 0, loss is: 0.11909470573189246\n",
      "epoch: 19, iter: 1, loss is: 0.19383415498160558\n",
      "epoch: 19, iter: 2, loss is: 0.1624879842728421\n",
      "epoch: 19, iter: 3, loss is: 0.047535426175292966\n",
      "epoch: 19, iter: 4, loss is: 0.1395417790210093\n",
      "epoch: 20, iter: 0, loss is: 0.17979317290423066\n",
      "epoch: 20, iter: 1, loss is: 0.12620358922350403\n",
      "epoch: 20, iter: 2, loss is: 0.1461655457800497\n",
      "epoch: 20, iter: 3, loss is: 0.09625858068554108\n",
      "epoch: 20, iter: 4, loss is: 0.030646508956442824\n",
      "epoch: 21, iter: 0, loss is: 0.21176998016828283\n",
      "epoch: 21, iter: 1, loss is: 0.1844321316421809\n",
      "epoch: 21, iter: 2, loss is: 0.1017091551669157\n",
      "epoch: 21, iter: 3, loss is: 0.07009587053926364\n",
      "epoch: 21, iter: 4, loss is: 0.10193664265311833\n",
      "epoch: 22, iter: 0, loss is: 0.17845722569099856\n",
      "epoch: 22, iter: 1, loss is: 0.13370925998440086\n",
      "epoch: 22, iter: 2, loss is: 0.10939106820131034\n",
      "epoch: 22, iter: 3, loss is: 0.16523564527662907\n",
      "epoch: 22, iter: 4, loss is: 0.03245321282709421\n",
      "epoch: 23, iter: 0, loss is: 0.19189001313686596\n",
      "epoch: 23, iter: 1, loss is: 0.15743377309136075\n",
      "epoch: 23, iter: 2, loss is: 0.09844764647774432\n",
      "epoch: 23, iter: 3, loss is: 0.17337817327503102\n",
      "epoch: 23, iter: 4, loss is: 0.06161738331276684\n",
      "epoch: 24, iter: 0, loss is: 0.21556543356450575\n",
      "epoch: 24, iter: 1, loss is: 0.16271523845174396\n",
      "epoch: 24, iter: 2, loss is: 0.15306372488008743\n",
      "epoch: 24, iter: 3, loss is: 0.13429249064687557\n",
      "epoch: 24, iter: 4, loss is: 0.05632986934529527\n",
      "epoch: 25, iter: 0, loss is: 0.21188544605350643\n",
      "epoch: 25, iter: 1, loss is: 0.12510961488311756\n",
      "epoch: 25, iter: 2, loss is: 0.1764470948002437\n",
      "epoch: 25, iter: 3, loss is: 0.1979796231690685\n",
      "epoch: 25, iter: 4, loss is: 0.035400331448606576\n",
      "epoch: 26, iter: 0, loss is: 0.19618277709631804\n",
      "epoch: 26, iter: 1, loss is: 0.2628006277225833\n",
      "epoch: 26, iter: 2, loss is: 0.15068484463368872\n",
      "epoch: 26, iter: 3, loss is: 0.15145300606467202\n",
      "epoch: 26, iter: 4, loss is: 0.5964863887189962\n",
      "epoch: 27, iter: 0, loss is: 0.27281934483048825\n",
      "epoch: 27, iter: 1, loss is: 0.23511355475805248\n",
      "epoch: 27, iter: 2, loss is: 0.18426747068098398\n",
      "epoch: 27, iter: 3, loss is: 0.18642358472016413\n",
      "epoch: 27, iter: 4, loss is: 0.07397352087587697\n",
      "epoch: 28, iter: 0, loss is: 0.20102889216848432\n",
      "epoch: 28, iter: 1, loss is: 0.23024848891834807\n",
      "epoch: 28, iter: 2, loss is: 0.2667784010208774\n",
      "epoch: 28, iter: 3, loss is: 0.264771286846838\n",
      "epoch: 28, iter: 4, loss is: 0.703126891216829\n",
      "epoch: 29, iter: 0, loss is: 0.25216017285883885\n",
      "epoch: 29, iter: 1, loss is: 0.30399468476029096\n",
      "epoch: 29, iter: 2, loss is: 0.34400059594004856\n",
      "epoch: 29, iter: 3, loss is: 0.1921260186370335\n",
      "epoch: 29, iter: 4, loss is: 0.07909288913285495\n",
      "epoch: 30, iter: 0, loss is: 0.32354299236626516\n",
      "epoch: 30, iter: 1, loss is: 0.28835307184696624\n",
      "epoch: 30, iter: 2, loss is: 0.20730924922500457\n",
      "epoch: 30, iter: 3, loss is: 0.344622128457707\n",
      "epoch: 30, iter: 4, loss is: 0.08733336312672407\n",
      "epoch: 31, iter: 0, loss is: 0.42546304678638014\n",
      "epoch: 31, iter: 1, loss is: 0.31332965327322554\n",
      "epoch: 31, iter: 2, loss is: 0.28839444662941927\n",
      "epoch: 31, iter: 3, loss is: 0.15120825969792145\n",
      "epoch: 31, iter: 4, loss is: 0.003392838037171792\n",
      "epoch: 32, iter: 0, loss is: 0.2559826573970781\n",
      "epoch: 32, iter: 1, loss is: 0.3158019502254224\n",
      "epoch: 32, iter: 2, loss is: 0.21193703484794418\n",
      "epoch: 32, iter: 3, loss is: 0.3770122258021225\n",
      "epoch: 32, iter: 4, loss is: 0.12111788500118092\n",
      "epoch: 33, iter: 0, loss is: 0.2521872966333822\n",
      "epoch: 33, iter: 1, loss is: 0.3511020242148073\n",
      "epoch: 33, iter: 2, loss is: 0.25971094415769047\n",
      "epoch: 33, iter: 3, loss is: 0.23974440288842602\n",
      "epoch: 33, iter: 4, loss is: 0.0038627240948621916\n",
      "epoch: 34, iter: 0, loss is: 0.29243262221742033\n",
      "epoch: 34, iter: 1, loss is: 0.3252751270535294\n",
      "epoch: 34, iter: 2, loss is: 0.18431701790434715\n",
      "epoch: 34, iter: 3, loss is: 0.2356647857002166\n",
      "epoch: 34, iter: 4, loss is: 0.020330318769753913\n",
      "epoch: 35, iter: 0, loss is: 0.15050745406103855\n",
      "epoch: 35, iter: 1, loss is: 0.3066515078291367\n",
      "epoch: 35, iter: 2, loss is: 0.19518129330700262\n",
      "epoch: 35, iter: 3, loss is: 0.34150139133658025\n",
      "epoch: 35, iter: 4, loss is: 0.06008037609453605\n",
      "epoch: 36, iter: 0, loss is: 0.1843968277035114\n",
      "epoch: 36, iter: 1, loss is: 0.3039730640808008\n",
      "epoch: 36, iter: 2, loss is: 0.22285884744780837\n",
      "epoch: 36, iter: 3, loss is: 0.23052068046563917\n",
      "epoch: 36, iter: 4, loss is: 0.00605851869256449\n",
      "epoch: 37, iter: 0, loss is: 0.2847683838231597\n",
      "epoch: 37, iter: 1, loss is: 0.1949383866782856\n",
      "epoch: 37, iter: 2, loss is: 0.2085749190250952\n",
      "epoch: 37, iter: 3, loss is: 0.1637388392264224\n",
      "epoch: 37, iter: 4, loss is: 0.004108428172935364\n",
      "epoch: 38, iter: 0, loss is: 0.21519932240233308\n",
      "epoch: 38, iter: 1, loss is: 0.2144639882847873\n",
      "epoch: 38, iter: 2, loss is: 0.22479916182775217\n",
      "epoch: 38, iter: 3, loss is: 0.15456541480858862\n",
      "epoch: 38, iter: 4, loss is: 0.04526752392004087\n",
      "epoch: 39, iter: 0, loss is: 0.1580723366617323\n",
      "epoch: 39, iter: 1, loss is: 0.14571400729693737\n",
      "epoch: 39, iter: 2, loss is: 0.17498048140834155\n",
      "epoch: 39, iter: 3, loss is: 0.2763658900654409\n",
      "epoch: 39, iter: 4, loss is: 0.006406300118150932\n",
      "epoch: 40, iter: 0, loss is: 0.3388353086483078\n",
      "epoch: 40, iter: 1, loss is: 0.1668850038699051\n",
      "epoch: 40, iter: 2, loss is: 0.11068371369405156\n",
      "epoch: 40, iter: 3, loss is: 0.10623389621362964\n",
      "epoch: 40, iter: 4, loss is: 0.011871510037137328\n",
      "epoch: 41, iter: 0, loss is: 0.12015321467518776\n",
      "epoch: 41, iter: 1, loss is: 0.15181577066776816\n",
      "epoch: 41, iter: 2, loss is: 0.2830693070221521\n",
      "epoch: 41, iter: 3, loss is: 0.13483625034753197\n",
      "epoch: 41, iter: 4, loss is: 0.04693870492805126\n",
      "epoch: 42, iter: 0, loss is: 0.08274021255589253\n",
      "epoch: 42, iter: 1, loss is: 0.11440258251585073\n",
      "epoch: 42, iter: 2, loss is: 0.14513451424494433\n",
      "epoch: 42, iter: 3, loss is: 0.3099451411741719\n",
      "epoch: 42, iter: 4, loss is: 0.00207287983538288\n",
      "epoch: 43, iter: 0, loss is: 0.06745970427924532\n",
      "epoch: 43, iter: 1, loss is: 0.09014052348225647\n",
      "epoch: 43, iter: 2, loss is: 0.2545413150656888\n",
      "epoch: 43, iter: 3, loss is: 0.18796371042785118\n",
      "epoch: 43, iter: 4, loss is: 0.45750357847795414\n",
      "epoch: 44, iter: 0, loss is: 0.1368268214238167\n",
      "epoch: 44, iter: 1, loss is: 0.17509458825177723\n",
      "epoch: 44, iter: 2, loss is: 0.17665051783043711\n",
      "epoch: 44, iter: 3, loss is: 0.09255637165288039\n",
      "epoch: 44, iter: 4, loss is: 0.009134608586030922\n",
      "epoch: 45, iter: 0, loss is: 0.08918180606772605\n",
      "epoch: 45, iter: 1, loss is: 0.1682396044453383\n",
      "epoch: 45, iter: 2, loss is: 0.12820013722787352\n",
      "epoch: 45, iter: 3, loss is: 0.17167886681136374\n",
      "epoch: 45, iter: 4, loss is: 0.1259775678707983\n",
      "epoch: 46, iter: 0, loss is: 0.1473443530928806\n",
      "epoch: 46, iter: 1, loss is: 0.15719926909865223\n",
      "epoch: 46, iter: 2, loss is: 0.13088738821350293\n",
      "epoch: 46, iter: 3, loss is: 0.10064092677137662\n",
      "epoch: 46, iter: 4, loss is: 0.060094742546523604\n",
      "epoch: 47, iter: 0, loss is: 0.1566581466000867\n",
      "epoch: 47, iter: 1, loss is: 0.17441507451281776\n",
      "epoch: 47, iter: 2, loss is: 0.09221044825322641\n",
      "epoch: 47, iter: 3, loss is: 0.09897226785509904\n",
      "epoch: 47, iter: 4, loss is: 0.005866703369736149\n",
      "epoch: 48, iter: 0, loss is: 0.2037008456149044\n",
      "epoch: 48, iter: 1, loss is: 0.1773912343831797\n",
      "epoch: 48, iter: 2, loss is: 0.04051620259883246\n",
      "epoch: 48, iter: 3, loss is: 0.08159786507775496\n",
      "epoch: 48, iter: 4, loss is: 0.010439739309715809\n",
      "epoch: 49, iter: 0, loss is: 0.19638149368878538\n",
      "epoch: 49, iter: 1, loss is: 0.05781402468501879\n",
      "epoch: 49, iter: 2, loss is: 0.08466271103861174\n",
      "epoch: 49, iter: 3, loss is: 0.1474266514384609\n",
      "epoch: 49, iter: 4, loss is: 0.09969336291982636\n",
      "epoch: 50, iter: 0, loss is: 0.060486327329133324\n",
      "epoch: 50, iter: 1, loss is: 0.23698952415254337\n",
      "epoch: 50, iter: 2, loss is: 0.12967781217053828\n",
      "epoch: 50, iter: 3, loss is: 0.04775120046085802\n",
      "epoch: 50, iter: 4, loss is: 0.09588404844889102\n",
      "epoch: 51, iter: 0, loss is: 0.09060524505495388\n",
      "epoch: 51, iter: 1, loss is: 0.060237533594273626\n",
      "epoch: 51, iter: 2, loss is: 0.23758009819412862\n",
      "epoch: 51, iter: 3, loss is: 0.07971917591086349\n",
      "epoch: 51, iter: 4, loss is: 0.0028999013867438005\n",
      "epoch: 52, iter: 0, loss is: 0.10302279700364786\n",
      "epoch: 52, iter: 1, loss is: 0.10133822242483775\n",
      "epoch: 52, iter: 2, loss is: 0.0779632264077257\n",
      "epoch: 52, iter: 3, loss is: 0.17404688258925763\n",
      "epoch: 52, iter: 4, loss is: 0.014254031870047618\n",
      "epoch: 53, iter: 0, loss is: 0.12006757121034665\n",
      "epoch: 53, iter: 1, loss is: 0.14720148018350757\n",
      "epoch: 53, iter: 2, loss is: 0.047788001806301404\n",
      "epoch: 53, iter: 3, loss is: 0.1281233814430799\n",
      "epoch: 53, iter: 4, loss is: 0.04841668488249721\n",
      "epoch: 54, iter: 0, loss is: 0.08667017029905143\n",
      "epoch: 54, iter: 1, loss is: 0.21319714508468857\n",
      "epoch: 54, iter: 2, loss is: 0.07497283048429881\n",
      "epoch: 54, iter: 3, loss is: 0.05911853645541678\n",
      "epoch: 54, iter: 4, loss is: 0.024453086211053975\n",
      "epoch: 55, iter: 0, loss is: 0.08614699547291009\n",
      "epoch: 55, iter: 1, loss is: 0.13226082865727187\n",
      "epoch: 55, iter: 2, loss is: 0.07636028890036842\n",
      "epoch: 55, iter: 3, loss is: 0.1246704758183898\n",
      "epoch: 55, iter: 4, loss is: 0.1639846439245828\n",
      "epoch: 56, iter: 0, loss is: 0.046987871963051916\n",
      "epoch: 56, iter: 1, loss is: 0.04762209799920916\n",
      "epoch: 56, iter: 2, loss is: 0.08189135598406154\n",
      "epoch: 56, iter: 3, loss is: 0.24113904789207186\n",
      "epoch: 56, iter: 4, loss is: 0.007870852593622548\n",
      "epoch: 57, iter: 0, loss is: 0.08539641173277036\n",
      "epoch: 57, iter: 1, loss is: 0.06955663581139021\n",
      "epoch: 57, iter: 2, loss is: 0.08975152399710697\n",
      "epoch: 57, iter: 3, loss is: 0.16410374263283103\n",
      "epoch: 57, iter: 4, loss is: 0.025974215745503692\n",
      "epoch: 58, iter: 0, loss is: 0.04684836403094928\n",
      "epoch: 58, iter: 1, loss is: 0.13533894280426625\n",
      "epoch: 58, iter: 2, loss is: 0.10106256063823495\n",
      "epoch: 58, iter: 3, loss is: 0.11596134301194772\n",
      "epoch: 58, iter: 4, loss is: 0.07462107975497245\n",
      "epoch: 59, iter: 0, loss is: 0.0911924955310293\n",
      "epoch: 59, iter: 1, loss is: 0.03292254800118665\n",
      "epoch: 59, iter: 2, loss is: 0.14906577999688947\n",
      "epoch: 59, iter: 3, loss is: 0.12063304795402566\n",
      "epoch: 59, iter: 4, loss is: 0.05023555850209043\n",
      "epoch: 60, iter: 0, loss is: 0.10642459476341479\n",
      "epoch: 60, iter: 1, loss is: 0.10364671769601949\n",
      "epoch: 60, iter: 2, loss is: 0.09232360804006212\n",
      "epoch: 60, iter: 3, loss is: 0.08423805953947268\n",
      "epoch: 60, iter: 4, loss is: 0.034860220505653225\n",
      "epoch: 61, iter: 0, loss is: 0.11972031686793352\n",
      "epoch: 61, iter: 1, loss is: 0.09301613232611074\n",
      "epoch: 61, iter: 2, loss is: 0.07141735830055557\n",
      "epoch: 61, iter: 3, loss is: 0.09224973365481068\n",
      "epoch: 61, iter: 4, loss is: 0.07945182114087626\n",
      "epoch: 62, iter: 0, loss is: 0.12452451743501801\n",
      "epoch: 62, iter: 1, loss is: 0.04926983699312855\n",
      "epoch: 62, iter: 2, loss is: 0.1250003440671651\n",
      "epoch: 62, iter: 3, loss is: 0.06859724108992292\n",
      "epoch: 62, iter: 4, loss is: 0.08248312270451177\n",
      "epoch: 63, iter: 0, loss is: 0.04376890641103397\n",
      "epoch: 63, iter: 1, loss is: 0.11212019564054\n",
      "epoch: 63, iter: 2, loss is: 0.09976004647405702\n",
      "epoch: 63, iter: 3, loss is: 0.10236914065983102\n",
      "epoch: 63, iter: 4, loss is: 0.04783400056476792\n",
      "epoch: 64, iter: 0, loss is: 0.02818633131162131\n",
      "epoch: 64, iter: 1, loss is: 0.18596237155583503\n",
      "epoch: 64, iter: 2, loss is: 0.07011658383340563\n",
      "epoch: 64, iter: 3, loss is: 0.038941523196471525\n",
      "epoch: 64, iter: 4, loss is: 0.6824877304082808\n",
      "epoch: 65, iter: 0, loss is: 0.06664456441741595\n",
      "epoch: 65, iter: 1, loss is: 0.094126045387078\n",
      "epoch: 65, iter: 2, loss is: 0.05561967421603559\n",
      "epoch: 65, iter: 3, loss is: 0.12377183376952042\n",
      "epoch: 65, iter: 4, loss is: 0.04234996489170361\n",
      "epoch: 66, iter: 0, loss is: 0.05478742517974475\n",
      "epoch: 66, iter: 1, loss is: 0.06836827949548244\n",
      "epoch: 66, iter: 2, loss is: 0.11387562923766\n",
      "epoch: 66, iter: 3, loss is: 0.08273555391767726\n",
      "epoch: 66, iter: 4, loss is: 0.3569751626720683\n",
      "epoch: 67, iter: 0, loss is: 0.11808333123205257\n",
      "epoch: 67, iter: 1, loss is: 0.04262574348360304\n",
      "epoch: 67, iter: 2, loss is: 0.05548879237750382\n",
      "epoch: 67, iter: 3, loss is: 0.1051987002617291\n",
      "epoch: 67, iter: 4, loss is: 0.1331683220130262\n",
      "epoch: 68, iter: 0, loss is: 0.03654345107037673\n",
      "epoch: 68, iter: 1, loss is: 0.10594109851829912\n",
      "epoch: 68, iter: 2, loss is: 0.08400640086740285\n",
      "epoch: 68, iter: 3, loss is: 0.08961196730872265\n",
      "epoch: 68, iter: 4, loss is: 0.058608957292305655\n",
      "epoch: 69, iter: 0, loss is: 0.05091924248019382\n",
      "epoch: 69, iter: 1, loss is: 0.05583373311402325\n",
      "epoch: 69, iter: 2, loss is: 0.07578096786067799\n",
      "epoch: 69, iter: 3, loss is: 0.12706997037358186\n",
      "epoch: 69, iter: 4, loss is: 0.044725457943580925\n",
      "epoch: 70, iter: 0, loss is: 0.08062227242342751\n",
      "epoch: 70, iter: 1, loss is: 0.08178335631231366\n",
      "epoch: 70, iter: 2, loss is: 0.04115467553583484\n",
      "epoch: 70, iter: 3, loss is: 0.09902137096152173\n",
      "epoch: 70, iter: 4, loss is: 0.013352719544640101\n",
      "epoch: 71, iter: 0, loss is: 0.12633715747597593\n",
      "epoch: 71, iter: 1, loss is: 0.046164216050184975\n",
      "epoch: 71, iter: 2, loss is: 0.07515159421109123\n",
      "epoch: 71, iter: 3, loss is: 0.04869555353654889\n",
      "epoch: 71, iter: 4, loss is: 0.01033433581535063\n",
      "epoch: 72, iter: 0, loss is: 0.06079631071614277\n",
      "epoch: 72, iter: 1, loss is: 0.11030581703219967\n",
      "epoch: 72, iter: 2, loss is: 0.03473408951995703\n",
      "epoch: 72, iter: 3, loss is: 0.08494497941035066\n",
      "epoch: 72, iter: 4, loss is: 0.01622880254290143\n",
      "epoch: 73, iter: 0, loss is: 0.061997007138954625\n",
      "epoch: 73, iter: 1, loss is: 0.0834806786796683\n",
      "epoch: 73, iter: 2, loss is: 0.05327253489981456\n",
      "epoch: 73, iter: 3, loss is: 0.08732081464364308\n",
      "epoch: 73, iter: 4, loss is: 0.00815284087998706\n",
      "epoch: 74, iter: 0, loss is: 0.07672382754943145\n",
      "epoch: 74, iter: 1, loss is: 0.05426243277115565\n",
      "epoch: 74, iter: 2, loss is: 0.0742521357899368\n",
      "epoch: 74, iter: 3, loss is: 0.0758788782120381\n",
      "epoch: 74, iter: 4, loss is: 0.015144675424214618\n",
      "epoch: 75, iter: 0, loss is: 0.09008758274292548\n",
      "epoch: 75, iter: 1, loss is: 0.030135197084181817\n",
      "epoch: 75, iter: 2, loss is: 0.051458340437085665\n",
      "epoch: 75, iter: 3, loss is: 0.07302901243902268\n",
      "epoch: 75, iter: 4, loss is: 0.8098356797207441\n",
      "epoch: 76, iter: 0, loss is: 0.0450493889886351\n",
      "epoch: 76, iter: 1, loss is: 0.08027930109719934\n",
      "epoch: 76, iter: 2, loss is: 0.09150286334310216\n",
      "epoch: 76, iter: 3, loss is: 0.05295549664280893\n",
      "epoch: 76, iter: 4, loss is: 0.10361773865956934\n",
      "epoch: 77, iter: 0, loss is: 0.09285411305115886\n",
      "epoch: 77, iter: 1, loss is: 0.04776021635176324\n",
      "epoch: 77, iter: 2, loss is: 0.051643043406610835\n",
      "epoch: 77, iter: 3, loss is: 0.07731245684249517\n",
      "epoch: 77, iter: 4, loss is: 0.006984878692782817\n",
      "epoch: 78, iter: 0, loss is: 0.06333538344949603\n",
      "epoch: 78, iter: 1, loss is: 0.036849964290762385\n",
      "epoch: 78, iter: 2, loss is: 0.06784396806582943\n",
      "epoch: 78, iter: 3, loss is: 0.09249202512814035\n",
      "epoch: 78, iter: 4, loss is: 0.13404411369565766\n",
      "epoch: 79, iter: 0, loss is: 0.04083807604782288\n",
      "epoch: 79, iter: 1, loss is: 0.11666122696938568\n",
      "epoch: 79, iter: 2, loss is: 0.05078732811985768\n",
      "epoch: 79, iter: 3, loss is: 0.04377466989887943\n",
      "epoch: 79, iter: 4, loss is: 0.2272653197944164\n",
      "epoch: 80, iter: 0, loss is: 0.081368491181741\n",
      "epoch: 80, iter: 1, loss is: 0.04972537206906969\n",
      "epoch: 80, iter: 2, loss is: 0.07704627151166808\n",
      "epoch: 80, iter: 3, loss is: 0.04294767069824808\n",
      "epoch: 80, iter: 4, loss is: 0.13249970032018377\n",
      "epoch: 81, iter: 0, loss is: 0.10606382963567673\n",
      "epoch: 81, iter: 1, loss is: 0.05614363930693815\n",
      "epoch: 81, iter: 2, loss is: 0.04505262052467412\n",
      "epoch: 81, iter: 3, loss is: 0.045133135882685284\n",
      "epoch: 81, iter: 4, loss is: 0.019005265442478575\n",
      "epoch: 82, iter: 0, loss is: 0.060426886020789675\n",
      "epoch: 82, iter: 1, loss is: 0.05010962368389312\n",
      "epoch: 82, iter: 2, loss is: 0.07389391061696586\n",
      "epoch: 82, iter: 3, loss is: 0.06453985956248004\n",
      "epoch: 82, iter: 4, loss is: 0.05619523595985731\n",
      "epoch: 83, iter: 0, loss is: 0.05395110102924019\n",
      "epoch: 83, iter: 1, loss is: 0.039888409461709906\n",
      "epoch: 83, iter: 2, loss is: 0.07880816617764638\n",
      "epoch: 83, iter: 3, loss is: 0.07301110683903414\n",
      "epoch: 83, iter: 4, loss is: 0.0832750014179788\n",
      "epoch: 84, iter: 0, loss is: 0.04135087299161345\n",
      "epoch: 84, iter: 1, loss is: 0.04586533189189486\n",
      "epoch: 84, iter: 2, loss is: 0.10893348395436264\n",
      "epoch: 84, iter: 3, loss is: 0.049466539671273374\n",
      "epoch: 84, iter: 4, loss is: 0.02888819842616143\n",
      "epoch: 85, iter: 0, loss is: 0.09643852784484402\n",
      "epoch: 85, iter: 1, loss is: 0.038845207129977195\n",
      "epoch: 85, iter: 2, loss is: 0.052439142872301704\n",
      "epoch: 85, iter: 3, loss is: 0.05478592690519119\n",
      "epoch: 85, iter: 4, loss is: 0.011870957506279787\n",
      "epoch: 86, iter: 0, loss is: 0.03640009348790734\n",
      "epoch: 86, iter: 1, loss is: 0.09972384605835362\n",
      "epoch: 86, iter: 2, loss is: 0.04610623807068488\n",
      "epoch: 86, iter: 3, loss is: 0.05766523373722273\n",
      "epoch: 86, iter: 4, loss is: 0.012658228637766515\n",
      "epoch: 87, iter: 0, loss is: 0.04220441620126258\n",
      "epoch: 87, iter: 1, loss is: 0.04116700839648857\n",
      "epoch: 87, iter: 2, loss is: 0.0805886079183244\n",
      "epoch: 87, iter: 3, loss is: 0.07317207428122433\n",
      "epoch: 87, iter: 4, loss is: 0.02539195017009032\n",
      "epoch: 88, iter: 0, loss is: 0.10152011508903945\n",
      "epoch: 88, iter: 1, loss is: 0.04699220413218279\n",
      "epoch: 88, iter: 2, loss is: 0.04360088455181813\n",
      "epoch: 88, iter: 3, loss is: 0.04136755231731022\n",
      "epoch: 88, iter: 4, loss is: 0.051222216855215774\n",
      "epoch: 89, iter: 0, loss is: 0.04210026088046693\n",
      "epoch: 89, iter: 1, loss is: 0.05235529644395525\n",
      "epoch: 89, iter: 2, loss is: 0.09072389874317378\n",
      "epoch: 89, iter: 3, loss is: 0.04253493414303037\n",
      "epoch: 89, iter: 4, loss is: 0.13799833361310945\n",
      "epoch: 90, iter: 0, loss is: 0.05604184820127713\n",
      "epoch: 90, iter: 1, loss is: 0.078527450854431\n",
      "epoch: 90, iter: 2, loss is: 0.04442197259354442\n",
      "epoch: 90, iter: 3, loss is: 0.050628470522088145\n",
      "epoch: 90, iter: 4, loss is: 0.03092294343229383\n",
      "epoch: 91, iter: 0, loss is: 0.05284196625838921\n",
      "epoch: 91, iter: 1, loss is: 0.034918704646408574\n",
      "epoch: 91, iter: 2, loss is: 0.045533648572262796\n",
      "epoch: 91, iter: 3, loss is: 0.09600350753231446\n",
      "epoch: 91, iter: 4, loss is: 0.012374562747377587\n",
      "epoch: 92, iter: 0, loss is: 0.07026865673032032\n",
      "epoch: 92, iter: 1, loss is: 0.05402558978522719\n",
      "epoch: 92, iter: 2, loss is: 0.04119135851916744\n",
      "epoch: 92, iter: 3, loss is: 0.059629274103123124\n",
      "epoch: 92, iter: 4, loss is: 0.04110019944744909\n",
      "epoch: 93, iter: 0, loss is: 0.04568322545791994\n",
      "epoch: 93, iter: 1, loss is: 0.05391599252227927\n",
      "epoch: 93, iter: 2, loss is: 0.05018049049404864\n",
      "epoch: 93, iter: 3, loss is: 0.07460606133928734\n",
      "epoch: 93, iter: 4, loss is: 0.037690947613100205\n",
      "epoch: 94, iter: 0, loss is: 0.0390543106776523\n",
      "epoch: 94, iter: 1, loss is: 0.04731814775822213\n",
      "epoch: 94, iter: 2, loss is: 0.042155085450958134\n",
      "epoch: 94, iter: 3, loss is: 0.09232021277469445\n",
      "epoch: 94, iter: 4, loss is: 0.0909733440453746\n",
      "epoch: 95, iter: 0, loss is: 0.06182073895274854\n",
      "epoch: 95, iter: 1, loss is: 0.04124015778335873\n",
      "epoch: 95, iter: 2, loss is: 0.07365387699457654\n",
      "epoch: 95, iter: 3, loss is: 0.04441068551248161\n",
      "epoch: 95, iter: 4, loss is: 0.030664954229468883\n",
      "epoch: 96, iter: 0, loss is: 0.08112225323325259\n",
      "epoch: 96, iter: 1, loss is: 0.0501485416505595\n",
      "epoch: 96, iter: 2, loss is: 0.042965283397706944\n",
      "epoch: 96, iter: 3, loss is: 0.04266907720313801\n",
      "epoch: 96, iter: 4, loss is: 0.08738579794790281\n",
      "epoch: 97, iter: 0, loss is: 0.042356676700525654\n",
      "epoch: 97, iter: 1, loss is: 0.08482719671768253\n",
      "epoch: 97, iter: 2, loss is: 0.04332192441373324\n",
      "epoch: 97, iter: 3, loss is: 0.047849027371309125\n",
      "epoch: 97, iter: 4, loss is: 0.00661316213559876\n",
      "epoch: 98, iter: 0, loss is: 0.05171702416676264\n",
      "epoch: 98, iter: 1, loss is: 0.08868789931150016\n",
      "epoch: 98, iter: 2, loss is: 0.037561286470279454\n",
      "epoch: 98, iter: 3, loss is: 0.03727133001429419\n",
      "epoch: 98, iter: 4, loss is: 0.055072869693411915\n",
      "epoch: 99, iter: 0, loss is: 0.06334332372484501\n",
      "epoch: 99, iter: 1, loss is: 0.03569833692165964\n",
      "epoch: 99, iter: 2, loss is: 0.05280968003651467\n",
      "epoch: 99, iter: 3, loss is: 0.06192264745649955\n",
      "epoch: 99, iter: 4, loss is: 0.05300519567925763\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = load_data()\r\n",
    "\r\n",
    "# 创建网络\r\n",
    "net = Network(13)\r\n",
    "# 启动训练\r\n",
    "losses = net.train(train_data, num_epochs=100, batch_size=100, eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画出损失函数的变化趋势\r\n",
    "plot_x = np.arange(len(losses))\r\n",
    "plot_y = np.array(losses)\r\n",
    "plt.plot(plot_x, plot_y)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference result is [[23.47615216]], the corresponding label is 19.5\n"
     ]
    }
   ],
   "source": [
    "# 从上边已加载的测试集中，随机选择一条作为测试数据\r\n",
    "idx = np.random.randint(0, test_data.shape[0]) \r\n",
    "one_data, label = test_data[idx, :-1], test_data[idx, -1]\r\n",
    "# 修改该条数据shape为[1,13]\r\n",
    "one_data =  one_data.reshape([1,-1])\r\n",
    "\r\n",
    "_, _, predict=net.forward(one_data)\r\n",
    "\r\n",
    "# 对结果做反归一化处理\r\n",
    "predict = predict * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "# 对label数据做反归一化处理\r\n",
    "label = label * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "\r\n",
    "print(\"Inference result is {}, the corresponding label is {}\".format(predict, label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
