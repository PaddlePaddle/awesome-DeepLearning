## 深度学习发展历史

#### 1943年

神经科学家麦卡洛克（W.S.McCilloch）和数学家皮兹（W.Pitts）在《数学生物物理学公告》上发表论文《神经活动中内在思想的逻辑演算》，建立了神经网络和数学模型，成为MCP模型。所谓MCP模型，其实就是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，也就诞生了所谓的“模拟大脑”，人工神经网络的大门由此开启。

MCP当时是希望能够用计算机来模拟人的神经元反应的过程，改模型将神经元简化为了三个过程：输入信号线性加权、求和、非线性激活（阈值法）。

#### 1958年

计算机科学家罗森布拉特（Rosenblatt）提出了两层神经元组成的神经网络，称之为“感知器”。第一次将MCP用于机器学习分类。感知器算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年，该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。

#### 1969年

1969年，美国数学家及人工智能先驱Marvin Minsky在其著作中证明了感知器本质上是一种线性模型，只能处理线性分类问题，就连最简单的XOR问题都无法正确分类。直接宣判了感知器的死刑，神经网络的研究也陷入了近20年的停滞。

#### 1986年

由神经网络之父Geoffrey Hinton在1986年发明了适用于多层感知器（MLP）的BP算法，并采用了Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。

Sigmoid 函数是一个在生物学中常见的S型的函数，也称为S型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间。
$$
S(x)=\left(\frac{1}{1+e^{-x}}\right)
$$

#### 90年代时期

1991年BP算法被指出存在梯度消失问题，也就是说在误差梯度后项传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该问题直接阻碍了深度学习的进一步发展。

此外90年代中期，支持向量机算法诞生（SVM算法）等各种浅层机器学习模型被提出，SVM也是一种有监督的学习模型，应用于模式识别，分类以及回归分析等。支持向量机以统计学为基础，和神经网络有明显的差异，支持向量机等算法的提出再次阻碍了深度学习的发展。

#### 发展期2006年-2012年

2006年，加拿大多伦多大学教授、机器学习领域泰斗、神经网络之父—— Geoffrey Hinton 和他的学生 Ruslan Salakhutdinov 在顶尖学术刊物《科学》上发表了一篇文章，该文章提出了深层网络训练中梯度消失问题的解决方案：**无监督预训练对权值进行初始化+有监督训练微调**。斯坦福大学、纽约大学、加拿大蒙特利尔大学等成为研究深度学习的重镇，至此开启了深度学习在学术界和工业界的浪潮。

2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。2011年以来，微软首次将DL应用在语音识别上，取得了重大突破。微软研究院和Google的语音识别研究人员先后采用DNN技术降低语音识别错误率20％~30％，是语音识别领域十多年来最大的突破性进展。2012年，DNN技术在图像识别领域取得惊人的效果，在ImageNet评测上将错误率从26％降低到15％。在这一年，DNN还被应用于制药公司的DrugeActivity预测问题，并获得世界最好成绩。

#### 爆发期2012年至今

2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。

**AlexNet的创新点在于:**

(1)首次采用ReLU激活函数，极大增大收敛速度且从根本上解决了梯度消失问题。

(2)由于ReLU方法可以很好抑制梯度消失问题，AlexNet抛弃了“预训练+微调”的方法，完全采用有监督训练。也正因为如此，DL的主流学习方法也因此变为了纯粹的有监督学习。

(3)扩展了LeNet5结构，添加Dropout层减小过拟合，LRN层增强泛化能力/减小过拟合。

(4)第一次使用GPU加速模型计算。



## 人工智能、机器学习、深度学习的区别和联系

![relationship](images\relationship.png)

##### 人工智能：为机器赋予人的智能

早在1956年夏天那次会议，人工智能的先驱们就梦想着用当时刚刚出现的计算机来构造复杂的、拥有与人类智慧同样本质特性的机器。这就是我们现在所说的“强人工智能”（General AI）。这个无所不能的机器，它有着我们所有的感知（甚至比人更多），我们所有的理性，可以像我们一样思考。

我们目前能实现的，一般被称为“弱人工智能”（Narrow AI）。弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。例如，图像分类技术和人脸识别技术。

##### 机器学习：一种实现人工智能的方法

机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。

机器学习直接来源于早期的人工智能领域。传统算法包括决策树学习、推导逻辑规划、聚类、分类、回归、强化学习和贝叶斯网络等等。众所周知，我们还没有实现强人工智能。早期机器学习方法甚至都无法实现弱人工智能。

机器学习最成功的应用领域是计算机视觉，虽然也还是需要大量的手工编码来完成工作。人们需要手工编写分类器、边缘检测滤波器，以便让程序能识别物体从哪里开始，到哪里结束；写形状检测程序来判断检测对象是不是有八条边；写分类器来识别字母“STOP”。使用以上这些手工编写的分类器，人们总算可以开发算法来感知图像，判断图像是不是一个停止标志牌。

##### 深度学习：一种实现机器学习的技术

人工神经网络是早期机器学习中的一个重要的算法，历经数十年风风雨雨。神经网络的原理是受我们大脑的生理结构——互相交叉相连的神经元启发。但与大脑中一个神经元可以连接一定距离内的任意神经元不同，人工神经网络具有离散的层、连接和数据传播的方向。每一个神经元都为它的输入分配权重，这个权重的正确与否与其执行的任务直接相关。最终的输出由这些权重加总来决定。

神经网络是调制、训练出来的，时不时还是很容易出错的。它最需要的，就是训练。需要成百上千的数据来进行训练，直到神经元的输入权值都被调制得十分精确，每次都能得到正确的结果。



### 神经元、单层感知机和多层感知机

##### 神经元

一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。突触之间的交流通过神经递质实现。

![neuron](images\neuron.png)

当 把树突到细胞核的阶段简化为线性加权的过程，再把突触之间的信号传递简化为对求和结果的非线性变换，那么神经元的模型就如上图所示。

##### 单层感知机

神经元的基本模型其实就是单层感知机的基本模型，单层感知机的概念最早出现于1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。在网络中，输出层和输出层直接相连，简化后的模型和神经元模型一致。

设单层感知机的输入为x，权值向量为W，激活函数为f，则单层感知机的输出为：
$$
y=f(W*x+w_0)
$$
其中，常用的激活函数有Sigmoid函数、tanh函数和Relu函数。

##### 多层感知机

多层感知机的模型如下图：

![](images\perceptrons.png)

相对于单层感知器，输出端从一个变到了多个；输入端和输出端之间也不光只有一层，现在有两层：输出层和隐藏层。多层感知机的隐藏层计算同样使用了激活函数和权值。相比单层感知机，多层感知机可以解决异或问题。并且随着隐藏层层数的增加，神经元的数量也会增加，也越能够表达更加复杂的模型。



### 前向传播算法

前向传播通过对一层的结点以及对应的连接权值进行加权和运算，结果加上一个偏置项，然后通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，得到的结果就是下一层结点的输出。从第一层（输入层）开始不断的通过这种方法一层层的运算，最后得到输出层结果。

![](images\forward.png)

如上图所示，假设输出A=3，B=4，在忽略激活函数和偏置项的情况下，C的结果为3x1+4x2=11，同理得D的结果为3x2+4x1=10，最后可以求得输出E的值为11x3 - 1x10=23

在前向传播过程中，无论有多少隐藏层，都可以用如下公式表示：
$$
x^2=f(x^1*W^2+b^2)
$$
其中上标代表层数，星号代表卷积，b代表偏置项bias，f代表激活函数。



### 反向传播算法

反向传播算法（BackPropagation）是多层神经网络的训练中举足轻重的算法。反向传播算法本质上是梯度下降法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。即误差的反向传播。

**反向传播的主要思想如下：**

（1）将训练集数据输入到ANN的输入层，经过隐藏层，最后达到输出层并输出结果，这是ANN的前向传播过程；

（2）由于ANN的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；

（3）在反向传播的过程中，根据误差调整各种参数的值；不断迭代上述过程，直至收敛。

![](images\backward.png)

上图是一个三层的人工神经网络，在此定义一些变量：

w<sub>jk</sub><sup>l</sup>表示第（l-1）层的第k个神经元连接到第l层的第j个神经元的权重；

b<sub>j</sub><sup>l</sup>表示第l层的第j个神经元的偏置；

z<sub>j</sub><sup>l</sup>表示第l层的第j个神经元的输入，即：
$$
z^l_j=\sum_{k}{w^l_{jk}a^{l-1}_k+b^l_j}
$$
a<sub>j</sub><sup>l</sup>表示第l层的第j个神经元的输出，即：
$$
a^l_j=f(\sum_{k}{w^l_{jk}a^{l-1}_k+b^l_j})
$$
其中，f表示激活函数。

然后计算代价函数，代价函数被用来计算ANN输出值与实际值之间的误差。常用的代价函数是二次代价函数：
$$
C=\frac{1}{2n}\sum_{k}{||y(x)-a^L(x)||^2}
$$
其中，x表示输入的样本，y表示实际的分类，a<sup>L</sup>表示预测的输出，L表示神经网络的最大层数。

##### 反向传播算法过程

输入训练集

对于训练集中的每个样本x，设置输入层对应的激活值a<sup>1</sup>:

##### 			前向传播：

$$
Z^l=W^la^{l-1}+b^l,a^l=\sigma(z^l)
$$

##### 			计算输出层产生的错误：

$$
\delta^L=\nabla_aC\bigodot\sigma^\prime(z^L)
$$

##### 			反向传播错误：

$$
\delta^l=((w^{l+1})^T\delta^{l+1})\bigodot\sigma^\prime(z^l)
$$

##### 			使用梯度下降法，训练参数：

$$
w^l\rightarrow w^l-\frac{\eta}{m}\sum_{x}\delta^{x,l}(a^{x,l-1})^T
$$

$$
b^l\rightarrow b^l-\frac{\eta}{m}\sum_{x}{\delta^{x,l}}
$$

