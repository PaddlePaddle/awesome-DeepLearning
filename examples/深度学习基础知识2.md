```python
# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原
# View dataset directory. 
# This directory will be recovered automatically after resetting environment. 
!ls /home/aistudio/data
```

1.补充目前awesome-DeepLearning repo缺少的损失函数

1.Dice Loss
用来度量集合相似度的度量函数，通常用于计算两个样本之间的像素相似度，公式如下：
![](https://ai-studio-static-online.cdn.bcebos.com/83912eea711346139835f6ec6fe948bbb26dc0a15c3d4f75ae75923d40ef3eed)


Dice是公式后面部分,是两个样本A和B的相似度度量。分子是矩阵A和B逐个元素相乘（点乘），再求和，即相交。分母是矩阵分别求和（矩阵内所有元素加起来），再相加。对于二分类问题，Target分割图是只有 ,  两个值，因此 可以有效忽视背景像素，只关注要检测的目标部分，预测结果Prediction和真实标签Target越相似，Dice 系数越高，Dice Loss越小。
Dice Loss适用于目标样本极度不均衡的情况，但目标很小时，使用Dice Loss会因为分割不好导致Loss很大，对反向传播有不利的影响，使得训练不稳定。

![](https://ai-studio-static-online.cdn.bcebos.com/103dd435993c43f4ac5cbba6f5d7040dd8ce4f43fdc14b3b8bdfe2a24f3c5e72)


2.损失函数python代码实现


```python
#Dice 
import torch.nn as nn
import torch.nn.functional as F

class SoftDiceLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(SoftDiceLoss, self).__init__()
    def forward(self, logits, targets):
        num = targets.size(0)
        smooth = 1
        probs = F.sigmoid(logits)
        m1 = probs.view(num, -1)
        m2 = targets.view(num, -1)
        intersection = (m1 * m2)
        score = 2. * (intersection.sum(1) + smooth) / (m1.sum(1) + m2.sum(1) + smooth)
        score = 1 - score.sum() / num
        return score
```


```python
# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.
# View personal work directory. 
# All changes under this directory will be kept even after reset. 
# Please clean unnecessary files in time to speed up environment loading. 
!ls /home/aistudio/work
```


```python
#Focal
import torch
import torch.nn as nn

#二分类
class FocalLoss(nn.Module):

    def __init__(self, gamma=2,alpha=0.25):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha=alpha
    def forward(self, input, target):
        # input:size is M*2. M　is the batch　number
        # target:size is M.
        pt=torch.softmax(input,dim=1)
        p=pt[:,1]
        loss = -self.alpha*(1-p)**self.gamma*(target*torch.log(p))-\
               (1-self.alpha)*p**self.gamma*((1-target)*torch.log(1-p))
        return loss.mean()
```

3.池化方法补充

空间金字塔池化(SPP)
SPP 显著特点
1) 不管输入尺寸是怎样，SPP 可以产生固定大小的输出
2) 使用多个窗口(pooling window)
3) SPP 可以使用同一图像不同尺寸(scale)作为输入, 得到同样长度的池化特征。

其它特点
1) 由于对输入图像的不同纵横比和不同尺寸，SPP同样可以处理，所以提高了图像的尺度不变(scale-invariance)和降低了过拟合(over-fitting)
2) 实验表明训练图像尺寸的多样性比单一尺寸的训练图像更容易使得网络收敛(convergence)
3) SPP 对于特定的CNN网络设计和结构是独立的。(也就是说，只要把SPP放在最后一层卷积层后面，对网络的结构是没有影响的， 它只是替换了原来的pooling层)
4) 不仅可以用于图像分类而且可以用来目标检测

感兴趣区域池化（Region of interest pooling）
重叠池化
随机池化

4.数据增强方法修改及补充

移位
移位只涉及沿X或Y方向（或两者）移动图像。
![](https://ai-studio-static-online.cdn.bcebos.com/d95b3b70972a4a689cb481584726d7e57fc22b739f344f12813e3214131f6135)


使用条件GAN将图像从一个域转换为图像到另一个域
![](https://ai-studio-static-online.cdn.bcebos.com/2042dd8cde034ba8b52869de0d3351a488b25673939b4044953b24d29c0e90c8)


5.图像分类方法综述

图像分类算法包括传统的算法和基于深度学习的图像分类算法

1). 底层特征提取: 通常从图像中按照固定步长、尺度提取大量局部特征描述。常用的局部特征包括SIFT(Scale-Invariant Feature Transform, 尺度不变特征转换) 、HOG(Histogram of Oriented Gradient, 方向梯度直方图) 、LBP(Local Bianray Pattern, 局部二值模式)等，一般也采用多种特征描述，防止丢失过多的有用信息。

　　2). 特征编码: 底层特征中包含了大量冗余与噪声，为了提高特征表达的鲁棒性，需要使用一种特征变换算法对底层特征进行编码，称作特征编码。常用的特征编码方法包括向量量化编码、稀疏编码、局部线性约束编码、Fisher向量编码等。

　　3). 空间特征约束: 特征编码之后一般会经过空间特征约束，也称作特征汇聚。特征汇聚是指在一个空间范围内，对每一维特征取最大值或者平均值，可以获得一定特征不变形的特征表达。金字塔特征匹配是一种常用的特征汇聚方法，这种方法提出将图像均匀分块，在分块内做特征汇聚。

　　4). 通过分类器分类: 经过前面步骤之后一张图像可以用一个固定维度的向量进行描述，接下来就是经过分类器对图像进行分类。通常使用的分类器包括SVM(Support Vector Machine, 支持向量机)、随机森林等。而使用核方法的SVM是最为广泛的分类器，在传统图像分类任务上性能很好。
  

深度学习算法
1.CNN
2.VGG
3.GoogLeNet
4.ResNet

请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>
Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. 
