# 损失函数
## L1范数损失 L1Loss
计算 output 和 target 之差的绝对值。

torch.nn.L1Loss(reduction='mean')

## 均方误差损失 MSELoss
计算 output 和 target 之差的均方差。

torch.nn.MSELoss(reduction='mean')

## 交叉熵损失 CrossEntropyLoss
当训练有 C 个类别的分类问题时很有效。可选参数 weight 必须是一个1维Tensor，权重将被分配给各个类别。对于不平衡的训练集非常有效。
在多分类任务中，经常采用 softmax 激活函数+交叉熵损失函数，因为交叉熵描述了两个概率分布的差异，然而神经网络输出的是向量，并不是概率分布的形式。所以需要 softmax 激活函数将一个向量进行“归一化”成概率分布的形式，再采用交叉熵损失函数计算 loss。
![Image text](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9nS2F4akl4NmJhZ0pGVVlDSndFemhKbU43S0M2WWtPbGo5Sk10T3NIUjlEQ0hjWnpTNVl1cmljOVBpYm1XNjJ6TkxFcDBlOWxybWFvZ1JHWThtN0FpYTZVQS82NDA?x-oss-process=image/format,png
)

torch.nn.CrossEntropyLoss(weight=None,ignore_index=-100, reduction='mean')

## KL 散度损失 KLDivLoss
计算 input 和 target 之间的 KL 散度。KL 散度可用于衡量不同的连续分布之间的距离，在连续的输出分布的空间上(离散采样)上进行直接回归时很有效。

torch.nn.KLDivLoss(reduction='mean')

## 二进制交叉熵损失 BCELoss
二分类任务时的交叉熵计算函数。用于测量重构的误差，例如自动编码机。注意目标的值 t[i] 的范围为0到1之间。

torch.nn.BCELoss(weight=None, reduction='mean')

## BCEWithLogitsLoss
BCEWithLogitsLoss损失函数把 Sigmoid 层集成到了 BCELoss 类中。该版比用一个简单的 Sigmoid 层和 BCELoss 在数值上更稳定, 因为把这两个操作合并为一个层之后, 可以利用 log-sum-exp 的技巧来实现数值稳定。

torch.nn.BCEWithLogitsLoss(weight=None, reduction='mean', pos_weight=None)

# 池化方法
## 一般池化（General Pooling）
池化作用于图像中不重合的区域（这与卷积操作不同），过程如下图。
我们定义池化窗口的大小为sizeX，即下图中红色正方形的边长，定义两个相邻池化窗口的水平位移/竖直位移为stride。一般池化由于每一池化窗口都是不重复的，所以sizeX=stride。
![Image text](https://img-blog.csdn.net/20150105213214237?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZGFuaWVsamlhbmZlbmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

最常见的池化操作为平均池化mean pooling和最大池化max pooling：

平均池化：计算图像区域的平均值作为该区域池化后的值。

最大池化：选图像区域的最大值作为该区域池化后的值。

# 数据增强
1.Flip
对图像进行水平翻转或者垂直翻转。

2.Rotation
将图像绕着某个点旋转一定的角度。

3.Resize
对图像进行缩放，以保证模型具有尺度不变性。

4.Croping
对图像进行随机裁剪，只截取部分图像进行训练（语义分割中经常用到）。

5.Noise
在图像中加入噪音。

# 图像分类
## KNN
"Nearest Neighbor"是处理图像分类问题一个较为简单、直接、粗暴的方法：首先在系统中“记住”所有已经标注好类别的图像（其实这就是训练集），当遇到一个要判断的还未标注的图像（也就是测试集中的某个图像）时，就去比较这个图像与“记住的”图像的“相似性”，找到那个最相似的已经标注好的图像，用那个图像的类别作为正在分类的图像的类别，这也就是"Nearest Neighbor"名称的含义。

## Linear Classification
Linear Classification利用一个“全连接层”(Fully Connected Layer)，输入图像的所有像素值，经过全连接层的运算后输出每个类别的“得分”，最终选取得分最高的类别作为图像的类别。

## Convolutional Neural Network(CNN，卷积神经网络)
CNN网络主要特点是使用卷积层，这其实是模拟了人的视觉神经，单个神经元只能对某种特定的图像特征产生响应，比如横向或者纵向的边缘，本身是非常简单的，但是这些简单的神经元构成一层，在层数足够多后，就可以获取足够丰富的特征。从机制上讲，卷积神经网络与人的视觉神经十分相似。
