# <center>作业二——深度学习基础知识</center>

**<p align="right">张森源</p>** 

**<p align="right">18040500018</p>**

---

### **目录**
> * 损失函数方法补充及代码实现
> * 池化方法补充
> * 数据增强方法修改及补充
> * 图像分类方法综述

---

损失函数方法补充
--------


### 0-1损失函数(zero-one loss)

0-1损失是指预测值和目标值不相等为1， 否则为0:

特点：



(1)0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用.

(2)感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足 |Y-f(x)|<t时认为相等，

```python
def zero_one_loss(Y, f):
    if Y == f:
        return 1
    else:
        return 0
```

### 绝对值损失函数

返回差的绝对值
```python
def abs_loss(Y, f):
    return abs(Y-f)
```



### Hinge 损失函数



在机器学习中，Hinge loss作为损失函数，通常被用于最大间隔算法(maximum-margin)，而最大间隔算法又是SVM(支持向量机support vector machines)用到的重要算法。



Hinge loss专用于二分类问题，标签值y=±1，预测值$\hat{y}$∈R。该二分类问题的目标函数的要求如下：当$\hat{y}$等于+1或者小于等于-1时，都是分类器确定的分类结果，此时的损失函数loss为0；而当$\hat{y}$预测值∈(−1,1)时，分类器对分类结果不确定，loss不为0。显然，当y^=0时，loss达到最大值。





```python

def update_weights_Hinge(m1, m2, b, X1, X2, Y, learning_rate):

    m1_deriv = 0

    m2_deriv = 0

    b_deriv = 0

    N = len(X1)

    for i in range(N):

        # 计算偏导数

        if Y[i]*(m1*X1[i] + m2*X2[i] + b) <= 1:

            m1_deriv += -X1[i] * Y[i]

            m2_deriv += -X2[i] * Y[i]

            b_deriv += -Y[i]

        # 否则偏导数为0

    # 我们减去它，因为导数指向最陡的上升方向

    m1 -= (m1_deriv / float(N)) * learning_rate

    m2 -= (m2_deriv / float(N)) * learning_rate

    b -= (b_deriv / float(N)) * learning_rate

return m1, m2, b

```

<br\>
<br\>



##池化方法补充

### 重叠池化
重叠池化正如其名字所说的，相邻池化窗口之间会有重叠区域，此时sizeX>stride。

###均值池化
一般池化的基础上，计算每个池化窗口对应图像区域的平均值，作为该区域池化后的值。

###全局池化
Global Pooling就是池化窗口的大小 = 整张特征图的大小。这样，每个 W×H×C 的特征图输入就会被转化为 1×1×C 的输出，也等同于每个位置权重都为 1/(W×H) 的全连接层操作。

### 随机池化
Stochastic-pooling（随机池化）：只需对feature map中的元素按照其概率值大小随机选择，即元素值大的被选中的概率也大。而不像max-pooling那样，永远只取那个最大值元素。

<br\>
<br\>


## 数据增强方法补充



### 色彩抖动



在实际工程中为了消除图像在不同背景中存在的差异性，通常会做一些色彩抖动操作，扩充数据集合。色彩抖动主要是在图像的颜色方面做增强，主要调整的是图像的亮度，饱和度和对比度。工程中不是任何数据集都适用，通常如果不同背景的图像较多，加入色彩抖动操作会有很好的提升。



### 几何变换类



几何变换类即对图像进行几何变换，包括翻转，旋转，裁剪，变形，缩放等各类操作。



### 颜色变换



包括噪声、模糊、颜色变换、擦除、填充等等。



### GAN



通过生成对抗网络生成同类型的数据。比如生成汽车、人脸图片。通过图像风格迁移的手段，还可以生成同一物体再不同环境下的图片。

### Autoaugmentation[5]

AutoAugment是Google提出的自动选择最优数据增强方案的研究，这是无监督数据增强的重要研究方向。它的基本思路是使用增强学习从数据本身寻找最佳图像变换策略，对于不同的任务学习不同的增强方法，流程如下：

(1) 准备16个常用的数据增强操作。

(2) 从16个中选择5个操作，随机产生使用该操作的概率和相应的幅度，将其称为一个sub-policy，一共产生5个sub-polices。

(3) 对训练过程中每一个batch的图片，随机采用5个sub-polices操作中的一种。

(4) 通过模型在验证集上的泛化能力来反馈，使用的优化方法是增强学习方法。

(5) 经过80~100个epoch后网络开始学习到有效的sub-policies。

(6) 之后串接这5个sub-policies，然后再进行最后的训练。

总的来说，就是学习已有数据增强的组合策略，对于门牌数字识别等任务，研究表明剪切和平移等几何变换能够获得最佳效果。
<br\>
<br\>
## 图像分类方法综述



### 传统方法

#### KNN

K-最近邻（K-NN）是一种非参数的惰性学习算法，用于分类和分类

回归。该算法简单地依赖于特征向量和分类器之间的距离

通过在k-最近的例子中找到最常见的类来获得未知的数据点。

#### 决策树分类
决策树(Decision Tree）是在已知各种情况发生概率的基础上，通过构成决策树来求取净现值的期望值大于等于零的概率，评价项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法。由于这种决策分支画成图形很像一棵树的枝干，故称决策树。在机器学习中，决策树是一个预测模型，他代表的是对象属性与对象值之间的一种映射关系。Entropy = 系统的凌乱程度，使用算法ID3, C4.5和C5.0生成树算法使用熵。这一度量是基于信息学理论中熵的概念。
决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。
分类树（决策树）是一种十分常用的分类方法。它是一种监督学习，所谓监督学习就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。这样的机器学习就被称之为监督学习。
####贝叶斯分类
贝叶斯分类算法是统计学的一种分类方法，它是一类利用概率统计知识进行分类的算法。在许多场合，朴素贝叶斯(Naïve Bayes，NB)分类算法可以与决策树和神经网络分类算法相媲美，该算法能运用到大型数据库中，而且方法简单、分类准确率高、速度快。
由于贝叶斯定理假设一个属性值对给定类的影响独立于其它属性的值，而此假设在实际情况中经常是不成立的，因此其分类准确率可能会下降。为此，就衍生出许多降低独立性假设的贝叶斯分类算法，如TAN(tree augmented Bayes network)算法。

#### SVM支持向量机

支持向量机（SVM）是一种强大而灵活的有监督机器学习算法是多维空间中超平面上不同类的表示。目标是分裂

将数据集分成类，寻找最大边缘超平面。它建立了一个超平面或一组

高维空间中的超平面和两类之间的良好分离是通过

到任何类中最近的训练数据点距离最大的超平面。真正的力量

该算法的性能取决于所使用的核函数。
<br/>

### 深度学习方法


####  VGG
牛津大学VGG(Visual Geometry Group)组在2014年ILSVRC提出的模型被称作VGG模型。该模型相比以往模型进一步加宽和加深了网络结构，它的核心是五组卷积操作，每两组之间做Max-Pooling空间降维。同一组内采用多次连续的3X3卷积，卷积核的数目由较浅组的64增多到最深组的512，同一组内的卷积核数目是一样的。卷积之后接两层全连接层，之后是分类层。

由于每组内卷积层的不同，有11、13、16、19层这几种模型，下图展示一个16层的网络结构。VGG模型结构相对简洁，提出之后也有很多文章基于此模型进行研究，如在ImageNet上首次公开超过人眼识别的模型就是借鉴VGG模型的结构。
####  DenseNet
####  Inception
####  ResNet
####  LetNet



