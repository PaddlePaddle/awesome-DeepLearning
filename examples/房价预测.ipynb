{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.fluid as fluid\r\n",
    "import paddle\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deal_data():\r\n",
    "    # 读取文件数据，此时数据形状是(870,2)\r\n",
    "    \r\n",
    "    df =  pd.read_csv(\"./房价预测/data/data.txt\",sep=',')  # 用pandas读取数据，分隔符为逗号\r\n",
    "    housingdata = np.array(df)  # 将DF数据类型转化为numpy数据类型\r\n",
    "\r\n",
    "    # 规范数据格式。\r\n",
    "    housingdata = np.array(housingdata).reshape((-1, 2))  # 此时数据形状为(870,2)\r\n",
    "\r\n",
    "    # 归一化操作\r\n",
    "    for i in range(1):\r\n",
    "        Max = np.max(housingdata[:, i])\r\n",
    "        Min = np.min(housingdata[:, i])\r\n",
    "        housingdata[:, i] = (housingdata[:, i] - Min) / (Max - Min)\r\n",
    "\r\n",
    "    # 依据2-8原则，80%的数据作为训练数据，20%数据作为测试数据；\r\n",
    "    Splitdata = round(len(housingdata) * 0.8)\r\n",
    "    train_data = housingdata[:Splitdata]  # 训练数据集\r\n",
    "    test_data = housingdata[Splitdata:]  # 测试数据集\r\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / iter   0, loss = 342581.5894\n",
      "Epoch   0 / iter   1, loss = 455283.3876\n",
      "Epoch   0 / iter   2, loss = 327978.1744\n",
      "Epoch   0 / iter   3, loss = 44382721.9571\n",
      "Epoch   0 / iter   4, loss = 1219024898605514.5000\n",
      "Epoch   0 / iter   5, loss = 22507045196532430485370179694249377792.0000\n",
      "Epoch   0 / iter   6, loss = 140994458106770890917894732279488226582959336242648569025836835123034324341470367978615104148702234673152.0000\n",
      "Epoch   1 / iter   0, loss = 34405111187769251034132548186360561734344792723919748688512217728782643995379343380256294367299243298223085555503570002642779105299283624496857818690300679449028993895910170976369405191587198882290692102713787347598854092459626542811876004073845131291077604136232337181439167576479711049887533068737052672.0000\n",
      "Epoch   1 / iter   1, loss = inf\n",
      "Epoch   1 / iter   2, loss = nan\n",
      "Epoch   1 / iter   3, loss = nan\n",
      "Epoch   1 / iter   4, loss = nan\n",
      "Epoch   1 / iter   5, loss = nan\n",
      "Epoch   1 / iter   6, loss = nan\n",
      "Epoch   2 / iter   0, loss = nan\n",
      "Epoch   2 / iter   1, loss = nan\n",
      "Epoch   2 / iter   2, loss = nan\n",
      "Epoch   2 / iter   3, loss = nan\n",
      "Epoch   2 / iter   4, loss = nan\n",
      "Epoch   2 / iter   5, loss = nan\n",
      "Epoch   2 / iter   6, loss = nan\n",
      "Epoch   3 / iter   0, loss = nan\n",
      "Epoch   3 / iter   1, loss = nan\n",
      "Epoch   3 / iter   2, loss = nan\n",
      "Epoch   3 / iter   3, loss = nan\n",
      "Epoch   3 / iter   4, loss = nan\n",
      "Epoch   3 / iter   5, loss = nan\n",
      "Epoch   3 / iter   6, loss = nan\n",
      "Epoch   4 / iter   0, loss = nan\n",
      "Epoch   4 / iter   1, loss = nan\n",
      "Epoch   4 / iter   2, loss = nan\n",
      "Epoch   4 / iter   3, loss = nan\n",
      "Epoch   4 / iter   4, loss = nan\n",
      "Epoch   4 / iter   5, loss = nan\n",
      "Epoch   4 / iter   6, loss = nan\n",
      "Epoch   5 / iter   0, loss = nan\n",
      "Epoch   5 / iter   1, loss = nan\n",
      "Epoch   5 / iter   2, loss = nan\n",
      "Epoch   5 / iter   3, loss = nan\n",
      "Epoch   5 / iter   4, loss = nan\n",
      "Epoch   5 / iter   5, loss = nan\n",
      "Epoch   5 / iter   6, loss = nan\n",
      "Epoch   6 / iter   0, loss = nan\n",
      "Epoch   6 / iter   1, loss = nan\n",
      "Epoch   6 / iter   2, loss = nan\n",
      "Epoch   6 / iter   3, loss = nan\n",
      "Epoch   6 / iter   4, loss = nan\n",
      "Epoch   6 / iter   5, loss = nan\n",
      "Epoch   6 / iter   6, loss = nan\n",
      "Epoch   7 / iter   0, loss = nan\n",
      "Epoch   7 / iter   1, loss = nan\n",
      "Epoch   7 / iter   2, loss = nan\n",
      "Epoch   7 / iter   3, loss = nan\n",
      "Epoch   7 / iter   4, loss = nan\n",
      "Epoch   7 / iter   5, loss = nan\n",
      "Epoch   7 / iter   6, loss = nan\n",
      "Epoch   8 / iter   0, loss = nan\n",
      "Epoch   8 / iter   1, loss = nan\n",
      "Epoch   8 / iter   2, loss = nan\n",
      "Epoch   8 / iter   3, loss = nan\n",
      "Epoch   8 / iter   4, loss = nan\n",
      "Epoch   8 / iter   5, loss = nan\n",
      "Epoch   8 / iter   6, loss = nan\n",
      "Epoch   9 / iter   0, loss = nan\n",
      "Epoch   9 / iter   1, loss = nan\n",
      "Epoch   9 / iter   2, loss = nan\n",
      "Epoch   9 / iter   3, loss = nan\n",
      "Epoch   9 / iter   4, loss = nan\n",
      "Epoch   9 / iter   5, loss = nan\n",
      "Epoch   9 / iter   6, loss = nan\n",
      "Epoch  10 / iter   0, loss = nan\n",
      "Epoch  10 / iter   1, loss = nan\n",
      "Epoch  10 / iter   2, loss = nan\n",
      "Epoch  10 / iter   3, loss = nan\n",
      "Epoch  10 / iter   4, loss = nan\n",
      "Epoch  10 / iter   5, loss = nan\n",
      "Epoch  10 / iter   6, loss = nan\n",
      "Epoch  11 / iter   0, loss = nan\n",
      "Epoch  11 / iter   1, loss = nan\n",
      "Epoch  11 / iter   2, loss = nan\n",
      "Epoch  11 / iter   3, loss = nan\n",
      "Epoch  11 / iter   4, loss = nan\n",
      "Epoch  11 / iter   5, loss = nan\n",
      "Epoch  11 / iter   6, loss = nan\n",
      "Epoch  12 / iter   0, loss = nan\n",
      "Epoch  12 / iter   1, loss = nan\n",
      "Epoch  12 / iter   2, loss = nan\n",
      "Epoch  12 / iter   3, loss = nan\n",
      "Epoch  12 / iter   4, loss = nan\n",
      "Epoch  12 / iter   5, loss = nan\n",
      "Epoch  12 / iter   6, loss = nan\n",
      "Epoch  13 / iter   0, loss = nan\n",
      "Epoch  13 / iter   1, loss = nan\n",
      "Epoch  13 / iter   2, loss = nan\n",
      "Epoch  13 / iter   3, loss = nan\n",
      "Epoch  13 / iter   4, loss = nan\n",
      "Epoch  13 / iter   5, loss = nan\n",
      "Epoch  13 / iter   6, loss = nan\n",
      "Epoch  14 / iter   0, loss = nan\n",
      "Epoch  14 / iter   1, loss = nan\n",
      "Epoch  14 / iter   2, loss = nan\n",
      "Epoch  14 / iter   3, loss = nan\n",
      "Epoch  14 / iter   4, loss = nan\n",
      "Epoch  14 / iter   5, loss = nan\n",
      "Epoch  14 / iter   6, loss = nan\n",
      "Epoch  15 / iter   0, loss = nan\n",
      "Epoch  15 / iter   1, loss = nan\n",
      "Epoch  15 / iter   2, loss = nan\n",
      "Epoch  15 / iter   3, loss = nan\n",
      "Epoch  15 / iter   4, loss = nan\n",
      "Epoch  15 / iter   5, loss = nan\n",
      "Epoch  15 / iter   6, loss = nan\n",
      "Epoch  16 / iter   0, loss = nan\n",
      "Epoch  16 / iter   1, loss = nan\n",
      "Epoch  16 / iter   2, loss = nan\n",
      "Epoch  16 / iter   3, loss = nan\n",
      "Epoch  16 / iter   4, loss = nan\n",
      "Epoch  16 / iter   5, loss = nan\n",
      "Epoch  16 / iter   6, loss = nan\n",
      "Epoch  17 / iter   0, loss = nan\n",
      "Epoch  17 / iter   1, loss = nan\n",
      "Epoch  17 / iter   2, loss = nan\n",
      "Epoch  17 / iter   3, loss = nan\n",
      "Epoch  17 / iter   4, loss = nan\n",
      "Epoch  17 / iter   5, loss = nan\n",
      "Epoch  17 / iter   6, loss = nan\n",
      "Epoch  18 / iter   0, loss = nan\n",
      "Epoch  18 / iter   1, loss = nan\n",
      "Epoch  18 / iter   2, loss = nan\n",
      "Epoch  18 / iter   3, loss = nan\n",
      "Epoch  18 / iter   4, loss = nan\n",
      "Epoch  18 / iter   5, loss = nan\n",
      "Epoch  18 / iter   6, loss = nan\n",
      "Epoch  19 / iter   0, loss = nan\n",
      "Epoch  19 / iter   1, loss = nan\n",
      "Epoch  19 / iter   2, loss = nan\n",
      "Epoch  19 / iter   3, loss = nan\n",
      "Epoch  19 / iter   4, loss = nan\n",
      "Epoch  19 / iter   5, loss = nan\n",
      "Epoch  19 / iter   6, loss = nan\n",
      "Epoch  20 / iter   0, loss = nan\n",
      "Epoch  20 / iter   1, loss = nan\n",
      "Epoch  20 / iter   2, loss = nan\n",
      "Epoch  20 / iter   3, loss = nan\n",
      "Epoch  20 / iter   4, loss = nan\n",
      "Epoch  20 / iter   5, loss = nan\n",
      "Epoch  20 / iter   6, loss = nan\n",
      "Epoch  21 / iter   0, loss = nan\n",
      "Epoch  21 / iter   1, loss = nan\n",
      "Epoch  21 / iter   2, loss = nan\n",
      "Epoch  21 / iter   3, loss = nan\n",
      "Epoch  21 / iter   4, loss = nan\n",
      "Epoch  21 / iter   5, loss = nan\n",
      "Epoch  21 / iter   6, loss = nan\n",
      "Epoch  22 / iter   0, loss = nan\n",
      "Epoch  22 / iter   1, loss = nan\n",
      "Epoch  22 / iter   2, loss = nan\n",
      "Epoch  22 / iter   3, loss = nan\n",
      "Epoch  22 / iter   4, loss = nan\n",
      "Epoch  22 / iter   5, loss = nan\n",
      "Epoch  22 / iter   6, loss = nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  23 / iter   0, loss = nan\n",
      "Epoch  23 / iter   1, loss = nan\n",
      "Epoch  23 / iter   2, loss = nan\n",
      "Epoch  23 / iter   3, loss = nan\n",
      "Epoch  23 / iter   4, loss = nan\n",
      "Epoch  23 / iter   5, loss = nan\n",
      "Epoch  23 / iter   6, loss = nan\n",
      "Epoch  24 / iter   0, loss = nan\n",
      "Epoch  24 / iter   1, loss = nan\n",
      "Epoch  24 / iter   2, loss = nan\n",
      "Epoch  24 / iter   3, loss = nan\n",
      "Epoch  24 / iter   4, loss = nan\n",
      "Epoch  24 / iter   5, loss = nan\n",
      "Epoch  24 / iter   6, loss = nan\n",
      "Epoch  25 / iter   0, loss = nan\n",
      "Epoch  25 / iter   1, loss = nan\n",
      "Epoch  25 / iter   2, loss = nan\n",
      "Epoch  25 / iter   3, loss = nan\n",
      "Epoch  25 / iter   4, loss = nan\n",
      "Epoch  25 / iter   5, loss = nan\n",
      "Epoch  25 / iter   6, loss = nan\n",
      "Epoch  26 / iter   0, loss = nan\n",
      "Epoch  26 / iter   1, loss = nan\n",
      "Epoch  26 / iter   2, loss = nan\n",
      "Epoch  26 / iter   3, loss = nan\n",
      "Epoch  26 / iter   4, loss = nan\n",
      "Epoch  26 / iter   5, loss = nan\n",
      "Epoch  26 / iter   6, loss = nan\n",
      "Epoch  27 / iter   0, loss = nan\n",
      "Epoch  27 / iter   1, loss = nan\n",
      "Epoch  27 / iter   2, loss = nan\n",
      "Epoch  27 / iter   3, loss = nan\n",
      "Epoch  27 / iter   4, loss = nan\n",
      "Epoch  27 / iter   5, loss = nan\n",
      "Epoch  27 / iter   6, loss = nan\n",
      "Epoch  28 / iter   0, loss = nan\n",
      "Epoch  28 / iter   1, loss = nan\n",
      "Epoch  28 / iter   2, loss = nan\n",
      "Epoch  28 / iter   3, loss = nan\n",
      "Epoch  28 / iter   4, loss = nan\n",
      "Epoch  28 / iter   5, loss = nan\n",
      "Epoch  28 / iter   6, loss = nan\n",
      "Epoch  29 / iter   0, loss = nan\n",
      "Epoch  29 / iter   1, loss = nan\n",
      "Epoch  29 / iter   2, loss = nan\n",
      "Epoch  29 / iter   3, loss = nan\n",
      "Epoch  29 / iter   4, loss = nan\n",
      "Epoch  29 / iter   5, loss = nan\n",
      "Epoch  29 / iter   6, loss = nan\n",
      "Epoch  30 / iter   0, loss = nan\n",
      "Epoch  30 / iter   1, loss = nan\n",
      "Epoch  30 / iter   2, loss = nan\n",
      "Epoch  30 / iter   3, loss = nan\n",
      "Epoch  30 / iter   4, loss = nan\n",
      "Epoch  30 / iter   5, loss = nan\n",
      "Epoch  30 / iter   6, loss = nan\n",
      "Epoch  31 / iter   0, loss = nan\n",
      "Epoch  31 / iter   1, loss = nan\n",
      "Epoch  31 / iter   2, loss = nan\n",
      "Epoch  31 / iter   3, loss = nan\n",
      "Epoch  31 / iter   4, loss = nan\n",
      "Epoch  31 / iter   5, loss = nan\n",
      "Epoch  31 / iter   6, loss = nan\n",
      "Epoch  32 / iter   0, loss = nan\n",
      "Epoch  32 / iter   1, loss = nan\n",
      "Epoch  32 / iter   2, loss = nan\n",
      "Epoch  32 / iter   3, loss = nan\n",
      "Epoch  32 / iter   4, loss = nan\n",
      "Epoch  32 / iter   5, loss = nan\n",
      "Epoch  32 / iter   6, loss = nan\n",
      "Epoch  33 / iter   0, loss = nan\n",
      "Epoch  33 / iter   1, loss = nan\n",
      "Epoch  33 / iter   2, loss = nan\n",
      "Epoch  33 / iter   3, loss = nan\n",
      "Epoch  33 / iter   4, loss = nan\n",
      "Epoch  33 / iter   5, loss = nan\n",
      "Epoch  33 / iter   6, loss = nan\n",
      "Epoch  34 / iter   0, loss = nan\n",
      "Epoch  34 / iter   1, loss = nan\n",
      "Epoch  34 / iter   2, loss = nan\n",
      "Epoch  34 / iter   3, loss = nan\n",
      "Epoch  34 / iter   4, loss = nan\n",
      "Epoch  34 / iter   5, loss = nan\n",
      "Epoch  34 / iter   6, loss = nan\n",
      "Epoch  35 / iter   0, loss = nan\n",
      "Epoch  35 / iter   1, loss = nan\n",
      "Epoch  35 / iter   2, loss = nan\n",
      "Epoch  35 / iter   3, loss = nan\n",
      "Epoch  35 / iter   4, loss = nan\n",
      "Epoch  35 / iter   5, loss = nan\n",
      "Epoch  35 / iter   6, loss = nan\n",
      "Epoch  36 / iter   0, loss = nan\n",
      "Epoch  36 / iter   1, loss = nan\n",
      "Epoch  36 / iter   2, loss = nan\n",
      "Epoch  36 / iter   3, loss = nan\n",
      "Epoch  36 / iter   4, loss = nan\n",
      "Epoch  36 / iter   5, loss = nan\n",
      "Epoch  36 / iter   6, loss = nan\n",
      "Epoch  37 / iter   0, loss = nan\n",
      "Epoch  37 / iter   1, loss = nan\n",
      "Epoch  37 / iter   2, loss = nan\n",
      "Epoch  37 / iter   3, loss = nan\n",
      "Epoch  37 / iter   4, loss = nan\n",
      "Epoch  37 / iter   5, loss = nan\n",
      "Epoch  37 / iter   6, loss = nan\n",
      "Epoch  38 / iter   0, loss = nan\n",
      "Epoch  38 / iter   1, loss = nan\n",
      "Epoch  38 / iter   2, loss = nan\n",
      "Epoch  38 / iter   3, loss = nan\n",
      "Epoch  38 / iter   4, loss = nan\n",
      "Epoch  38 / iter   5, loss = nan\n",
      "Epoch  38 / iter   6, loss = nan\n",
      "Epoch  39 / iter   0, loss = nan\n",
      "Epoch  39 / iter   1, loss = nan\n",
      "Epoch  39 / iter   2, loss = nan\n",
      "Epoch  39 / iter   3, loss = nan\n",
      "Epoch  39 / iter   4, loss = nan\n",
      "Epoch  39 / iter   5, loss = nan\n",
      "Epoch  39 / iter   6, loss = nan\n",
      "Epoch  40 / iter   0, loss = nan\n",
      "Epoch  40 / iter   1, loss = nan\n",
      "Epoch  40 / iter   2, loss = nan\n",
      "Epoch  40 / iter   3, loss = nan\n",
      "Epoch  40 / iter   4, loss = nan\n",
      "Epoch  40 / iter   5, loss = nan\n",
      "Epoch  40 / iter   6, loss = nan\n",
      "Epoch  41 / iter   0, loss = nan\n",
      "Epoch  41 / iter   1, loss = nan\n",
      "Epoch  41 / iter   2, loss = nan\n",
      "Epoch  41 / iter   3, loss = nan\n",
      "Epoch  41 / iter   4, loss = nan\n",
      "Epoch  41 / iter   5, loss = nan\n",
      "Epoch  41 / iter   6, loss = nan\n",
      "Epoch  42 / iter   0, loss = nan\n",
      "Epoch  42 / iter   1, loss = nan\n",
      "Epoch  42 / iter   2, loss = nan\n",
      "Epoch  42 / iter   3, loss = nan\n",
      "Epoch  42 / iter   4, loss = nan\n",
      "Epoch  42 / iter   5, loss = nan\n",
      "Epoch  42 / iter   6, loss = nan\n",
      "Epoch  43 / iter   0, loss = nan\n",
      "Epoch  43 / iter   1, loss = nan\n",
      "Epoch  43 / iter   2, loss = nan\n",
      "Epoch  43 / iter   3, loss = nan\n",
      "Epoch  43 / iter   4, loss = nan\n",
      "Epoch  43 / iter   5, loss = nan\n",
      "Epoch  43 / iter   6, loss = nan\n",
      "Epoch  44 / iter   0, loss = nan\n",
      "Epoch  44 / iter   1, loss = nan\n",
      "Epoch  44 / iter   2, loss = nan\n",
      "Epoch  44 / iter   3, loss = nan\n",
      "Epoch  44 / iter   4, loss = nan\n",
      "Epoch  44 / iter   5, loss = nan\n",
      "Epoch  44 / iter   6, loss = nan\n",
      "Epoch  45 / iter   0, loss = nan\n",
      "Epoch  45 / iter   1, loss = nan\n",
      "Epoch  45 / iter   2, loss = nan\n",
      "Epoch  45 / iter   3, loss = nan\n",
      "Epoch  45 / iter   4, loss = nan\n",
      "Epoch  45 / iter   5, loss = nan\n",
      "Epoch  45 / iter   6, loss = nan\n",
      "Epoch  46 / iter   0, loss = nan\n",
      "Epoch  46 / iter   1, loss = nan\n",
      "Epoch  46 / iter   2, loss = nan\n",
      "Epoch  46 / iter   3, loss = nan\n",
      "Epoch  46 / iter   4, loss = nan\n",
      "Epoch  46 / iter   5, loss = nan\n",
      "Epoch  46 / iter   6, loss = nan\n",
      "Epoch  47 / iter   0, loss = nan\n",
      "Epoch  47 / iter   1, loss = nan\n",
      "Epoch  47 / iter   2, loss = nan\n",
      "Epoch  47 / iter   3, loss = nan\n",
      "Epoch  47 / iter   4, loss = nan\n",
      "Epoch  47 / iter   5, loss = nan\n",
      "Epoch  47 / iter   6, loss = nan\n",
      "Epoch  48 / iter   0, loss = nan\n",
      "Epoch  48 / iter   1, loss = nan\n",
      "Epoch  48 / iter   2, loss = nan\n",
      "Epoch  48 / iter   3, loss = nan\n",
      "Epoch  48 / iter   4, loss = nan\n",
      "Epoch  48 / iter   5, loss = nan\n",
      "Epoch  48 / iter   6, loss = nan\n",
      "Epoch  49 / iter   0, loss = nan\n",
      "Epoch  49 / iter   1, loss = nan\n",
      "Epoch  49 / iter   2, loss = nan\n",
      "Epoch  49 / iter   3, loss = nan\n",
      "Epoch  49 / iter   4, loss = nan\n",
      "Epoch  49 / iter   5, loss = nan\n",
      "Epoch  49 / iter   6, loss = nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Network(object):\r\n",
    "    def __init__(self, num_net1,num_net2):\r\n",
    "        # 随机产生w的初始值\r\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\r\n",
    "        np.random.seed(1)\r\n",
    "        self.w0 = np.random.randn(num_net1*num_net2,1).reshape(num_net1,num_net2)\r\n",
    "        self.b0 = np.random.randn(num_net1,1).reshape(1,num_net1)\r\n",
    "        self.w1 = np.random.randn(num_net2, 1)\r\n",
    "        self.b1 = np.random.randn(1,1)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        mid = np.dot(x,self.w0) + self.b0\r\n",
    "        z = np.dot(mid, self.w1) + self.b1\r\n",
    "        return mid,z\r\n",
    "    \r\n",
    "    def loss(self, z, y):\r\n",
    "        error = z - y\r\n",
    "        cost = error * error\r\n",
    "        cost = np.mean(cost)\r\n",
    "        return cost\r\n",
    "    \r\n",
    "    def gradient(self, x, y):\r\n",
    "        mid,z = self.forward(x)\r\n",
    "\r\n",
    "        gradient_w1 = np.mean((z-y) * mid, axis=0)\r\n",
    "        gradient_w1 = gradient_w1[:, np.newaxis]\r\n",
    "        gradient_b1 = np.mean(z-y)\r\n",
    "\r\n",
    "        gradient_w0 = np.zeros(shape=(1,1))\r\n",
    "        for i in range(len(x)):\r\n",
    "            data = x[i, :]\r\n",
    "            data = data[:, np.newaxis]\r\n",
    "            w1 = self.w1.reshape(1, 1)\r\n",
    "            gradient_w01 = (z - y)[i] * np.dot(data, w1)\r\n",
    "            gradient_w0 += gradient_w01\r\n",
    "        gradient_w0 = gradient_w0 / len(x)\r\n",
    "        w2 = self.w1.reshape(1, 1)\r\n",
    "        gradient_b0 = np.mean((z - y) * w2, axis=0)\r\n",
    "\r\n",
    "        return gradient_w1, gradient_b1, gradient_w0, gradient_b0\r\n",
    "\r\n",
    "    \r\n",
    "    def update(self, gradient_w1, gradient_b1, gradient_w0, gradient_b0, eta):\r\n",
    "        self.w1 = self.w1 - eta * gradient_w1\r\n",
    "        self.b1 = self.b1 - eta * gradient_b1\r\n",
    "        self.w0 = self.w0 - eta * gradient_w0\r\n",
    "        self.b0 = self.b0 - eta * gradient_b0\r\n",
    "            \r\n",
    "                \r\n",
    "    def train(self, training_data, num_epochs, batch_size, eta):\r\n",
    "        n = len(training_data)\r\n",
    "        losses = []\r\n",
    "        for epoch_id in range(num_epochs):\r\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\r\n",
    "            # 然后再按每次取batch_size条数据的方式取出\r\n",
    "            np.random.shuffle(training_data)\r\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\r\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\r\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "                #print(self.w.shape)\r\n",
    "                #print(self.b)\r\n",
    "                x = mini_batch[:, :-1]\r\n",
    "                y = mini_batch[:, -1:]\r\n",
    "                _,a = self.forward(x)\r\n",
    "                loss = self.loss(a, y)\r\n",
    "                gradient_w1, gradient_b1, gradient_w0, gradient_b0 = self.gradient(x, y)  # 计算梯度\r\n",
    "                self.update(gradient_w1, gradient_b1, gradient_w0, gradient_b0, eta)  # 更新梯度\r\n",
    "                losses.append(loss)\r\n",
    "                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\r\n",
    "                                 format(epoch_id, iter_id, loss))\r\n",
    "        \r\n",
    "        return losses\r\n",
    "\r\n",
    "# 获取数据\r\n",
    "train_data, test_data = deal_data()\r\n",
    "\r\n",
    "# 创建网络\r\n",
    "net = Network(1,1)\r\n",
    "# 启动训练\r\n",
    "losses = net.train(train_data, num_epochs=50, batch_size=100, eta=0.01)\r\n",
    "\r\n",
    "# 画出损失函数的变化趋势\r\n",
    "plot_x = np.arange(len(losses))\r\n",
    "plot_y = np.array(losses)\r\n",
    "plt.plot(plot_x, plot_y)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载飞桨、Numpy和相关类库\r\n",
    "import paddle\r\n",
    "from paddle.nn import Linear\r\n",
    "import paddle.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import random\r\n",
    "import paddle.fluid as fluid\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    fname = './房价预测/data/data.txt'\r\n",
    "    data = np.loadtxt(fname, delimiter=',',dtype='float32')\r\n",
    "    \r\n",
    "   \r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算训练集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\r\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\r\n",
    "    feature_num = 2\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        #print(maximums[i], minimums[i], avgs[i])\r\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\r\n",
    "    \r\n",
    "    #  # 因为数据为一个特征所以我们需要加一个偏置作为一个恒定输入\r\n",
    "    # a = data[:,0]\r\n",
    "    # b = np.ones(870)/5\r\n",
    "    # c = np.c_[a,b]\r\n",
    "    # d = data[:,1]\r\n",
    "    # data = np.c_[c,d]\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Regressor(paddle.nn.Layer):\r\n",
    "\r\n",
    "    # self代表类的实例自身\r\n",
    "    def __init__(self):\r\n",
    "        # 初始化父类中的一些参数\r\n",
    "        super(Regressor, self).__init__()\r\n",
    "        \r\n",
    "        # 定义一层全连接层，输入维度是13，输出维度是1\r\n",
    "        \r\n",
    "        self.fc1 = Linear(in_features=1, out_features=4)\r\n",
    "        \r\n",
    "        self.fc2 = Linear(in_features=4, out_features=1)\r\n",
    "    \r\n",
    "    # 网络的前向计算\r\n",
    "    def forward(self, inputs):\r\n",
    "        \r\n",
    "        middle2 = self.fc1(inputs)\r\n",
    "        paddle.to_tensor(middle2)\r\n",
    "        m = paddle.nn.ReLU()\r\n",
    "        out = m(middle2)\r\n",
    "        x = self.fc2(out)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 声明定义好的线性回归模型\r\n",
    "model = Regressor()\r\n",
    "# 开启模型训练模式\r\n",
    "model.train()\r\n",
    "# 加载数据\r\n",
    "training_data, test_data = load_data()\r\n",
    "# 定义优化算法，使用随机梯度下降SGD\r\n",
    "# 学习率设置为0.01\r\n",
    "opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss is: [0.00328743]\n",
      "epoch: 0, iter: 20, loss is: [0.00305114]\n",
      "epoch: 0, iter: 40, loss is: [0.00921501]\n",
      "epoch: 0, iter: 60, loss is: [0.00976808]\n",
      "epoch: 1, iter: 0, loss is: [0.00330687]\n",
      "epoch: 1, iter: 20, loss is: [0.0036918]\n",
      "epoch: 1, iter: 40, loss is: [0.01353603]\n",
      "epoch: 1, iter: 60, loss is: [0.00288837]\n",
      "epoch: 2, iter: 0, loss is: [0.01082382]\n",
      "epoch: 2, iter: 20, loss is: [0.00586678]\n",
      "epoch: 2, iter: 40, loss is: [0.00492271]\n",
      "epoch: 2, iter: 60, loss is: [0.01478204]\n",
      "epoch: 3, iter: 0, loss is: [0.00748355]\n",
      "epoch: 3, iter: 20, loss is: [0.00942113]\n",
      "epoch: 3, iter: 40, loss is: [0.00779717]\n",
      "epoch: 3, iter: 60, loss is: [0.00445309]\n",
      "epoch: 4, iter: 0, loss is: [0.00768874]\n",
      "epoch: 4, iter: 20, loss is: [0.00486786]\n",
      "epoch: 4, iter: 40, loss is: [0.02915761]\n",
      "epoch: 4, iter: 60, loss is: [0.02648658]\n",
      "epoch: 5, iter: 0, loss is: [0.00584669]\n",
      "epoch: 5, iter: 20, loss is: [0.00319418]\n",
      "epoch: 5, iter: 40, loss is: [0.00594664]\n",
      "epoch: 5, iter: 60, loss is: [0.00264934]\n",
      "epoch: 6, iter: 0, loss is: [0.00569919]\n",
      "epoch: 6, iter: 20, loss is: [0.0048675]\n",
      "epoch: 6, iter: 40, loss is: [0.00718348]\n",
      "epoch: 6, iter: 60, loss is: [0.00277935]\n",
      "epoch: 7, iter: 0, loss is: [0.00511865]\n",
      "epoch: 7, iter: 20, loss is: [0.00531008]\n",
      "epoch: 7, iter: 40, loss is: [0.0067943]\n",
      "epoch: 7, iter: 60, loss is: [0.00354562]\n",
      "epoch: 8, iter: 0, loss is: [0.00455836]\n",
      "epoch: 8, iter: 20, loss is: [0.00575348]\n",
      "epoch: 8, iter: 40, loss is: [0.00473614]\n",
      "epoch: 8, iter: 60, loss is: [0.00267038]\n",
      "epoch: 9, iter: 0, loss is: [0.00777708]\n",
      "epoch: 9, iter: 20, loss is: [0.00592851]\n",
      "epoch: 9, iter: 40, loss is: [0.01338637]\n",
      "epoch: 9, iter: 60, loss is: [0.011479]\n"
     ]
    }
   ],
   "source": [
    "EPOCH_NUM = 10   # 设置外层循环次数\r\n",
    "BATCH_SIZE = 10  # 设置batch大小\r\n",
    "\r\n",
    "# 定义外层循环\r\n",
    "for epoch_id in range(EPOCH_NUM):\r\n",
    "    # 在每轮迭代开始之前，将训练数据的顺序随机的打乱\r\n",
    "    np.random.shuffle(training_data)\r\n",
    "    # 将训练数据进行拆分，每个batch包含10条数据\r\n",
    "    mini_batches = [training_data[k:k+BATCH_SIZE] for k in range(0, len(training_data), BATCH_SIZE)]\r\n",
    "    # 定义内层循环\r\n",
    "    for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "        x = np.array(mini_batch[:, :-1]) # 获得当前批次训练数据\r\n",
    "        y = np.array(mini_batch[:, -1:]) # 获得当前批次训练标签（真实房价）\r\n",
    "        # 将numpy数据转为飞桨动态图tensor形式\r\n",
    "        house_features = paddle.to_tensor(x)\r\n",
    "        prices = paddle.to_tensor(y)\r\n",
    "        \r\n",
    "        # 前向计算\r\n",
    "        predicts = model(house_features)\r\n",
    "        \r\n",
    "        # 计算损失\r\n",
    "        loss = F.square_error_cost(predicts, label=prices)\r\n",
    "        avg_loss = paddle.mean(loss)\r\n",
    "        if iter_id%20==0:\r\n",
    "            print(\"epoch: {}, iter: {}, loss is: {}\".format(epoch_id, iter_id, avg_loss.numpy()))\r\n",
    "        \r\n",
    "        # 反向传播\r\n",
    "        avg_loss.backward()\r\n",
    "        # 最小化loss,更新参数\r\n",
    "        opt.step()\r\n",
    "        # 清除梯度\r\n",
    "        opt.clear_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存成功，模型参数保存在LR_model.pdparams中\n"
     ]
    }
   ],
   "source": [
    "# 保存模型参数，文件名为LR_model.pdparams\r\n",
    "paddle.save(model.state_dict(), 'LR_model.pdparams')\r\n",
    "print(\"模型保存成功，模型参数保存在LR_model.pdparams中\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
