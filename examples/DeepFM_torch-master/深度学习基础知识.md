# 深度学习基础知识
------
## 一、CNN-DSSM

①概念：CNN-DSSM是针对 DSSM 词袋模型丢失上下文信息的缺点，CLSM（convolutional latent semantic model）应运而生，又叫 CNN-DSSM。


②模型：CNN-DSSM的架构图如下：
<br></br>
<center> <img src="https://note.youdao.com/yws/api/personal/file/B59880C041B746398CF35F77DBBD82E4?method=download&shareKey=0a152e179dd97e221c88566574d18cda" width="600" hegiht="">
</center>  
<br></br>

输入：Query是代表用户输入，document是数据库中的文档。

word-n-gram层：是对输入做了一个获取上下文信息的窗口，图中是word-trigram，取连续的3个单词。  
Letter-trigram：是把上层的三个单词通过3个字母的形式映射到3w维，然后把3个单词连接起来成9w维的空间。  
Convolutional layer：是通过Letter-trigram层乘上卷积矩阵获得，是普通的卷积操作。  
Max-pooling：是把卷积结果经过池化操作。  
Semantic layer：是语义层，是池化层经过全连接得到的。  
获得128维的语义后就可以计算文本之间的相似度了，计算相似度的过程和DNN-DSSM的过程是一样的。可以发现CNN-DSSM和DNN-DSSM基本流程是差不多的，就是用卷积和池化的操作代替了DNN的操作。


③作用：既可以用来预测两个句子的语义相似度，又可以获得某句子的低纬语义向量表达。


④场景：搜索场景下query和Doc的语义相似度、feeds场景下Doc和Doc的语义相似度、机器翻译场景下A句子和B句子的语义相似度等等


⑤优缺点：  
优点：CNN-DSSM 通过卷积层提取了滑动窗口下的上下文信息，又通过池化层提取了全局的上下文信息，上下文信息得到较为有效的保留。  
缺点：对于间隔较远的上下文信息，难以有效保留。举个例子，I grew up in France... I speak fluent French，显然 France 和 French 是具有上下文依赖关系的，但是由于 CNN-DSSM 滑动窗口（卷积核）大小的限制，导致无法捕获该上下文信息。


## 二、LSTM-DSSM

①概念：LSTM-DSSM用来针对 CNN-DSSM 无法捕获较远距离上下文特征的缺点结合LSTM进行改进。


②模型：LSTM-DSSM 其实用的是 LSTM 的一个变种——加入了peephole的 LSTM。如下图所示：
<br></br>
<center> <img src="https://blog-10039692.file.myqcloud.com/1501556197309_9865_1501556198338.png" width="600" hegiht="">
</center>  
<br></br>

看起来有点复杂，我们换一个图，可以看的更清晰：
<br></br>
<center> <img src="https://blog-10039692.file.myqcloud.com/1501556209287_3423_1501556210288.png" width="600" hegiht="">
</center>  
<br></br>

这里三条黑线就是所谓的 peephole，传统的 LSTM 中遗忘门、输入门和输出门只用了 h(t-1) 和 xt 来控制门缝的大小，peephole 的意思是说不但要考虑 h(t-1) 和 xt，也要考虑 Ct-1 和 Ct，其中遗忘门和输入门考虑了 Ct-1，而输出门考虑了 Ct。总体来说需要考虑的信息更丰富了。  
LSTM-DSSM 整体的网络结构：
<br></br>
<center> <img src="https://blog-10039692.file.myqcloud.com/1501556241446_432_1501556242436.png" width="600" hegiht="">
</center>  
<br></br>

③作用：既可以用来预测两个句子的语义相似度，又可以获得某句子的低纬语义向量表达。


④场景：搜索场景下query和Doc的语义相似度、feeds场景下Doc和Doc的语义相似度、机器翻译场景下A句子和B句子的语义相似度等等


⑤优缺点：  
优点：LSTM-DSSM 针对 CNN-DSSM 无法捕获较远距离上下文特征的缺点做了修补。  


## 三、MMoE

①概念：MMOE模型，全称为：Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts（建模任务关系在多任务学习多门混合专家）。


②模型：
<br></br>
<center> <img src="https://ai-studio-static-online.cdn.bcebos.com/c45b2bff88a04536bf7c2cd34953c083f3acee2a9d6e4748955a9ec37e981625" width="600" hegiht="">
  
</center>  
<br></br>

对于不同的任务，模型的权重选择是不同的，所以作者为每个任务都配备一个 Gate 模型。对于不同的任务，特定的 Gate k 的输出表示不同的 Expert 被选择的概率，将多个 Expert 加权求和，得到$f^k\left(x\right)$，并输出给特定的 Tower 模型，用于最终的输出。函数表达式  
$f^k\left(x\right) = \sum_{i=1}^n g_i^k\left(x\right) f_i\left(x\right)$

$g^k\left(x\right) = softmax\left(W_{g^k\left(x\right)}\right)$

其中： $g\left(x\right)$ 表示 gate 门的输出，为多层感知机模型，实现时为简单的线性变换加 softmax 层。

③作用：学习任务之间的关系，学习特定任务功能，自动分配参数捕获共享任务信息或特定任务信息，避免每次添加新参数。


④场景：许多基于DNN的多任务学习存在着对数据分布不平衡、任务相关性等问题，内在的任务差异冲突会损害一些任务的预测。
也有一些论文提出新的建模技术来处理多任务学习中的任务差异，但技术常设计为每个模型增加更多模型参数，导致计算开销变大。


⑤优缺点：  
优点：多任务模型通过学习不同任务的联系和差异，可提高每个任务的学习效率和质量。  
（1）多任务学习的的框架广泛采用shared-bottom的结构，不同任务间共用底部的隐层。这种结构本质上可以减少过拟合的风险，但是效果上可能受到任务差异和数据分布带来的影响。  
（2）也有一些其他结构，比如两个任务的参数不共用，但是通过对不同任务的参数增加L2范数的限制；也有一些对每个任务分别学习一套隐层然后学习所有隐层的组合。和shared-bottom结构相比，这些模型对增加了针对任务的特定参数，在任务差异会影响公共参数的情况下对最终效果有提升。  
缺点：模型增加了参数量所以需要更大的数据量来训练模型，而且模型更复杂并不利于在真实生产环境中实际部署使用。

## 四、ShareBottom

①概念：在多任务学习模型当中，最常见的一种模型就是Shared-bottom模型。首先每个任务共享底部的network，然后再根据任务的个数在上面划分出多个tower network来分别学习不同的目标。


②模型：
<br></br>
<center> <img src="https://pic4.zhimg.com/80/v2-f1fc3d8ba2c6dd4f1f47ecdc70f733b3_720w.jpg" width="200" hegiht="">
</center>  
<br></br>

在多任务学习模型当中，最常见的一种模型就是Shared-bottom模型。首先每个任务共享底部的network，然后再根据任务的个数在上面划分出多个tower network来分别学习不同的目标。转换成公式是  
$y_k = h^k\left(f\left(x\right)\right)$  
其中f表示shared-bottom network， hk表示第k个tower network，yk是最终的输出。


③作用：一个模型里面可以同时学习到多个目标.


④场景：在推荐系统当中，除了想要预估item的点击率的同时，我们可能还想要预估其他目标，例如点赞，评论，时长等等。我们希望在一个模型里面可以同时学习到多个目标。这就是多任务学习。


⑤优缺点：   
优点：降低overfit风险，利用任务之间的关联性使模型学习效果更强。  
缺点：任务之间的相关性将严重影响模型效果。假如任务之间相关性较低，模型的效果相对会较差。


## 五、youtube视频推荐的经典架构

①概念：Deep Neural Networks for YouTube Recommendations，是世界上最大的视频内容平台youtube，为用户所做的视频推荐系统。


②模型：  
召回层 + 排序层的推荐系统架构
<br></br>
<center> <img src="https://img-blog.csdnimg.cn/20210122171931662.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDEyNzMyNw==,size_16,color_FFFFFF,t_70#pic_center" width="500" hegiht="">
</center>  
<br></br>

其推荐过程可以分成二级。第一级是用候选集生成模型（Candidate Generation Model）完成候选视频的快速筛选，在这一步，候选视频集合由百万降低到几百量级，这就相当于经典推荐系统架构中的召回层。第二级是用排序模型（Ranking Model）完成几百个候选视频的精排，这相当于经典推荐系统架构中的排序层。  
无论是候选集生成模型还是排序模型，YouTube 都采用了深度学习的解决方案。

③作用：对海量的视频进行快速、准确的排序.


④场景：作为全球最大的视频分享网站，YouTube 平台中几乎所有的视频都来自 UGC（User Generated Content，用户原创内容），这样的内容产生模式有两个特点：

一是其商业模式不同于 Netflix，以及国内的腾讯视频、爱奇艺这样的流媒体，这些流媒体的大部分内容都是采购或自制的电影、剧集等头部内容，而 YouTube 的内容都是用户上传的自制视频，种类风格繁多，头部效应没那么明显；  
二是由于 YouTube 的视频基数巨大，用户难以发现喜欢的内容。



⑤优缺点：   
优点：1、更新较快，一方面视频更新频繁，另一方面用户行为更新频繁。
2、可以处理非常大的数据量和用户信息。

		
