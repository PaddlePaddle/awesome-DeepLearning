# 可变性卷积详解

### 提出背景

视觉识别的一个关键挑战是如何适应物体尺度、姿态、视点和零件变形的几何变化或模型几何变换。
一般来说，有两种方法。首先是建立具有足够期望变化的训练数据集。这通常是通过增加现有的数据样本来实现的，例如通过仿射变换。鲁棒表示可以从数据中学习，但通常以昂贵的训练和复杂的模型参数为代价。二是利用变换不变的特征和算法。这一类包含了许多众所周知的技术，例如SIFT（scale invariant feature transform）[38]和基于滑动窗口的目标检测范式。

上述方法有两个缺点。首先，假设几何变换是固定的和已知的。利用这些先验知识对数据进行扩充，设计特征和算法。这种假设防止了泛化到具有未知几何变换的新任务，而这些几何变换没有正确建模。第二，对于过于复杂的变换，手工设计不变的特征和算法可能是困难的或不可行的，即使它们是已知的。

简言之，CNN本质上局限于对大型未知变换建模。这种局限性源于CNN模块的固定几何结构：卷积单元在固定位置对输入特征图进行采样；汇集层以固定比率降低空间分辨率；一个ROI（感兴趣区域）池层将一个ROI分割成固定的空间单元等，缺乏处理几何变换的内部机制。这会引起明显的问题。例如，同一CNN层中所有激活单元的感受野大小是相同的。这对于在空间位置上编码语义的高级CNN层是不需要的。由于不同的位置可能对应于具有不同尺度或变形的对象，因此对于具有精细定位的视觉识别（例如，使用完全卷积网络的语义分割）而言，尺度或感受野大小的自适应确定是可取的。另一个例子是，虽然目标检测近年来取得了显著而迅速的进展，但是所有的方法仍然依赖于基于原始边界框的特征提取。这是次优的，特别是对于非刚性对象。

### 可变形卷积

##### DCN v1

可变形卷积顾名思义就是卷积的位置是可变形的，并非在传统的N × N的网格上做卷积，这样的好处就是更准确地提取到我们想要的特征（传统的卷积仅仅只能提取到矩形框的特征），通过一张图我们可以更直观地了解：

![picture1](.\images\picture1.png)

在上面这张图里面，左边传统的卷积显然没有提取到完整绵羊的特征，而右边的可变形卷积则提取到了完整的不规则绵羊的特征。

那可变卷积实际上是怎么做的呢？*其实就是在每一个卷积采样点加上了一个偏移量*，如下图所示：

![picture2](.\images\picture2.png)

(a) 所示的正常卷积规律的采样 9 个点（绿点）。(b)(c)(d) 为可变形卷积，在正常的采样坐标上加上一个位移量（蓝色箭头），其中 (d) 作为 (b) 的特殊情况，展示了可变形卷积可以作为尺度变换，比例变换和旋转变换等特殊情况。

普通的卷积，以3x3卷积为例对于每个输出y(p0)，都要从x上采样9个位置，这9个位置都在中心位置x(p0)向四周扩散，(-1,-1)代表x(p0)的左上角，(1,1)代表x(p0)的右下角。
$$
\mathrm{R} = 	\{(-1,-1),(-1,0),...,(0,1),(1,1)\}
$$
所以传统卷积的输出就是（其中$\mathrm{p}_n$就是网格中的n个点）：
$$
y(\mathrm{p}_0)=\sum_{\mathrm{p}_n\in\mathrm{R}}\mathrm{w}(\mathrm{p}_n) \cdot \mathrm{x}(\mathrm{p}_0+\mathrm{p}_n)
$$
正如上面阐述的可变形卷积，就是在传统的卷积操作上加入了一个偏移量，正是这个偏移量才让卷积变形为不规则的卷积，这里要注意这个偏移量可以是小数，所以下面的式子的特征值需要通过***双线性插值***的方法来计算。：
$$
y(\mathrm{p}_0)=\sum_{\mathrm{p}_n\in\mathrm{R}}\mathrm{w}(\mathrm{p}_n) \cdot \mathrm{x}(\mathrm{p}_0+\mathrm{p}_n+\Delta \mathrm{p}_n)
$$
那这个偏移量如何算呢？我们来看：

![picture3](.\images\picture3.png)

对于输入的一张feature map，假设原来的卷积操作是3×3的，那么为了学习偏移量offset，我们定义另外一个3×3的卷积层（图中上面的那层），输出的维度其实就是原来feature map大小，channel数等于2N（分别表示x,y方向的偏移）。下面的可变形卷积可以看作先基于上面那部分生成的offset做了一个插值操作，然后再执行普通的卷积。

##### DCN v2

DCN v1听起来不错，但其实也有问题：我们的可变形卷积有可能*引入了无用的上下文（区域）来干扰我们的特征提取*，这显然会降低算法的表现。

DCN v2 在DCN v1基础（添加offset）上添加每个采样点的权重

为了解决引入了一些无关区域的问题，在DCN v2中我们不只添加每一个采样点的偏移，还添加了一个权重系数$\Delta m_k$，来区分我们引入的区域是否为我们感兴趣的区域，假如这个采样点的区域我们不感兴趣，则把权重学习为0即可：

$$
y(\mathrm{p}_0)=\sum_{\mathrm{p}_n\in\mathrm{R}}\mathrm{w}(\mathrm{p}_n) \cdot \mathrm{x}(\mathrm{p}_0+\mathrm{p}_n+\Delta \mathrm{p}_n) \cdot \Delta m_k
$$

##### **paddle中的API**

`paddle.vision.ops.deform_conv2d(*x*, *offset*, *weight*, *bias=None*, *stride=1*, *padding=0*, *dilation=1*, *deformable_groups=1*, *groups=1*, *mask=None*, *name=None*);`

**核心参数解析：**

输入：

1. input 形状：$(N,C_{in},H_{in},W_{in})$
2. 卷积核形状：$(C_{out},C_{in},H_f,W_f)$
3. offset形状：$(N,2*H_f,*W_f,H_{out},W_{out})$
4. mask形状：$(N,H_f*W_f,H_{out},W_{out})$

输出：

输出形状：$(N,C_{out},H_{out},W_{out})$

其中：
$$
H_{out}=\frac{(H_{in}+2*paddings[0]-(dilations[0]*(H_f-1)+1))}
{strides[0]}+1 \\
W_{out}=\frac{(W_{in}+2*paddings[1]-(dilations[1]*(W_f-1)+1))}
{strides[1]}+1
$$


### 作用及意义

总结来说，*DCN v1中引入的offset是要寻找有效信息的区域位置，DCN v2中引入权重系数是要给找到的这个位置赋予权重，这两方面保证了有效信息的准确提取*。

使用可变形卷积，可以更加高效的从图片中获取到目标的特征信息，可以起到提升Faster R-CNN和R-FCN在物体检测和分割上的性能。只要增加很少的计算量，就可以得到性能的提升。

### 应用实例

**DCN v1**：

![picture4](.\images\picture4.png)

可以从上图看到，可以看到当绿色点在目标上时，红色点所在区域也集中在目标位置，并且基本能够覆盖不同尺寸的目标，因此经过可变形卷积，我们可以更好地提取出感兴趣物体的完整特征，效果是非常不错的。

**DCN v2**:

![picture5](.\images\picture5.png)

DCN v1听起来不错，但其实也有问题：我们的可变形卷积有可能*引入了无用的上下文（区域）来干扰我们的特征提取*，这显然会降低算法的表现。

通过对比实验结果我们也可以看到DCN v2更能集中在物体的完整有效的区域

### 参考文献

> [1] Dai J ,  Qi H ,  Xiong Y , et al. Deformable Convolutional Networks[J]. IEEE, 2017.
>
> [2] Zhu X ,  Hu H ,  Lin S , et al. Deformable ConvNets V2: More Deformable, Better Results[C]// 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019.
>
> [3] https://blog.csdn.net/cristiano20/article/details/107931844
>
> [4] https://www.zhihu.com/question/303900394/answer/540818451
>
> [5] https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/ops/deform_conv2d_cn.html
