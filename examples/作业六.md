# CNN-DSSM概念

针对 DSSM 词袋模型丢失上下文信息的缺点，CLSM（convolutional latent semantic model）应运而生，又叫 CNN-DSSM。CNN-DSSM 与 DSSM 的区别主要在于输入层和表示层。

# 模型

### 1.输入层

1.1  英文

英文的处理方式，除了上文提到的 letter-trigram，CNN-DSSM 还在输入层增加了word-trigram

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/91d2ab0a45e5416ab6f742daa6a9e8b22a23676b9b7840c8b02013fe28d736a7" width="400" hegiht="" ></center>

如上图所示，word-trigram其实就是一个包含了上下文信息的滑动窗口。

1.2 中文

英文的处理方式（word-trigram letter-trigram）在中文中并不可取，因为英文中虽然用了 word-ngram 把样本空间拉成了百万级，但是经过 letter-trigram 又把向量空间降到可控级别，只有 3*30K（9 万）。而中文如果用 word-trigram，那向量空间就是百万级的了，显然还是字向量（1.5 万维）比较可控。

### 2.表示层

CNN-DSSM 的表示层由一个卷积神经网络组成，如下图所示：

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/6b6dfed259d44fac984b4bc3a573a565a070a7dae4b24fcca971634f1bad281d" width="400" hegiht="" ></center>

（1）卷积层——Convolutional layer

卷积层的作用是提取滑动窗口下的上下文特征。以下图为例，假设输入层是一个 302*90000（302 行，9 万列）的矩阵，代表 302 个字向量（query 的和 Doc 的长度一般小于 300，这里少了就补全，多了就截断），每个字向量有 9 万维。而卷积核是一个 3*90000 的权值矩阵，卷积核以步长为 1 向下移动，得到的 feature map 是一个 300*1 的矩阵，feature map 的计算公式是(输入层维数 302-卷积核大小 3 步长 1)/步长 1=300。而这样的卷积核有 300 个，所以形成了 300 个 300*1 的 feature map 矩阵。

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/d08c4c2e50ab43339044226aaa722bfda9b662415956484fa8f662e8827742f7" width="400" hegiht="" ></center>

（2）池化层——Max pooling layer

池化层的作用是为句子找到全局的上下文特征。池化层以 Max-over-time pooling 的方式，每个 feature map 都取最大值，得到一个 300 维的向量。Max-over-pooling 可以解决可变长度的句子输入问题（因为不管 Feature Map 中有多少个值，只需要提取其中的最大值）。不过在上一步已经做了句子的定长处理（固定句子长度为 302），所以就没有可变长度句子的问题。最终池化层的输出为各个 Feature Map 的最大值，即一个 300*1 的向量。

（3）全连接层——Semantic layer

最后通过全连接层把一个 300 维的向量转化为一个 128 维的低维语义向量。全连接层采用 tanh 函数：

（2）池化层——Max pooling layer

池化层的作用是为句子找到全局的上下文特征。池化层以 Max-over-time pooling 的方式，每个 feature map 都取最大值，得到一个 300 维的向量。Max-over-pooling 可以解决可变长度的句子输入问题（因为不管 Feature Map 中有多少个值，只需要提取其中的最大值）。不过我们在上一步已经做了句子的定长处理（固定句子长度为 302），所以就没有可变长度句子的问题。最终池化层的输出为各个 Feature Map 的最大值，即一个 300*1 的向量。这里多提一句，之所以 Max pooling 层要保持固定的输出维度，是因为下一层全链接层要求有固定的输入层数，才能进行训练。

（3）全连接层——Semantic layer

最后通过全连接层把一个 300 维的向量转化为一个 128 维的低维语义向量。全连接层采用 tanh 函数：

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/f18f1abf9143432f90b5462f90ffd1240c74d179a6de4540ad8463cf318a9e72" width="400" hegiht="" ></center>

### 3.匹配层

Query 和 Doc 的语义相似性可以用这两个语义向量(128 维) 的 cosine 距离来表示：

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/4b78c505266641a98d78f67fe006dce997ce10139f8d4cfba0da04be03f0e036" width="400" hegiht="" ></center>


通过softmax 函数可以把Query 与正样本 Doc 的语义相似性转化为一个后验概率：

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/15521993474840068541d26a3c732affcbabeed9fb19468ba3bfb26c9053f08f" width="400" hegiht="" ></center>


其中 r 为 softmax 的平滑因子，D 为 Query 下的正样本，D-为 Query 下的负样本（采取随机负采样），D 为 Query 下的整个样本空间。

在训练阶段，通过极大似然估计，我们最小化损失函数：

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/5269d1088c0645da83de1f666f674ab477c3b53dbafc4da68a0105dceb7fbb7f" width="400" hegiht="" ></center>


残差会在表示层的 DNN 中反向传播，最终通过随机梯度下降（SGD）使模型收敛，得到各网络层的参数{Wi,bi}。

# 优缺点

优点：CNN-DSSM 通过卷积层提取了滑动窗口下的上下文信息，又通过池化层提取了全局的上下文信息，上下文信息得到较为有效的保留。

缺点：对于间隔较远的上下文信息，难以有效保留。举个例子，I grew up in France... I speak fluent French，显然 France 和 French 是具有上下文依赖关系的，但是由于 CNN-DSSM 滑动窗口（卷积核）大小的限制，导致无法捕获该上下文信息。

# LSTM-DSSM概念

LSTM(（Long-Short-Term Memory）是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM-DSSM 其实用的是 LSTM 的一个变种——加入了peephole的 LSTM。

# 模型
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/7fd5068e95e840e294e6aeb8835f5b3e7bfe629df6e54d8b89563f5a8e44159d" width="400" hegiht="" ></center>

这里三条黑线就是所谓的 peephole，传统的 LSTM 中遗忘门、输入门和输出门只用了 h(t-1) 和 xt 来控制门缝的大小，peephole 的意思是说不但要考虑 h(t-1) 和 xt，也要考虑 Ct-1 和 Ct，其中遗忘门和输入门考虑了 Ct-1，而输出门考虑了 Ct。总体来说需要考虑的信息更丰富了。

整体结构：

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/8f574b4dc9ea4ecab1f7d35e79e7c68b9ce4a18b4f204d41bcd4d1f831f871f2" width="400" hegiht="" ></center>

红色的部分可以清晰的看到残差传递的方向。

# 作用

和其他神经网络用途类似，主要用于分类或预测。

# 优缺点
[1] 优点：改善了RNN中存在的长期依赖问题；LSTM的表现通常比时间递归神经网络及隐马尔科夫模型（HMM）更好；作为非线性模型，LSTM可作为复杂的非线性单元用于构造更大型深度神经网络。

[2] 缺点：一个缺点是RNN的梯度问题在LSTM及其变种里面得到了一定程度的解决，但还是不够。它可以处理100个量级的序列，而对于1000个量级，或者更长的序列则依然会显得很棘手；另一个缺点是每一个LSTM的cell里面都意味着有4个全连接层(MLP)，如果LSTM的时间跨度很大，并且网络又很深，这个计算量会很大，很耗时。

# MMoE的概念

底层共享,上层拆塔,是多任务学习的常规实现. 

# 场景

推荐系统的多目标(ctr,互动率,转化率,etc.)

# 网络构架

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/ee1b8abe32cd43b0b6650dc06a77a5332f6f41c6e16940d6ada03c48fcfa425a" width="400" hegiht="" ></center>

图: 多任务学习的网络架构演化, c 为MMoE网络架构

MMoE模型为每一个task设置了一个gate，使不同的任务和不同的数据可以多样化的使用共享层。
门控网络采用输入特性和输出Softmax门，以不同的权重组装专家，允许不同的任务以不同的方式利用专家。 然 后将组装专家的结果传递到特定任务的塔网络中。这样，不同任务的门控网络可以学习专家组装的不同混合模式， 从而捕获任务关系。
每个任务的共享层的输出不同，第k个任务的共享层输出计算公式如下：

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/09a1d9e1c7e2424ea08a05da85069aa53699b6e3309d4ab18be861eb8affc88e" width="400" hegiht="" ></center>


随后每个任务对应的共享层输出，经过多层全连接神经网络得到每个任务的输出：

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/e23b26cf6d7f46c8b6ae92ce5f0d7f1b7421fc889617411abe42802b9a50f4b4" width="400" hegiht="" ></center>


如果两个任务并不十分相关，经过Gate之后，二者得到的权重系数会差别比较大，可以利用部分expert网络输出的信息，近似于多个单任务学习模型。如果两个任务紧密相关，经过Gate得到的权重分布相差不多，类似于一般的多任务学习框架。

# 优点

1.MMoE能够对于任务之间的关系明确地建模。

2.gating networks是轻量的，而expert networks是共享的，因此并不会引入造成计算代价。

3.更容易训练

# ShareBottom的概念

多目标建模目前业内有两种模式，一种叫Shared-Bottom模式，另一种叫MOE，MOE又包含MMOE和OMOE两种。

# 网络构架

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/d7363b589b384dffa8a57f4ef1ba1e90a0b28bab40154c5aa834577610473009" width="800" hegiht="" ></center>

（a）为sharebottom

Shared-Bottom的思路就是多个目标底层共用一套共享layer，在这之上基于不同的目标构建不同的Tower。这样的好处就是底层的layer复用，减少计算量，同时也可以防止过拟合的情况出现。

# 优缺点
Shared-Bottom 优点：降低overfit风险，利用任务之间的关联性使模型学习效果更强

Shared-Bottom 缺点：任务之间的相关性将严重影响模型效果。假如任务之间相关性较低，模型的效果相对会较差。

<center><img src="" width="400" hegiht="" ></center>

# YouTube深度学习视频推荐系统

作为全球最大的视频分享网站，YouTube 平台中几乎所有的视频都来自 UGC（User Generated Content，用户原创内容），这样的内容产生模式有两个特点：

1.其商业模式不同于 Netflix，以及国内的腾讯视频、爱奇艺这样的流媒体，这些流媒体的大部分内容都是采购或自制的电影、剧集等头部内容，而 YouTube 的内容都是用户上传的自制视频，种类风格繁多，头部效应没那么明显；

2.由于 YouTube 的视频基数巨大，用户难以发现喜欢的内容。

# 网络架构

为了对海量的视频进行快速、准确的排序，YouTube 也采用了经典的召回层 + 排序层的推荐系统架构。

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/9729424f6dba47909119bc87a2a28271c03a6d8e5bac4a5faf9a25983b09f021" width="400" hegiht="" ></center>

其推荐过程可以分成二级。第一级是用候选集生成模型（Candidate Generation Model）完成候选视频的快速筛选，在这一步，候选视频集合由百万降低到几百量级，这就相当于经典推荐系统架构中的召回层。第二级是用排序模型（Ranking Model）完成几百个候选视频的精排，这相当于经典推荐系统架构中的排序层。

无论是候选集生成模型还是排序模型，YouTube 都采用了深度学习的解决方案。

# 候选集生成模型

用于视频召回的候选集生成模型，架构如下图所示。

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/f579b2cb1bfc477ab5e51deea8227eec218de130160f41fcb01140b18d901786" width="400" hegiht="" ></center>

最底层是它的输入层，输入的特征包括用户历史观看视频的 Embedding 向量，以及搜索词的 Embedding 向量。对于这些 Embedding 特征，YouTube 是利用用户的观看序列和搜索序列，采用了类似 Item2vec 的预训练方式生成的。

除了视频和搜索词 Embedding 向量，特征向量中还包括用户的地理位置 Embedding、年龄、性别等特征。这里我们需要注意的是，对于样本年龄这个特征，YouTube 不仅使用了原始特征值，还把经过平方处理的特征值也作为一个新的特征输入模型。
这个操作其实是为了挖掘特征非线性的特性。

确定好了特征，这些特征会在 concat 层中连接起来，输入到上层的 ReLU 神经网络进行训练。

三层 ReLU 神经网络过后，YouTube 又使用了 softmax 函数作为输出层。值得一提的是，这里的输出层不是要预测用户会不会点击这个视频，而是要预测用户会点击哪个视频，这就跟一般深度推荐模型不一样。

总的来讲，YouTube 推荐系统的候选集生成模型，是一个标准的利用了 Embedding 预训练特征的深度推荐模型，它遵循Embedding MLP 模型的架构，只是在最后的输出层有所区别。

# 排序模型

<center><img src="https://img-blog.csdnimg.cn/20210122173512448.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDEyNzMyNw==,size_16,color_FFFFFF,t_70#pic_center" width="400" hegiht="" ></center>

输入层，相比于候选集生成模型需要对几百万候选集进行粗筛，排序模型只需对几百个候选视频进行排序，因此可以引入更多特征进行精排。具体来说，YouTube 的输入层从左至右引入的特征依次是：

impression video ID embedding：当前候选视频的 Embedding；

watched video IDs average embedding：用户观看过的最后 N 个视频 Embedding 的平均值；

language embedding：用户语言的 Embedding 和当前候选视频语言的 Embedding；

time since last watch：表示用户上次观看同频道视频距今的时间；

#previous impressions：该视频已经被曝光给该用户的次数；

这 5 类特征连接起来之后，需要再经过三层 ReLU 网络进行充分的特征交叉，然后就到了输出层。这里重点注意，排序模型的输出层与候选集生成模型又有所不同。不同主要有两点：一是候选集生成模型选择了 softmax 作为其输出层，而排序模型选择了 weighted logistic regression（加权逻辑回归）作为模型输出层；二是候选集生成模型预测的是用户会点击“哪个视频”，排序模型预测的是用户“要不要点击当前视频”。

# 训练和测试样本的处理
为了能够提高模型的训练效率和预测准确率，Youtube采取了诸多处理训练样本的工程措施，主要有3点：

1.候选集生成模型把推荐模型转换成 多分类问题，在预测下一次观看的场景中，每一个备选视频都会是一个分类，而如果采用softmax对其训练是很低效的。

2.Youtube采用word2vec中常用的 负采样训练方法减少每次预测的分类数量，从而加快整个模型的收敛速度。

3.在对训练集的预处理过程中，Youtube没有采用原始的用户日志，而是 对每个用户提取等数量的训练样本。

YouTube这样做的目的是减少高度活跃用户对模型损失的过度影响，使模型过于偏向活跃用户的行为模式，忽略数量更广大的长尾用户体验。在处理测试集时，Youtube没有采用经典的随机留一法，而是一定要以用户最近一次观看的行为作为测试集。只留最后一次观看行为做测试集主要是为了避免引入未来信息(future information)，产生于事实不符的数据穿越问题。

# 总结

YouTube 推荐系统的架构是一个典型的召回层加排序层的架构，其中候选集生成模型负责从百万候选集中召回几百个候选视频，排序模型负责几百个候选视频的精排，最终选出几十个推荐给用户。

候选集生成模型是一个典型的 Embedding MLP 的架构，要注意的是它的输出层一个多分类的输出层，预测的是用户点击了“哪个”视频。在候选集生成模型的 serving 过程中，需要从输出层提取出视频 Embedding，从最后一层 ReLU 层得到用户 Embedding，然后利用 最近邻搜索快速 得到候选集。

排序模型同样是一个 Embedding MLP 的架构，不同的是，它的输入层包含了更多的用户和视频的特征，输出层采用了 Weighted LR 作为输出层，并且使用观看时长作为正样本权重，让模型能够预测出观看时长，这更接近 YouTube 要达成的商业目标。

