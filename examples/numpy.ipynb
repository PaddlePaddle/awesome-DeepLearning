{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / iter   0, loss = 7.0961\n",
      "Epoch   0 / iter   1, loss = 0.6654\n",
      "Epoch   0 / iter   2, loss = 0.3429\n",
      "Epoch   0 / iter   3, loss = 0.4864\n",
      "Epoch   0 / iter   4, loss = 0.6278\n",
      "Epoch   1 / iter   0, loss = 0.6507\n",
      "Epoch   1 / iter   1, loss = 0.9969\n",
      "Epoch   1 / iter   2, loss = 1.5353\n",
      "Epoch   1 / iter   3, loss = 2.4381\n",
      "Epoch   1 / iter   4, loss = 1.5588\n",
      "Epoch   2 / iter   0, loss = 2.6289\n",
      "Epoch   2 / iter   1, loss = 0.3820\n",
      "Epoch   2 / iter   2, loss = 0.3189\n",
      "Epoch   2 / iter   3, loss = 0.3115\n",
      "Epoch   2 / iter   4, loss = 0.3770\n",
      "Epoch   3 / iter   0, loss = 0.4648\n",
      "Epoch   3 / iter   1, loss = 1.0960\n",
      "Epoch   3 / iter   2, loss = 1.7831\n",
      "Epoch   3 / iter   3, loss = 2.3209\n",
      "Epoch   3 / iter   4, loss = 1.9400\n",
      "Epoch   4 / iter   0, loss = 2.4583\n",
      "Epoch   4 / iter   1, loss = 1.7921\n",
      "Epoch   4 / iter   2, loss = 1.2824\n",
      "Epoch   4 / iter   3, loss = 0.4456\n",
      "Epoch   4 / iter   4, loss = 0.2668\n",
      "Epoch   5 / iter   0, loss = 0.2010\n",
      "Epoch   5 / iter   1, loss = 0.0806\n",
      "Epoch   5 / iter   2, loss = 0.0985\n",
      "Epoch   5 / iter   3, loss = 0.1003\n",
      "Epoch   5 / iter   4, loss = 0.0213\n",
      "Epoch   6 / iter   0, loss = 0.0857\n",
      "Epoch   6 / iter   1, loss = 0.1020\n",
      "Epoch   6 / iter   2, loss = 0.0745\n",
      "Epoch   6 / iter   3, loss = 0.1016\n",
      "Epoch   6 / iter   4, loss = 0.0608\n",
      "Epoch   7 / iter   0, loss = 0.1109\n",
      "Epoch   7 / iter   1, loss = 0.0782\n",
      "Epoch   7 / iter   2, loss = 0.0686\n",
      "Epoch   7 / iter   3, loss = 0.1057\n",
      "Epoch   7 / iter   4, loss = 0.0413\n",
      "Epoch   8 / iter   0, loss = 0.0860\n",
      "Epoch   8 / iter   1, loss = 0.0980\n",
      "Epoch   8 / iter   2, loss = 0.1106\n",
      "Epoch   8 / iter   3, loss = 0.0609\n",
      "Epoch   8 / iter   4, loss = 0.2167\n",
      "Epoch   9 / iter   0, loss = 0.1077\n",
      "Epoch   9 / iter   1, loss = 0.0881\n",
      "Epoch   9 / iter   2, loss = 0.0854\n",
      "Epoch   9 / iter   3, loss = 0.0820\n",
      "Epoch   9 / iter   4, loss = 0.0279\n",
      "Epoch  10 / iter   0, loss = 0.0839\n",
      "Epoch  10 / iter   1, loss = 0.0999\n",
      "Epoch  10 / iter   2, loss = 0.0793\n",
      "Epoch  10 / iter   3, loss = 0.1007\n",
      "Epoch  10 / iter   4, loss = 0.0302\n",
      "Epoch  11 / iter   0, loss = 0.0823\n",
      "Epoch  11 / iter   1, loss = 0.1250\n",
      "Epoch  11 / iter   2, loss = 0.0761\n",
      "Epoch  11 / iter   3, loss = 0.0787\n",
      "Epoch  11 / iter   4, loss = 0.0748\n",
      "Epoch  12 / iter   0, loss = 0.0838\n",
      "Epoch  12 / iter   1, loss = 0.0889\n",
      "Epoch  12 / iter   2, loss = 0.0953\n",
      "Epoch  12 / iter   3, loss = 0.0945\n",
      "Epoch  12 / iter   4, loss = 0.0235\n",
      "Epoch  13 / iter   0, loss = 0.0868\n",
      "Epoch  13 / iter   1, loss = 0.0794\n",
      "Epoch  13 / iter   2, loss = 0.1103\n",
      "Epoch  13 / iter   3, loss = 0.0791\n",
      "Epoch  13 / iter   4, loss = 0.2515\n",
      "Epoch  14 / iter   0, loss = 0.0935\n",
      "Epoch  14 / iter   1, loss = 0.1037\n",
      "Epoch  14 / iter   2, loss = 0.0800\n",
      "Epoch  14 / iter   3, loss = 0.0839\n",
      "Epoch  14 / iter   4, loss = 0.0234\n",
      "Epoch  15 / iter   0, loss = 0.0542\n",
      "Epoch  15 / iter   1, loss = 0.1219\n",
      "Epoch  15 / iter   2, loss = 0.1013\n",
      "Epoch  15 / iter   3, loss = 0.0820\n",
      "Epoch  15 / iter   4, loss = 0.0583\n",
      "Epoch  16 / iter   0, loss = 0.0988\n",
      "Epoch  16 / iter   1, loss = 0.0868\n",
      "Epoch  16 / iter   2, loss = 0.0705\n",
      "Epoch  16 / iter   3, loss = 0.1043\n",
      "Epoch  16 / iter   4, loss = 0.0341\n",
      "Epoch  17 / iter   0, loss = 0.0718\n",
      "Epoch  17 / iter   1, loss = 0.0959\n",
      "Epoch  17 / iter   2, loss = 0.0901\n",
      "Epoch  17 / iter   3, loss = 0.1013\n",
      "Epoch  17 / iter   4, loss = 0.0535\n",
      "Epoch  18 / iter   0, loss = 0.0999\n",
      "Epoch  18 / iter   1, loss = 0.1068\n",
      "Epoch  18 / iter   2, loss = 0.0817\n",
      "Epoch  18 / iter   3, loss = 0.0694\n",
      "Epoch  18 / iter   4, loss = 0.0507\n",
      "Epoch  19 / iter   0, loss = 0.0933\n",
      "Epoch  19 / iter   1, loss = 0.0722\n",
      "Epoch  19 / iter   2, loss = 0.0990\n",
      "Epoch  19 / iter   3, loss = 0.0963\n",
      "Epoch  19 / iter   4, loss = 0.0035\n",
      "Epoch  20 / iter   0, loss = 0.1169\n",
      "Epoch  20 / iter   1, loss = 0.0797\n",
      "Epoch  20 / iter   2, loss = 0.0888\n",
      "Epoch  20 / iter   3, loss = 0.0733\n",
      "Epoch  20 / iter   4, loss = 0.0408\n",
      "Epoch  21 / iter   0, loss = 0.1010\n",
      "Epoch  21 / iter   1, loss = 0.0942\n",
      "Epoch  21 / iter   2, loss = 0.0777\n",
      "Epoch  21 / iter   3, loss = 0.0835\n",
      "Epoch  21 / iter   4, loss = 0.0432\n",
      "Epoch  22 / iter   0, loss = 0.0809\n",
      "Epoch  22 / iter   1, loss = 0.1149\n",
      "Epoch  22 / iter   2, loss = 0.0966\n",
      "Epoch  22 / iter   3, loss = 0.0656\n",
      "Epoch  22 / iter   4, loss = 0.0253\n",
      "Epoch  23 / iter   0, loss = 0.0919\n",
      "Epoch  23 / iter   1, loss = 0.0790\n",
      "Epoch  23 / iter   2, loss = 0.0907\n",
      "Epoch  23 / iter   3, loss = 0.0876\n",
      "Epoch  23 / iter   4, loss = 0.2217\n",
      "Epoch  24 / iter   0, loss = 0.0985\n",
      "Epoch  24 / iter   1, loss = 0.0632\n",
      "Epoch  24 / iter   2, loss = 0.0985\n",
      "Epoch  24 / iter   3, loss = 0.0940\n",
      "Epoch  24 / iter   4, loss = 0.0896\n",
      "Epoch  25 / iter   0, loss = 0.1082\n",
      "Epoch  25 / iter   1, loss = 0.0599\n",
      "Epoch  25 / iter   2, loss = 0.1010\n",
      "Epoch  25 / iter   3, loss = 0.0852\n",
      "Epoch  25 / iter   4, loss = 0.0758\n",
      "Epoch  26 / iter   0, loss = 0.0604\n",
      "Epoch  26 / iter   1, loss = 0.0761\n",
      "Epoch  26 / iter   2, loss = 0.1237\n",
      "Epoch  26 / iter   3, loss = 0.0932\n",
      "Epoch  26 / iter   4, loss = 0.0629\n",
      "Epoch  27 / iter   0, loss = 0.0970\n",
      "Epoch  27 / iter   1, loss = 0.0955\n",
      "Epoch  27 / iter   2, loss = 0.0912\n",
      "Epoch  27 / iter   3, loss = 0.0695\n",
      "Epoch  27 / iter   4, loss = 0.0437\n",
      "Epoch  28 / iter   0, loss = 0.0919\n",
      "Epoch  28 / iter   1, loss = 0.1037\n",
      "Epoch  28 / iter   2, loss = 0.0862\n",
      "Epoch  28 / iter   3, loss = 0.0676\n",
      "Epoch  28 / iter   4, loss = 0.1252\n",
      "Epoch  29 / iter   0, loss = 0.0718\n",
      "Epoch  29 / iter   1, loss = 0.0964\n",
      "Epoch  29 / iter   2, loss = 0.0942\n",
      "Epoch  29 / iter   3, loss = 0.0898\n",
      "Epoch  29 / iter   4, loss = 0.0290\n",
      "Epoch  30 / iter   0, loss = 0.0958\n",
      "Epoch  30 / iter   1, loss = 0.0808\n",
      "Epoch  30 / iter   2, loss = 0.0885\n",
      "Epoch  30 / iter   3, loss = 0.0862\n",
      "Epoch  30 / iter   4, loss = 0.0641\n",
      "Epoch  31 / iter   0, loss = 0.0884\n",
      "Epoch  31 / iter   1, loss = 0.0707\n",
      "Epoch  31 / iter   2, loss = 0.1095\n",
      "Epoch  31 / iter   3, loss = 0.0822\n",
      "Epoch  31 / iter   4, loss = 0.0691\n",
      "Epoch  32 / iter   0, loss = 0.1006\n",
      "Epoch  32 / iter   1, loss = 0.0775\n",
      "Epoch  32 / iter   2, loss = 0.0965\n",
      "Epoch  32 / iter   3, loss = 0.0729\n",
      "Epoch  32 / iter   4, loss = 0.1161\n",
      "Epoch  33 / iter   0, loss = 0.0660\n",
      "Epoch  33 / iter   1, loss = 0.0993\n",
      "Epoch  33 / iter   2, loss = 0.0723\n",
      "Epoch  33 / iter   3, loss = 0.1043\n",
      "Epoch  33 / iter   4, loss = 0.2440\n",
      "Epoch  34 / iter   0, loss = 0.0961\n",
      "Epoch  34 / iter   1, loss = 0.0621\n",
      "Epoch  34 / iter   2, loss = 0.0837\n",
      "Epoch  34 / iter   3, loss = 0.1053\n",
      "Epoch  34 / iter   4, loss = 0.0988\n",
      "Epoch  35 / iter   0, loss = 0.0601\n",
      "Epoch  35 / iter   1, loss = 0.0895\n",
      "Epoch  35 / iter   2, loss = 0.1174\n",
      "Epoch  35 / iter   3, loss = 0.0807\n",
      "Epoch  35 / iter   4, loss = 0.0588\n",
      "Epoch  36 / iter   0, loss = 0.1226\n",
      "Epoch  36 / iter   1, loss = 0.0779\n",
      "Epoch  36 / iter   2, loss = 0.0791\n",
      "Epoch  36 / iter   3, loss = 0.0686\n",
      "Epoch  36 / iter   4, loss = 0.0498\n",
      "Epoch  37 / iter   0, loss = 0.0663\n",
      "Epoch  37 / iter   1, loss = 0.1100\n",
      "Epoch  37 / iter   2, loss = 0.0802\n",
      "Epoch  37 / iter   3, loss = 0.0938\n",
      "Epoch  37 / iter   4, loss = 0.0169\n",
      "Epoch  38 / iter   0, loss = 0.0938\n",
      "Epoch  38 / iter   1, loss = 0.0768\n",
      "Epoch  38 / iter   2, loss = 0.0884\n",
      "Epoch  38 / iter   3, loss = 0.0861\n",
      "Epoch  38 / iter   4, loss = 0.1006\n",
      "Epoch  39 / iter   0, loss = 0.0782\n",
      "Epoch  39 / iter   1, loss = 0.1171\n",
      "Epoch  39 / iter   2, loss = 0.0899\n",
      "Epoch  39 / iter   3, loss = 0.0604\n",
      "Epoch  39 / iter   4, loss = 0.0816\n",
      "Epoch  40 / iter   0, loss = 0.0758\n",
      "Epoch  40 / iter   1, loss = 0.0547\n",
      "Epoch  40 / iter   2, loss = 0.1084\n",
      "Epoch  40 / iter   3, loss = 0.1090\n",
      "Epoch  40 / iter   4, loss = 0.0049\n",
      "Epoch  41 / iter   0, loss = 0.1033\n",
      "Epoch  41 / iter   1, loss = 0.0853\n",
      "Epoch  41 / iter   2, loss = 0.0883\n",
      "Epoch  41 / iter   3, loss = 0.0676\n",
      "Epoch  41 / iter   4, loss = 0.1087\n",
      "Epoch  42 / iter   0, loss = 0.0811\n",
      "Epoch  42 / iter   1, loss = 0.0762\n",
      "Epoch  42 / iter   2, loss = 0.0886\n",
      "Epoch  42 / iter   3, loss = 0.0999\n",
      "Epoch  42 / iter   4, loss = 0.0425\n",
      "Epoch  43 / iter   0, loss = 0.0853\n",
      "Epoch  43 / iter   1, loss = 0.0757\n",
      "Epoch  43 / iter   2, loss = 0.1041\n",
      "Epoch  43 / iter   3, loss = 0.0797\n",
      "Epoch  43 / iter   4, loss = 0.0325\n",
      "Epoch  44 / iter   0, loss = 0.0881\n",
      "Epoch  44 / iter   1, loss = 0.0893\n",
      "Epoch  44 / iter   2, loss = 0.0661\n",
      "Epoch  44 / iter   3, loss = 0.0963\n",
      "Epoch  44 / iter   4, loss = 0.1669\n",
      "Epoch  45 / iter   0, loss = 0.1119\n",
      "Epoch  45 / iter   1, loss = 0.0827\n",
      "Epoch  45 / iter   2, loss = 0.0789\n",
      "Epoch  45 / iter   3, loss = 0.0706\n",
      "Epoch  45 / iter   4, loss = 0.0483\n",
      "Epoch  46 / iter   0, loss = 0.0826\n",
      "Epoch  46 / iter   1, loss = 0.0641\n",
      "Epoch  46 / iter   2, loss = 0.1039\n",
      "Epoch  46 / iter   3, loss = 0.0844\n",
      "Epoch  46 / iter   4, loss = 0.2465\n",
      "Epoch  47 / iter   0, loss = 0.0973\n",
      "Epoch  47 / iter   1, loss = 0.0878\n",
      "Epoch  47 / iter   2, loss = 0.0793\n",
      "Epoch  47 / iter   3, loss = 0.0713\n",
      "Epoch  47 / iter   4, loss = 0.2322\n",
      "Epoch  48 / iter   0, loss = 0.0675\n",
      "Epoch  48 / iter   1, loss = 0.1085\n",
      "Epoch  48 / iter   2, loss = 0.0939\n",
      "Epoch  48 / iter   3, loss = 0.0745\n",
      "Epoch  48 / iter   4, loss = 0.0039\n",
      "Epoch  49 / iter   0, loss = 0.1095\n",
      "Epoch  49 / iter   1, loss = 0.0649\n",
      "Epoch  49 / iter   2, loss = 0.0780\n",
      "Epoch  49 / iter   3, loss = 0.0908\n",
      "Epoch  49 / iter   4, loss = 0.0300\n",
      "Inference result is [[29.92211405]], the corresponding label is 42.27574257425745\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # 从文件导入数据\n",
    "    datafile = './data/housing.data'\n",
    "    data = np.fromfile(datafile, sep=' ')\n",
    "\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',                       'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "    feature_num = len(feature_names)\n",
    "\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
    "\n",
    "    # 将原数据集拆分成训练集和测试集\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\n",
    "    # 测试集和训练集必须是没有交集的\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0] * ratio)\n",
    "    training_data = data[:offset]\n",
    "\n",
    "    # 计算训练集的最大值，最小值，平均值\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0),                                  training_data.sum(axis=0) / training_data.shape[0]\n",
    "\n",
    "    global max_values\n",
    "    global min_values\n",
    "    global avg_values\n",
    "    max_values=maximums\n",
    "    min_values=minimums\n",
    "    avg_values=avgs\n",
    "\n",
    "    # 对数据进行归一化处理\n",
    "    for i in range(feature_num):\n",
    "        #print(maximums[i], minimums[i], avgs[i])\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\n",
    "\n",
    "    # 训练集和测试集的划分比例\n",
    "    training_data = data[:offset]\n",
    "    test_data = data[offset:]\n",
    "    return training_data, test_data\n",
    "\n",
    "\n",
    "# 定义隐藏层使用的激活函数\n",
    "def Sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "# 创建一个两层神经网络的类\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(321)\n",
    "        #初始化参数\n",
    "        self.w1 = np.random.randn(num_of_weights, 128)\n",
    "        self.b1 = 0.\n",
    "        self.w2=np.random.randn(128,1)\n",
    "        self.b2=0.\n",
    "\n",
    "    def forward(self, x):\n",
    "        #正向传播\n",
    "        z1 = np.dot(x, self.w1) + self.b1\n",
    "        o1=Sigmoid(z1)\n",
    "        o2= np.dot(o1,self.w2)+self.b2\n",
    "        return z1,o1,o2\n",
    "\n",
    "    #计算损失\n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "\n",
    "    def gradient(self, x, y, z1,o1,o2):\n",
    "\n",
    "        #此处略复杂，经过手工计算梯度反向传播公式，然后分别计算相应的梯度\n",
    "        N = o1.shape[0]\n",
    "        gradient_w2 = 1. / N * np.sum((o2-y) * o1, axis=0)\n",
    "        gradient_w2 = gradient_w2[:, np.newaxis]\n",
    "        gradient_b2 = 1. / N * np.sum(o2-y)\n",
    "\n",
    "        M=x.shape[0]\n",
    "        gradient_w1=1. /M*np.sum(np.dot(self.w2.T,np.dot(o1.T,(o2-y)))*np.dot(x.T,(1-o1)),axis=0)\n",
    "        gradient_w1 = gradient_w1[:, np.newaxis]\n",
    "        gradient_b1=1. /M*np.sum(np.dot(self.w2.T,np.dot(o1.T,(o2-y)))*(1-o1),axis=0)\n",
    "\n",
    "        return gradient_w1, gradient_b1,gradient_w2, gradient_b2\n",
    "\n",
    "    #根据计算出的梯度分别对两层的参数进行更新\n",
    "    def update(self, gradient_w1, gradient_b1, gradient_w2, gradient_b2, eta = 0.01):\n",
    "        self.w2 = self.w2 - eta * gradient_w2\n",
    "        self.b2 = self.b2 - eta * gradient_b2\n",
    "        self.w1 = self.w1 - eta * gradient_w1.T\n",
    "        self.b1 = self.b1 - eta * gradient_b1\n",
    "\n",
    "    # 训练过程\n",
    "    def train(self, training_data, num_epochs, batch_size=10, eta=0.01):\n",
    "        n = len(training_data)\n",
    "        losses = []\n",
    "        for epoch_id in range(num_epochs):\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\n",
    "            # 然后再按每次取batch_size条数据的方式取出\n",
    "            np.random.shuffle(training_data)\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\n",
    "                #print(self.w.shape)\n",
    "                #print(self.b)\n",
    "                x = mini_batch[:, :-1]\n",
    "                y = mini_batch[:, -1:]\n",
    "                z1,o1,o2 = self.forward(x)\n",
    "                loss = self.loss(o2, y)\n",
    "                gradient_w1, gradient_b1, gradient_w2, gradient_b2= self.gradient(x, y, z1,o1,o2)\n",
    "                self.update(gradient_w1, gradient_b1, gradient_w2, gradient_b2, eta)\n",
    "                losses.append(loss)\n",
    "                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\n",
    "                                 format(epoch_id, iter_id, loss))\n",
    "\n",
    "        return losses\n",
    "\n",
    "\n",
    "train_data, test_data = load_data()\n",
    "\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "# 启动训练\n",
    "losses = net.train(train_data, num_epochs=50, batch_size=100, eta=0.001)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(len(losses))\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def load_one_example():\n",
    "    # 从上边已加载的测试集中，随机选择一条作为测试数据\n",
    "    idx = np.random.randint(0, test_data.shape[0])\n",
    "    one_data, label = test_data[idx, :-1], test_data[idx, -1]\n",
    "    # 修改该条数据shape为[1,13]\n",
    "    one_data =  one_data.reshape([1,-1])\n",
    "\n",
    "    return one_data, label\n",
    "\n",
    "\n",
    "\n",
    "one_data, label = load_one_example()\n",
    "_, _, predict=net.forward(one_data)\n",
    "\n",
    "# 对结果做反归一化处理\n",
    "predict = predict * (max_values[-1] - min_values[-1]) + avg_values[-1]\n",
    "# 对label数据做反归一化处理\n",
    "label = label * (max_values[-1] - min_values[-1]) + avg_values[-1]\n",
    "\n",
    "print(\"Inference result is {}, the corresponding label is {}\".format(predict, label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
