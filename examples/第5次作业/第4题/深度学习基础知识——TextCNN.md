# 深度学习基础知识——TextCNN

 对于文本分类问题，常见的方法无非就是抽取文本的特征，比如使用doc2evc或者LDA模型将文本转换成一个固定维度的特征向量，然后在基于抽取的特征训练一个分类器。 然而研究证明，TextCnn在文本分类问题上有着更加卓越的表现。从直观上理解，TextCNN通过一维卷积来获取句子中n-gram的特征表示。

TextCNN的结构比较简单，输入数据首先通过一个embedding layer，得到输入语句的embedding表示，然后通过一个convolution layer，提取语句的特征，最后通过一个fully connected layer得到最终的输出，整个模型的结构如下图：

![img](https://img-blog.csdn.net/20180319223936424?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3UwMTI3NjI0MTk=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

## TextCNN的网络结构

### 嵌入层

  这一层的主要作用是将输入的自然语言编码成distributed representation。可以使用预训练好的词向量，也可以直接在训练textcnn的过程中训练出一套词向量，不过前者比或者快100倍不止。如果使用预训练好的词向量，又分为static方法和no-static方法，前者是指在训练textcnn过程中不再调节词向量的参数，后者在训练过程中调节词向量的参数，所以，后者的结果比前者要好。更为一般的做法是：不要在每一个batch中都调节emdbedding层，而是每个100个batch调节一次，这样可以减少训练的时间，又可以微调词向量。

### 卷积层

这一层主要是通过卷积，提取不同的n-gram特征。输入的语句或者文本，通过embedding layer后，会转变成一个二维矩阵，假设文本的长度为 $|T|$，词向量的大小为 $|d|$，则该二维矩阵的大小为 $|T|\times |d|$，接下的卷积工作就是对这一个 $|T|\times |d|$​的二维矩阵进行的。卷积核的大小一般设定为:
$$
n\times |d|
$$
n是卷积核的长度，$|d|$ 是卷积核的宽度，这个宽度和词向量的维度是相同的，也就是卷积只是沿着文本序列进行的，n可以有多种选择，比如2、3、4、5等。对于一个 $|T|\times |d|$的文本，如果选择卷积核kernel的大小为$2\times|d|$，则卷积后得到的结果是$|T-2+1|\times 1$的一个向量。在 TextCNN 网络中，需要同时使用多个不同类型的kernel，同时每个size的kernel又可以有多个。如果我们使用的kernel size大小为2、3、4、$5\times|d|$​，每个种类的size又有128个kernel，则卷积网络一共有4x128个卷积核。积层本质上是一个n-gram特征提取器，不同的卷积核提取的特征不同，以文本分类为例，有的卷积核可能提取到娱乐类的n-gram，比如范冰冰、电影等n-gram；有的卷积核可能提取到经济类的n-gram，比如去产能、调结构等。分类的时候，不同领域的文本包含的n-gram是不同的，激活对应的卷积核，就会被分到对应的类。

### 池化层

最大池化层，对卷积后得到的若干个一维向量取最大值，然后拼接在一块，作为本层的输出值。

![img](https://img-blog.csdnimg.cn/20181218204918185.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BpcGlzb3JyeQ==,size_16,color_FFFFFF,t_70)

CNN中采用**Max Pooling操作有几个好处**：**首先**，这个操作可以**保证特征的位置与旋转不变性**，因为不论这个强特征在哪个位置出现，都会不考虑其出现位置而能把它提出来。但是对于NLP来说，这个特性其实并不一定是好事，因为在很多NLP的应用场合，特征的出现位置信息是很重要的，比如主语出现位置一般在句子头，宾语一般出现在句子尾等等。**其次**，MaxPooling能**减少模型参数数量**，有利于减少模型过拟合问题。因为经过Pooling操作后，往往把2D或者1D的数组转换为单一数值，这样对于后续的Convolution层或者全联接隐层来说无疑单个Filter的参数或者隐层神经元个数就减少了。**再者**，对于NLP任务来说，可以**把变长的输入X整理成固定长度的输入**。因为CNN最后往往会接全联接层，而其神经元个数是需要事先定好的，如果输入是不定长的那么很难设计网络结构。

 但是，CNN模型采取 MaxPooling 也有**缺点**：首先特征的**位置信息在这一步骤完全丢失**。在卷积层其实是保留了特征的位置信息的，但是通过取唯一的最大值，现在在Pooling层只知道这个最大值是多少，但是其出现位置信息并没有保留；另外一个明显的缺点是：有时候有些强特征会出现多次，出现次数越多说明这个特征越强，但是因为Max Pooling只保留一个最大值，就是说**同一特征的强度信息丢失**了。

### 全连接层

这一层没有特别的地方，将max-pooling layer后再拼接一层，作为输出结果。实际中为了提高网络的学习能力，可以拼接多个全连接层。

