# 深度学习基础知识——层次 softmax 训练词向量

对于语义相近的词，他们经常会同时出现在某段文本的中，而且往往距离很近，所以我们就可以基于这种现象来训练出词向量。

既然语义相近的两个词（即差别很小的两个词向量）在文本中也会趋向于挨得近，那如果我们可以找到一个模型，它**可以在给定一个词向量时，计算出这个词附近出现每个词的概率**（即一个词就看成一个类别，词典中有多少词，就是多少个类别，计算出给定输入下，每个类别的概率）。softmax函数，即归一化指数函数完美的解决了这个需求。
$$
y_i=e^{z_i}/\sum\limits_{j=1}^ke^{z_j}
$$
其中，k为输出的个数，$z_i$为第$i$​个输出，**softmax函数将其映射为0到1之间，其本质是在输入特征的条件下划入各类别的后验条件概率**，具有概率拥有的两个性质：一是取值非负，二是概率和为一。

## 1. Hierarchical Softmax模型

我们先回顾下传统的神经网络词向量语言模型，里面一般有三层，输入层（词向量），隐藏层和输出层（softmax层）。里面最大的**问题在于从隐藏层到输出的softmax层的计算量很大**，因为要计算所有词的softmax概率，再去找概率最大的值。这个模型如下图所示。其中V是词汇表的大小，

![img](https://images2017.cnblogs.com/blog/1042406/201707/1042406-20170727105326843-18935623.png)

 

**word2vec**对这个模型做了改进，首先，对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。

第二个改进就是从隐藏层到输出的softmax层这里的计算量个改进。为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。

由于我们把之前所有都要计算的从输出softmax层的概率计算变成了一颗二叉霍夫曼树，那么我们的softmax概率计算只需要沿着树形结构进行就可以了。如下图所示，我们可以沿着霍夫曼树从根节点一直走到我们的叶子节点的词$w_2$。

​                                                      ![img](https://images2017.cnblogs.com/blog/1042406/201707/1042406-20170727105752968-819608237.png)

和之前的神经网络语言模型相比，霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。**在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的**，因此这种softmax取名为"Hierarchical Softmax"。

在word2vec中，我们采用了二元逻辑回归的方法，即规定沿着左子树走，那么就是负类(霍夫曼树编码1)，沿着右子树走，那么就是正类(霍夫曼树编码0)。判别正类和负类的方法是使用sigmoid函数，即：
$$
P(+)=\sigma(x^{T}_{w}θ)=\dfrac{1}{1+e^{-x^{T}_{w}θ}}
$$


其中$x_w$是当前内部节点的词向量，而$\theta$则是我们需要从训练样本求出的逻辑回归的模型参数。

这个改进有**两个优点**。首先，由于是二叉树，之前计算量为$V$​,现在变成了$\log_2V$​。第二，由于使用霍夫曼树是高频的词靠近树根，这样高频词需要更少的时间会被找到，这符合我们的贪心优化思想。

容易理解，被划分为左子树而成为负类的概率为$P(−)=1−P(+)$。在某一个内部节点，要判断是沿左子树还是右子树走的标准就是看$P(−),P(+)$​谁的概率值大。而控制$P(−),P(+)$谁的概率值大的因素一个是当前节点的词向量，另一个是当前节点的模型参数$\theta$。

对于上图中的$w_2$​，如果它是一个训练样本的输出，那么我们期望对于里面的隐藏节点$n(w_2,1)$​的$P(−)$​概率大，$n(w_2,2)$的$P(−)$概率大，$n(w2,3)$的$P(+)$概率大。

回到基于Hierarchical Softmax的word2vec本身，我们的目标就是找到合适的所有节点的词向量和所有内部节点$\theta$, 使训练样本达到最大似然。

## 3. 基于Hierarchical Softmax的CBOW模型

由于word2vec有两种模型：CBOW和Skip-Gram,我们先看看基于CBOW模型时， Hierarchical Softmax如何使用。

首先我们要定义词向量的维度大小**M**，以及CBOW的上下文大小**2c**,这样我们对于训练样本中的每一个词，其前面的c个词和后面的c个词作为了CBOW模型的输入,该词本身作为样本的输出，期望softmax概率最大。

在做CBOW模型前，我们需要先将词汇表建立成一颗霍夫曼树。

对于从输入层到隐藏层（投影层），这一步比较简单，就是对w周围的2c个词向量求和取平均即可，即：
$$
x_w=\dfrac{1}{2c}\sum\limits_{i=1}^{2c}x_i
$$


第二步，通过梯度上升法来更新我们的$\theta_{j-1}^{w}$和$x_w$，注意这里的$x_w$​是由2c个词向量相加而成，我们做梯度更新完毕后会用梯度项直接更新原始的各个词向量，即：
$$
\theta_{j-1}^{w}=\theta_{j-1}^{w}+\eta (1-d_j^w-\sigma(x_w^T\theta_{j-1}^{w}))x_w
$$

$$
x_i=x_i+\eta\sum\limits_{j=2}^{l_w}(1-d_j^w-\sigma(x_w^T\theta_{j-1}^{w}))\theta_{j-1}^{w}
$$

其中$\eta$为梯度上升法的步长。

这里总结下基于Hierarchical Softmax的CBOW模型算法流程，梯度迭代使用了随机梯度上升法：

输入：基于CBOW的语料训练样本，词向量的维度大小M，CBOW的上下文大小2c,步长η

输出：霍夫曼树的内部节点模型参数θ，所有的词向量$w$

1. 基于语料训练样本建立霍夫曼树；

2. 随机初始化所有的模型参数θ，所有的词向量w；

3. 进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w),w)$​做如下处理：

   ​	Step1. e=0，计算$x_w=\dfrac{1}{2c}\sum\limits_{i=1}^{2c}x_i$

   ​	Step2. for j = 2 to $l_w$​​，计算：
   $$
   f=\sigma(x_w^T\theta_{j-1}^{w})
   $$

   $$
   g=(1-d_j^w-f)
   $$

   $$
   e=e+g\theta_{j-1}^w
   $$

   $$
   \theta_{j-1}^w=\theta_{j-1}^w+gx_w
   $$

   

   ​	Step3. 对$context(w)$中的每一个词向量$x_i$进行更新：
   $$
   x_i=x_i+e
   $$
   ​	Step4. 如果梯度收敛，则结束迭代，否则重复这一过程。



## 4. 基于Hierarchical Softmax的Skip-Gram模型

现在我们先看看基于Skip-Gram模型时， Hierarchical Softmax如何使用。此时输入的只有一个词$w$,输出的为2c个词向量。

我们对于训练样本中的每一个词，该词本身作为样本的输入， 其前面的c个词和后面的c个词作为了Skip-Gram模型的输出,，期望这些词的softmax概率比其他的词大。

对于从输入层到隐藏层（投影层），这一步比CBOW简单，由于只有一个词，所以，即$x_w$就是词$w$对应的词向量。

第二步，通过梯度上升法来更新我们的$\theta_{j-1}^w$和$x_w$，注意这里的$x_w$周围有2c个词向量，此时如果我们期望$P(x_i|x_w),i\in (1,2...2c)$最大。此时我们注意到由于上下文是相互的，在期望最大化的同时，反过来我们也期望$P(x_w|x_i),i\in(1,2...2c)$最大。word2vec使用了后者，这样做的好处就是在一个迭代窗口内，我们不是只更新$z_w$一个词，而是2c个词。这样整体的迭代会更加的均衡。因为这个原因，Skip-Gram模型并没有和CBOW模型一样对输入进行迭代更新，而是对2c个输出进行迭代更新。

这里总结下基于Hierarchical Softmax的Skip-Gram模型算法流程，梯度迭代使用了随机梯度上升法：

输入：基于Skip-Gram的语料训练样本，词向量的维度大小M，Skip-Gram的上下文大小2c,步长$\eta$

输出：霍夫曼树的内部节点模型参数θ，所有的词向量w

1. 基于语料训练样本建立霍夫曼树。

2. 随机初始化所有的模型参数θ，所有的词向量w,

 3. 进行梯度上升迭代过程，对于训练集中的每一个样本$(context(w),w)$做如下处理：

    Step1. for i = 1 to 2c:

    ​            	e=0

    ​            	for j = 2 to $l_w$，计算：
    $$
    f=\sigma(x_w^T\theta_{j-1}^{w})
    $$

    $$
    g=(1-d_j^w-f)\eta
    $$

    $$
    e=e+g\theta_{j-1}^w
    $$

    $$
    \theta_{j-1}^w=\theta_{j-1}^w+gx_w
    $$

    ​               更新$x_i$：
    $$
    x_i=x_i+e
    $$
    

    Step2. 如果梯度收敛，则结束迭代，否则重复这一过程。