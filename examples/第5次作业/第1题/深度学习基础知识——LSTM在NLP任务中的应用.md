# 深度学习基础知识——LSTM在NLP任务中的应用

## 1. 序列到类别——文本分类

输入为序列，输出为类别，如文本分类中，输入数据为单词的序列，输出数据为文本的类别。

输入样本$x_1,x_2,...,x_T$ ，输出为y={1,2,3,4....C}  将x按照不同时刻输入到循环神经网络中，得到不同时刻的隐状态$h_t$ ， 可以将$h_T$看作整个序列的最终表示，然后再输入给分类器进行分类。

除了将最后时刻的状态作为整个序列的表示之后，我们还可以对所有序列的状态进行平均，利用这个平均的状态作为整个序列的表示。

## 2. 同步的序列到序列——词性标注

每一个时刻都有输入和输出，且其长度相同。如在词性标注中，每一个单词都需要标注其对应的词性标签。

输入样本$x_1,x_2,...,x_T$​ ，输出为序列$y_1,y_2,...,y_T$​​， 输入样本按不同时刻输入到循环神经网络中，并得到不同时刻的隐状态，每个时刻的都代表了当前时刻$x_t$信息和前一时刻隐状态的历史信息。

然后再输入给分类器，得到序列 $y_t$​​。对真实标签 $y$​​ 和预测的 $\hat{y}$​​ 进行交叉熵损失的计算，得到损失函数，再进行梯度下降更新参数和梯度。

## 3. 异步的序列到序列——机器翻译

又称为 编码器-解码器，输入和输出不需要有严格的对应关系，也不需要有一致的长度。 如在机器翻译中，输入源语言序列，输出为目标语言序列，其长度不一定等长。

一般通过先编码后解码的过程来进行，先将输入序列$x_1,x_2,...,x_T$ 按不同时刻输入到编码器中得到其编码，然后再使用另外一个循环神经网络（解码器）进行解码得到输出 $\hat{y}_1$,  再将 $\hat{y}_1$​ 和传递下来的隐状态作为下一个时刻的输入， 采用自回归模型，每个时刻的输入都是上一时刻的预测结果。

