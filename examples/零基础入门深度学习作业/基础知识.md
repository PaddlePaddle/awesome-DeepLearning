一、深度学习基础知识（10分每题）

1、深度学习发展史
神经网络思想的提出已经是75年前的事情了，现今的神经网络和深度学习的设计理论是一步步趋于完善的。在这漫长的发展岁月中，一些取得关键突破的闪光时刻，值得我们这些深度学习爱好者们铭记，如下图 所示
![](https://ai-studio-static-online.cdn.bcebos.com/431d29314dab428d89f4c3f5f8b03f4668bf1856a8564b5ab86024265e13a577)

·1940年代：首次提出神经元的结构，但权重是不可学的。
•	50-60年代：提出权重学习理论，神经元结构趋于完善，开启了神经网络的第一个黄金时代。
•	1969年：提出异或问题（人们惊讶的发现神经网络模型连简单的异或问题也无法解决，对其的期望从云端跌落到谷底），神经网络模型进入了被束之高阁的黑暗时代。
•	1986年：新提出的多层神经网络解决了异或问题，但随着90年代后理论更完备并且实践效果更好的SVM等机器学习模型的兴起，神经网络并未得到重视。
•	2010年左右：深度学习进入真正兴起时期。随着神经网络模型改进的技术在语音和计算机视觉任务上大放异彩，也逐渐被证明在更多的任务，如自然语言处理以及海量数据的任务上更加有效。至此，神经网络模型重新焕发生机，并有了一个更加响亮的名字：深度学习。


2、人工智能、机器学习、深度学习有什么区别联系？
人工智能、机器学习和深度学习覆盖的技术范畴是逐层递减的。人工智能是最宽泛的概念。机器学习是当前比较有效的一种实现人工智能的方式。深度学习是机器学习算法中最热门的一个分支，近些年取得了显著的进展，并替代了大多数传统机器学习算法。三者的关系如下图所示，即：人工智能 > 机器学习 > 深度学习。
![](https://ai-studio-static-online.cdn.bcebos.com/f186d3e9fe43451fa71c9d0d5b398d178f22b04c32b24562a404ddf264e95a85)


3、神经元、单层感知机、多层感知机
神经元：
人工神经网络与生物神经网络相似，都是由大量神经元组成，其中神经元又被称之为神经细胞，都具有细胞体、胞体、树突以及轴突等。
轴突和树突是神经元的突起，在神经元之间传递的是电信号，神经元的功能是接受信号并对其作出迅速反应、传导兴奋等。
当神经元之间相互连接，当某一个神经元处于兴奋 状态时，其相连神经元的电位将发生改变，若神经元电位改变超过一定的数值时，则相连的神经元被激活并处于兴奋状态，向下一级连接的神经元继续传递电位改变信息。信息从一个神经元一电传导的方式跨过之间的链接，传给另一个神经元，最终使肌肉收缩或分泌液体。
神经元就是一个多个输入、输出的信息处理单元，并且对信息的处理是非线性的。
1943年McCulloch 和Pitts提出MP模型，由固定权值和权重组成，是一种基于阈值逻辑算法的神经网络计算模型
在MP模型中，某个神经元接受来自其余多个神经元的传递信息号，多个输入与对应连接权重相乘后并对其求合，再与神经元预设的阈值进行比较，最后通过激活函数产生神经元输出。每个神经元具有空间整合特性和阈值特性。
单层感知机：
感知机是由Frank Rosenblatt在1957年提出的模型，与MP类似。
一般感知机指单层人工神经网络，以区别与多层感知机，其主要结构如下图所示：
![](https://ai-studio-static-online.cdn.bcebos.com/ac30b9f402bc4e31a2939d39cb320a31d1da2069b5384b9fbc7f23ed102c46d4)
假设有一个n维输入的单层感知机，x1到xn为n维输入向量的各个分量，w1到wn为各个输入分量连接到感知机的权重，θ为阈值，f为激活函数，y为标量输出。理想的激活函数f(.) 通常为阶跃函数或sigmoid函数。感知机的输出是输入向量x与权重向量的W求得内积 后，经激活函数f所得到的标量 ：
![](https://ai-studio-static-online.cdn.bcebos.com/c50521e774994428a7fa3cc1307c04bf343e9b0f207e461bbd331bab9850727a)
为了得到更好的拟合输出结果，权重W的初始值一般为随机值。通过计算得到的输出值，然后将实际输出值和理论值输出值做差，以此来调整每一个输出值的权值。学校规则是用来计算新的权值矩阵W及新的偏差B的算法。
多层感知机：
多层感知机，又叫深度前馈网络、前馈神经网络。最左边的是输入层，就是我们的输入数据，最右边的是输出层，中间的就是隐藏层（因为训练数据并没有直接表明隐藏层的每一层的所需输出），实际上就是由感知器构成。从现在开始，感知器就开始称为神经元，而这整个包含了输入层、隐藏层和输出层的结构就是大名鼎鼎的神经网络。可以看到相邻层之间的神经元是全连接的。
多层感知器的学习过程也是和感知器接近，主要都是计算训练数据的输出，根据预测输出和实际输出之间的差异去调整神经元的权值和阈值，不断迭代训练直到误差小于一定范围。同时也可能有一种情况，就是迭代次数已经很大可是模型依然无法得到很好的训练效果，这时我们就需要调整模型的超参数，比如神经元的数量、隐藏层的层数等等。基本上，这个过程和之前的线性回归等机器学习过程是一致的。
![](https://ai-studio-static-online.cdn.bcebos.com/3891fe0d98534644847ee833637dae645fed7bfbed5c43ca9a8819e16407f4b1)


4、什么是前向传播？
前向传播：通过输入层输入，一路向前，通过输出层输出的一个结果。如图指的是1 、 x1、x2、xn、与权重（weights）相乘，并且加上偏置值b0，然后进行总的求和，同时通过激活函数激活之后算出结果。这个过程就是前向传播。![](https://ai-studio-static-online.cdn.bcebos.com/a861885f378a449191f82f1536c7f9f8f2f3948ad2064f068fb624708fd87442)


5、什么是反向传播？
通过输出反向更新权重的过程。具体的说输出位置会产生一个模型的输出，通过这个输出以及原数据计算一个差值。将前向计算过程反过来计算。通过差值和学习率更新权重。
![](https://ai-studio-static-online.cdn.bcebos.com/97b8db128fa8459a93dd333568929ffbe1db60f9467f49118d9b627a09fa4a63)


二、代码实践（50分）


```python
# 导入需要用到的package
import numpy as np
import json
import matplotlib.pyplot as plt

def load_data():
    # 从文件导入数据
    datafile = './work/housing.data'
    data = np.fromfile(datafile, sep=' ')

    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数
    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \
                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]
    feature_num = len(feature_names)

    # 将原始数据进行Reshape，变成[N, 14]这样的形状
    data = data.reshape([data.shape[0] // feature_num, feature_num])

    # 将原数据集拆分成训练集和测试集
    # 这里使用80%的数据做训练，20%的数据做测试
    # 测试集和训练集必须是没有交集的
    ratio = 0.8
    offset = int(data.shape[0] * ratio)
    training_data = data[:offset]

    # 计算训练集的最大值，最小值，平均值
    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \
                                 training_data.sum(axis=0) / training_data.shape[0]

    # 对数据进行归一化处理
    for i in range(feature_num):
        #print(maximums[i], minimums[i], avgs[i])
        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])

    # 训练集和测试集的划分比例
    training_data = data[:offset]
    test_data = data[offset:]
    return training_data, test_data



class Network(object):
    def __init__(self, num_of_weights,mid_weights):
        # 随机产生w的初始值
        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights, mid_weights)
        self.b = np.zeros(mid_weights)
        self.w2 = np.random.randn(mid_weights,1)
        self.b2 = 0.

    def sigmoid(self,x):
        # sigmoid激活函数
        return 1 / (1 + np.exp(-x))

    def forward(self, x):
        #前向传播
        z = np.dot(x, self.w) + self.b
        z = self.sigmoid(z)
        z = np.dot(z,self.w2) + self.b2
        return z

    def loss(self, z, y):
        error = z - y
        cost = error * error
        cost = np.mean(cost)
        return cost

    def gradient(self, x, y):
        # 梯度计算
        z = self.forward(x)
        output1 = self.sigmoid(np.dot(x, self.w) + self.b)
        gradient_w = x.T.dot((z-y).dot(self.w2.T) * ((output1)*(1-output1)))
        gradient_b = np.mean((z-y).dot(self.w2.T) * ((output1)*(1-output1)), axis=0)
        gradient_w2 = np.mean((z-y)*output1,axis=0)
        gradient_w2 = gradient_w2[:,np.newaxis]
        gradient_b2 = np.mean((z-y))
        return gradient_w, gradient_b, gradient_w2, gradient_b2

    def update(self, gradient_w, gradient_w2, gradient_b, gradient_b2, eta=0.01):
        self.w = self.w - eta * gradient_w
        self.b = self.b - eta * gradient_b
        self.w2 = self.w2 - eta * gradient_w2
        self.b2 = self.b2 - eta * gradient_b2

    def train(self, x, y, iterations=100, eta=0.01):
        losses = []
        for i in range(iterations):
            z = self.forward(x)
            L = self.loss(z, y)
            gradient_w, gradient_b ,gradient_w2,gradient_b2= self.gradient(x, y)
            self.update(gradient_w, gradient_w2,gradient_b,gradient_b2,eta)
            losses.append(L)
            if (i + 1) % 10 == 0:
                print('iter {}, loss {}'.format(i, L))
        return losses


# 获取数据
train_data, test_data = load_data()
x = train_data[:, :-1]
y = train_data[:, -1:]
# 创建网络
net = Network(13,2)
num_iterations = 1000
# 启动训练
losses = net.train(x, y, iterations=num_iterations, eta=0.01)

# 画出损失函数的变化趋势
plot_x = np.arange(num_iterations)
plot_y = np.array(losses)
plt.plot(plot_x, plot_y)
plt.show()
```

    iter 9, loss 0.18416924832057396
    iter 19, loss 0.12997355753312106
    iter 29, loss 0.09909506957471798
    iter 39, loss 0.07991418645890326
    iter 49, loss 0.06759101678667934
    iter 59, loss 0.059547721441832666
    iter 69, loss 0.054249965591762164
    iter 79, loss 0.05073807563359743
    iter 89, loss 0.048397108532051016
    iter 99, loss 0.04682783533417876
    iter 109, loss 0.04576905567884512
    iter 119, loss 0.045049021959573395
    iter 129, loss 0.0445544133177313
    iter 139, loss 0.044210265966544586
    iter 149, loss 0.043966886342396
    iter 159, loss 0.04379127197642227
    iter 169, loss 0.04366146654214889
    iter 179, loss 0.04356283556921187
    iter 189, loss 0.04348560436102546
    iter 199, loss 0.04342322777690757
    iter 209, loss 0.043371309441774156
    iter 219, loss 0.04332688444417435
    iter 229, loss 0.04328794282713525
    iter 239, loss 0.04325311276363975
    iter 249, loss 0.043221449724071645
    iter 259, loss 0.04319229605115723
    iter 269, loss 0.04316518733692949
    iter 279, loss 0.04313978993041563
    iter 289, loss 0.04311585916517872
    iter 299, loss 0.0430932113864743
    iter 309, loss 0.04307170517566122
    iter 319, loss 0.043051228709559755
    iter 329, loss 0.04303169121621936
    iter 339, loss 0.043013017169423066
    iter 349, loss 0.042995142317247374
    iter 359, loss 0.042978010941487156
    iter 369, loss 0.042961573945482404
    iter 379, loss 0.0429457875015694
    iter 389, loss 0.042930612078446784
    iter 399, loss 0.04291601172811266
    iter 409, loss 0.042901953551617966
    iter 419, loss 0.042888407289297714
    iter 429, loss 0.04287534499878423
    iter 439, loss 0.04286274079590172
    iter 449, loss 0.04285057064143926
    iter 459, loss 0.0428388121620989
    iter 469, loss 0.042827444497479626
    iter 479, loss 0.042816448167365864
    iter 489, loss 0.04280580495522184
    iter 499, loss 0.04279549780490762
    iter 509, loss 0.042785510728400825
    iter 519, loss 0.042775828722840525
    iter 529, loss 0.04276643769558632
    iter 539, loss 0.042757324396253474
    iter 549, loss 0.04274847635488086
    iter 559, loss 0.04273988182553373
    iter 569, loss 0.04273152973475262
    iter 579, loss 0.042723409634346406
    iter 589, loss 0.04271551165809395
    iter 599, loss 0.04270782648197472
    iter 609, loss 0.042700345287593484
    iter 619, loss 0.042693059728502604
    iter 629, loss 0.04268596189915749
    iter 639, loss 0.04267904430626935
    iter 649, loss 0.04267229984234275
    iter 659, loss 0.04266572176120793
    iter 669, loss 0.04265930365537553
    iter 679, loss 0.04265303943505897
    iter 689, loss 0.0426469233087239
    iter 699, loss 0.04264094976503765
    iter 709, loss 0.04263511355610367
    iter 719, loss 0.042629409681875854
    iter 729, loss 0.04262383337565811
    iter 739, loss 0.042618380090602336
    iter 749, loss 0.042613045487126244
    iter 759, loss 0.042607825421178795
    iter 769, loss 0.04260271593328815
    iter 779, loss 0.04259771323833194
    iter 789, loss 0.04259281371597525
    iter 799, loss 0.042588013901726146
    iter 809, loss 0.042583310478562925
    iter 819, loss 0.04257870026909118
    iter 829, loss 0.042574180228191684
    iter 839, loss 0.04256974743612426
    iter 849, loss 0.04256539909205458
    iter 859, loss 0.04256113250797429
    iter 869, loss 0.04255694510298677
    iter 879, loss 0.04255283439793307
    iter 889, loss 0.04254879801033497
    iter 899, loss 0.042544833649633074
    iter 909, loss 0.04254093911270043
    iter 919, loss 0.04253711227961308
    iter 929, loss 0.04253335110966053
    iter 939, loss 0.04252965363758031
    iter 949, loss 0.04252601797000225
    iter 959, loss 0.04252244228208869
    iter 969, loss 0.04251892481435822
    iter 979, loss 0.04251546386968136
    iter 989, loss 0.042512057810437126
    iter 999, loss 0.04250870505582093



![png](https://github.com/jiegege527/awesome-DeepLearning/blob/master/housing%20price%20loss.png)

