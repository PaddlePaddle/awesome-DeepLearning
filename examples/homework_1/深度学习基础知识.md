# 深度学习基础知识

## 深度学习发展历史
1.第一次兴起--神经网络启蒙
1943年 由神经科学家麦卡洛克(W.S.McCilloch) 和数学家皮兹（W.Pitts）在《数学生物物理学公告》上发表论文《神经活动中内在思想的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity）。建立了神经网络和数学模型，称为MCP模型。所谓MCP模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，也就诞生了所谓的“模拟大脑”，人工神经网络的大门由此开启。MCP当时是希望能够用计算机来模拟人的神经元反应的过程，该模型将神经元简化为了三个过程：输入信号线性加权，求和，非线性激活（阈值法）
1958年 计算机科学家罗森布拉特（ Rosenblatt）提出了两层神经元组成的神经网络，称之为“感知器”(Perceptrons)。第一次将MCP用于机器学习（machine learning）分类(classification)。“感知器”算法算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年,该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。

2.第一次低谷---成也萧何败也萧何
1969年，美国数学家及人工智能先驱 Marvin Minsky 在其著作中证明了感知器本质上是一种线性模型（linear model），只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了将近20年的停滞。

3.第二次兴起--BP网络与激活函数
1986年由神经网络之父 Geoffrey Hinton 在1986年发明了适用于多层感知器（MLP）的BP（Backpropagation）算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。注：Sigmoid 函数是一个在生物学中常见的S型的函数，也称为S型生长曲线。在信息科学中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间

4.第二次低谷--屋漏偏逢连夜雨
1991年BP算法被指出存在梯度消失问题，也就是说在误差梯度后项传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该问题直接阻碍了深度学习的进一步发展。
此外90年代中期，支持向量机算法诞生（SVM算法）等各种浅层机器学习模型被提出，SVM也是一种有监督的学习模型，应用于模式识别，分类以及回归分析等。支持向量机以统计学为基础，和神经网络有明显的差异，支持向量机等算法的提出再次阻碍了深度学习的发展。

5.第三次兴起--待到秋来九月八，我花开后百花杀。冲天香阵透长安，满城尽带黄金甲。
发展期 2006年 - 2012年
2006年，加拿大多伦多大学教授、机器学习领域泰斗、神经网络之父—— Geoffrey Hinton 和他的学生 Ruslan Salakhutdinov 在顶尖学术刊物《科学》上发表了一篇文章，该文章提出了深层网络训练中梯度消失问题的解决方案：无监督预训练对权值进行初始化+有监督训练微调。斯坦福大学、纽约大学、加拿大蒙特利尔大学等成为研究深度学习的重镇，至此开启了深度学习在学术界和工业界的浪潮。
2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。2011年以来，微软首次将DL应用在语音识别上，取得了重大突破。微软研究院和Google的语音识别研究人员先后采用DNN技术降低语音识别错误率20％~30％，是语音识别领域十多年来最大的突破性进展。2012年，DNN技术在图像识别领域取得惊人的效果，在ImageNet评测上将错误率从26％降低到15％。
在这一年，DNN还被应用于制药公司的DrugeActivity预测问题，并获得世界最好成绩。
6.爆发期 2012 - 2017
2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。

## 人工智能，机器学习，深度学习有什么区别和联系
机器学习是实现人工智能的一种方法。机器学习的概念来自早期的人工智能研究者，已经研究出的算法包括决策树学习、归纳逻辑编程、增强学习和贝叶斯网络等。简单来说，机器学习就是使用算法分析数据，从中学习并做出推断或预测。与传统的使用特定指令集手写软件不同，我们使用大量数据和算法来“训练”机器，由此带来机器学习如何完成任务。
深度学习是实现机器学习的一种技术。早期机器学习研究者中还开发了一种叫人工神经网络的算法，但是发明之后数十年都默默无闻。神经网络是受人类大脑的启发而来的：神经元之间的相互连接关系。但是，人类大脑中的神经元可以与特定范围内的任意神经元连接，而人工神经网络中数据传播要经历不同的层，传播方向也不同。
![](C:\Users\jasonzhao\jason\awesome-DeepLearning\examples\homework_1\aimldl.png)

## 神经元，单层感知机，多层感知机
神经元的基本模型实际就是一个单层感知机的模型，多层感知机是多个单层感知机逐层连接形成的一种模型，相对于只能处理线性问题的单层感知机来说，多层感知机更加适用于非线性问题，比如异或运算至少需要两层感知机才能实现。
神经元：
![](C:\Users\jasonzhao\jason\awesome-DeepLearning\examples\homework_1\sjy.png)

![](C:\Users\jasonzhao\jason\awesome-DeepLearning\examples\homework_1\sjy1.png)
单层感知机：
![](C:\Users\jasonzhao\jason\awesome-DeepLearning\examples\homework_1\gzj.png)
多层感知机：
![](C:\Users\jasonzhao\jason\awesome-DeepLearning\examples\homework_1\dcgzj.png)


## 什么是前向传播

如图所示，前向传播的思想比较简单。
举个例子，假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。
最终不断的通过这种方法一层层的运算，得到输出层结果。

![](C:\Users\jasonzhao\jason\awesome-DeepLearning\examples\homework_1\qxcb.png)

## 什么是反向传播
就是训练模型参数，在所有参数上用梯度下降等方法通过损失函数对参数求导得到的结果对权重和偏置进行逐层调整，使NN 模型在下一步的训练数据上的损失函数最小