## 层归一化方法详解
**概念**
深度神经网络的训练是具有高度的计算复杂性的。减少训练的时间成本的一种方法是对神经元的输入进行规范化处理进而加快网络的收敛速度。层规范化是在训练时和测试时对数据同时进行处理，通过对输入同一层的数据进行汇总，计算平均值和方差，来对每一层的输入数据做规范化处理。层规范化是基于批规范化进行优化得到的。相比较而言，批规范化是对一个神经元输入的数据以mini-batch为单位来进行汇总，计算平均值和方法，再用这个数据对每个训练样例的输入进行规整。层规范化在面对RNN等问题的时候效果更加优越，也不会受到mini-batch选值的影响。

**算法流程**
1.计算网络层的均值与方差
2.进行归一化
3.设置可训练的缩放及偏移

**作用**
层归一化和批归一化不同的是，层归一化是对一个中间层的所有神经元进行归一化。

**应用场景**
在标准循环神经网络中，循环神经网络的净输入一般会随着时间慢慢变大或变小，从而导致梯度爆炸或消失。而层归一化的循环神经网络可以有效地缓解这种状况。

对于 k个样本的一个小批量集合，层归一化是矩阵Zl对每一列进行归一化，而批量归一化是对每一行进行归一化。一般而言，批归一化是一种更好的选择，当小批量样本数量比较小时，可以选择层归一化。


## 可变形卷积方法详解
**DCN v1**
1.背景
在计算机视觉领域，同一物体在不同场景，角度中未知的几何变换是检测/识别的一大挑战，通常来说我们有两种做法:

(1)通过充足的数据增强，扩充足够多的样本去增强模型适应尺度变换的能力。

(2)设置一些针对几何变换不变的特征或者算法，比如SIFT和sliding windows。

两种方法都有缺陷，第一种方法因为样本的局限性显然模型的泛化能力比较低，无法泛化到一般场景中，第二种方法则因为手工设计的不变特征和算法对于过于复杂的变换是很难的而无法设计。所以作者提出了Deformable Conv（可变形卷积）和 Deformable Pooling（可变形池化）来解决这个问题。

2.可变形卷积
可变形卷积顾名思义就是卷积的位置是可变形的，并非在传统的N × N的网格上做卷积，这样的好处就是更准确地提取到我们想要的特征（传统的卷积仅仅只能提取到矩形框的特征），通过一张图我们可以更直观地了解：
在上面这张图里面，左边传统的卷积显然没有提取到完整绵羊的特征，而右边的可变形卷积则提取到了完整的不规则绵羊的特征。

那可变卷积实际上是怎么做的呢？其实就是在每一个卷积采样点加上了一个偏移量，如下图所示：

(a) 所示的正常卷积规律的采样 9 个点（绿点），(b)(c)(d) 为可变形卷积，在正常的采样坐标上加上一个位移量（蓝色箭头），其中 (c)(d) 作为 (b) 的特殊情况，展示了可变形卷积可以作为尺度变换，比例变换和旋转变换等特殊情况。
我们先看普通的卷积，以3x3卷积为例对于每个输出y(p0)，都要从x上采样9个位置，这9个位置都在中心位置x(p0)向四周扩散，(-1,-1)代表x(p0)的左上角，(1,1)代表x(p0)的右下角。

所以传统的卷积输出就是（其中就是网格中的n个点）：

正如我们上面阐述的可变形卷积，他就是在传统的卷积操作上加入了一个偏移量，正是这个偏移量才让卷积变形为不规则的卷积，这里要注意这个偏移量可以是小数，所以下面的式子的特征值需要通过双线性插值的方法来计算。：

那这个偏移量如何算呢？我们来看：

对于输入的一张feature map，假设原来的卷积操作是3×3的，那么为了学习偏移量offset，我们定义另外一个3×3的卷积层（图中上面的那层），输出的维度其实就是原来feature map大小，channel数等于2N（分别表示x,y方向的偏移）。下面的可变形卷积可以看作先基于上面那部分生成的offset做了一个插值操作，然后再执行普通的卷积。

3.池化
理解了可变形卷积之后，Deformable RoIPooling（可变形池化）就比较好理解了。原始的RoIPooling在操作过程中是将RoI划分为k×k个子区域。而可变形池化的偏移量其实就是子区域的偏移。同理每一个子区域都有一个偏移，偏移量对应子区域有k×k个。与可变形卷积不同的是，可变形池化的偏移量是通过全连接层得到的。因为和可变形卷积类似，这里就不多讲了。


**DCN v2**
1.背景
DCN v1听起来不错，但其实也有问题：我们的可变形卷积有可能引入了无用的上下文（区域）来干扰我们的特征提取，这显然会降低算法的表现。作者也做了一个实验进行对比说明：

我们可以看到虽然DCN v1更能覆盖整个物体，但是同时也会引入一些无关的背景，这造成了干扰，所以作者提出了三个解决方法：
（1）More Deformable Conv Layers（使用更多的可变形卷积）。

（2）Modulated Deformable Modules（在DCNv1基础（添加offset）上添加每个采样点的权重）

（3）R-CNN Feature Mimicking（模拟R-CNN的feature）。

在DCN v1中只在conv 5中使用了三个可变形卷积，在DCN v2中把conv3到conv5都换成了可变形卷积，提高算法对几何形变的建模能力。

在DCNv1基础（添加offset）上添加每个采样点的权重。
