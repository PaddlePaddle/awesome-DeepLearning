层归一化方法详解

\*\*概念\*\*

深度神经网络的训练是具有高度的计算复杂性的。减少训练的时间成本的一种方法是对神经元的输入进行规范化处理进而加快网络的收敛速度。层规范化是在训练时和测试时对数据同时进行处理，通过对输入同一层的数据进行汇总，计算平均值和方差，来对每一层的输入数据做规范化处理。层规范化是基于批规范化进行优化得到的。相比较而言，批规范化是对一个神经元输入的数据以mini-batch为单位来进行汇总，计算平均值和方法，再用这个数据对每个训练样例的输入进行规整。层规范化在面对RNN等问题的时候效果更加优越，也不会受到mini-batch选值的影响。

\*\*算法流程\*\*

1.计算网络层的均值与方差

2.进行归一化

3.设置可训练的缩放及偏移

\*\*作用\*\*

层归一化和批归一化不同的是，层归一化是对一个中间层的所有神经元进行归一化。

\*\*应用场景\*\*

在标准循环神经网络中，循环神经网络的净输入一般会随着时间慢慢变大或变小，从而导致梯度爆炸或消失。而层归一化的循环神经网络可以有效地缓解这种状况。

对于
k个样本的一个小批量集合，层归一化是矩阵Zl对每一列进行归一化，而批量归一化是对每一行进行归一化。一般而言，批归一化是一种更好的选择，当小批量样本数量比较小时，可以选择层归一化。
