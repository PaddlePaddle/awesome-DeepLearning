# ResNet及其Vd系列

## 概述

​		ResNet系列模型是在2015年提出的，一举在ILSVRC2015比赛中取得冠军，top5错误率为3.57%。该网络创新性的提出了残差结构，通过堆叠多个残差结构从而构建了ResNet网络。实验表明使用残差块可以有效地提升收敛速度和精度。

​		斯坦福大学的Joyce Xu将ResNet称为「真正重新定义了我们看待神经网络的方式」的三大架构之一。由于ResNet卓越的性能，越来越多的来自学术界和工业界学者和工程师对其结构进行了改进，比较出名的有Wide-ResNet, ResNet-vc ,ResNet-vd, Res2Net等，其中ResNet-vc与ResNet-vd的参数量和计算量与ResNet几乎一致，所以在此我们将其与ResNet统一归为ResNet系列。

​		本次发布ResNet系列的模型包括ResNet50，ResNet50_vd，ResNet50_vd_ssld，ResNet200_vd等14个预训练模型。在训练层面上，ResNet的模型采用了训练ImageNet的标准训练流程，而其余改进版模型采用了更多的训练策略，如learning rate的下降方式采用了cosine decay，引入了label smoothing的标签正则方式，在数据预处理加入了mixup的操作，迭代总轮数从120个epoch增加到200个epoch。

​		其中，ResNet50_vd_v2与ResNet50_vd_ssld采用了知识蒸馏，保证模型结构不变的情况下，进一步提升了模型的精度，具体地，ResNet50_vd_v2的teacher模型是ResNet152_vd（top1准确率80.59%），数据选用的是ImageNet-1k的训练集，ResNet50_vd_ssld的teacher模型是ResNeXt101_32x16d_wsl（top1准确率84.2%），数据选用结合了ImageNet-1k的训练集和ImageNet-22k挖掘的400万数据。知识蒸馏的具体方法正在持续更新中。



​		通过前面几个经典模型学习，我们可以发现随着深度学习的不断发展，模型的层数越来越多，网络结构也越来越复杂。那么是否加深网络结构，就一定会得到更好的效果呢？从理论上来说，假设新增加的层都是恒等映射，只要原有的层学出跟原模型一样的参数，那么深模型结构就能达到原模型结构的效果。换句话说，原模型的解只是新模型的解的子空间，在新模型解的空间里应该能找到比原模型解对应的子空间更好的结果。但是实践表明，增加网络的层数之后，训练误差往往不降反升。

​		Kaiming He等人提出了残差网络ResNet来解决上述问题，其基本思想如 **图6**所示。

- 图6(a)：表示增加网络的时候，将*x*映射成*y*=*F*(*x*)输出。
- 图6(b)：对图6(a)作了改进，输出*y*=*F*(*x*)+*x*。这时不是直接学习输出特征y的表示，而是学习*y*−*x*
  - 如果想学习出原模型的表示，只需将*F*(*x*)的参数全部设置为0，则*y*=*x*是恒等映射。
  - *F*(*x*)=*y*−*x*也叫做残差项，如*x*→*y*的映射接近恒等映射，图6(b)中通过学习残差项也比图6(a)学习完整映射形式更加容易。

![img](https://ai-studio-static-online.cdn.bcebos.com/e10f22f054704daabf4261ab46719629a36749631db74eb0a368499de3e5d3d6)


图6：残差块设计思想





图6(b)的结构是残差网络的基础，这种结构也叫做残差块（Residual block）。输入*x*通过跨层连接，能更快的向前传播数据，或者向后传播梯度。通俗的比喻，在火热的电视节目《王牌对王牌》上有一个“传声筒”的游戏，排在队首的嘉宾把看到的影视片段表演给后面一个嘉宾看，经过四五个嘉宾后，最后一个嘉宾如果能表演出更多原剧的内容，就能取得高分。我们常常会发现刚开始的嘉宾往往表演出最多的信息（类似于Loss），而随着表演的传递，有效的表演信息越来越少（类似于梯度弥散）。如果每个嘉宾都能看到原始的影视片段，那么相信传声筒的效果会好很多。类似的，由于ResNet每层都存在直连的旁路，相当于每一层都和最终的损失有“直接对话”的机会，自然可以更好的解决梯度弥散的问题。残差块的具体设计方案如 **图**7 所示，这种设计方案也常称作瓶颈结构（BottleNeck）。1*1的卷积核可以非常方便的调整中间层的通道数，在进入3*3的卷积层之前减少通道数（256->64），经过该卷积层后再恢复通道数(64->256)，可以显著减少网络的参数量。这个结构（256->64->256）像一个中间细，两头粗的瓶颈，所以被称为“BottleNeck”。



![img](https://ai-studio-static-online.cdn.bcebos.com/322b26358d43401ba81546dd134a310cfb11ecafb3314aab88b5885ff642870b)


图7：残差块结构示意图





下图表示出了ResNet-50的结构，一共包含49层卷积和1层全连接，所以被称为ResNet-50。





![img](https://ai-studio-static-online.cdn.bcebos.com/8f42b3b5b7b34e45847a9c61580f1f8239a80ca6fa67448e8baeeb0209a2d556)


图8：ResNet-50模型网络结构示意图











- 下面给出了ResNet的核心结构：**残差模块**
  ![img](https://img-blog.csdnimg.cn/20200606174503445.png)
- 在残差结构中，一个支路经过各种卷积运行，另一个支路直接连接到输出，这两个支路相加之后得到输出，相当于卷积计算的支路只需要计算残差项，这大大降低了模型训练过程中的学习难度。下面也给出了ResNet18的结构。

![img](https://img-blog.csdnimg.cn/20200606182546330.png)

- 上面是标准的ResNet结构，李沐大神及其团队在后来对ResNet做了一系列的改进。下面给出了最左边是最开始的ResNet-Va结构，Vb对这个左边的特征变换通路的降采样卷积做了调整，把降采样的步骤从最开始的第一个1x1卷积调整到中间的3x3卷积中；Vc结构则是将最开始这个7x7的卷积变成3个3x3的卷积，在感受野不变的情况下减少了存储；而Vd是修改了降采样残差模块右边的特征通路。把降采样的过程由平均池化这个操作去替代了，这一系列的改进，几乎没有带来新增的预测耗时，结合适当的训练策略，比如说标签平滑以及mixup这种数据增广方式，精度可以提升高达2.5%。

![img](https://img-blog.csdnimg.cn/20200606183032524.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI1MjYwMDM=,size_16,color_FFFFFF,t_70)

更多关于改进结构的描述，可以参考这篇论文：https://arxiv.org/abs/1812.01187