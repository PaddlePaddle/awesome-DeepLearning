{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "import paddle.nn.functional as F\r\n",
    "import math\r\n",
    "\r\n",
    "\r\n",
    "class DNNLayer(nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(DNNLayer, self).__init__()\r\n",
    "        self.layer1 = 39\r\n",
    "        self.layer2 = 64\r\n",
    "        self.layer3 = 90\r\n",
    "        self.layer4 = 32\r\n",
    "        self.fclayer1 = nn.Linear(in_features=self.layer1,out_features=self.layer2)\r\n",
    "        self.fclayer2 = nn.Linear(in_features=self.layer2,out_features=self.layer3) \r\n",
    "        self.fclayer3 = nn.Linear(in_features=self.layer3,out_features=self.layer4)\r\n",
    "        self.fclayer4 = nn.Linear(in_features=self.layer4,out_features=2)\r\n",
    "        self.dropout = nn.Dropout(p=0.5)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.fclayer1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.dropout(x)\r\n",
    "        x = self.fclayer2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.fclayer3(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        output = self.fclayer4(x)\r\n",
    "        return output\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train_data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5ff31df37c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data/train_data.txt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0moutput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;31m# print(output_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0moutput_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5ff31df37c9e>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moutput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train_data.txt'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from paddle.io import IterableDataset\r\n",
    "\r\n",
    "\r\n",
    "class RecDataset(IterableDataset):\r\n",
    "    def __init__(self, file_list, config):\r\n",
    "        super(RecDataset, self).__init__()\r\n",
    "        self.file_list = file_list\r\n",
    "        self.init()\r\n",
    "\r\n",
    "    def init(self):\r\n",
    "        from operator import mul\r\n",
    "        padding = 0\r\n",
    "        sparse_slots = \"click 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\"\r\n",
    "        self.sparse_slots = sparse_slots.strip().split(\" \")\r\n",
    "        self.dense_slots = [\"dense_feature\"]\r\n",
    "        self.dense_slots_shape = [13]\r\n",
    "        self.slots = self.sparse_slots + self.dense_slots\r\n",
    "        self.slot2index = {}\r\n",
    "        self.visit = {}\r\n",
    "        for i in range(len(self.slots)):\r\n",
    "            self.slot2index[self.slots[i]] = i\r\n",
    "            self.visit[self.slots[i]] = False\r\n",
    "        self.padding = padding\r\n",
    "\r\n",
    "    def __iter__(self):\r\n",
    "        full_lines = []\r\n",
    "        self.data = []\r\n",
    "        output_list = []\r\n",
    "        for file in self.file_list:\r\n",
    "            with open(file, \"r\") as rf:\r\n",
    "                for l in rf:\r\n",
    "                    line = l.strip().split(\" \")\r\n",
    "                    output = [(i, []) for i in self.slots]\r\n",
    "                    for i in line:\r\n",
    "                        slot_feasign = i.split(\":\")\r\n",
    "                        slot = slot_feasign[0]\r\n",
    "                        if slot not in self.slots:\r\n",
    "                            continue\r\n",
    "                        if slot in self.sparse_slots:\r\n",
    "                            feasign = int(slot_feasign[1])\r\n",
    "                        else:\r\n",
    "                            feasign = float(slot_feasign[1])\r\n",
    "                        output[self.slot2index[slot]][1].append(feasign)\r\n",
    "                        self.visit[slot] = True\r\n",
    "                    for i in self.visit:\r\n",
    "                        slot = i\r\n",
    "                        if not self.visit[slot]:\r\n",
    "                            if i in self.dense_slots:\r\n",
    "                                output[self.slot2index[i]][1].extend(\r\n",
    "                                    [self.padding] *\r\n",
    "                                    self.dense_slots_shape[self.slot2index[i]])\r\n",
    "                            else:\r\n",
    "                                output[self.slot2index[i]][1].extend(\r\n",
    "                                    [self.padding])\r\n",
    "                        else:\r\n",
    "                            self.visit[slot] = False\r\n",
    "                    # sparse\r\n",
    "                    \r\n",
    "                    for key, value in output[:-1]:\r\n",
    "                        # output_list.append(np.array(value).astype('int64'))\r\n",
    "                        output_list.append((value[0]))\r\n",
    "                    # dense\r\n",
    "                    # output_list.append(\r\n",
    "                        # np.array(output[-1][1]).astype(\"float32\"))\r\n",
    "                    for k in output[-1][1]:\r\n",
    "                       output_list.append(k)\r\n",
    "                    # list\r\n",
    "                    # print(output)\r\n",
    "        return output_list\r\n",
    "rec = RecDataset([\"data/train_data.txt\"],1)\r\n",
    "rec.init()\r\n",
    "output_list = rec.__iter__()\r\n",
    "# print(output_list)\r\n",
    "output_set = np.array(output_list)\r\n",
    "output_set = np.reshape(output_set,[80,40])\r\n",
    "print(output_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 39)\n",
      "(80, 1)\n",
      "Tensor(shape=[80], dtype=int64, place=CPUPlace, stop_gradient=True,\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n",
      "Tensor(shape=[80], dtype=int64, place=CPUPlace, stop_gradient=True,\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "#训练函数\r\n",
    "from paddle.metric import Accuracy\r\n",
    "input_dim =39\r\n",
    "layer1_output = 64\r\n",
    "layer2_output = 80\r\n",
    "layer3_output = 32\r\n",
    "data_num=80\r\n",
    "# model =DNNLayer(input_dim,layer1_output,layer2_output,layer3_output)\r\n",
    "val_acc_history = []\r\n",
    "val_loss_history = []\r\n",
    "train_set = output_set[:,1:]\r\n",
    "# train_set.dtype='float32'\r\n",
    "print(train_set.shape)\r\n",
    "label = np.reshape(output_set[:,0],[data_num,1])\r\n",
    "label = np.float64(label)\r\n",
    "print(label.shape)\r\n",
    "# train_dataset = paddle.empty(shape=[data_num,2])\r\n",
    "\r\n",
    "# train_dataset = np.hstack((train_set,label))\r\n",
    "# train_dataset = paddle.to_tensor(train_set)\r\n",
    "# for i in range(data_num):\r\n",
    "#     train_dataset[i,0]=paddle.to_tensor(train_set[i,:],dtype='float32')\r\n",
    "#     train_dataset[i,1]=paddle.to_tensor(label[i,0])\r\n",
    "\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "from paddle.io import Dataset\r\n",
    "\r\n",
    "# define a random dataset\r\n",
    "class RandomDataset(Dataset):\r\n",
    "    def __init__(self, num_samples,train_set,label_set):\r\n",
    "        self.num_samples = num_samples\r\n",
    "        self.train_set = train_set\r\n",
    "        self.label_set = label_set\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        example = paddle.to_tensor(self.train_set[idx,:],dtype='float32')\r\n",
    "        label = paddle.to_tensor(self.label_set[idx,0],dtype='int64')\r\n",
    "        return example, label\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return self.num_samples\r\n",
    "\r\n",
    "dataset = RandomDataset(80,train_set,label)\r\n",
    "for i in range(2):\r\n",
    "    print(dataset[:][1])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ... \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DNNLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-45bee5dc2e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         )\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-45bee5dc2e48>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start training ... '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDNNLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 用Model封装模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DNNLayer' is not defined"
     ]
    }
   ],
   "source": [
    "def train(epochs):\r\n",
    "    print('start training ... ')\r\n",
    "\r\n",
    "    model = paddle.Model(DNNLayer())   # 用Model封装模型\r\n",
    "    optim = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())\r\n",
    "    \r\n",
    "    # 配置模型\r\n",
    "    model.prepare(\r\n",
    "        optim,\r\n",
    "        paddle.nn.CrossEntropyLoss(),\r\n",
    "        Accuracy()\r\n",
    "    )\r\n",
    "    model.fit(dataset,\r\n",
    "        # dataset[:][1],\r\n",
    "        epochs=epochs,\r\n",
    "        batch_size=8,\r\n",
    "        verbose=1\r\n",
    "        )\r\n",
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-888e8fd0a412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"feat_dict needed for criteo reader.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mcriteo_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mcriteo_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_from_stdin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-888e8fd0a412>\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, feat_dict_name)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinuous_range_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_range_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_dict_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_dict_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "import sys\r\n",
    "import paddle.fluid.incubate.data_generator as dg\r\n",
    "try:\r\n",
    "    import cPickle as pickle\r\n",
    "except ImportError:\r\n",
    "    import pickle\r\n",
    "from collections import Counter\r\n",
    "import os\r\n",
    "\r\n",
    "\r\n",
    "class CriteoDataset(dg.MultiSlotDataGenerator):\r\n",
    "    def setup(self, feat_dict_name):\r\n",
    "        self.cont_min_ = [0, -3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\r\n",
    "        self.cont_max_ = [\r\n",
    "            5775, 257675, 65535, 969, 23159456, 431037, 56311, 6047, 29019, 46,\r\n",
    "            231, 4008, 7393\r\n",
    "        ]\r\n",
    "        self.cont_diff_ = [\r\n",
    "            self.cont_max_[i] - self.cont_min_[i]\r\n",
    "            for i in range(len(self.cont_min_))\r\n",
    "        ]\r\n",
    "        self.continuous_range_ = range(1, 14)\r\n",
    "        self.categorical_range_ = range(14, 40)\r\n",
    "        self.feat_dict_ = pickle.load(open(feat_dict_name, 'rb'))\r\n",
    "\r\n",
    "    def _process_line(self, line):\r\n",
    "        features = line.rstrip('\\n').split('\\t')\r\n",
    "        feat_idx = []\r\n",
    "        feat_value = []\r\n",
    "        for idx in self.continuous_range_:\r\n",
    "            if features[idx] == '':\r\n",
    "                feat_idx.append(0)\r\n",
    "                feat_value.append(0.0)\r\n",
    "            else:\r\n",
    "                feat_idx.append(self.feat_dict_[idx])\r\n",
    "                feat_value.append(\r\n",
    "                    (float(features[idx]) - self.cont_min_[idx - 1]) /\r\n",
    "                    self.cont_diff_[idx - 1])\r\n",
    "        for idx in self.categorical_range_:\r\n",
    "            if features[idx] == '' or features[idx] not in self.feat_dict_:\r\n",
    "                feat_idx.append(0)\r\n",
    "                feat_value.append(0.0)\r\n",
    "            else:\r\n",
    "                feat_idx.append(self.feat_dict_[features[idx]])\r\n",
    "                feat_value.append(1.0)\r\n",
    "        label = [int(features[0])]\r\n",
    "        return feat_idx, feat_value, label\r\n",
    "\r\n",
    "    def test(self, filelist):\r\n",
    "        def local_iter():\r\n",
    "            for fname in filelist:\r\n",
    "                with open(fname.strip(), 'r') as fin:\r\n",
    "                    for line in fin:\r\n",
    "                        feat_idx, feat_value, label = self._process_line(line)\r\n",
    "                        yield [feat_idx, feat_value, label]\r\n",
    "\r\n",
    "        return local_iter\r\n",
    "\r\n",
    "    def generate_sample(self, line):\r\n",
    "        def data_iter():\r\n",
    "            feat_idx, feat_value, label = self._process_line(line)\r\n",
    "            yield [('feat_idx', feat_idx), ('feat_value', feat_value), ('label',\r\n",
    "                                                                        label)]\r\n",
    "\r\n",
    "        return data_iter\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    import paddle\r\n",
    "    paddle.enable_static()\r\n",
    "    criteo_dataset = CriteoDataset()\r\n",
    "    if len(sys.argv) <= 1:\r\n",
    "        sys.stderr.write(\"feat_dict needed for criteo reader.\")\r\n",
    "        exit(1)\r\n",
    "    criteo_dataset.setup(sys.argv[1])\r\n",
    "    criteo_dataset.run_from_stdin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7b7cd3ab3466>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfluid\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfluid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcriteo_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCriteoDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetwork_conf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mctr_deepfm_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'args'"
     ]
    }
   ],
   "source": [
    "import logging\r\n",
    "import numpy as np\r\n",
    "import pickle\r\n",
    "\r\n",
    "# disable gpu training for this example \r\n",
    "import os\r\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\r\n",
    "import paddle\r\n",
    "import paddle.fluid as fluid\r\n",
    "\r\n",
    "from args import parse_args\r\n",
    "from criteo_reader import CriteoDataset\r\n",
    "from network_conf import ctr_deepfm_model\r\n",
    "import utils\r\n",
    "\r\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s')\r\n",
    "logger = logging.getLogger('fluid')\r\n",
    "logger.setLevel(logging.INFO)\r\n",
    "\r\n",
    "\r\n",
    "def infer():\r\n",
    "    args = parse_args()\r\n",
    "\r\n",
    "    place = fluid.CPUPlace()\r\n",
    "    inference_scope = fluid.Scope()\r\n",
    "\r\n",
    "    test_files = [\r\n",
    "        os.path.join(args.test_data_dir, x)\r\n",
    "        for x in os.listdir(args.test_data_dir)\r\n",
    "    ]\r\n",
    "    criteo_dataset = CriteoDataset()\r\n",
    "    criteo_dataset.setup(args.feat_dict)\r\n",
    "    test_reader = fluid.io.batch(\r\n",
    "        criteo_dataset.test(test_files), batch_size=args.batch_size)\r\n",
    "\r\n",
    "    startup_program = fluid.framework.Program()\r\n",
    "    test_program = fluid.framework.Program()\r\n",
    "    cur_model_path = os.path.join(args.model_output_dir,\r\n",
    "                                  'epoch_' + args.test_epoch)\r\n",
    "\r\n",
    "    with fluid.scope_guard(inference_scope):\r\n",
    "        with fluid.framework.program_guard(test_program, startup_program):\r\n",
    "            loss, auc, data_list, auc_states = ctr_deepfm_model(\r\n",
    "                args.embedding_size, args.num_field, args.num_feat,\r\n",
    "                args.layer_sizes, args.act, args.reg)\r\n",
    "\r\n",
    "            exe = fluid.Executor(place)\r\n",
    "            feeder = fluid.DataFeeder(feed_list=data_list, place=place)\r\n",
    "            main_program = fluid.default_main_program()\r\n",
    "            fluid.load(main_program, cur_model_path, exe)\r\n",
    "            for var in auc_states:  # reset auc states\r\n",
    "                set_zero(var.name, scope=inference_scope, place=place)\r\n",
    "\r\n",
    "            loss_all = 0\r\n",
    "            num_ins = 0\r\n",
    "            for batch_id, data_test in enumerate(test_reader()):\r\n",
    "                loss_val, auc_val = exe.run(test_program,\r\n",
    "                                            feed=feeder.feed(data_test),\r\n",
    "                                            fetch_list=[loss.name, auc.name])\r\n",
    "                num_ins += len(data_test)\r\n",
    "                loss_all += loss_val\r\n",
    "                logger.info('TEST --> batch: {} loss: {} auc_val: {}'.format(\r\n",
    "                    batch_id, loss_all / num_ins, auc_val))\r\n",
    "\r\n",
    "            print(\r\n",
    "                'The last log info is the total Logloss and AUC for all test data. '\r\n",
    "            )\r\n",
    "\r\n",
    "\r\n",
    "def set_zero(var_name,\r\n",
    "             scope=fluid.global_scope(),\r\n",
    "             place=fluid.CPUPlace(),\r\n",
    "             param_type=\"int64\"):\r\n",
    "    \"\"\"\r\n",
    "    Set tensor of a Variable to zero.\r\n",
    "    Args:\r\n",
    "        var_name(str): name of Variable\r\n",
    "        scope(Scope): Scope object, default is fluid.global_scope()\r\n",
    "        place(Place): Place object, default is fluid.CPUPlace()\r\n",
    "        param_type(str): param data type, default is int64\r\n",
    "    \"\"\"\r\n",
    "    param = scope.var(var_name).get_tensor()\r\n",
    "    param_array = np.zeros(param._get_dims()).astype(param_type)\r\n",
    "    param.set(param_array, place)\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    import paddle\r\n",
    "    paddle.enable_static()\r\n",
    "    utils.check_version()\r\n",
    "    infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'args'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0c8beaa0d42d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfluid\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfluid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetwork_conf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mctr_deepfm_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'args'"
     ]
    }
   ],
   "source": [
    "from args import parse_args\r\n",
    "import os\r\n",
    "import paddle.fluid as fluid\r\n",
    "import sys\r\n",
    "from network_conf import ctr_deepfm_model\r\n",
    "import time\r\n",
    "import numpy\r\n",
    "import pickle\r\n",
    "import utils\r\n",
    "\r\n",
    "\r\n",
    "def train():\r\n",
    "    args = parse_args()\r\n",
    "    # add ce\r\n",
    "    if args.enable_ce:\r\n",
    "        SEED = 102\r\n",
    "        fluid.default_main_program().random_seed = SEED\r\n",
    "        fluid.default_startup_program().random_seed = SEED\r\n",
    "\r\n",
    "    print('---------- Configuration Arguments ----------')\r\n",
    "    for key, value in args.__dict__.items():\r\n",
    "        print(key + ':' + str(value))\r\n",
    "\r\n",
    "    if not os.path.isdir(args.model_output_dir):\r\n",
    "        os.mkdir(args.model_output_dir)\r\n",
    "\r\n",
    "    loss, auc, data_list, auc_states = ctr_deepfm_model(\r\n",
    "        args.embedding_size, args.num_field, args.num_feat, args.layer_sizes,\r\n",
    "        args.act, args.reg)\r\n",
    "    optimizer = fluid.optimizer.SGD(\r\n",
    "        learning_rate=args.lr,\r\n",
    "        regularization=fluid.regularizer.L2DecayRegularizer(args.reg))\r\n",
    "    optimizer.minimize(loss)\r\n",
    "\r\n",
    "    exe = fluid.Executor(fluid.CPUPlace())\r\n",
    "    exe.run(fluid.default_startup_program())\r\n",
    "\r\n",
    "    dataset = fluid.DatasetFactory().create_dataset()\r\n",
    "    dataset.set_use_var(data_list)\r\n",
    "    pipe_command = 'python criteo_reader.py {}'.format(args.feat_dict)\r\n",
    "    dataset.set_pipe_command(pipe_command)\r\n",
    "    dataset.set_batch_size(args.batch_size)\r\n",
    "    dataset.set_thread(args.num_thread)\r\n",
    "    train_filelist = [\r\n",
    "        os.path.join(args.train_data_dir, x)\r\n",
    "        for x in os.listdir(args.train_data_dir)\r\n",
    "    ]\r\n",
    "\r\n",
    "    print('---------------------------------------------')\r\n",
    "    for epoch_id in range(args.num_epoch):\r\n",
    "        start = time.time()\r\n",
    "        dataset.set_filelist(train_filelist)\r\n",
    "        exe.train_from_dataset(\r\n",
    "            program=fluid.default_main_program(),\r\n",
    "            dataset=dataset,\r\n",
    "            fetch_list=[loss, auc],\r\n",
    "            fetch_info=['epoch %d batch loss' % (epoch_id + 1), \"auc\"],\r\n",
    "            print_period=1000,\r\n",
    "            debug=False)\r\n",
    "        model_dir = os.path.join(args.model_output_dir,\r\n",
    "                                 'epoch_' + str(epoch_id + 1))\r\n",
    "        sys.stderr.write('epoch%d is finished and takes %f s\\n' % (\r\n",
    "            (epoch_id + 1), time.time() - start))\r\n",
    "        main_program = fluid.default_main_program()\r\n",
    "        fluid.io.save(main_program, model_dir)\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    import paddle\r\n",
    "    paddle.enable_static()\r\n",
    "    utils.check_version()\r\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
