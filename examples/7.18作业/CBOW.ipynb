{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 15.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totoally 253854 different words in the corpus\n",
      "word the, its id 0, its word freq 1061396\n",
      "word of, its id 1, its word freq 593677\n",
      "word and, its id 2, its word freq 416629\n",
      "word one, its id 3, its word freq 411764\n",
      "word in, its id 4, its word freq 372201\n",
      "word a, its id 5, its word freq 325873\n",
      "word to, its id 6, its word freq 316376\n",
      "word zero, its id 7, its word freq 264975\n",
      "word nine, its id 8, its word freq 250430\n",
      "word two, its id 9, its word freq 192644\n",
      "8745977 tokens in the corpus\n",
      "[5233, 3080, 194, 3133, 58, 127, 741, 476, 10571, 27349, 854, 15067, 58112, 150, 854, 3580, 5, 10712, 1324, 454]\n",
      "center_word originated, target anarchism, label 1\n",
      "center_word originated, target bhiwani, label 0\n",
      "center_word originated, target qahtanite, label 0\n",
      "center_word originated, target multiport, label 0\n",
      "center_word originated, target retrogames, label 0\n",
      "center_word term, target anarchism, label 1\n",
      "center_word term, target kvetoslav, label 0\n",
      "center_word term, target neared, label 0\n",
      "center_word term, target hoffuitbanner, label 0\n",
      "center_word term, target varus, label 0\n",
      "center_word anarchism, target term, label 1\n",
      "center_word anarchism, target pittacus, label 0\n",
      "center_word anarchism, target eyb, label 0\n",
      "center_word anarchism, target aweigh, label 0\n",
      "center_word anarchism, target eidgen, label 0\n",
      "center_word originated, target term, label 1\n",
      "center_word originated, target topmoxie, label 0\n",
      "center_word originated, target stoppe, label 0\n",
      "center_word originated, target televizi, label 0\n",
      "center_word originated, target shih, label 0\n",
      "center_word abuse, target term, label 1\n",
      "center_word abuse, target diversis, label 0\n",
      "center_word abuse, target reveries, label 0\n",
      "center_word abuse, target savas, label 0\n",
      "center_word abuse, target verson, label 0\n",
      "step 1000, loss 0.695\n",
      "step 2000, loss 0.686\n",
      "step 3000, loss 0.618\n",
      "step 4000, loss 0.493\n",
      "step 5000, loss 0.398\n",
      "step 6000, loss 0.296\n",
      "step 7000, loss 0.263\n",
      "step 8000, loss 0.240\n",
      "step 9000, loss 0.205\n",
      "step 10000, loss 0.231\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is historically\n",
      "for word movie, the similar word is connection\n",
      "for word movie, the similar word is teams\n",
      "for word movie, the similar word is military\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is joined\n",
      "for word one, the similar word is songwriter\n",
      "for word one, the similar word is unit\n",
      "for word one, the similar word is states\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is geography\n",
      "for word who, the similar word is notice\n",
      "for word who, the similar word is colleges\n",
      "for word who, the similar word is financial\n",
      "step 11000, loss 0.218\n",
      "step 12000, loss 0.169\n",
      "step 13000, loss 0.209\n",
      "step 14000, loss 0.228\n",
      "step 15000, loss 0.196\n",
      "step 16000, loss 0.309\n",
      "step 17000, loss 0.258\n",
      "step 18000, loss 0.170\n",
      "step 19000, loss 0.217\n",
      "step 20000, loss 0.243\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is existent\n",
      "for word movie, the similar word is splashdown\n",
      "for word movie, the similar word is happier\n",
      "for word movie, the similar word is intolerance\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is seven\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is eight\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is discussing\n",
      "for word who, the similar word is alaskan\n",
      "for word who, the similar word is checkers\n",
      "for word who, the similar word is marriages\n",
      "step 21000, loss 0.155\n",
      "step 22000, loss 0.205\n",
      "step 23000, loss 0.181\n",
      "step 24000, loss 0.197\n",
      "step 25000, loss 0.203\n",
      "step 26000, loss 0.146\n",
      "step 27000, loss 0.196\n",
      "step 28000, loss 0.165\n",
      "step 29000, loss 0.172\n",
      "step 30000, loss 0.193\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is ghosts\n",
      "for word movie, the similar word is existent\n",
      "for word movie, the similar word is nostrils\n",
      "for word movie, the similar word is seaweed\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is seven\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is eight\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is airstrips\n",
      "for word who, the similar word is apocryphal\n",
      "for word who, the similar word is marriages\n",
      "for word who, the similar word is quelled\n",
      "step 31000, loss 0.178\n",
      "step 32000, loss 0.234\n",
      "step 33000, loss 0.207\n",
      "step 34000, loss 0.206\n",
      "step 35000, loss 0.163\n",
      "step 36000, loss 0.140\n",
      "step 37000, loss 0.151\n",
      "step 38000, loss 0.119\n",
      "step 39000, loss 0.131\n",
      "step 40000, loss 0.172\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is gaebelein\n",
      "for word movie, the similar word is seward\n",
      "for word movie, the similar word is engined\n",
      "for word movie, the similar word is guiana\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is seven\n",
      "for word one, the similar word is nine\n",
      "for word one, the similar word is three\n",
      "for word one, the similar word is princess\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is chose\n",
      "for word who, the similar word is father\n",
      "for word who, the similar word is kings\n",
      "for word who, the similar word is wife\n",
      "step 41000, loss 0.139\n",
      "step 42000, loss 0.130\n",
      "step 43000, loss 0.167\n",
      "step 44000, loss 0.155\n",
      "step 45000, loss 0.159\n",
      "step 46000, loss 0.177\n",
      "step 47000, loss 0.113\n",
      "step 48000, loss 0.117\n",
      "step 49000, loss 0.104\n",
      "step 50000, loss 0.154\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is ghosts\n",
      "for word movie, the similar word is anime\n",
      "for word movie, the similar word is weiss\n",
      "for word movie, the similar word is pounds\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is bavaria\n",
      "for word one, the similar word is seven\n",
      "for word one, the similar word is heavyweight\n",
      "for word one, the similar word is hans\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is newport\n",
      "for word who, the similar word is grandfather\n",
      "for word who, the similar word is persisted\n",
      "for word who, the similar word is them\n",
      "step 51000, loss 0.185\n",
      "step 52000, loss 0.178\n",
      "step 53000, loss 0.133\n",
      "step 54000, loss 0.103\n",
      "step 55000, loss 0.207\n",
      "step 56000, loss 0.145\n",
      "step 57000, loss 0.106\n",
      "step 58000, loss 0.116\n",
      "step 59000, loss 0.140\n",
      "step 60000, loss 0.141\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is ghosts\n",
      "for word movie, the similar word is anime\n",
      "for word movie, the similar word is maximilian\n",
      "for word movie, the similar word is television\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is skater\n",
      "for word one, the similar word is bavaria\n",
      "for word one, the similar word is geoffroy\n",
      "for word one, the similar word is princess\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is succeeded\n",
      "for word who, the similar word is persisted\n",
      "for word who, the similar word is wife\n",
      "for word who, the similar word is them\n",
      "step 61000, loss 0.114\n",
      "step 62000, loss 0.098\n",
      "step 63000, loss 0.119\n",
      "step 64000, loss 0.122\n",
      "step 65000, loss 0.151\n",
      "step 66000, loss 0.163\n",
      "step 67000, loss 0.128\n",
      "step 68000, loss 0.108\n",
      "step 69000, loss 0.081\n",
      "step 70000, loss 0.052\n",
      "for word movie, the similar word is movie\n",
      "for word movie, the similar word is anime\n",
      "for word movie, the similar word is nominations\n",
      "for word movie, the similar word is musical\n",
      "for word movie, the similar word is ramos\n",
      "for word one, the similar word is one\n",
      "for word one, the similar word is bavaria\n",
      "for word one, the similar word is geoffroy\n",
      "for word one, the similar word is greensboro\n",
      "for word one, the similar word is chess\n",
      "for word who, the similar word is who\n",
      "for word who, the similar word is succeeded\n",
      "for word who, the similar word is xylonite\n",
      "for word who, the similar word is persisted\n",
      "for word who, the similar word is mccartney\n",
      "step 71000, loss 0.108\n",
      "step 72000, loss 0.064\n",
      "step 73000, loss 0.065\n",
      "step 74000, loss 0.068\n",
      "step 75000, loss 0.057\n",
      "step 76000, loss 0.050\n",
      "step 77000, loss 0.080\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "\r\n",
    "import io\r\n",
    "import os\r\n",
    "import sys\r\n",
    "import requests\r\n",
    "from collections import OrderedDict \r\n",
    "import math\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "from paddle.nn import Embedding\r\n",
    "import paddle.nn.functional as F\r\n",
    "\r\n",
    "\r\n",
    "# In[41]:\r\n",
    "\r\n",
    "\r\n",
    "def download():\r\n",
    "    #可以从百度云服务器下载一些开源数据集（dataset.bj.bcebos.com）\r\n",
    "    corpus_url = \"https://dataset.bj.bcebos.com/word2vec/text8.txt\"\r\n",
    "    #使用python的requests包下载数据集到本地\r\n",
    "    web_request = requests.get(corpus_url)\r\n",
    "    corpus = web_request.content\r\n",
    "    #把下载后的文件存储在当前目录的text8.txt文件内\r\n",
    "    with open(\"./text8.txt\", \"wb\") as f:\r\n",
    "        f.write(corpus)\r\n",
    "    f.close()\r\n",
    "\r\n",
    "download()\r\n",
    "\r\n",
    "\r\n",
    "# In[53]:\r\n",
    "\r\n",
    "\r\n",
    "def readdata():\r\n",
    "    corpus_url = \"text8.txt\"\r\n",
    "    with open(corpus_url, \"r\") as f:  # 打开文件\r\n",
    "        corpus = f.read().strip(\"\\n\")  # 读取文件\r\n",
    "        print(corpus)\r\n",
    "    f.close()\r\n",
    "    return corpus\r\n",
    "corpus = readdata()\r\n",
    "\r\n",
    "corpus[:250]\r\n",
    "\r\n",
    "\r\n",
    "# In[54]:\r\n",
    "\r\n",
    "\r\n",
    "def data_preprocess(corpus):\r\n",
    "    # 由于英文单词出现在句首的时候经常要大写，所以我们把所有英文字符都转换为小写，\r\n",
    "    # 以便对语料进行归一化处理（Apple vs apple等）\r\n",
    "    corpus = corpus.strip().lower()\r\n",
    "    corpus = corpus.split(\" \")\r\n",
    "    return corpus\r\n",
    "\r\n",
    "corpus = data_preprocess(corpus)\r\n",
    "corpus[:50]\r\n",
    "\r\n",
    "\r\n",
    "# In[55]:\r\n",
    "\r\n",
    "\r\n",
    "def build_dict(corpus):\r\n",
    "    # 首先统计每个不同词的频率（出现的次数），使用一个词典记录\r\n",
    "    word_freq_dict = dict()\r\n",
    "    for word in corpus:\r\n",
    "        if word not in word_freq_dict:\r\n",
    "            word_freq_dict[word] = 0\r\n",
    "        word_freq_dict[word] += 1\r\n",
    "\r\n",
    "    # 将这个词典中的词，按照出现次数排序，出现次数越高，排序越靠前\r\n",
    "    # 一般来说，出现频率高的高频词往往是：I，the，you这种代词，而出现频率低的词，往往是一些名词，如：nlp\r\n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)\r\n",
    "\r\n",
    "    # 构造3个不同的词典，分别存储，\r\n",
    "    # 每个词到id的映射关系：word2id_dict\r\n",
    "    # 每个id出现的频率：word2id_freq\r\n",
    "    # 每个id到词的映射关系：id2word_dict\r\n",
    "    word2id_dict = dict()\r\n",
    "    word2id_freq = dict()\r\n",
    "    id2word_dict = dict()\r\n",
    "\r\n",
    "    # 按照频率，从高到低，开始遍历每个单词，并为这个单词构造一个独一无二的id\r\n",
    "    for word, freq in word_freq_dict:\r\n",
    "        curr_id = len(word2id_dict)\r\n",
    "        word2id_dict[word] = curr_id\r\n",
    "        word2id_freq[word2id_dict[word]] = freq\r\n",
    "        id2word_dict[curr_id] = word\r\n",
    "\r\n",
    "    return word2id_freq, word2id_dict, id2word_dict\r\n",
    "\r\n",
    "word2id_freq, word2id_dict, id2word_dict = build_dict(corpus)\r\n",
    "vocab_size = len(word2id_freq)\r\n",
    "\r\n",
    "\r\n",
    "# In[56]:\r\n",
    "\r\n",
    "\r\n",
    "print(\"there are totoally %d different words in the corpus\" % vocab_size)\r\n",
    "for _, (word, word_id) in zip(range(10), word2id_dict.items()):\r\n",
    "    print(\"word %s, its id %d, its word freq %d\" % (word, word_id, word2id_freq[word_id]))\r\n",
    "\r\n",
    "\r\n",
    "# In[57]:\r\n",
    "\r\n",
    "\r\n",
    "def convert_corpus_to_id(corpus, word2id_dict):\r\n",
    "    # 使用一个循环，将语料中的每个词替换成对应的id，以便于神经网络进行处理\r\n",
    "    corpus = [word2id_dict[word] for word in corpus]\r\n",
    "    return corpus\r\n",
    "\r\n",
    "corpus = convert_corpus_to_id(corpus, word2id_dict)\r\n",
    "\r\n",
    "\r\n",
    "# In[58]:\r\n",
    "\r\n",
    "\r\n",
    "def subsampling(corpus, word2id_freq):\r\n",
    "\r\n",
    "    # 这个discard函数决定了一个词会不会被替换，这个函数是具有随机性的，每次调用结果不同\r\n",
    "    # 如果一个词的频率很大，那么它被遗弃的概率就很大\r\n",
    "    def discard(word_id):\r\n",
    "        return random.uniform(0, 1) < 1 - math.sqrt(\r\n",
    "            1e-4 / word2id_freq[word_id] * len(corpus))\r\n",
    "\r\n",
    "    corpus = [word for word in corpus if not discard(word)]\r\n",
    "    return corpus\r\n",
    "\r\n",
    "corpus = subsampling(corpus, word2id_freq)\r\n",
    "print(\"%d tokens in the corpus\" % len(corpus))\r\n",
    "print(corpus[:20])\r\n",
    "\r\n",
    "\r\n",
    "# In[59]:\r\n",
    "\r\n",
    "\r\n",
    "def build_data(corpus, word2id_dict, word2id_freq, max_window_size = 3, negative_sample_num = 4):\r\n",
    "\r\n",
    "    #使用一个list存储处理好的数据\r\n",
    "    dataset = []\r\n",
    "    center_word_idx=0\r\n",
    "\r\n",
    "    #从左到右，开始枚举每个中心点的位置\r\n",
    "    while center_word_idx < len(corpus):\r\n",
    "        #以max_window_size为上限，随机采样一个window_size，这样会使得训练更加稳定\r\n",
    "        window_size = random.randint(1, max_window_size)\r\n",
    "        #当前的中心词就是center_word_idx所指向的词，可以当作正样本\r\n",
    "        positive_word = corpus[center_word_idx]\r\n",
    "\r\n",
    "        #以当前中心词为中心，左右两侧在window_size内的词就是上下文\r\n",
    "        context_word_range = (max(0, center_word_idx - window_size), min(len(corpus) - 1, center_word_idx + window_size))\r\n",
    "        context_word_candidates = [corpus[idx] for idx in range(context_word_range[0], context_word_range[1]+1) if idx != center_word_idx]\r\n",
    "\r\n",
    "        #对于每个正样本来说，随机采样negative_sample_num个负样本，用于训练\r\n",
    "        for context_word in context_word_candidates:\r\n",
    "            #首先把（上下文，正样本，label=1）的三元组数据放入dataset中，\r\n",
    "            #这里label=1表示这个样本是个正样本\r\n",
    "            dataset.append((context_word, positive_word, 1))\r\n",
    "\r\n",
    "            #开始负采样\r\n",
    "            i = 0\r\n",
    "            while i < negative_sample_num:\r\n",
    "                negative_word_candidate = random.randint(0, vocab_size-1)\r\n",
    "\r\n",
    "                if negative_word_candidate is not positive_word:\r\n",
    "                    #把（上下文，负样本，label=0）的三元组数据放入dataset中，\r\n",
    "                    #这里label=0表示这个样本是个负样本\r\n",
    "                    dataset.append((context_word, negative_word_candidate, 0))\r\n",
    "                    i += 1\r\n",
    "\r\n",
    "        center_word_idx = min(len(corpus) - 1, center_word_idx + window_size)\r\n",
    "        if center_word_idx == (len(corpus) - 1):\r\n",
    "            center_word_idx += 1\r\n",
    "\r\n",
    "    return dataset\r\n",
    "\r\n",
    "corpus_light = corpus[:int(len(corpus)*0.2)]\r\n",
    "dataset = build_data(corpus_light, word2id_dict, word2id_freq)\r\n",
    "\r\n",
    "\r\n",
    "# In[60]:\r\n",
    "\r\n",
    "\r\n",
    "for _, (center_word, target_word, label) in zip(range(25), dataset):\r\n",
    "    print(\"center_word %s, target %s, label %d\" % (id2word_dict[center_word],\r\n",
    "                                                   id2word_dict[target_word], label))\r\n",
    "\r\n",
    "\r\n",
    "# In[61]:\r\n",
    "\r\n",
    "\r\n",
    "def build_batch(dataset, batch_size, epoch_num):\r\n",
    "\r\n",
    "    #context_word_batch缓存batch_size个中心词\r\n",
    "    context_word_batch = []\r\n",
    "    #target_word_batch缓存batch_size个目标词（可以是正样本或者负样本）\r\n",
    "    target_word_batch = []\r\n",
    "    #label_batch缓存了batch_size个0或1的标签，用于模型训练\r\n",
    "    label_batch = []\r\n",
    "    #eval_word_batch每次随机生成几个样例，用于在运行阶段对模型做评估，以便更好地可视化训练效果。\r\n",
    "    eval_word_batch = []\r\n",
    "\r\n",
    "\r\n",
    "    for epoch in range(epoch_num):\r\n",
    "        #每次开启一个新epoch之前，都对数据进行一次随机打乱，提高训练效果\r\n",
    "        random.shuffle(dataset)\r\n",
    "\r\n",
    "        for context_word, target_word, label in dataset:\r\n",
    "            #遍历dataset中的每个样本，并将这些数据送到不同的tensor里\r\n",
    "            context_word_batch.append([context_word])\r\n",
    "            target_word_batch.append([target_word])\r\n",
    "            label_batch.append(label)\r\n",
    "\r\n",
    "            #构造训练中评估的样本，这里我们生成'one','king','who'三个词的同义词，\r\n",
    "            #看模型认为的同义词有哪些\r\n",
    "            if len(eval_word_batch) == 0:\r\n",
    "                eval_word_batch.append([word2id_dict['one']])\r\n",
    "            elif len(eval_word_batch) == 1:\r\n",
    "                eval_word_batch.append([word2id_dict['king']])\r\n",
    "            elif len(eval_word_batch) ==2:\r\n",
    "                eval_word_batch.append([word2id_dict['who']])\r\n",
    "\r\n",
    "            #当样本积攒到一个batch_size后，我们把数据都返回回来\r\n",
    "            #在这里我们使用numpy的array函数把list封装成tensor\r\n",
    "            #并使用python的迭代器机制，将数据yield出来\r\n",
    "            #使用迭代器的好处是可以节省内存\r\n",
    "            if len(context_word_batch) == batch_size:\r\n",
    "                yield epoch,                    np.array(context_word_batch).astype(\"int64\"),                    np.array(target_word_batch).astype(\"int64\"),                    np.array(label_batch).astype(\"float32\"),                    np.array(eval_word_batch).astype(\"int64\")\r\n",
    "                context_word_batch = []\r\n",
    "                target_word_batch = []\r\n",
    "                label_batch = []\r\n",
    "                eval_word_batch = []\r\n",
    "\r\n",
    "    if len(context_word_batch) > 0:\r\n",
    "        yield epoch,            np.array(context_word_batch).astype(\"int64\"),            np.array(target_word_batch).astype(\"int64\"),            np.array(label_batch).astype(\"float32\"),            np.array(eval_word_batch).astype(\"int64\")\r\n",
    "\r\n",
    "\r\n",
    "# In[62]:\r\n",
    "\r\n",
    "\r\n",
    "for _, batch in zip(range(10), build_batch(dataset, 128, 3)):\r\n",
    "    if 0 :\r\n",
    "        print(batch)\r\n",
    "\r\n",
    "\r\n",
    "# In[63]:\r\n",
    "\r\n",
    "\r\n",
    "class CBOW(paddle.nn.Layer):\r\n",
    "    def __init__(self, vocab_size, embedding_size, init_scale=0.1):\r\n",
    "        # vocab_size定义了这个skipgram这个模型的词表大小\r\n",
    "        # embedding_size定义了词向量的维度是多少\r\n",
    "        # init_scale定义了词向量初始化的范围，一般来说，比较小的初始化范围有助于模型训练\r\n",
    "        super(CBOW, self).__init__()\r\n",
    "        self.vocab_size = vocab_size\r\n",
    "        self.embedding_size = embedding_size\r\n",
    "\r\n",
    "        # 使用Embedding函数构造一个词向量参数\r\n",
    "        # 这个参数的大小为：[self.vocab_size, self.embedding_size]\r\n",
    "        # 数据类型为：float32\r\n",
    "        # 这个参数的初始化方式为在[-init_scale, init_scale]区间进行均匀采样\r\n",
    "        self.embedding = Embedding( \r\n",
    "            num_embeddings = self.vocab_size,\r\n",
    "            embedding_dim = self.embedding_size,\r\n",
    "            weight_attr=paddle.ParamAttr(\r\n",
    "                initializer=paddle.nn.initializer.Uniform( \r\n",
    "                    low=-init_scale, high=init_scale)))\r\n",
    "\r\n",
    "        # 使用Embedding函数构造另外一个词向量参数\r\n",
    "        # 这个参数的大小为：[self.vocab_size, self.embedding_size]\r\n",
    "        # 这个参数的初始化方式为在[-init_scale, init_scale]区间进行均匀采样\r\n",
    "        self.embedding_out = Embedding(\r\n",
    "            num_embeddings = self.vocab_size,\r\n",
    "            embedding_dim = self.embedding_size,\r\n",
    "            weight_attr=paddle.ParamAttr(\r\n",
    "                initializer=paddle.nn.initializer.Uniform(\r\n",
    "                    low=-init_scale, high=init_scale)))\r\n",
    "\r\n",
    "    # 定义网络的前向计算逻辑\r\n",
    "    # center_words是一个tensor（mini-batch），表示中心词\r\n",
    "    # target_words是一个tensor（mini-batch），表示目标词\r\n",
    "    # label是一个tensor（mini-batch），表示这个词是正样本还是负样本（用0或1表示）\r\n",
    "    # 用于在训练中计算这个tensor中对应词的同义词，用于观察模型的训练效果\r\n",
    "    def forward(self, context_words, target_words, label, eval_words):\r\n",
    "        # 首先，通过self.embedding参数，将mini-batch中的词转换为词向量\r\n",
    "        # 这里center_words和eval_words_emb查询的是一个相同的参数\r\n",
    "        # 而target_words_emb查询的是另一个参数\r\n",
    "        context_words_emb = self.embedding(context_words)\r\n",
    "        target_words_emb = self.embedding_out(target_words)\r\n",
    "        eval_words_emb = self.embedding(eval_words)\r\n",
    "\r\n",
    "        # 我们通过点乘的方式计算中心词到目标词的输出概率，并通过sigmoid函数估计这个词是正样本还是负样本的概率。\r\n",
    "        word_sim = paddle.multiply(context_words_emb, target_words_emb)\r\n",
    "        word_sim = paddle.sum(word_sim, axis=-1)\r\n",
    "        word_sim = paddle.reshape(word_sim, shape=[-1])\r\n",
    "        pred = F.sigmoid(word_sim)\r\n",
    "\r\n",
    "        # 通过估计的输出概率定义损失函数，注意我们使用的是binary_cross_entropy_with_logits函数\r\n",
    "        # 将sigmoid计算和cross entropy合并成一步计算可以更好的优化，所以输入的是word_sim，而不是pred\r\n",
    "        loss = F.binary_cross_entropy_with_logits(word_sim, label)\r\n",
    "        loss = paddle.mean(loss)\r\n",
    "\r\n",
    "        #我们通过一个矩阵乘法，来对每个词计算他的同义词\r\n",
    "        #on_fly在机器学习或深度学习中往往指在在线计算中做什么，\r\n",
    "        #比如我们需要在训练中做评估，就可以说evaluation_on_fly\r\n",
    "        # word_sim_on_fly = paddle.matmul(eval_words_emb, \r\n",
    "        #     self.embedding._w, transpose_y = True)\r\n",
    "\r\n",
    "        # 返回前向计算的结果，飞桨会通过backward函数自动计算出反向结果。\r\n",
    "        return pred, loss # , word_sim_on_fly\r\n",
    "\r\n",
    "\r\n",
    "# In[39]:\r\n",
    "\r\n",
    "\r\n",
    "batch_size = 512\r\n",
    "epoch_num = 3\r\n",
    "embedding_size = 200\r\n",
    "step = 0\r\n",
    "learning_rate = 0.001\r\n",
    "\r\n",
    "#定义一个使用word-embedding查询同义词的函数\r\n",
    "#这个函数query_token是要查询的词，k表示要返回多少个最相似的词，embed是我们学习到的word-embedding参数\r\n",
    "#我们通过计算不同词之间的cosine距离，来衡量词和词的相似度\r\n",
    "#具体实现如下，x代表要查询词的Embedding，Embedding参数矩阵W代表所有词的Embedding\r\n",
    "#两者计算Cos得出所有词对查询词的相似度得分向量，排序取top_k放入indices列表\r\n",
    "def get_similar_tokens(query_token, k, embed):\r\n",
    "    W = embed.numpy()\r\n",
    "    x = W[word2id_dict[query_token]]\r\n",
    "    cos = np.dot(W, x) / np.sqrt(np.sum(W * W, axis=1) * np.sum(x * x) + 1e-9)\r\n",
    "    flat = cos.flatten()\r\n",
    "    indices = np.argpartition(flat, -k)[-k:]\r\n",
    "    indices = indices[np.argsort(-flat[indices])]\r\n",
    "    for i in indices:\r\n",
    "        print('for word %s, the similar word is %s' % (query_token, str(id2word_dict[i])))\r\n",
    "\r\n",
    "# 将模型放到GPU上训练\r\n",
    "paddle.set_device('gpu:0')\r\n",
    "\r\n",
    "# 通过我们定义的CBOW类，来构造一个Skip-gram模型网络\r\n",
    "cbow_model = CBOW(vocab_size, embedding_size)\r\n",
    "\r\n",
    "# 构造训练这个网络的优化器\r\n",
    "adam = paddle.optimizer.Adam(learning_rate=learning_rate, parameters = cbow_model.parameters())\r\n",
    "\r\n",
    "# 使用build_batch函数，以mini-batch为单位，遍历训练数据，并训练网络\r\n",
    "for epoch_num, context_words, target_words, label, eval_words in build_batch(\r\n",
    "    dataset, batch_size, epoch_num):\r\n",
    "    # 使用paddle.to_tensor，将一个numpy的tensor，转换为飞桨可计算的tensor\r\n",
    "    context_words_var = paddle.to_tensor(context_words)\r\n",
    "    target_words_var = paddle.to_tensor(target_words)\r\n",
    "    label_var = paddle.to_tensor(label)\r\n",
    "    eval_words_var = paddle.to_tensor(eval_words)\r\n",
    "\r\n",
    "    # 将转换后的tensor送入飞桨中，进行一次前向计算，并得到计算结果\r\n",
    "    pred, loss  = cbow_model(\r\n",
    "        context_words_var, target_words_var, label_var, eval_words_var)\r\n",
    "\r\n",
    "    # 程序自动完成反向计算\r\n",
    "    loss.backward()\r\n",
    "    # 程序根据loss，完成一步对参数的优化更新\r\n",
    "    adam.step()\r\n",
    "    # 清空模型中的梯度，以便于下一个mini-batch进行更新\r\n",
    "    adam.clear_grad()\r\n",
    "\r\n",
    "    # 每经过1000个mini-batch，打印一次当前的loss，看看loss是否在稳定下降\r\n",
    "    step += 1\r\n",
    "    if step % 1000 == 0:\r\n",
    "        print(\"step %d, loss %.3f\" % (step, loss.numpy()[0]))\r\n",
    "\r\n",
    "    # 每隔10000步，打印一次模型对以下查询词的相似词，这里我们使用词和词之间的向量点积作为衡量相似度的方法，只打印了5个最相似的词\r\n",
    "    if step % 10000 ==0:\r\n",
    "        get_similar_tokens('movie', 5, cbow_model.embedding.weight)\r\n",
    "        get_similar_tokens('one', 5, cbow_model.embedding.weight)\r\n",
    "        get_similar_tokens('who', 5, cbow_model.embedding.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "从打印结果可以看到，经过一定步骤的训练，loss逐渐下降并趋于稳定。\n",
    "\n",
    "跟who比较接近的词有\"who, he, his, whose\"。\n",
    "\n",
    "跟one比较接近的词是\"one, five, four, two, nine\"\n",
    "\n",
    "跟king比较接近的词有\"king, alfonso（葡萄牙的六位国王和西班牙几个古老地区的国王的名字）, throne（王位）, economist\"\n",
    "\n",
    "从结果可以看出来，对于出现频率最高的数字以及who, he等代词，模型可以很快地学习出它们之间的相似性，但对于king这类出现频率相对较低的词来说，模型很难学习出它们的相似性。这也是CBOW算法的局限性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
