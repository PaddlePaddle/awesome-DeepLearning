# 一，损失函数方法补充

## Focal loss

该 Focal loss 损失函数出自于论文《Focal Loss for Dense Object Detection》，主要是解决正负样本之间的不平衡问题。通过降低 easy example 中的损失值，间接提高了 hard example 中损失值的权重。Focal loss 是基于交叉熵进行改进的：

Focalloss=−αt(1−pt)γlog(pt)

可以看到，在交叉熵前增加了(1−pt)γ，当图片被错分时，pt会很小，(1−pt)接近于 1，所以损失受到的影响不大；而增加参数γ是为了平滑降低 esay example 的权重。当γ=0时，Focal loss 退化成交叉熵。对于不同的γ，其影响如下图所示。

![](https://ai-studio-static-online.cdn.bcebos.com/58e2d0a05ad743aabb4dabe9ac182aaf2af90c7404ef4dc9b70e9d9e3191613a)


## L1，L2，smooth L1损失函数
 
利用L1,L2或者smooth L1损失函数，来对4个坐标值进行回归。smooth L1损失函数是在Fast R-CNN中提出的。三个损失函数，如下所示：

         L1=|x|
         L2=x2
smoothL1=

       0.5x2    if|x|<1

       |x|−0.5  otherwise
       
从损失函数对x的导数可知：

L1损失函数对x的导数为常数，在训练后期，x很小时，如果learning rate 不变，损失函数会在稳定值附近波动，很难收敛到更高的精度。

L2损失函数对x的导数在x值很大时，其导数也非常大，在训练初期不稳定。smooth L1完美的避开了L1和L2损失的缺点。

 在一般的目标检测中，通常是计算4个坐标值与GT框之间的差异，然后将这4个loss进行相加，构成regression loss。

但使用上述的3个损失函数，会存在以下的不足：

上面的三种Loss用于计算目标检测的Bounding Box Loss时，独立的求出4个点的Loss，然后进行相加得到最终的Bounding Box Loss，这种做法的假设是4个点是相互独立的，实际是有一定相关性的；

# 二，池化方法补充

## 1. Max Pooling(最大池化)

定义：最大池化(Max Pooling)是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。

## 2. Average Pooling(平均池化)

定义：平均池化(Average Pooling)是将输入的图像划分为若干个矩形区域，对每个子区域输出所有元素的平均值。

## 3.Global Average Pooling(全局平均池化)

定义：全局平均池化是一种特殊的平均池化，只不过它不划分若干矩形区域，而是将整个特征图中所有的元素取平均输出到下一层。

## 4. Mix Pooling(混合池化)

定义：为了提高训练较大CNN模型的正则化性能，受Dropout(将一半激活函数随机设置为0)的启发，Dingjun Yu等人提出了一种随机池化Mix Pooling[2]的方法，随机池化用随机过程代替了常规的确定性池化操作，在模型训练期间随机采用了最大池化和平均池化方法，并在一定程度上有助于防止网络过拟合现象。

## 5. Stochastic Pooling(随机池化)

定义：随机池化Stochastic Pooling[3]是Zeiler等人于ICLR2013提出的一种池化操作。随机池化的计算过程如下：

先将方格中的元素同时除以它们的和sum，得到概率矩阵。
按照概率随机选中方格。
pooling得到的值就是方格位置的值。

# 三，数据增强方法补充

目前深度学习中的数据增强方法大致有三类：

## 空间变换

## 颜色失真

## 信息丢弃

空间变换涉及到一组基本的数据扩充方法，如随机尺度、裁剪、翻转和随机旋转等，在模型训练中得到了广泛的应用。

颜色失真，包括亮度、色调等的变化，也用于一些模型。这两种方法的目的是通过改变一些信息通道，将训练数据转化为更好地模拟真实世界的数据。

近年来，信息丢弃因其有效性和/或高效性而得到广泛应用。它包括random erasing、CutOut和hide-and-seek （HaS）。众所周知，通过删除图像中的某一级别信息，CNNs可以学习原来不那么敏感或重要的信息，增加感受野，从而显著提高模型的鲁棒性。

避免对连续区域的过度删除和保留是信息丢弃方法的核心要求。有趣的是，一个成功的信息丢弃方法应该在图像区域信息的删除和保留之间达到合理的平衡。

一方面，过多地删除一个或几个区域会导致完全的对象删除和上下文信息的删除。因此，剩余信息不足以被分类，图像更像是噪声数据。

另一方面，过多的保存区域会使一些对象保持不变。它们都是微不足道的图像，可能会导致网络鲁棒性的降低。

# 图像分类方法综述 


## kNN

思路比较简单：将二维向量拉直成一个一维向量，基于距离度量以判断向量间的相似性。

显而易见，这种不带特征提取的朴素办法，丢掉了二维向量中最重要的四周相邻像素的信息。在比较干净的数据集MNIST还有不错的表现，准确率为96.927%。此外，kNN模型训练慢。


## MLP

多层感知器MLP (Multi Layer Perceptron)亦即三层的前馈神经网络，所采用的特征与kNN方法类似——每一个像素点的灰度值对应于输入层的一个神经元，隐藏层的神经元数为700（一般介于输入层与输出层的数量之间）。

sklearn的MLPClassifier实现MLP分类，下面给出基于keras的MLP实现。没怎么细致地调参，准确率大概在98.530%左右。


## CNN

LeCun早在1989年发表的论文中提出了用CNN (Convolutional Neural Networks)来做手写数字识别，后来又改进到Lenet-5，其网络结构如下所示：

卷积、池化、卷积、池化，然后套2个全连接层，最后接个Guassian连接层。

众所周知，CNN自带特征提取功能，不需要刻意地设计特征提取器。


