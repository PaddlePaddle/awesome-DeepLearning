```python
# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原
# View dataset directory. 
# This directory will be recovered automatically after resetting environment. 
!ls /home/aistudio/data
```


```python
# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.
# View personal work directory. 
# All changes under this directory will be kept even after reset. 
# Please clean unnecessary files in time to speed up environment loading. 
!ls /home/aistudio/work
```


```python
# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:
# If a persistence installation is required, 
# you need to use the persistence path as the following: 
!mkdir /home/aistudio/external-libraries
!pip install beautifulsoup4 -t /home/aistudio/external-libraries
```

    mkdir: cannot create directory ‘/home/aistudio/external-libraries’: File exists
    Looking in indexes: https://mirror.baidu.com/pypi/simple/
    Collecting beautifulsoup4
    [?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)
    [K     |████████████████████████████████| 122kB 14.9MB/s eta 0:00:01
    [?25hCollecting soupsieve>1.2; python_version >= "3.0" (from beautifulsoup4)
      Downloading https://mirror.baidu.com/pypi/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl
    Installing collected packages: soupsieve, beautifulsoup4
    Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1
    [33mWARNING: Target directory /home/aistudio/external-libraries/beautifulsoup4-4.9.3.dist-info already exists. Specify --upgrade to force replacement.[0m
    [33mWARNING: Target directory /home/aistudio/external-libraries/bs4 already exists. Specify --upgrade to force replacement.[0m
    [33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve-2.2.1.dist-info already exists. Specify --upgrade to force replacement.[0m
    [33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve already exists. Specify --upgrade to force replacement.[0m



```python
# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: 
# Also add the following code, 
# so that every time the environment (kernel) starts, 
# just run the following code: 
import sys 
sys.path.append('/home/aistudio/external-libraries')
```

请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>
Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. 

Bengio等人2003年提出的神经语言模型，Collobert和Weston在2008年提出的C&W模型，以及Mikolov等人在2013年提出的word2vec模型。作者认为，降低最后的softmax层的计算复杂度是设计更好词向量模型所面临的主要挑战，同时也是机器翻译（Jean等[10]）和语言建模(Jozefowicz等[6])的共性挑战。

本篇文章列举了近几年内新提出的几种替代softmax层的方法。其中一些方法目前还只在语言建模和机器学习中尝试过。关于超参数的讨论将安排在后续的系列文章中介绍。

先来复习一遍上篇文章里用到的符号：假设有一份训练文档集，它包括了T个训练词语w1,w2,w3,⋯,wT，它们构成大小为|V|的词语集合V。语言模型通常只考虑由当前词语wi的左右n个词语组成的上下文ci。每个词语有一个d维的输入词向量vW(即embedding层的词向量)和输出词向量v’W（即softmax层的权重矩阵所表示的词语）。最后，针对模型参数θ来优化目标函数Jθ。

若指定上下文c，用softmax方法计算词语w出现的概率可以用公式表示为：

![](https://ai-studio-static-online.cdn.bcebos.com/67bc6702ec3f475a8e64d0f6612b483ac354045ab432477080685656139c7584)
基于softmax的方法
分层Softmax
Hierarchical softmax （H-Softmax）是由Morin和Bengio[3]受到二叉树的启发而提出。H-Softmax本质上是用层级关系替代了扁平化的softmax层，如图1所示，每个叶子节点表示一个词语。于是，计算单个词语概率值的计算过程被拆解为一系列的概率计算，这样可以避免对所有词语进行标准化计算。用H-Softmax替换softmax层之后，词语的预测速度可以提升至少50倍，速度的提升对于低延时要求的实时系统至关重要，比如谷歌新推出的消息应用Allo。

我们可以把原来的softmax看做深度为1的树，词表V中的每一个词语表示一个叶子节点。计算一个词语的softmax概率需要对|V|个节点的概率值做标准化。如果把softmax改为二叉树结构，每个word表示叶子节点，那么只需要沿着通向该词语的叶子节点的路径搜索，而不需要考虑其它的节点。

平衡二叉树的深度是log2(|V|)，因此，最多只需要计算log2(|V|)个节点就能得到目标词语的概率值。注意，得到的概率值已经经过了标准化，因为二叉树所有叶子节点组成一个概率分布，所有叶子节点的概率值总和等于1。我们可以简单地验证一下，在图1的根节点（Node o）处，两个分枝的概率和必须为1。之后的每个节点，它的两个子节点的概率值之和等于节点本身的概率值。因为整条搜索路径没有概率值的损失，所以最底层所有叶子节点的概率值之和必定等于1，hierarchical softmax定义了词表V中所有词语的标准化概率分布。

具体说来，当遍历树的时候，我们需要能够计算左侧分枝或是右侧分枝的概率值。为此，给每个节点分配一个向量表示。与常规的softmax做法不同，这里不是给每个输出词语w生成词向量v’w，而是给每个节点n计算一个向量v’n。总共有|V|-1个节点，每个节点都有自己独一无二的向量表示，H-Softmax方法用到的参数与常规的softmax几乎一样。于是，在给定上下文c时，就能够计算节点n左右两个分枝的概率：![](https://ai-studio-static-online.cdn.bcebos.com/1a74e27cc90e4731aae33ba14b6fb0197865b02599484956be2893b753fb83c3)

上式与常规的softmax大致相同。现在需要计算h与树的每个节点的向量v’n的内积，而不是与每个输出词语的向量计算。而且，现在只需要计算一个概率值，这里就是偏向n节点右枝的概率值。相反的，偏向左枝的概率值是1−p(right|n,c)
如图2所示，假设已知出现了词语“the”、“dog”、“and”、“the”，则出现词语“cat”的概率值就是在节点1向左偏的概率值、在节点2向右偏的概率以及在节点5向右偏的概率值的乘积。Hugo Lachorelle在他的视频教程中给了更详细的解释。Rong[7]的文章也详细地解释了这些概念，并推导了H-Softmax。

显然，树形的结构非常重要。若我们让模型在各个节点的预测更方便，比如路径相近的节点概率值也相近，那么凭直觉系统的性能肯定还会提升。沿着这个思路，Morin和Bengio使用WordNet的同义词集作为树簇。然而性能依旧不如常规的softmax方法。Mnih和Hinton[8]将聚类算法融入到树形结构的学习过程，递归地将词集分为两个集合，效果终于和softmax方法持平，计算量有所减小。

值得注意的是，此方法只是加速了训练过程，因为我们可以提前知道将要预测的词语（以及其搜索路径）。在测试过程中，被预测词语是未知的，仍然无法避免计算所有词语的概率值。

在实践中，一般不用“左节点”和“右节点”，而是给每个节点赋一个索引向量，这个向量表示该节点的搜索路径。如图2所示，如果约定该位为0表示向左搜索，该位为1表示向右搜索，那词语“cat”的向量就是011。

上文中提到平衡二叉树的深度不超过log2(|V|)。若词表的大小是|V|=10000，那么搜索路径的平均长度就是13.3。因此，词表中的每个词语都能表示为一个平均长度为13.3比特的向量，即信息量为13.3比特。

关于信息量

在信息论中，人们习惯于将词语w概率值的负对数定义为信息量I(w)：

I(w)=−log2p(w)

而熵H则是词表中所有词语的信息量的期望值：

H=∑i∈Vp(wi)I(wi)

熵也代表着根据信息的概率分布对信息编码所需要的最短平均编码长度。 抛硬币事件需要用1比特来编码正反两个时间，对于永恒不变的事件则只需0比特。若用平衡二叉树的节点来表示词表中的词语，还是假设词表的大小|V|=10000，词表中词语的概率值均相等，那么熵H与平均搜索路径的长度恰好相等：
![](https://ai-studio-static-online.cdn.bcebos.com/5843e795e40c4cf58e6569dc87adb2bd00ada7fc6291459ab2a6f99d672eb2f0)
之前我们一再强调了树结构的重要性，因为利用好树结构不仅能提升系统的性能，还能加快运算速度。若我们给树加入额外的信息，就能缩短某些携带信息量少的词语的搜索路径。Morin和Bengio就是利用了词表中各个词语出现概率不相等这一信息。他们认为词表中的一些词语出现的概率总是大于其它词语，那这些词语就应该用更短的向量编码。他们所用的文档集（|V|=10000）的熵大约是9.16。

于是，考虑词频之后，文档集中每个词语的平均编码长度从13.3比特减为9.16比特，运算速度也提升了31%。Mikolov等人在他们关于hierarchical softmax的论文[1]里就用到了霍夫曼树，即词频越高的词语编码长度越短。比如，“the”是英语中最常见的词语，那“the”在霍夫曼树中的编码长度最短，词频第二高的词语编码长度仅次于“the”，以此类推。整篇文档的平均编码长度因此降低。

霍夫曼编码通常也被称作熵编码，因为每个词语的编码长度与它的熵几乎成正比。香农通过实验[5]得出英语字母的信息量通常在0.6~1.3之间。假设单词的平均长度是4.5个字母，那么所携带的信息量就是2.7~5.85比特。

再回到语言模型：衡量语言模型好坏的指标perplexity是2H，H表示熵。熵为9.16的unigram模型的perplexity达到29.16=572.0。我们可以直观的理解为，平均情况下用该unigram模型预测下一个词语时，有572个词语是等可能的候选词语。目前，Jozefowicz在2016年的论文中提到最好的模型perplexity=24.2。因为24.6=24.2，所以这个模型平均只需要4.6比特来表示一个词语，已经非常接近香农记录的实验下限值了。这个模型是否能用于改进网络的hierarchical softmax层，仍需要人们进一步探索。

分片Softmax
Chen等人在论文中介绍了一种传统softmax层的变换形式，称作Differentiated Softmax (D-Softmax)。D-Softmax基于的假设是并不是所有词语都需要相同数量的参数：多次出现的高频词语需要更多的参数去拟合，而较少见的词语就可以用较少的参数。

传统的softmax层用到了dx|V|的稠密矩阵来存放输出的词向量表示v′w∈ℝd，论文中采用了稀疏矩阵。他们将词向量v′w按照词频分块，每块区域的向量维度各不相同。分块数量和对应的维度是超参数，可以根据需要调整。


由于大多数的词语只需要相对较少的参数，计算softmax的复杂度得到降低，训练速度因此提升。相对于H-Softmax方法，D-Softmax的优化方法在测试阶段仍然有效。Chen在2015年的论文中提到D-Softmax是测试阶段最快的方法，同时也是准确率最高的之一。但是，由于低频词语的参数较少，D-Softmax对这部分数据的建模能力较弱。

CNN-Softmax
传统softmax层的另一种改进是受到Kim[3]的论文启发，Kim对输入词向量vw采用了字符级别的CNN模型。相反，Jozefowicz在2016年将同样的方法用于输出词向量v′w，并将这种方法称为CNN-Softmax。如图4所示，如果我们在输入端和输出端加上CNN模型，输出端CNN生成的向量v′w与输入端CNN生成的向量必然不相同，因为输入和输出的词向量矩阵就不一样。
尽管这个方法仍需要计算常规softmax的标准化，但模型的参数却大大的减少：只需要保留CNN模型的参数，不需要保存整个词向量矩阵dx|V|。在测试阶段，输出词向量v′w可以提前计算，所以性能损失不大。

但是，由于字符串是在连续空间内表示，而且模型倾向于用平滑函数将字符串映射到词向量，因此基于字符的模型往往无法区分拼写相似但是含义不同的词语。为了消除上述影响，论文的作者增加了一个矫正因数，显著地缩小了CNN-Softmax与传统方法的性能差距。通过调整矫正因数的维度，作者可以方便地取舍模型的大小与计算性能。

论文的作者还提到，前一层h的输出可以传入字符级别的LSTM模型，这个模型每次预测输出词语的一个字母。但是，这个方法的性能并不令人满意。Ling等人在2014年的论文中采用类类似的方法来解决机器翻译任务，取得了不错的效果。


LSTM被广泛用于许多序列任务（包括天然气负荷预测，股票市场预测，语言建模，机器翻译），并且比其他序列模型（例如RNN）表现更好，尤其是在有大量数据的情况下。 LSTM经过精心设计，可以避免RNN的梯度消失问题。消失梯度的主要实际限制是模型无法学习长期的依赖关系。但是，通过避免消失的梯度问题，与常规RNN相比，LSTM可以存储更多的记忆（数百个时间步长）。与仅维护单个隐藏状态的RNN相比，LSTM具有更多参数，可以更好地控制在特定时间步长保存哪些记忆以及丢弃哪些记忆。例如，在每个训练步骤中都必须更新隐藏状态，因此RNN无法确定要保存的记忆和要丢弃的记忆。

     

LSTM可以看作是一个更高级的RNN系列，主要由五个不同部分组成。

●单元状态：这是LSTM单元的内部单元状态（例如，记忆）。

●隐藏状态：这是用于计算预测结果的外部隐藏状态

●输入门：确定发送到单元状态的当前输入量

●忘记门：确定发送到当前单元状态的先前单元状态的数量

●输出门：确定隐藏状态下输出的单元状态数

可以将RNN加载到单元架构中，如下所示。该单元输出的状态取决于前一个单元的状态和当前输入（使用非线性激活函数）。但是，在RNN中，单元状态始终随每个输入而变化。这将不断更改RNN的单元状态。此行为对于保留长期依赖关系非常不利。 LSTM可以决定何时以单位状态替换，更新或忘记存储在每个神经元中的信息。这意味着LSTM具有防止单元状态改变的机制，从而保留了长期依赖关系。

采用引入门控机制来实现这种效果。对于单元需要执行的每个操作，LSTM都有一个相应的门。门在0和1之间连续（通常是S型函数）。 0表示没有信息通过门，1表示所有信息都通过门。 LSTM对单元中的每个神经元使用一个这样的门。如前所述，这些门控制以下内容：

●当前有多少输入被写入单元状态（输入门）

●先前单元状态（忘记门）遗忘的信息量

●从单元状态到最终隐藏状态（输出门）的信息输出量
LSTM 中引入了3个门，即输入门（input gate）、遗忘门（forget gate）和输出门（output gate），以及与隐藏状态形状相同的记忆细胞（某些文献把记忆细胞当成一种特殊的隐藏状态），从而记录额外的信息。


