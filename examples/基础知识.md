## DSSM

DSSM [1]（Deep Structured Semantic Models）的原理很简单，通过搜索引擎里 Query 和 Title 的海量的点击曝光日志，用 DNN 把 Query 和 Title 表达为低纬语义向量，并通过 cosine 距离来计算两个语义向量的距离，最终训练出语义相似度模型。该模型既可以用来预测两个句子的语义相似度，又可以获得某句子的低纬语义向量表达。

DSSM 从下往上可以分为三层结构：输入层、表示层、匹配层

[![img](https://camo.githubusercontent.com/70002145bba7ae390624e0ac8ba74c7b13687a4204846a4991866a2678ef3fe1/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353239363630365f313034385f313530313535353239373534382e706e67)](https://camo.githubusercontent.com/70002145bba7ae390624e0ac8ba74c7b13687a4204846a4991866a2678ef3fe1/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353239363630365f313034385f313530313535353239373534382e706e67)

### 输入层

输入层做的事情是把句子映射到一个向量空间里并输入到 DNN 中，这里英文和中文的处理方式有很大的不同。

（1）英文

英文的输入层处理方式是通过word hashing。举个例子，假设用 letter-trigams 来切分单词（3 个字母为一组，#表示开始和结束符），boy 这个单词会被切为 #-b-o, b-o-y, o-y-#

[![img](https://camo.githubusercontent.com/7427f3fedde186985f8b65cf442d8f046076d48c5c4dca820b7c6503117ce35f/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353332353637305f323331365f313530313535353332363539352e706e67)](https://camo.githubusercontent.com/7427f3fedde186985f8b65cf442d8f046076d48c5c4dca820b7c6503117ce35f/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353332353637305f323331365f313530313535353332363539352e706e67)

这样做的好处有两个：首先是压缩空间，50 万个词的 one-hot 向量空间可以通过 letter-trigram 压缩为一个 3 万维的向量空间。其次是增强范化能力，三个字母的表达往往能代表英文中的前缀和后缀，而前缀后缀往往具有通用的语义。

这里之所以用 3 个字母的切分粒度，是综合考虑了向量空间和单词冲突：

[![img](https://camo.githubusercontent.com/8f86b1994f4aa70ea2c6ba613983889f6f3e0f6c0ce4599c127d70dbec70af33/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353334373730395f373338355f313530313535353334383632362e706e67)](https://camo.githubusercontent.com/8f86b1994f4aa70ea2c6ba613983889f6f3e0f6c0ce4599c127d70dbec70af33/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353334373730395f373338355f313530313535353334383632362e706e67)

以 50 万个单词的词库为例，2 个字母的切分粒度的单词冲突为 1192（冲突的定义：至少有两个单词的 letter-bigram 向量完全相同），而 3 个字母的单词冲突降为 22 效果很好，且转化后的向量空间 3 万维不是很大，综合考虑选择 3 个字母的切分粒度。

（2）中文

中文的输入层处理方式与英文有很大不同，首先中文分词是个让所有 NLP 从业者头疼的事情，即便业界号称能做到 95%左右的分词准确性，但分词结果极为不可控，往往会在分词阶段引入误差。所以这里我们不分词，而是仿照英文的处理方式，对应到中文的最小粒度就是单字了。（曾经有人用偏旁部首切的，感兴趣的朋友可以试试）

由于常用的单字为 1.5 万左右，而常用的双字大约到百万级别了，所以这里出于向量空间的考虑，采用字向量（one-hot）作为输入，向量空间约为 1.5 万维。

### 表示层

DSSM 的表示层采用 BOW（Bag of words）的方式，相当于把字向量的位置信息抛弃了，整个句子里的词都放在一个袋子里了，不分先后顺序。当然这样做会有问题，我们先为 CNN-DSSM 和 LSTM-DSSM 埋下一个伏笔。

紧接着是一个含有多个隐层的 DNN，如下图所示：

[![img](https://camo.githubusercontent.com/45377660fe983cd394960c432ddff60faa3493865c3620d568a00ffc516cca8f/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353338343132325f343631375f313530313535353338353234352e706e67)](https://camo.githubusercontent.com/45377660fe983cd394960c432ddff60faa3493865c3620d568a00ffc516cca8f/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353338343132325f343631375f313530313535353338353234352e706e67)

用 Wi 表示第 i 层的权值矩阵，bi 表示第 i 层的 bias 项。则第一隐层向量 l1（300 维），第 i 个隐层向量 li（300 维），输出向量 y（128 维）可以分别表示为：

[![img](https://camo.githubusercontent.com/01785819504f4f3c23cacabf1ab17e5f63e058f6cbd90f2483584a9e3354c1e5/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353530333639375f393430375f313530313535353530343633362e706e67)](https://camo.githubusercontent.com/01785819504f4f3c23cacabf1ab17e5f63e058f6cbd90f2483584a9e3354c1e5/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353530333639375f393430375f313530313535353530343633362e706e67)

用 tanh 作为隐层和输出层的激活函数：

[![img](https://camo.githubusercontent.com/f8dc41990f047fa3459c8cbaaedf3cb11c4d1c03e9520a97bbd3b8793ce43b69/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353532313232345f3330315f313530313535353532323132312e706e67)](https://camo.githubusercontent.com/f8dc41990f047fa3459c8cbaaedf3cb11c4d1c03e9520a97bbd3b8793ce43b69/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353532313232345f3330315f313530313535353532323132312e706e67)

最终输出一个 128 维的低纬语义向量。

### 匹配层

Query 和 Doc 的语义相似性可以用这两个语义向量(128 维) 的 cosine 距离来表示：

[![img](https://camo.githubusercontent.com/5c10e0463de7e28d07689a5b7284fbccc07d4d1fb16dbf5d9a8d25d830588191/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353534353531395f343130375f313530313535353534363432372e706e67)](https://camo.githubusercontent.com/5c10e0463de7e28d07689a5b7284fbccc07d4d1fb16dbf5d9a8d25d830588191/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353534353531395f343130375f313530313535353534363432372e706e67)

通过softmax 函数可以把Query 与正样本 Doc 的语义相似性转化为一个后验概率：

[![img](https://camo.githubusercontent.com/0f5a6dbaeb6e447c12c2e440aa968b418b9122ecb84f38ec5b4e6943923db385/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353539303834325f393533395f313530313535353539313735352e706e67)](https://camo.githubusercontent.com/0f5a6dbaeb6e447c12c2e440aa968b418b9122ecb84f38ec5b4e6943923db385/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353539303834325f393533395f313530313535353539313735352e706e67)

其中 r 为 softmax 的平滑因子，D 为 Query 下的正样本，D-为 Query 下的负样本（采取随机负采样），D 为 Query 下的整个样本空间。

在训练阶段，通过极大似然估计，我们最小化损失函数：

[![img](https://camo.githubusercontent.com/70e1df0a10aecd5504725a18728937dcd100c2a2560aafd3f051ebf038ed6c5b/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353630323633345f3231395f313530313535353630333534322e706e67)](https://camo.githubusercontent.com/70e1df0a10aecd5504725a18728937dcd100c2a2560aafd3f051ebf038ed6c5b/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353630323633345f3231395f313530313535353630333534322e706e67)

残差会在表示层的 DNN 中反向传播，最终通过随机梯度下降（SGD）使模型收敛，得到各网络层的参数{Wi,bi}。

### 优缺点

优点：DSSM 用字向量作为输入既可以减少切词的依赖，又可以提高模型的范化能力，因为每个汉字所能表达的语义是可以复用的。另一方面，传统的输入层是用 Embedding 的方式（如 Word2Vec 的词向量）或者主题模型的方式（如 LDA 的主题向量）来直接做词的映射，再把各个词的向量累加或者拼接起来，由于 Word2Vec 和 LDA 都是无监督的训练，这样会给整个模型引入误差，DSSM 采用统一的有监督训练，不需要在中间过程做无监督模型的映射，因此精准度会比较高。

缺点：上文提到 DSSM 采用词袋模型（BOW），因此丧失了语序信息和上下文信息。另一方面，DSSM 采用弱监督、端到端的模型，预测结果不可控。

## CNN-DSSM

针对 DSSM 词袋模型丢失上下文信息的缺点，CLSM[2]（convolutional latent semantic model）应运而生，又叫 CNN-DSSM。CNN-DSSM 与 DSSM 的区别主要在于输入层和表示层。

### 输入层

（1）英文

英文的处理方式，除了上文提到的 letter-trigram，CNN-DSSM 还在输入层增加了word-trigram

[![img](https://camo.githubusercontent.com/acbce9bd5454fb411c091185f48a639c0f520614ccdcccff4d9d66d160d3c228/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353638353232385f363935375f313530313535353638363338322e706e67)](https://camo.githubusercontent.com/acbce9bd5454fb411c091185f48a639c0f520614ccdcccff4d9d66d160d3c228/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353638353232385f363935375f313530313535353638363338322e706e67)

如上图所示，word-trigram其实就是一个包含了上下文信息的滑动窗口。举个例子：把<`s`> online auto body ... <`s`>这句话提取出前三个词<`s`> online auto，之后再分别对这三个词进行letter-trigram映射到一个 3 万维的向量空间里，然后把三个向量 concat 起来，最终映射到一个 9 万维的向量空间里。

（2）中文

英文的处理方式（word-trigram letter-trigram）在中文中并不可取，因为英文中虽然用了 word-ngram 把样本空间拉成了百万级，但是经过 letter-trigram 又把向量空间降到可控级别，只有 3`*`30K（9 万）。而中文如果用 word-trigram，那向量空间就是百万级的了，显然还是字向量（1.5 万维）比较可控。

### 表示层

CNN-DSSM 的表示层由一个卷积神经网络组成，如下图所示：

[![img](https://camo.githubusercontent.com/4193748ac6a70a1f4af4ecf9bd8d43664ef8734354a8b9ddfc79c76668338582/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353831383831375f333434345f313530313535353832303037382e706e67)](https://camo.githubusercontent.com/4193748ac6a70a1f4af4ecf9bd8d43664ef8734354a8b9ddfc79c76668338582/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353831383831375f333434345f313530313535353832303037382e706e67)

（1）卷积层——Convolutional layer

卷积层的作用是提取滑动窗口下的上下文特征。以下图为例，假设输入层是一个 302`*`90000（302 行，9 万列）的矩阵，代表 302 个字向量（query 的和 Doc 的长度一般小于 300，这里少了就补全，多了就截断），每个字向量有 9 万维。而卷积核是一个 3`*`90000 的权值矩阵，卷积核以步长为 1 向下移动，得到的 feature map 是一个 300`*`1 的矩阵，feature map 的计算公式是(输入层维数 302-卷积核大小 3 步长 1)/步长 1=300。而这样的卷积核有 300 个，所以形成了 300 个 300`*`1 的 feature map 矩阵。

[![img](https://camo.githubusercontent.com/2d2b776a90827c2ecca7b3c7a26cd16be27969bdb0c914f28ec929d57f18ebc4/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353836393234345f393832345f313530313535353837303239332e706e67)](https://camo.githubusercontent.com/2d2b776a90827c2ecca7b3c7a26cd16be27969bdb0c914f28ec929d57f18ebc4/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353836393234345f393832345f313530313535353837303239332e706e67)

（2）池化层——Max pooling layer

池化层的作用是为句子找到全局的上下文特征。池化层以 Max-over-time pooling 的方式，每个 feature map 都取最大值，得到一个 300 维的向量。Max-over-pooling 可以解决可变长度的句子输入问题（因为不管 Feature Map 中有多少个值，只需要提取其中的最大值）。不过我们在上一步已经做了句子的定长处理（固定句子长度为 302），所以就没有可变长度句子的问题。最终池化层的输出为各个 Feature Map 的最大值，即一个 300`*`1 的向量。这里多提一句，之所以 Max pooling 层要保持固定的输出维度，是因为下一层全链接层要求有固定的输入层数，才能进行训练。

（3）全连接层——Semantic layer

最后通过全连接层把一个 300 维的向量转化为一个 128 维的低维语义向量。全连接层采用 tanh 函数：

[![img](https://camo.githubusercontent.com/25695cdc91204cdf69c50905e5596c0b80fdf4b1fa51cb2493a4750892525b22/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353931323837365f343638305f313530313535353931333830332e706e67)](https://camo.githubusercontent.com/25695cdc91204cdf69c50905e5596c0b80fdf4b1fa51cb2493a4750892525b22/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353931323837365f343638305f313530313535353931333830332e706e67)

### 匹配层

CNN-DSSM 的匹配层和 DSSM 的一样，这里省略。

### 优缺点

优点：CNN-DSSM 通过卷积层提取了滑动窗口下的上下文信息，又通过池化层提取了全局的上下文信息，上下文信息得到较为有效的保留。

缺点：对于间隔较远的上下文信息，难以有效保留。举个例子，I grew up in France... I speak fluent French，显然 France 和 French 是具有上下文依赖关系的，但是由于 CNN-DSSM 滑动窗口（卷积核）大小的限制，导致无法捕获该上下文信息。

## LSTM-DSSM

针对 CNN-DSSM 无法捕获较远距离上下文特征的缺点，有人提出了用LSTM-DSSM[3]（Long-Short-Term Memory）来解决该问题。不过说 LSTM 之前，要先介绍它的"爸爸""RNN。

### RNN

RNN（Recurrent Neural Networks）可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。如果我们将这个循环展开：

[![img](https://camo.githubusercontent.com/1e5ef4629c2b491039ff1bfce04e724354682bb309c874ffc1c86a9adad41f5e/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353935353233355f323931395f313530313535353935363233392e706e67)](https://camo.githubusercontent.com/1e5ef4629c2b491039ff1bfce04e724354682bb309c874ffc1c86a9adad41f5e/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353935353233355f323931395f313530313535353935363233392e706e67)

假设输入 xi 为一个 query 中几个连续的词，hi 为输出。那么上一个神经元的输出 h(t-1) 与当前细胞的输入 Xt 拼接后经过 tanh 函数会输出 ht，同时把 ht 传递给下一个细胞。

[![img](https://camo.githubusercontent.com/6efdf6dfbe8c1fc367d4d9815618742d6d55eef409bf4931a56843c0eb458b60/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353937353832365f393938395f313530313535353937363834362e706e67)](https://camo.githubusercontent.com/6efdf6dfbe8c1fc367d4d9815618742d6d55eef409bf4931a56843c0eb458b60/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353937353832365f393938395f313530313535353937363834362e706e67)

不幸的是，在这个间隔不断增大时，RNN 会逐渐丧失学习到远距离信息的能力。因为 RNN 随着距离的加长，会导致梯度消失。简单来说，由于求导的链式法则，直接导致梯度被表示为连乘的形式，以至梯度消失（几个小于 1 的数相乘会逐渐趋向于 0）。

### LSTM

LSTM[4](（Long-Short-Term Memory）是一种 RNN 特殊的类型，可以学习长期依赖信息。我们分别来介绍它最重要的几个模块：

[![img](https://camo.githubusercontent.com/ae93d6b20c61cb98caefe5e1722bbc632825883e2bf571ad6d45f6de5f365c6e/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353939333030305f363633305f313530313535353939333935392e706e67)](https://camo.githubusercontent.com/ae93d6b20c61cb98caefe5e1722bbc632825883e2bf571ad6d45f6de5f365c6e/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535353939333030305f363633305f313530313535353939333935392e706e67)

（0）细胞状态

细胞状态这条线可以理解成是一条信息的传送带，只有一些少量的线性交互。在上面流动可以保持信息的不变性。

[![img](https://camo.githubusercontent.com/edb65bf840ffa49130f53fe5927cd90865f137a07eacc9d68cbd7fd2b58e42c1/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535363037353636365f34345f313530313535363037363635352e706e67)](https://camo.githubusercontent.com/edb65bf840ffa49130f53fe5927cd90865f137a07eacc9d68cbd7fd2b58e42c1/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535363037353636365f34345f313530313535363037363635352e706e67)

（1）遗忘门

遗忘门 [5]由 Gers 提出，它用来控制细胞状态 cell 有哪些信息可以通过，继续往下传递。如下图所示，上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（遗忘门）产生一个从 0 到 1 的数值 ft，然后与细胞状态 C(t-1) 相乘，最终决定有多少细胞状态可以继续往后传递。

[![img](https://camo.githubusercontent.com/5a01f427434c536ec008fec0250e608b81c68463db19150a0d8437ac9e044931/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535363130363638315f353330365f313530313535363130373637362e706e67)](https://camo.githubusercontent.com/5a01f427434c536ec008fec0250e608b81c68463db19150a0d8437ac9e044931/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535363130363638315f353330365f313530313535363130373637362e706e67)

（2）输入门

输入门决定要新增什么信息到细胞状态，这里包含两部分：一个 sigmoid 输入门和一个 tanh 函数。sigmoid 决定输入的信号控制，tanh 决定输入什么内容。如下图所示，上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（输入门）产生一个从 0 到 1 的数值 it，同样的信息经过 tanh 网络做非线性变换得到结果 Ct，sigmoid 的结果和 tanh 的结果相乘，最终决定有哪些信息可以输入到细胞状态里。

[![img](https://camo.githubusercontent.com/216fe2cf9f07513806118f900297abf861a3b756b076dddee31e902d29b36e5b/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535363132313534375f333839385f313530313535363132323830362e706e67)](https://camo.githubusercontent.com/216fe2cf9f07513806118f900297abf861a3b756b076dddee31e902d29b36e5b/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535363132313534375f333839385f313530313535363132323830362e706e67)

（3）输出门

输出门决定从细胞状态要输出什么信息，这里也包含两部分：一个 sigmoid 输出门和一个 tanh 函数。sigmoid 决定输出的信号控制，tanh 决定输出什么内容。如下图所示，上一层的输出 h(t-1) concat 上本层的输入 xt，经过一个 sigmoid 网络（输出门）产生一个从 0 到 1 的数值 Ot，细胞状态 Ct 经过 tanh 网络做非线性变换，得到结果再与 sigmoid 的结果 Ot 相乘，最终决定有哪些信息可以输出，输出的结果 ht 会作为这个细胞的输出，也会作为传递个下一个细胞。

[![img](https://camo.githubusercontent.com/373dd7c665468d955d4d3223b5463c95246dfd0454b1534b63b44ba62aa1d0fe/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535363134333338335f343738385f313530313535363134343338322e706e67)](https://camo.githubusercontent.com/373dd7c665468d955d4d3223b5463c95246dfd0454b1534b63b44ba62aa1d0fe/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535363134333338335f343738385f313530313535363134343338322e706e67)

### LSTM-DSSM

LSTM-DSSM 其实用的是 LSTM 的一个变种：

[![img](https://camo.githubusercontent.com/f237c3e8677e5830f0917f6652dcce727196ff352ba6aff5a33682b519185b62/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535363230393238375f333432335f313530313535363231303238382e706e67)](https://camo.githubusercontent.com/f237c3e8677e5830f0917f6652dcce727196ff352ba6aff5a33682b519185b62/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535363230393238375f333432335f313530313535363231303238382e706e67)

这里三条黑线就是所谓的 peephole，传统的 LSTM 中遗忘门、输入门和输出门只用了 h(t-1) 和 xt 来控制门缝的大小，peephole 的意思是说不但要考虑 h(t-1) 和 xt，也要考虑 Ct-1 和 Ct，其中遗忘门和输入门考虑了 Ct-1，而输出门考虑了 Ct。总体来说需要考虑的信息更丰富了。

好了，来看一个 LSTM-DSSM 整体的网络结构：

[![img](https://camo.githubusercontent.com/2b2b80b257299e42ee4b5847a1ece83aa4ac99cfc46ba276837bfa81d51be8ca/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535363234313434365f3433325f313530313535363234323433362e706e67)](https://camo.githubusercontent.com/2b2b80b257299e42ee4b5847a1ece83aa4ac99cfc46ba276837bfa81d51be8ca/68747470733a2f2f626c6f672d31303033393639322e66696c652e6d7971636c6f75642e636f6d2f313530313535363234313434365f3433325f313530313535363234323433362e706e67)

红色的部分可以清晰的看到残差传递的方向。



## YouTube深度学习视频推荐系统

1. 推荐系统的应用场景 作为全球最大的视频分享网站，YouTube 平台中几乎所有的视频都来自 UGC（User Generated Content，用户原创内容），这样的内容产生模式有两个特点：

一是其商业模式不同于 Netflix，以及国内的腾讯视频、爱奇艺这样的流媒体，这些流媒体的大部分内容都是采购或自制的电影、剧集等头部内容，而 YouTube 的内容都是用户上传的自制视频，种类风格繁多，头部效应没那么明显； 二是由于 YouTube 的视频基数巨大，用户难以发现喜欢的内容。 2.YouTube 推荐系统架构 为了对海量的视频进行快速、准确的排序，YouTube 也采用了经典的召回层 + 排序层的推荐系统架构。

[![在这里插入图片描述](https://camo.githubusercontent.com/c2ebc781d753d9f1fde3330f9bf765412a7bf6c9e3a7a74505549bc1442282fa/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303132323137313933313636322e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c33646c61586870626c38304e4445794e7a4d794e773d3d2c73697a655f31362c636f6c6f725f4646464646462c745f3730237069635f63656e746572)](https://camo.githubusercontent.com/c2ebc781d753d9f1fde3330f9bf765412a7bf6c9e3a7a74505549bc1442282fa/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303132323137313933313636322e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c33646c61586870626c38304e4445794e7a4d794e773d3d2c73697a655f31362c636f6c6f725f4646464646462c745f3730237069635f63656e746572)

其推荐过程可以分成二级。第一级是用候选集生成模型（Candidate Generation Model）完成候选视频的快速筛选，在这一步，候选视频集合由百万降低到几百量级，这就相当于经典推荐系统架构中的召回层。第二级是用排序模型（Ranking Model）完成几百个候选视频的精排，这相当于经典推荐系统架构中的排序层。

无论是候选集生成模型还是排序模型，YouTube 都采用了深度学习的解决方案。

3.候选集生成模型 用于视频召回的候选集生成模型，架构如下图所示。

[![在这里插入图片描述](https://camo.githubusercontent.com/9bd035b66ee0b1b62dfe19066d691a762031ffff8325d37d7375a573d8e9b7a2/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303132323137333431303136342e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c33646c61586870626c38304e4445794e7a4d794e773d3d2c73697a655f31362c636f6c6f725f4646464646462c745f3730237069635f63656e746572)](https://camo.githubusercontent.com/9bd035b66ee0b1b62dfe19066d691a762031ffff8325d37d7375a573d8e9b7a2/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303132323137333431303136342e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c33646c61586870626c38304e4445794e7a4d794e773d3d2c73697a655f31362c636f6c6f725f4646464646462c745f3730237069635f63656e746572)

最底层是它的输入层，输入的特征包括用户历史观看视频的 Embedding 向量，以及搜索词的 Embedding 向量。对于这些 Embedding 特征，YouTube 是利用用户的观看序列和搜索序列，采用了类似 Item2vec 的预训练方式生成的。

除了视频和搜索词 Embedding 向量，特征向量中还包括用户的地理位置 Embedding、年龄、性别等特征。这里我们需要注意的是，对于样本年龄这个特征，YouTube 不仅使用了原始特征值，还把经过平方处理的特征值也作为一个新的特征输入模型。 这个操作其实是为了挖掘特征非线性的特性。

确定好了特征，这些特征会在 concat 层中连接起来，输入到上层的 ReLU 神经网络进行训练。

三层 ReLU 神经网络过后，YouTube 又使用了 softmax 函数作为输出层。值得一提的是，这里的输出层不是要预测用户会不会点击这个视频，而是要预测用户会点击哪个视频，这就跟一般深度推荐模型不一样。

总的来讲，YouTube 推荐系统的候选集生成模型，是一个标准的利用了 Embedding 预训练特征的深度推荐模型，它遵循Embedding MLP 模型的架构，只是在最后的输出层有所区别。

1. 候选集生成模型独特的线上服务方法
2. 排序模型

[![YouTube的深度学习排序模型的架构](https://camo.githubusercontent.com/fdc174100977e980161a138ffda89fb0dec952915bb723e0bb92d3340ee3f734/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303132323137333531323434382e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c33646c61586870626c38304e4445794e7a4d794e773d3d2c73697a655f31362c636f6c6f725f4646464646462c745f3730237069635f63656e746572)](https://camo.githubusercontent.com/fdc174100977e980161a138ffda89fb0dec952915bb723e0bb92d3340ee3f734/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303231303132323137333531323434382e706e673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c736861646f775f31302c746578745f6148523063484d364c7939696247396e4c6d4e7a5a473475626d56304c33646c61586870626c38304e4445794e7a4d794e773d3d2c73697a655f31362c636f6c6f725f4646464646462c745f3730237069635f63656e746572)

输入层，相比于候选集生成模型需要对几百万候选集进行粗筛，排序模型只需对几百个候选视频进行排序，因此可以引入更多特征进行精排。具体来说，YouTube 的输入层从左至右引入的特征依次是：

impression video ID embedding：当前候选视频的 Embedding； watched video IDs average embedding：用户观看过的最后 N 个视频 Embedding 的平均值； language embedding：用户语言的 Embedding 和当前候选视频语言的 Embedding； time since last watch：表示用户上次观看同频道视频距今的时间； #previous impressions：该视频已经被曝光给该用户的次数； 这 5 类特征连接起来之后，需要再经过三层 ReLU 网络进行充分的特征交叉，然后就到了输出层。这里重点注意，排序模型的输出层与候选集生成模型又有所不同。不同主要有两点：一是候选集生成模型选择了 softmax 作为其输出层，而排序模型选择了 weighted logistic regression（加权逻辑回归）作为模型输出层；二是候选集生成模型预测的是用户会点击“哪个视频”，排序模型预测的是用户“要不要点击当前视频”。

其实，排序模型采用不同输出层的根本原因就在于，YouTube 想要更精确地预测 用户的观看时长，因为观看时长才是 YouTube 最看中的商业指标，而使用 Weighted LR 作为输出层，就可以实现这样的目标。

在 Weighted LR 的训练中，我们需要为每个样本设置一个权重，权重的大小，代表了这个样本的重要程度。为了能够预估观看时长，YouTube 将正样本的权重设置为用户观看这个视频的时长，然后再用 Weighted LR 进行训练，就可以让模型学到用户观看时长的信息。

对于排序模型，必须使用 TensorFlow Serving 等模型服务平台，来进行模型的线上推断。

1. 训练和测试样本的处理 为了能够提高模型的训练效率和预测准确率，Youtube采取了诸多处理训练样本的工程措施，主要有3点：

候选集生成模型把推荐模型转换成 多分类问题，在预测下一次观看的场景中，每一个备选视频都会是一个分类，而如果采用softmax对其训练是很低效的。 Youtube采用word2vec中常用的 负采样训练方法减少每次预测的分类数量，从而加快整个模型的收敛速度。 在对训练集的预处理过程中，Youtube没有采用原始的用户日志，而是 对每个用户提取等数量的训练样本。 YouTube这样做的目的是减少高度活跃用户对模型损失的过度影响，使模型过于偏向活跃用户的行为模式，忽略数量更广大的长尾用户体验。 在处理测试集时，Youtube没有采用经典的随机留一法，而是一定要以用户最近一次观看的行为作为测试集。 只留最后一次观看行为做测试集主要是为了避免引入未来信息(future information)，产生于事实不符的数据穿越问题。 7. 处理用户对新视频的爱好 8. 总结 YouTube 推荐系统的架构是一个典型的召回层加排序层的架构，其中候选集生成模型负责从百万候选集中召回几百个候选视频，排序模型负责几百个候选视频的精排，最终选出几十个推荐给用户。

候选集生成模型是一个典型的 Embedding MLP 的架构，要注意的是它的输出层一个多分类的输出层，预测的是用户点击了“哪个”视频。在候选集生成模型的 serving 过程中，需要从输出层提取出视频 Embedding，从最后一层 ReLU 层得到用户 Embedding，然后利用 最近邻搜索快速 得到候选集。

排序模型同样是一个 Embedding MLP 的架构，不同的是，它的输入层包含了更多的用户和视频的特征，输出层采用了 Weighted LR 作为输出层，并且使用观看时长作为正样本权重，让模型能够预测出观看时长，这更接近 YouTube 要达成的商业目标。