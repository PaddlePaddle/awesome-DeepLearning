# LSTM 文本生成

* LSTM介绍
* 文本生成任务

## LSTM介绍

LSTM的网络结构：

![lstm](C:\Users\SongWood\shujiaxuexi\baidushixi\nwp\images\lstm.png)

C是一个记忆单元, U和W是网络LSTM模型的参数（权值矩阵），

i、f、o分别称之为输入门、遗忘门、输出门。

σ表示sigmoid激活函数 ；s(t)是t时刻，LSTM隐藏层的激活值

输出就是t时刻隐层神经元激活值S(t)

![lstm2](C:\Users\SongWood\shujiaxuexi\baidushixi\nwp\images\lstm2.png)

## 文本生成

​	文本生成是一种语言建模问题。语言建模是许多自然语言处理任务的核心问题，如语音到文本、对话系统和文本摘要。经过训练的语言模型根据文本中先前使用的单词序列来学习单词出现的可能性。语言模型可以在字符级、n-gram、句子级甚至段落级进行操作。



​	前馈神经网络的激活输出只向一个方向传播，而在循环神经网络中，神经元的激活输出向两个方向传播(从输入到输出，以及从输出到输入)。这在神经网络结构中形成了循环，充当神经元的记忆状态。这种状态使神经元有能力记住到目前为止所学到的东西。

​	与传统神经网络相比，RNN的记忆状态具有优势，但与之相关的是一个叫做消失梯度的问题。在这个问题中，当学习大量的层时，网络很难学习和调优较早的层的参数。为了解决这个问题，人们开发了一种称为LSTMs (Long-Short Memory，长期记忆)模型的新型RNNs。

​	LSTM还有一种额外的状态称为cell状态，网络通过它对信息流进行调整。这种状态的好处是，模型可以更有选择性地记住或忘记倾斜。要了解更多关于LSTMs的信息，这里有一个很好的帖子。让我们在代码中构建一个LSTM模型。我在模型中总共添加了三个层。

1. 输入层:将单词序列作为输入
2. LSTM层:使用LSTM单位计算输出。一层中增加了100个神经单元（可以根据需要修改）
3. dropout层:一个正则化层，它随机地删除LSTM层中一些神经元的激活。它有助于防止过度拟合（为了防止过拟合的，可以不添加）。
4. 输出层:计算输出下一个单词的最佳可能性