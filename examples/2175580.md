```python

```

# 损失函数

## 0-1

这应该是最直接，也是简单的损失函数了。如果预测值与实际值相同，那么他们之间就没有损失，反之，如果预测值与实际值不同，那么他们之间就存在损失。

问题是，如何用数学语言描述没有损失和存在损失，最直接的方法就是，如果没有损失就用0代表，如果存在损失就用1代表。（这就有点像为何使用随机变量的意思，将描述性语言转化为数学表达式）

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/0dc9a9e52ffa4d37b7cb6f48899434d16a88d42bea734c49ae4e0ee031e241dc" width="400" hegiht="" ></center>


## 绝对值损失函数

基于上面的说明，如何描述预测值与实际值之间的差距。用绝对值描述也是一个很直观的感受。毕竟绝对值本身就带有描述距离的性质。

<center><img src="![](https://ai-studio-static-online.cdn.bcebos.com/3e1841d3f60947f6958cb4af3297d589124c43af529e4e2497e8b9cd1e10345b)" width="400" hegiht="" ></center>


说明：

1.无论是0-1损失函数还是绝对值损失函数，他们都有一个求解上的问题。在描述距离的层面上，他们的直观感受是最直接的，但是不利于求解。

2.主要原因是：通过最小化损失函数过程中，参数θ \thetaθ是多维的，一般都是采用迭代的方法进行求解（以梯度下降为例），这就涉及到一个问题：求导或求偏导的问题。阶跃和绝对值在分界点出都是不可求导的。所有0-1损失函数和绝对值损失函数的局限性就比较大了，所以不是很常用，但是可以依据这种思想设计更优秀的损失函数。

## 平方损失函数

放在二维及以上的空间中，怎么描述距离呢？最直接的就是欧氏距离。这是符合直观感受的，也是合理的。就好像绝对值损失函数升华了一样（虽然本质上他们不同，尤其求导时）。

最小二乘法

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/f888c8a2db82405484309490865b8e280fbfd14b803540219ca5b198c341df13" width="400" hegiht="" ></center>


比如在线性回归模型中，就是使用了平方损失函数。

理论背景：假设样本和噪声都符合高斯分布（中心极限定理），然后通过最大似然估计（MLE），推出最小二乘法。

# 损失函数代码

import numpy as np


class MSE(object):
    """
    L2
    SquaredError
    最小均方误差
    回归结果,神经元输出后结果计算损失
    """
    def __str__(self):
        return 'MSE'

    def __call__(self, y, y_pred):
        return self.loss(y, y_pred)

    def loss(self, y, y_pred):
        """
        loss = 1/2 * (y - y_pred)^2
        :param y:class:`ndarray <numpy.ndarray>` 样本结果(n, m)
        :param y_pred:class:`ndarray <numpy.ndarray>` 样本预测结果(n, m)
        :return: shape(n, m)
        """
        return 0.5 * np.sum((y_pred - y) ** 2, axis=-1)

    def grad(self, y, y_pred):
        """
        一阶导数： y_pred - y
        :param y: class:`ndarray <numpy.ndarray>`样本结果(n, m)
        :param y_pred:class:`ndarray <numpy.ndarray>` 样本预测结果(n, m)
        :return: shape(n, m)
        """
        return y_pred - y


class CrossEntropy(object): 
   def __init__(self):
        self.eps = np.finfo(float).eps

    def __str__(self):
        return 'CrossEntropy'

    def __call__(self, y, y_pred):
        return self.loss(y, y_pred)

    def loss(self, y, y_pred):
        """
        loss = - sum_x p(x) log q(x)
        :param y:class:`ndarray <numpy.ndarray>` 样本结果(n, m)
        :param y_pred:class:`ndarray <numpy.ndarray>` 样本预测结果(n, m)
        :return: shape(n, m)
        """
        loss = -np.sum(y * np.log(y_pred + self.eps), axis=-1)
        return loss

    def grad(self, y, y_pred):
        """
        这儿的一阶导数包括了softmax部分
        :param y:class:`ndarray <numpy.ndarray>` 样本结果(n, m)
        :param y_pred:class:`ndarray <numpy.ndarray>` 样本预测结果(n, m)
        :return: shape(n, m)
        """
        grad = y_pred - y
        return grad


# 池化方法
<center><img src="" width="400" hegiht="" ></center>
## 1.  一般池化（General Pooling）

池化作用于图像中不重合的区域（这与卷积操作不同），过程如下图。

我们定义池化窗口的大小为sizeX，即下图中红色正方形的边长，定义两个相邻池化窗口的水平位移/竖直位移为stride。一般池化由于每一池化窗口都是不重复的，所以sizeX=stride。

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/2803791c086447f4af81810453954143c0556c7b4bd94a9989f5412fb290ef53" width="400" hegiht="" ></center>

最常见的池化操作为平均池化mean pooling和最大池化max pooling：

平均池化：计算图像区域的平均值作为该区域池化后的值。

最大池化：选图像区域的最大值作为该区域池化后的值。


## 2. 重叠池化（OverlappingPooling）

重叠池化正如其名字所说的，相邻池化窗口之间会有重叠区域，此时sizeX>stride。

## 3.空金字塔池化（Spatial Pyramid Pooling）

空间金字塔池化可以把任何尺度的图像的卷积特征转化成相同维度，这不仅可以让CNN处理任意尺度的图像，还能避免cropping和warping操作，导致一些信息的丢失，具有非常重要的意义。

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/29513ebfcbff4fa38ecd302976fe249efd02d04de5924b43bd7493ecc2ea2960" width="400" hegiht="" ></center>



# 数据增强方法

## 有监督的数据增强

有监督数据增强，即采用预设的数据变换规则，在已有数据的基础上进行数据的扩增，包含单样本数据增强和多样本数据增强，其中单样本又包括几何操作类，颜色变换类。

### 单样本数据增强

所谓单样本数据增强，即增强一个样本的时候，全部围绕着该样本本身进行操作，包括几何变换类，颜色变换类等。

### 多样本数据增强

不同于单样本数据增强，多样本数据增强方法利用多个样本来产生新的样本，下面介绍几种方法。

(1) SMOTE[1]

SMOTE即Synthetic Minority Over-sampling Technique方法，它是通过人工合成新样本来处理样本不平衡问题，从而提升分类器性能。

类不平衡现象是很常见的，它指的是数据集中各类别数量不近似相等。如果样本类别之间相差很大，会影响分类器的分类效果。假设小样本数据数量极少，如仅占总体的1%，则即使小样本被错误地全部识别为大样本，在经验风险最小化策略下的分类器识别准确率仍能达到99%，但由于没有学习到小样本的特征，实际分类效果就会很差。

SMOTE方法是基于插值的方法，它可以为小样本类合成新的样本，主要流程为：

第一步，定义好特征空间，将每个样本对应到特征空间中的某一点，根据样本不平衡比例确定好一个采样倍率N；

第二步，对每一个小样本类样本(x,y)，按欧氏距离找出K个最近邻样本，从中随机选取一个样本点，假设选择的近邻点为(xn,yn)。


第三步，重复以上的步骤，直到大、小样本数量平衡。


在python中，SMOTE算法已经封装到了imbalanced-learn库中，如下图为算法实现的数据增强的实例，左图为原始数据特征空间图，右图为SMOTE算法处理后的特征空间图。



(2) SamplePairing[2]

SamplePairing方法的原理非常简单，从训练集中随机抽取两张图片分别经过基础数据增强操作(如随机翻转等)处理后经像素以取平均值的形式叠加合成一个新的样本，标签为原样本标签中的一种。这两张图片甚至不限制为同一类别，这种方法对于医学图像比较有效。


经SamplePairing处理后可使训练集的规模从N扩增到N×N。实验结果表明，因SamplePairing数据增强操作可能引入不同标签的训练样本，导致在各数据集上使用SamplePairing训练的误差明显增加，而在验证集上误差则有较大幅度降低。

尽管SamplePairing思路简单，性能上提升效果可观，符合奥卡姆剃刀原理，但遗憾的是可解释性不强。

(3) mixup[3]

mixup是Facebook人工智能研究院和MIT在“Beyond Empirical Risk Minimization”中提出的基于邻域风险最小化原则的数据增强方法，它使用线性插值得到新样本数据。

令(xn,yn)是插值生成的新数据，(xi,yi)和(xj,yj)是训练集随机选取的两个数据

λ的取指范围介于0到1。提出mixup方法的作者们做了丰富的实验，实验结果表明可以改进深度学习模型在ImageNet数据集、CIFAR数据集、语音数据集和表格数据集中的泛化误差，降低模型对已损坏标签的记忆，增强模型对对抗样本的鲁棒性和训练生成对抗网络的稳定性。

SMOTE，SamplePairing，mixup三者思路上有相同之处，都是试图将离散样本点连续化来拟合真实样本分布，不过所增加的样本点在特征空间中仍位于已知小样本点所围成的区域内。如果能够在给定范围之外适当插值，也许能实现更好的数据增强效果。

## 无监督的数据增强

### GAN

关于GAN(generative adversarial networks)，我们已经说的太多了。它包含两个网络，一个是生成网络，一个是对抗网络，基本原理如下：

(1) G是一个生成图片的网络，它接收随机的噪声z，通过噪声生成图片，记做G(z) 。

(2) D是一个判别网络，判别一张图片是不是“真实的”，即是真实的图片，还是由G生成的图片。


GAN的以假乱真能力就不多说了。

### Autoaugmentation[5]

AutoAugment是Google提出的自动选择最优数据增强方案的研究，这是无监督数据增强的重要研究方向。它的基本思路是使用增强学习从数据本身寻找最佳图像变换策略，对于不同的任务学习不同的增强方法，流程如下：

(1) 准备16个常用的数据增强操作。

(2) 从16个中选择5个操作，随机产生使用该操作的概率和相应的幅度，将其称为一个sub-policy，一共产生5个sub-polices。

(3) 对训练过程中每一个batch的图片，随机采用5个sub-polices操作中的一种。

(4) 通过模型在验证集上的泛化能力来反馈，使用的优化方法是增强学习方法。

(5) 经过80~100个epoch后网络开始学习到有效的sub-policies。

(6) 之后串接这5个sub-policies，然后再进行最后的训练。

总的来说，就是学习已有数据增强的组合策略，对于门牌数字识别等任务，研究表明剪切和平移等几何变换能够获得最佳效果。




```python

```


```python

```


```python

```

请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>
Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. 
