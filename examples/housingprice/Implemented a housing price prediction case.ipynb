{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**深度学习发展历史**\n",
    "\t1943年Warren S.McCulloch和Walter H.Pitts Jr提出MCP人工神经元模型，当时是希望能够用计算机来模拟人的神经元反应的过程，该模型将神经元简化为了三个过程：输入信号线性加权，求和，非线性激活（阈值法）。\n",
    "\t1958年Rosenblatt发明了感知器算法。该算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年，该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。\n",
    "\t1969年，美国数学家及人工智能先驱Minsky在其著作中证明了感知器本质上是一种线性模型，只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了近20年的停滞。\n",
    "\t1986年Hinton发明了适用于多层感知器（MLP）的BP算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。\n",
    "\t1989年，Robert Hecht-Nielsen证明了MLP的万能逼近定理，即对于任何闭区间内的一个连续函数f，都可以用含有一个隐含层的BP网络来逼近该定理的发现极大的鼓舞了神经网络的研究人员。\n",
    "\t1989年，LeCun发明了卷积神经网络-LeNet，并将其用于数字识别，且取得了较好的成绩，不过当时并没有引起足够的注意。\n",
    "\t1997年，LSTM模型被发明，尽管该模型在序列建模上的特性非常突出，但由于正处于NN的下坡期，也没有引起足够的重视。\n",
    "\t1986年，决策树方法被提出，很快ID3，ID4，CART等改进的决策树方法相继出现，到目前仍然是非常常用的一种机器学习方法。该方法也是符号学习方法的代表。\n",
    "\t1995年，线性SVM被统计学家Vapnik提出。该方法的特点有两个：由非常完美的数学理论推导而来（统计学与凸优化等），符合人的直观感受（最大间隔）。\n",
    "\t1997年，AdaBoost被提出，该方法是PAC理论在机器学习实践上的代表，也催生了集成方法这一类。该方法通过一系列的弱分类器集成，达到强分类器的效果。\n",
    "\t2000年，KernelSVM被提出，核化的SVM通过一种巧妙的方式将原空间线性不可分的问题，通过Kernel映射成高维空间的线性可分问题，成功解决了非线性分类的问题，且分类效果非常好。至此也更加终结了NN时代。\n",
    "\t2001年，随机森林被提出，这是集成方法的另一代表，该方法的理论扎实，比AdaBoost更好的抑制过拟合问题，实际效果也非常不错。\n",
    "\t2006年，DL元年，这一年，Hinton提出了深层网络训练中梯度消失问题的解决方案：无监督预训练对权值进行初始化+有监督训练微调。其主要思想是先通过自学习的方法学习到训练数据的结构（自动编码器），然后在该结构上进行有监督训练微调。\n",
    "\t2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。\n",
    "\t2011年，微软首次将DL应用在语音识别上，取得了重大突破。\n",
    "\t2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。\n",
    "\t2015，DeepResidualNet发明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**人工智能、机器学习、深度学习的区别与联系**\n",
    "\t人工智能最初是一种概念，即人们梦想使用计算机创造出一种与人类有同样智慧特性的机器。现在人工智能是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器。\n",
    "\t机器学习是实现人工智能的一种方法，是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。，也是人工智能的核心。\n",
    "\t深度学习是机器学习重要的一个研究方向，使用深度神经网络来解决各种机器学习和人工智能领域的问题。\n",
    "\t他们三者是包含与被包含的关系，从大到小来看：人工智能——>机器学习——>深度学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**神经元**\n",
    "\t![](https://ai-studio-static-online.cdn.bcebos.com/0648951cf1a843cb956b2154436ac4a9e2a98d7e06cf4c0bb8d5f9e28e6bb382)\n",
    "\t一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。突触之间的交流通过神经递质实现。\n",
    "\t对神经元进行抽象处理得到他的数学模型，将神经元接受的输入信息当做神经元模型的多个输入，对各个输入进行各自权值处理并求和，即线性加权过程通过非线性函数之后将结果进行输出。\n",
    "   ![](https://ai-studio-static-online.cdn.bcebos.com/8b2b7be5147d4f1c998697f9f0f35dc2da0737dbfd454182a4ed6344daf3bfa4)\n",
    "**单层感知机**\n",
    "\t上面神经元的基本模型其实就是一个感知机的模型。1958年，计算科学家Rosenblatt提出的由两层神经元组成的神经网络。\n",
    "   在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。\n",
    "　　我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。有一些文献会按照网络拥有的层数来命名，例如把“感知器”称为两层神经网络。\n",
    "　　![](https://ai-studio-static-online.cdn.bcebos.com/4d9941e12672428bbd677a418bacdf16f12a35def6354ed5ac3e119e3e653811)\n",
    "**多层感知机**\n",
    "   多层感知机（MLP，Multilayer Perceptron）也叫人工神经网络（ANN，Artificial Neural Network），除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构。\n",
    "   多层感知机层与层之间是全连接的。多层感知机最底层是输入层，中间是隐藏层，最后是输出层。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**前向传播**\n",
    "\t设激活函数是σ ( z )，隐藏层和输出层的输出值为a，则对于下图的三层DNN，利用和感知机一样的思路，可以利用上一层的输出计算下一层的输出，也就是所谓的DNN前向传播算法。\n",
    "   DNN的前向传播算法也就是利用我们的若干个权重系数矩阵W和偏倚向量b来和输入值向量x进行一系列线性运算和激活运算，从输入层开始，一层层的向后计算，一直到运算到输出层，得到输出结果为止。\n",
    "   输入: 总层数L，所有隐藏层和输出层对应的矩阵W，偏倚向量b，输入值向量x\n",
    "输出：输出层的输出al\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/de60222eec73497ebf05e757a1213477d02e1f248b894833a551f5c01457376d)\n",
    "\n",
    "**反向传播**\n",
    "以分类为例，最终总是有误差的，那么怎么减少误差呢，当前应用广泛的一个算法就是梯度下降算法，但是求梯度就要求偏导数，下面以图中字母为例讲解一下。\n",
    "设最终总误差为EE，对于输出那么EE#对于输出结点yl的偏导数是yl - tl，其中tl是真实值∂yl∂zl∂yl∂zl是指上面提到的激活函数，zlzl是上面提到的加权和，那么这一层的EE对zlzl的偏导数为∂E∂zl=∂E∂yl∂yl∂zl∂E∂zl=∂E∂yl∂yl∂zl。同理，下一层也是这么计算，（只不过∂E∂yk∂E∂yk计算方法变了），一直反向传播到输入层，最后有∂E∂xi=∂E∂yj∂yj∂zj∂E∂xi=∂E∂yj∂yj∂zj 且 ∂zj∂xi=wij∂zj∂xi=wij\n",
    "然后调整这些过程中的权值，再不断进行前向传播和反向传播的过程，最终得到一个比较好的结果。\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b02f62969ea4430595236544000bfd652dc48c116879476ba6162384ea6a7018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data269\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "housing.data\r\n"
     ]
    }
   ],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/external-libraries’: File exists\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 20.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve-2.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/beautifulsoup4-4.9.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/bs4 already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9, loss 5.008962647338311\n",
      "iter 19, loss 4.113722853029151\n",
      "iter 29, loss 3.6423425680297323\n",
      "iter 39, loss 3.281732016539723\n",
      "iter 49, loss 2.9704776158578627\n",
      "iter 59, loss 2.6941275885361007\n",
      "iter 69, loss 2.447254104231053\n",
      "iter 79, loss 2.22639458799293\n",
      "iter 89, loss 2.02871339893448\n",
      "iter 99, loss 1.8517258880978686\n",
      "iter 109, loss 1.6932221509418974\n",
      "iter 119, loss 1.551230444372803\n",
      "iter 129, loss 1.4239903535743328\n",
      "iter 139, loss 1.3099299127577537\n",
      "iter 149, loss 1.2076453766401147\n",
      "iter 159, loss 1.1158831958420816\n",
      "iter 169, loss 1.033523929957471\n",
      "iter 179, loss 0.9595678851350226\n",
      "iter 189, loss 0.8931222903963989\n",
      "iter 199, loss 0.8333898476583954\n",
      "iter 209, loss 0.7796585082630183\n",
      "iter 219, loss 0.7312923446169984\n",
      "iter 229, loss 0.6877233996242835\n",
      "iter 239, loss 0.6484444091640602\n",
      "iter 249, loss 0.6130023040885555\n",
      "iter 259, loss 0.5809924082342142\n",
      "iter 269, loss 0.5520532578858193\n",
      "iter 279, loss 0.5258619761206873\n",
      "iter 289, loss 0.5021301425919316\n",
      "iter 299, loss 0.48060010567762235\n",
      "iter 309, loss 0.4610416896083046\n",
      "iter 319, loss 0.44324925426188266\n",
      "iter 329, loss 0.4270390698475693\n",
      "iter 339, loss 0.41224697274770256\n",
      "iter 349, loss 0.3987262723997758\n",
      "iter 359, loss 0.3863458823274528\n",
      "iter 369, loss 0.37498865131011194\n",
      "iter 379, loss 0.36454987325263155\n",
      "iter 389, loss 0.3549359566137372\n",
      "iter 399, loss 0.3460632363018002\n",
      "iter 409, loss 0.3378569127778838\n",
      "iter 419, loss 0.33025010474058225\n",
      "iter 429, loss 0.32318300322682586\n",
      "iter 439, loss 0.3166021162660877\n",
      "iter 449, loss 0.31045959438906234\n",
      "iter 459, loss 0.30471262833087837\n",
      "iter 469, loss 0.2993229111965752\n",
      "iter 479, loss 0.2942561581848829\n",
      "iter 489, loss 0.2894816777058967\n",
      "iter 499, loss 0.28497198838858634\n",
      "iter 509, loss 0.2807024770636631\n",
      "iter 519, loss 0.2766510933337741\n",
      "iter 529, loss 0.2727980768130233\n",
      "iter 539, loss 0.26912571353750653\n",
      "iter 549, loss 0.26561811842327715\n",
      "iter 559, loss 0.26226104098273595\n",
      "iter 569, loss 0.25904169180918313\n",
      "iter 579, loss 0.25594858760600736\n",
      "iter 589, loss 0.25297141277514895\n",
      "iter 599, loss 0.25010089579212713\n",
      "iter 609, loss 0.24732869878478797\n",
      "iter 619, loss 0.24464731890246638\n",
      "iter 629, loss 0.2420500002136177\n",
      "iter 639, loss 0.23953065500513776\n",
      "iter 649, loss 0.23708379347726094\n",
      "iter 659, loss 0.2347044609356805\n",
      "iter 669, loss 0.23238818167873979\n",
      "iter 679, loss 0.23013090886344756\n",
      "iter 689, loss 0.22792897971076972\n",
      "iter 699, loss 0.22577907547913542\n",
      "iter 709, loss 0.22367818569624284\n",
      "iter 719, loss 0.22162357619385117\n",
      "iter 729, loss 0.21961276053899179\n",
      "iter 739, loss 0.21764347449856625\n",
      "iter 749, loss 0.21571365321315888\n",
      "iter 759, loss 0.21382141079060013\n",
      "iter 769, loss 0.21196502206079967\n",
      "iter 779, loss 0.21014290626103904\n",
      "iter 789, loss 0.2083536124456158\n",
      "iter 799, loss 0.20659580643578987\n",
      "iter 809, loss 0.2048682591456799\n",
      "iter 819, loss 0.20316983613734363\n",
      "iter 829, loss 0.2014994882739784\n",
      "iter 839, loss 0.1998562433542019\n",
      "iter 849, loss 0.1982391986228914\n",
      "iter 859, loss 0.19664751406523948\n",
      "iter 869, loss 0.19508040640066812\n",
      "iter 879, loss 0.19353714370215305\n",
      "iter 889, loss 0.19201704057447475\n",
      "iter 899, loss 0.19051945383201294\n",
      "iter 909, loss 0.18904377862305408\n",
      "iter 919, loss 0.18758944495324206\n",
      "iter 929, loss 0.1861559145658676\n",
      "iter 939, loss 0.1847426781412065\n",
      "iter 949, loss 0.18334925278115446\n",
      "iter 959, loss 0.1819751797490073\n",
      "iter 969, loss 0.18062002243745293\n",
      "iter 979, loss 0.17928336454071822\n",
      "iter 989, loss 0.1779648084093726\n",
      "iter 999, loss 0.17666397356858898\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAG2dJREFUeJzt3XuYXHWd5/H3t259S3fn1rkndAIJEBMTYkNAEFdQhpugLDtyccaZQeM+6zq4Oo+P7Oysz+zc3N2RUWdQySLODOMwOogugwi4EEE0Ah0JkJAQcr+QS+fel3RXV9V3/6jTSafpS3XS1edU9ef1PPWcqlO/OvU9fZJP/ep3Tp1j7o6IiJSOWNgFiIjI8Ci4RURKjIJbRKTEKLhFREqMgltEpMQouEVESoyCW0SkxCi4RURKjIJbRKTEJIqx0MmTJ3tjY2MxFi0iUpbWrFlz0N0bCmlblOBubGykubm5GIsWESlLZraj0LYaKhERKTEKbhGREqPgFhEpMQpuEZESo+AWESkxCm4RkRKj4BYRKTGRCu5vPPMWz21qCbsMEZFIi1Rwf/u5LfxCwS0iMqhIBXcqESOdzYVdhohIpEUquJPxGOmMgltEZDCRCu5UXD1uEZGhRCq4KxLqcYuIDCVSwZ1ScIuIDCl6wa2hEhGRQUUruLVzUkRkSJEK7mQ8Rrd63CIigxoyuM3sfDNb2+t23Mw+V4xiNMYtIjK0IS9d5u5vAksBzCwO7AF+VIxiUokYXQpuEZFBDXeo5Gpgi7sXfG204dDOSRGRoQ03uG8DHi5GIQAV2jkpIjKkgoPbzFLATcC/DvD8CjNrNrPmlpYzO1GUfvIuIjK04fS4rwN+4+77+3vS3Ve6e5O7NzU0NJxRMamEjioRERnKcIL7doo4TAI6qkREpBAFBbeZ1QAfAh4tZjHaOSkiMrQhDwcEcPd2YFKRayEVj9GddXI5JxazYr+diEhJitQvJ1OJfDnqdYuIDCxawR3Pl6MdlCIiA4tWcPf0uLWDUkRkQNEMbvW4RUQGFK3gjqvHLSIylGgFd9Dj1ommREQGFqngrkrGAejszoZciYhIdEUquCtPBrd63CIiA4lYcOfLUY9bRGRgEQtuDZWIiAwlYsEd9Li1c1JEZECRCu6KRNDjTqvHLSIykEgF98mhkoyCW0RkIJEK7qqUxrhFRIYSqeCuTPQcVaIxbhGRgUQquBPxGImYqcctIjKISAU35Me51eMWERlYBIM7xgn1uEVEBlToNSfHm9kjZrbRzDaY2WXFKqgiEadLwS0iMqCCrjkJfB140t1vNbMUUF2sgiqTMR0OKCIyiCGD28zqgSuB3wNw9zSQLlZBVSmNcYuIDKaQoZK5QAvwXTN7xcweMLOaYhVUmYjrqBIRkUEUEtwJYBnwLXe/CGgHvtS3kZmtMLNmM2tuaWk544KqUnE69JN3EZEBFRLcu4Hd7v5i8PgR8kF+Gndf6e5N7t7U0NBwxgXVpBK0d2XO+PUiIuVuyOB2933ALjM7P5h1NfBGsQqqqVBwi4gMptCjSj4LfC84omQr8PvFKmhcRZw2BbeIyIAKCm53Xws0FbkWIOhxp7O4O2Y2Gm8pIlJSIvfLyZqKBNmc60rvIiIDiFxwj6vIfwnQOLeISP8iF9zVwTm527t0SKCISH8iF9w9PW7toBQR6V/kgrumZ6gkreAWEelPZINbPW4Rkf5FLri1c1JEZHCRC+6aip6dkwpuEZH+RC64T+2c1FElIiL9iVxw12ioRERkUJEL7mQ8RioRU3CLiAwgcsEN+eESHVUiItK/SAZ3TUVcPW4RkQFEMrjHVSRp7VRwi4j0J5LBPaE6ydET3WGXISISSREN7hRHOop2IXkRkZIWyeAeX53kaId63CIi/YlkcE+oTnG0I00u52GXIiISOZEM7vHVSXKOdlCKiPSjoGtOmtl2oBXIAhl3L+r1JydUpwA40pGmvjpZzLcSESk5hV7lHeAD7n6waJX0MqEmH9ZHOtI0UjMabykiUjIiOlSS73FrB6WIyDsVGtwOPG1ma8xsRX8NzGyFmTWbWXNLS8tZFdV7qERERE5XaHBf4e7LgOuAz5jZlX0buPtKd29y96aGhoazKmpCdc9QiXrcIiJ9FRTc7r4nmB4AfgRcUsyi6iqTxAyOqsctIvIOQwa3mdWYWW3PfeAaYF1Ri4oZ46tTHG5XcIuI9FXIUSVTgR+ZWU/7f3b3J4taFTCltoIDrV3FfhsRkZIzZHC7+1ZgySjUcpqpdZXsP9452m8rIhJ5kTwcEGBqXQX7jim4RUT6imxwT6ur5GBbF5lsLuxSREQiJbLBPaWukpzDwTbtoBQR6S2ywT2trhKAfRrnFhE5TXSDuz4Ibo1zi4icJrLBPaWuAoADrQpuEZHeIhvck2sqSMaNPUdPhF2KiEikRDa4YzFj9oRqdh7qCLsUEZFIiWxwA8yZVM0OBbeIyGkiHdyNk2rYebgDd117UkSkR6SDe87Eatq6MhzSyaZERE6KdHCfM6kaQMMlIiK9RDy489eb3HGoPeRKRESiI9LBPWdiNYmYsflAW9iliIhERqSDO5WIMa+hhk37W8MuRUQkMiId3AALptaycZ+CW0SkR+SD+4Jptew+coK2rkzYpYiIRELkg3vB1FoA3tJwiYgIMIzgNrO4mb1iZo8Xs6C+LpxeB8C6t4+P5tuKiETWcHrcdwMbilXIQGZNqGJSTYpXdx0d7bcWEYmkgoLbzGYBNwAPFLecft+bJbPHs1bBLSICFN7j/hrwRWDAC0Ca2Qozazaz5paWlhEprsfS2ePZ0tLG8c7uEV2uiEgpGjK4zexG4IC7rxmsnbuvdPcmd29qaGgYsQIhH9zu8PruYyO6XBGRUlRIj/ty4CYz2w78C3CVmf1TUavqY8ms8QAaLhERoYDgdvd73H2WuzcCtwHPuvvHi15ZL/XVSeZPGceL2w6P5tuKiERS5I/j7nHZuZNo3n6YdGbAYXYRkTFhWMHt7j939xuLVcxg3nvuJDrSWV7breESERnbSqbHvXzuJMxg9ZZDYZciIhKqkgnuCTUpLpxWx+qtCm4RGdtKJrghGOfecYTO7mzYpYiIhKakgvt98yeTzuT4tXrdIjKGlVRwXzpvElXJOM9uPBB2KSIioSmp4K5Mxrn8vMk8u/EA7h52OSIioSip4Aa4+sIp7D5ygrd0HUoRGaNKLrg/cP4UAA2XiMiYVXLBPa2+knfNqOOZDfvDLkVEJBQlF9wAH7xwKmt2HKGltSvsUkRERl1JBvf1i6eTc3hq/b6wSxERGXUlGdwLpo5jXkMNT7y+N+xSRERGXUkGt5lxw+Lp/HrrIQ62abhERMaWkgxuODVc8vR67aQUkbGlZIP7gmm1zJus4RIRGXtKNrjNjOsXT2f11kM6ukRExpSSDW6Am5fOIJtz/u3Vt8MuRURk1JR0cM+fWsvimfU8+srusEsRERk1Qwa3mVWa2Utm9qqZrTezPx2Nwgp1y7KZrNtznE37W8MuRURkVBTS4+4CrnL3JcBS4Fozu7S4ZRXuw0tmkIgZj/5mT9iliIiMiiGD2/N6TsWXDG6ROafq5HEVvH9BAz9+ZQ/ZXGTKEhEpmoLGuM0sbmZrgQPAz9z9xX7arDCzZjNrbmlpGek6B3XLslnsO96pCwmLyJhQUHC7e9bdlwKzgEvMbFE/bVa6e5O7NzU0NIx0nYO6+sIp1Fcl+X7zrlF9XxGRMAzrqBJ3PwqsAq4tTjlnpjIZ55ZlM3ly3V4O6SfwIlLmCjmqpMHMxgf3q4APARuLXdhw3XHJHLqzziNrdGigiJS3Qnrc04FVZvYa8DL5Me7Hi1vW8M2fWsvFjRN4+KWd5LSTUkTKWCFHlbzm7he5+7vdfZG7/4/RKOxM3LF8DtsPdbB6q3ZSikj5KulfTvZ13aLpjK9O8s8v7gy7FBGRoimr4K5Mxrl12SyeWr+PvcdOhF2OiEhRlFVwA3zivY3k3PmHX+0IuxQRkaIou+CePbGaaxdN4+GXdtKRzoRdjojIiCu74Aa464q5HDvRzQ91aKCIlKGyDO5lcyawZPZ4Hvzldh0aKCJlpyyD28z45BVz2XawnWc3Hgi7HBGREVWWwQ1w3aJpzBxfxf3Pb8FdvW4RKR9lG9yJeIwVV87j5e1H+PXWw2GXIyIyYso2uAE+dvFsptRW8LfPvhV2KSIiI6asg7syGWfFlfP41ZZDNG9Xr1tEykNZBzfAncvPYVJNim88uznsUkRERkTZB3dVKs6nrpzH85taeGXnkbDLERE5a2Uf3AAfvzTf6/5fT76pI0xEpOSNieAeV5Hgs1edx+qth3hu0+heD1NEZKSNieAGuGP5OcyZWM1XfrpRv6YUkZI2ZoI7lYjxR791Phv3tfLjtXvCLkdE5IyNmeAGuHHxdBbPrOerT2+iszsbdjkiImekkIsFzzazVWb2hpmtN7O7R6OwYojFjHuuv4A9R0+w8vmtYZcjInJGCulxZ4AvuPtC4FLgM2a2sLhlFc97z53MDYunc9+qzew63BF2OSIiw1bIxYL3uvtvgvutwAZgZrELK6Y/vuFCYmb82eNvhF2KiMiwDWuM28wagYuAF/t5boWZNZtZc0tLtA+5mzG+is9efR5Pv7Gfn7+p076KSGkpOLjNbBzwQ+Bz7n687/PuvtLdm9y9qaGhYSRrLIpPXjGPeZNr+PJj6zmR1o5KESkdBQW3mSXJh/b33P3R4pY0OlKJGH/+0UXsONTBV59+M+xyREQKVshRJQZ8B9jg7vcWv6TR895zJ3Pn8jl855fbWLND5zERkdJQSI/7cuB3gKvMbG1wu77IdY2ae66/kBn1VXzxkVd1bLeIlIRCjip5wd3N3d/t7kuD2xOjUdxoGFeR4K9uWcyWlnbu/dmmsMsRERnSmPrl5ECuXNDAncvnsPL5rTyvk1CJSMQpuAN/cuNCFkwdx+d/8CotrV1hlyMiMiAFd6AyGedvb19Ga2c3n//BWp1BUEQiS8Hdy/nTavmTGxfyi7cO8s2f61JnIhJNCu4+7lw+h5uXzuCrP9vEsxv3h12OiMg7KLj7MDO+csu7WTi9jrsfXsuWlrawSxIROY2Cux9VqTgrf7eJVCLGin9s5lhHd9gliYicpOAewMzxVXzzzmXsPNzBpx5q1o9zRCQyFNyDWD5vEl/97aW8tO0wn//BWrI60kREIiARdgFRd9OSGRw43smf/2QDU2rf4MsfXkj+9C0iIuFQcBfgk++bx75jnTzwwjZqKuL80TXnK7xFJDQK7gL91+svpD2d5b5VWwAU3iISGgV3gWIx4y8+sgiA+1ZtwTC+cM0ChbeIjDoF9zCcCm/n71Ztpq0rw3+/cSGxmMJbREaPgnuY8uG9mJpUggde2EZLWxf3/vYSKhLxsEsTkTFCwX0GYjHjv924kKl1lfzFExs43Jbm2x9/D/XVybBLE5ExQMdxn4VPXTmPr31sKc07DnPzfS/w1v7WsEsSkTFAwX2WPnLRTB7+1KW0dWX5yH2/5Kn1+8IuSUTKXCEXC37QzA6Y2brRKKgUNTVO5N8+eznnTRnHpx9aw189sYF0Jhd2WSJSpgrpcf89cG2R6yh50+ur+P6nL+OO5XO4//mt/Ptv/YptB9vDLktEylAhFwt+Hjg8CrWUvMpknL/86GK+/fH3sOtIBzd84xc8/NJO3HWOExEZORrjLoJrF03jp3e/j6Wzx3PPo69zx/95Ub1vERkxIxbcZrbCzJrNrLmlRVdKn15fxT/dtZyv3LKYdW8f49qvPc99qzbTldHpYUXk7IxYcLv7SndvcvemhoaGkVpsSYvFjNsumcMzn38/Hzh/Cv/7qTf54L3P8cTrezV8IiJnTEMlo2BKXSXf/p338NBdl1CdTPCfvvcbPnb/r3ll55GwSxORElTI4YAPA6uB881st5ndVfyyytP75jfwkz+8gr/86GK2tLTx0W/+it/77ksKcBEZFivGV/ampiZvbm4e8eWWk7auDA+t3sHK57dwpKOb9y9o4NPvn8dl8ybpjIMiY5CZrXH3poLaKrjD1d6V4R9X7+CBX2zlUHuaC6bV8gdXzOWmJTOoTOrEVSJjhYK7BHV2Z3ls7ds8+MttbNzXyqSaFLcsm8l/aJrNgqm1YZcnIkWm4C5h7s7qLYf4h9XbeWbDATI5Z8msem5tms2H3z2d8dWpsEsUkSJQcJeJg21d/PiVPTyyZjcb97WSiBmXnTuJ6xZN55p3TWXyuIqwSxSREaLgLjPuzvq3j/OT1/fy09f3sv1QBzGDixsn8oELpnDl/AYunF6rnZoiJUzBXcbcnY37Wvnp63t5+o39bNyXPwf4lNoK3je/gSsXTObixonMGF8VcqUiMhwK7jFk//FOnt/UwnObWnhh80GOdnQDMHN8FRc3TqCpcSIXN07kvCnjiOvamCKRpeAeo7I5Z8Pe47y8/TDN24/w0vbDtLR2AVCdirNweh2LZtYHtzrOaxhHIq4fz4pEgYJbgPywys7DHby8/Qjr9hxj3Z5jvLH3OB3p/ImuKhIxzm0Yx3lTTr81TqohlVCgi4ym4QS3LhZcxsyMcybVcM6kGm59zywg3yvfdrCd9W/ng3zT/jbW7DjCY6++ffJ18ZgxZ2I1sydWM3tCVa/71cyeWEV9VVI7QkVCpOAeY+IxO9mzvnnpzJPzO9IZtra0s/lAG5sPtLGlpY1dRzp4dddRjp3oPm0ZtRUJZk6oYkpdJVNrK5hWX3ny/tS6SqbWVTJ5XErDMCJFouAWAKpTiZPj330dO9HN7iMd7Dp8Iph2sOfoCQ60dvHmvuO0tHaR6zPiFjMYX51iQnWSSTUVTKhJMrEmxcSaFBOqUyfvT6xJUV+VpLYySW1lgqTCXmRICm4ZUn1Vkvqqet41452hDvnhl0NtXew/3sX+453sb+1k//EuDrd3cbg9zeH2NNsPdrBmx1GOdKTJ9k35XqqSceqqEtRWJqmrDKZV+VCvC8K9tjJBVTJOTUWCqlScmlSC6lQ8uCWorohTnYyrxy9lS8EtZy0eM6bU5YdLFtN/uPdwd453Zk4G+uH2NMdPdNPa2c3xzkx+eiJDa1d+erQjzc7DHSfnp7O5gutKJWJUB8GeD/g4lcn8rSIRG3yajFGZyE8rEjEqBmhbkYiTisdIJoxUPEY8Zhr/l6JTcMuoMrOgB59k7uSaYb++sztLe1eGjnSWjnSW9nSGE+n8vBPdWdq7snSkM32ey3KiO5OfprMc7UjTlcnR2Z19x3SQLwMFrh8k4zFS8RipRIxk3PKPE/l5yXh+Xv65U/NOPk7YqfnBvIpgOYlYMA0+IHrmJWL5eYm45e8H7fJtTp+fiJ+aJmMx4sFzyWCZUhoU3FJSenrMk4qwbHcnk/N3hnp3js7M6dOuTJbO7izprJPO5OjO5ugOpl3ZHN0Zz8/L5khncqR73e/OOulsjrauzKnXBsvpadd98v7oXeLOjD4Bn/9ASMaMeBD0ibgR7/kA6dU2Hss/jp92ixE38tNYfvrONkbcTr0+1u9yerWJGzHLv2/PMnsvO2a927xzOb3b9CzznW1ixIxIf3NScIsEzOxkD3lcRTT+a7g73dlTHwLdWSebyz/O5JxsLj8vk3Uyufy87myObC4/r6ddJudksrmgXb5tflmnXp/N5ejuaZfrtczgNT3L7Q7m99SRzuTIup98z1zwAZgL3jcb3DK54LlsjpxDJpcjlwumEbwEa8zyw4CxvgFv+Q+YnuCPxTg5b3JNBT/4j5cVvbZo/OsUkX6ZGamElf0Potz7hHuv4B/4A+BU21PP9f4wGKhNr2UP0Sbrp79/zk+fZnOcuu9O7Sh94Bf0LmZ2LfB1IA484O5fKWpVIjKmWDB8kdBFnwpSyMWC48B9wHXAQuB2M1tY7MJERKR/hXz/ugTY7O5b3T0N/Atwc3HLEhGRgRQS3DOBXb0e7w7miYhICEZsj4eZrTCzZjNrbmlpGanFiohIH4UE9x5gdq/Hs4J5p3H3le7e5O5NDQ0NI1WfiIj0UUhwvwzMN7O5ZpYCbgMeK25ZIiIykCEPB3T3jJn9Z+Ap8ocDPuju64temYiI9Kug47jd/QngiSLXIiIiBSjKpcvMrAXYcYYvnwwcHMFySoHWeWzQOpe/s1nfc9y9oB2ERQnus2FmzYVed61caJ3HBq1z+Rut9S3vEyCIiJQhBbeISImJYnCvDLuAEGidxwatc/kblfWN3Bi3iIgMLoo9bhERGURkgtvMrjWzN81ss5l9Kex6RoqZzTazVWb2hpmtN7O7g/kTzexnZvZWMJ0QzDcz+0bwd3jNzJaFuwZnzsziZvaKmT0ePJ5rZi8G6/b94Je4mFlF8Hhz8HxjmHWfKTMbb2aPmNlGM9tgZpeV+3Y2s/8S/LteZ2YPm1lluW1nM3vQzA6Y2bpe84a9Xc3sE0H7t8zsE2dTUySCu8zP+Z0BvuDuC4FLgc8E6/Yl4Bl3nw88EzyG/N9gfnBbAXxr9EseMXcDG3o9/p/A37j7ecAR4K5g/l3AkWD+3wTtStHXgSfd/QJgCfl1L9vtbGYzgT8Emtx9EflfVt9G+W3nvweu7TNvWNvVzCYCXwaWkz9V9pd7wv6MuHvoN+Ay4Klej+8B7gm7riKt6/8FPgS8CUwP5k0H3gzu3w/c3qv9yXaldCN/MrJngKuAxwEj/8OERN9tTv50CpcF9xNBOwt7HYa5vvXAtr51l/N25tQpnycG2+1x4LfKcTsDjcC6M92uwO3A/b3mn9ZuuLdI9LgZI+f8Dr4aXgS8CEx1973BU/uAqcH9cvlbfA34IpALHk8Cjrp7Jnjce71OrnPw/LGgfSmZC7QA3w2Ghx4wsxrKeDu7+x7gr4GdwF7y220N5b2dewx3u47o9o5KcJc9MxsH/BD4nLsf7/2c5z+Cy+bwHjO7ETjg7mvCrmUUJYBlwLfc/SKgnVNfn4Gy3M4TyF8Nay4wA6jhnUMKZS+M7RqV4C7onN+lysyS5EP7e+7+aDB7v5lND56fDhwI5pfD3+Jy4CYz207+UndXkR//HW9mPSc2671eJ9c5eL4eODSaBY+A3cBud38xePwI+SAv5+38QWCbu7e4ezfwKPltX87bucdwt+uIbu+oBHfZnvPbzAz4DrDB3e/t9dRjQM+e5U+QH/vumf+7wd7pS4Fjvb6SlQR3v8fdZ7l7I/lt+ay73wmsAm4NmvVd556/xa1B+5Lqmbr7PmCXmZ0fzLoaeIMy3s7kh0guNbPq4N95zzqX7XbuZbjb9SngGjObEHxTuSaYd2bCHvTvNVh/PbAJ2AL8cdj1jOB6XUH+a9RrwNrgdj35sb1ngLeA/wdMDNob+SNstgCvk99jH/p6nMX6/zvg8eD+POAlYDPwr0BFML8yeLw5eH5e2HWf4bouBZqDbf1jYEK5b2fgT4GNwDrgIaCi3LYz8DD5Mfxu8t+s7jqT7Qr8QbDum4HfP5ua9MtJEZESE5WhEhERKZCCW0SkxCi4RURKjIJbRKTEKLhFREqMgltEpMQouEVESoyCW0SkxPx/Lr8zelU5MOgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#python numpy实现双层神经网络房价预测\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import json\r\n",
    "import matplotlib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from mpl_toolkits.mplot3d import Axes3D\r\n",
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    datafile = './work/housing.data'\r\n",
    "    data = np.fromfile(datafile, sep=' ')\r\n",
    "\r\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\r\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\r\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\r\n",
    "    feature_num = len(feature_names)\r\n",
    "\r\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\r\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\r\n",
    "\r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算训练集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\r\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\r\n",
    "\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        #print(maximums[i], minimums[i], avgs[i])\r\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data\r\n",
    "# 获取数据\r\n",
    "training_data, test_data = load_data()\r\n",
    "x = training_data[:, :-1]\r\n",
    "y = training_data[:, -1:]\r\n",
    "# 查看数据\r\n",
    "#print(x[0])\r\n",
    "#print(y[0])\r\n",
    "class Network(object):\r\n",
    "    def __init__(self, num_of_weights):\r\n",
    "        # 随机产生w的初始值\r\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\r\n",
    "        np.random.seed(0)\r\n",
    "        self.w1 = np.random.randn(num_of_weights, 1)\r\n",
    "        self.b1 = 0.2\r\n",
    "        np.random.seed(1)\r\n",
    "        self.w2 = np.random.randn(num_of_weights, 1)\r\n",
    "        self.b2 = 0.\r\n",
    "        \r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        z1 = np.dot(x, self.w1) + self.b1\r\n",
    "        z2 = np.dot(x, self.w2) + self.b1\r\n",
    "        z =z1 + z2 + self.b2\r\n",
    "        return z\r\n",
    "    \r\n",
    "    def loss(self, z, y):\r\n",
    "        error = z - y\r\n",
    "        num_samples = error.shape[0]\r\n",
    "        cost = error * error\r\n",
    "        cost = np.sum(cost) / num_samples\r\n",
    "        return cost\r\n",
    "    \r\n",
    "    def gradient(self, x, y):\r\n",
    "        z = self.forward(x)\r\n",
    "        z1 = self.forward(x)\r\n",
    "        z2 = self.forward(x)\r\n",
    "        gradient_b2 = (z - y)\r\n",
    "        gradient_b2 = np.mean(gradient_b2)\r\n",
    "        gradient_w1 = (z-y)*x\r\n",
    "        gradient_w1 = np.mean(gradient_w1, axis=0)\r\n",
    "        gradient_w1 = gradient_w1[:, np.newaxis]\r\n",
    "        gradient_w2 = (z-y)*x\r\n",
    "        gradient_w2 = np.mean(gradient_w2, axis=0)\r\n",
    "        gradient_w2 = gradient_w2[:, np.newaxis]\r\n",
    "        gradient_b1 = (z - y)\r\n",
    "        gradient_b1 = np.mean(gradient_b1)        \r\n",
    "        return gradient_b2,gradient_w1,gradient_w2,gradient_b1\r\n",
    "    \r\n",
    "    def update(self, gradient_b2,gradient_w1,gradient_w2,gradient_b1, eta = 0.01):\r\n",
    "        self.w1 = self.w1 - eta * gradient_w1\r\n",
    "        self.w2 = self.w2 - eta * gradient_w2\r\n",
    "        self.b1 = self.b1 - eta * gradient_b1\r\n",
    "        self.b2 = self.b2 - eta * gradient_b2\r\n",
    "        \r\n",
    "    def train(self, x, y, iterations=100, eta=0.01):\r\n",
    "        losses = []\r\n",
    "        for i in range(iterations):\r\n",
    "            z = self.forward(x)\r\n",
    "            L = self.loss(z, y)\r\n",
    "            gradient_b2,gradient_w1,gradient_w2,gradient_b1 = self.gradient(x, y)\r\n",
    "            self.update(gradient_b2,gradient_w1,gradient_w2,gradient_b1, eta)\r\n",
    "            losses.append(L)\r\n",
    "            if (i+1) % 10 == 0:\r\n",
    "                print('iter {}, loss {}'.format(i, L))\r\n",
    "        return losses\r\n",
    "\r\n",
    "# 获取数据\r\n",
    "train_data, test_data = load_data()\r\n",
    "x = train_data[:, :-1]\r\n",
    "y = train_data[:, -1:]\r\n",
    "# 创建网络\r\n",
    "net = Network(13)\r\n",
    "num_iterations=1000\r\n",
    "# 启动训练\r\n",
    "losses = net.train(x,y, iterations=num_iterations, eta=0.01)\r\n",
    "\r\n",
    "# 画出损失函数的变化趋势\r\n",
    "plot_x = np.arange(num_iterations)\r\n",
    "plot_y = np.array(losses)\r\n",
    "plt.plot(plot_x, plot_y)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss is: [0.05365043]\n",
      "epoch: 0, iter: 20, loss is: [0.09618521]\n",
      "epoch: 0, iter: 40, loss is: [0.0267882]\n",
      "epoch: 1, iter: 0, loss is: [0.02718393]\n",
      "epoch: 1, iter: 20, loss is: [0.04753923]\n",
      "epoch: 1, iter: 40, loss is: [0.10560463]\n",
      "epoch: 2, iter: 0, loss is: [0.11071634]\n",
      "epoch: 2, iter: 20, loss is: [0.0349807]\n",
      "epoch: 2, iter: 40, loss is: [0.03577711]\n",
      "epoch: 3, iter: 0, loss is: [0.04978322]\n",
      "epoch: 3, iter: 20, loss is: [0.04606231]\n",
      "epoch: 3, iter: 40, loss is: [0.15120089]\n",
      "epoch: 4, iter: 0, loss is: [0.02655281]\n",
      "epoch: 4, iter: 20, loss is: [0.06915595]\n",
      "epoch: 4, iter: 40, loss is: [0.02645713]\n",
      "epoch: 5, iter: 0, loss is: [0.07028259]\n",
      "epoch: 5, iter: 20, loss is: [0.02720477]\n",
      "epoch: 5, iter: 40, loss is: [0.05303243]\n",
      "epoch: 6, iter: 0, loss is: [0.03828095]\n",
      "epoch: 6, iter: 20, loss is: [0.03983556]\n",
      "epoch: 6, iter: 40, loss is: [0.05357898]\n",
      "epoch: 7, iter: 0, loss is: [0.00382383]\n",
      "epoch: 7, iter: 20, loss is: [0.03967518]\n",
      "epoch: 7, iter: 40, loss is: [0.01905488]\n",
      "epoch: 8, iter: 0, loss is: [0.05705719]\n",
      "epoch: 8, iter: 20, loss is: [0.02869581]\n",
      "epoch: 8, iter: 40, loss is: [0.02749251]\n",
      "epoch: 9, iter: 0, loss is: [0.00814354]\n",
      "epoch: 9, iter: 20, loss is: [0.02823475]\n",
      "epoch: 9, iter: 40, loss is: [0.01072248]\n",
      "模型保存成功，模型参数保存在LR_model.pdparams中\n",
      "Inference result is [[19.665342]], the corresponding label is 19.700000762939453\n"
     ]
    }
   ],
   "source": [
    "#paddle实现双层神经网络房价预测\r\n",
    "import paddle\r\n",
    "from paddle.nn import Linear\r\n",
    "import paddle.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "import matplotlib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import os\r\n",
    "import random\r\n",
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    datafile = './work/housing.data'\r\n",
    "    data = np.fromfile(datafile, sep=' ', dtype=np.float32)\r\n",
    "\r\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\r\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\r\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\r\n",
    "    feature_num = len(feature_names)\r\n",
    "\r\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\r\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\r\n",
    "\r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算train数据集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\r\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\r\n",
    "    \r\n",
    "    # 记录数据的归一化参数，在预测时对数据做归一化\r\n",
    "    global max_values\r\n",
    "    global min_values\r\n",
    "    global avg_values\r\n",
    "    max_values = maximums\r\n",
    "    min_values = minimums\r\n",
    "    avg_values = avgs\r\n",
    "\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data\r\n",
    "class Regressor(paddle.nn.Layer):\r\n",
    "\r\n",
    "    # self代表类的实例自身\r\n",
    "    def __init__(self):\r\n",
    "        # 初始化父类中的一些参数\r\n",
    "        super(Regressor, self).__init__()\r\n",
    "        \r\n",
    "        # 定义一层全连接层，输入维度是13，输出维度是1\r\n",
    "        self.fc = Linear(in_features=13, out_features=1)\r\n",
    "        self.act = paddle.nn.Sigmoid()\r\n",
    "        self.fc2 = paddle.nn.Linear(in_features=10, out_features=1)\r\n",
    "    \r\n",
    "    # 网络的前向计算\r\n",
    "    def forward(self, inputs):\r\n",
    "        x = self.fc(inputs)\r\n",
    "        return x\r\n",
    "# 声明定义好的线性回归模型\r\n",
    "model = Regressor()\r\n",
    "# 开启模型训练模式\r\n",
    "model.train()\r\n",
    "# 加载数据\r\n",
    "training_data, test_data = load_data()\r\n",
    "# 定义优化算法，使用随机梯度下降SGD\r\n",
    "# 学习率设置为0.01\r\n",
    "opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())\r\n",
    "EPOCH_NUM = 10   # 设置外层循环次数\r\n",
    "BATCH_SIZE = 10  # 设置batch大小\r\n",
    "\r\n",
    "# 定义外层循环\r\n",
    "for epoch_id in range(EPOCH_NUM):\r\n",
    "    # 在每轮迭代开始之前，将训练数据的顺序随机的打乱\r\n",
    "    np.random.shuffle(training_data)\r\n",
    "    # 将训练数据进行拆分，每个batch包含10条数据\r\n",
    "    mini_batches = [training_data[k:k+BATCH_SIZE] for k in range(0, len(training_data), BATCH_SIZE)]\r\n",
    "    # 定义内层循环\r\n",
    "    for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "        x = np.array(mini_batch[:, :-1]) # 获得当前批次训练数据\r\n",
    "        y = np.array(mini_batch[:, -1:]) # 获得当前批次训练标签（真实房价）\r\n",
    "        # 将numpy数据转为飞桨动态图tensor形式\r\n",
    "        house_features = paddle.to_tensor(x)\r\n",
    "        prices = paddle.to_tensor(y)\r\n",
    "        \r\n",
    "        # 前向计算\r\n",
    "        predicts = model(house_features)\r\n",
    "        \r\n",
    "        # 计算损失\r\n",
    "        loss = F.square_error_cost(predicts, label=prices)\r\n",
    "        avg_loss = paddle.mean(loss)\r\n",
    "        if iter_id%20==0:\r\n",
    "            print(\"epoch: {}, iter: {}, loss is: {}\".format(epoch_id, iter_id, avg_loss.numpy()))\r\n",
    "        \r\n",
    "        # 反向传播\r\n",
    "        avg_loss.backward()\r\n",
    "        # 最小化loss,更新参数\r\n",
    "        opt.step()\r\n",
    "        # 清除梯度\r\n",
    "        opt.clear_grad()\r\n",
    "\r\n",
    "# 保存模型参数，文件名为LR_model.pdparams\r\n",
    "paddle.save(model.state_dict(), 'LR_model.pdparams')\r\n",
    "print(\"模型保存成功，模型参数保存在LR_model.pdparams中\")\r\n",
    "def load_one_example():\r\n",
    "    # 从上边已加载的测试集中，随机选择一条作为测试数据\r\n",
    "    idx = np.random.randint(0, test_data.shape[0])\r\n",
    "    idx = -10\r\n",
    "    one_data, label = test_data[idx, :-1], test_data[idx, -1]\r\n",
    "    # 修改该条数据shape为[1,13]\r\n",
    "    one_data =  one_data.reshape([1,-1])\r\n",
    "\r\n",
    "    return one_data, label\r\n",
    "# 参数为保存模型参数的文件地址\r\n",
    "model_dict = paddle.load('LR_model.pdparams')\r\n",
    "model.load_dict(model_dict)\r\n",
    "model.eval()\r\n",
    "\r\n",
    "# 参数为数据集的文件地址\r\n",
    "one_data, label = load_one_example()\r\n",
    "# 将数据转为动态图的variable格式 \r\n",
    "one_data = paddle.to_tensor(one_data)\r\n",
    "predict = model(one_data)\r\n",
    "\r\n",
    "# 对结果做反归一化处理\r\n",
    "predict = predict * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "# 对label数据做反归一化处理\r\n",
    "label = label * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "\r\n",
    "print(\"Inference result is {}, the corresponding label is {}\".format(predict.numpy(), label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
