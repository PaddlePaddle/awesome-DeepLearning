**æ·±åº¦å­¦ä¹ å‘å±•å†å²**
	1943å¹´Warren S.McCullochå’ŒWalter H.Pitts Jræå‡ºMCPäººå·¥ç¥ç»å…ƒæ¨¡å‹ï¼Œå½“æ—¶æ˜¯å¸Œæœ›èƒ½å¤Ÿç”¨è®¡ç®—æœºæ¥æ¨¡æ‹Ÿäººçš„ç¥ç»å…ƒååº”çš„è¿‡ç¨‹ï¼Œè¯¥æ¨¡å‹å°†ç¥ç»å…ƒç®€åŒ–ä¸ºäº†ä¸‰ä¸ªè¿‡ç¨‹ï¼šè¾“å…¥ä¿¡å·çº¿æ€§åŠ æƒï¼Œæ±‚å’Œï¼Œéçº¿æ€§æ¿€æ´»ï¼ˆé˜ˆå€¼æ³•ï¼‰ã€‚
	1958å¹´Rosenblattå‘æ˜äº†æ„ŸçŸ¥å™¨ç®—æ³•ã€‚è¯¥ç®—æ³•ä½¿ç”¨MCPæ¨¡å‹å¯¹è¾“å…¥çš„å¤šç»´æ•°æ®è¿›è¡ŒäºŒåˆ†ç±»ï¼Œä¸”èƒ½å¤Ÿä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ä»è®­ç»ƒæ ·æœ¬ä¸­è‡ªåŠ¨å­¦ä¹ æ›´æ–°æƒå€¼ã€‚1962å¹´ï¼Œè¯¥æ–¹æ³•è¢«è¯æ˜ä¸ºèƒ½å¤Ÿæ”¶æ•›ï¼Œç†è®ºä¸å®è·µæ•ˆæœå¼•èµ·ç¬¬ä¸€æ¬¡ç¥ç»ç½‘ç»œçš„æµªæ½®ã€‚
	1969å¹´ï¼Œç¾å›½æ•°å­¦å®¶åŠäººå·¥æ™ºèƒ½å…ˆé©±Minskyåœ¨å…¶è‘—ä½œä¸­è¯æ˜äº†æ„ŸçŸ¥å™¨æœ¬è´¨ä¸Šæ˜¯ä¸€ç§çº¿æ€§æ¨¡å‹ï¼Œåªèƒ½å¤„ç†çº¿æ€§åˆ†ç±»é—®é¢˜ï¼Œå°±è¿æœ€ç®€å•çš„XORï¼ˆäº¦æˆ–ï¼‰é—®é¢˜éƒ½æ— æ³•æ­£ç¡®åˆ†ç±»ã€‚è¿™ç­‰äºç›´æ¥å®£åˆ¤äº†æ„ŸçŸ¥å™¨çš„æ­»åˆ‘ï¼Œç¥ç»ç½‘ç»œçš„ç ”ç©¶ä¹Ÿé™·å…¥äº†è¿‘20å¹´çš„åœæ»ã€‚
	1986å¹´Hintonå‘æ˜äº†é€‚ç”¨äºå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰çš„BPç®—æ³•ï¼Œå¹¶é‡‡ç”¨Sigmoidè¿›è¡Œéçº¿æ€§æ˜ å°„ï¼Œæœ‰æ•ˆè§£å†³äº†éçº¿æ€§åˆ†ç±»å’Œå­¦ä¹ çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•å¼•èµ·äº†ç¥ç»ç½‘ç»œçš„ç¬¬äºŒæ¬¡çƒ­æ½®ã€‚
	1989å¹´ï¼ŒRobert Hecht-Nielsenè¯æ˜äº†MLPçš„ä¸‡èƒ½é€¼è¿‘å®šç†ï¼Œå³å¯¹äºä»»ä½•é—­åŒºé—´å†…çš„ä¸€ä¸ªè¿ç»­å‡½æ•°fï¼Œéƒ½å¯ä»¥ç”¨å«æœ‰ä¸€ä¸ªéšå«å±‚çš„BPç½‘ç»œæ¥é€¼è¿‘è¯¥å®šç†çš„å‘ç°æå¤§çš„é¼“èˆäº†ç¥ç»ç½‘ç»œçš„ç ”ç©¶äººå‘˜ã€‚
	1989å¹´ï¼ŒLeCunå‘æ˜äº†å·ç§¯ç¥ç»ç½‘ç»œ-LeNetï¼Œå¹¶å°†å…¶ç”¨äºæ•°å­—è¯†åˆ«ï¼Œä¸”å–å¾—äº†è¾ƒå¥½çš„æˆç»©ï¼Œä¸è¿‡å½“æ—¶å¹¶æ²¡æœ‰å¼•èµ·è¶³å¤Ÿçš„æ³¨æ„ã€‚
	1997å¹´ï¼ŒLSTMæ¨¡å‹è¢«å‘æ˜ï¼Œå°½ç®¡è¯¥æ¨¡å‹åœ¨åºåˆ—å»ºæ¨¡ä¸Šçš„ç‰¹æ€§éå¸¸çªå‡ºï¼Œä½†ç”±äºæ­£å¤„äºNNçš„ä¸‹å¡æœŸï¼Œä¹Ÿæ²¡æœ‰å¼•èµ·è¶³å¤Ÿçš„é‡è§†ã€‚
	1986å¹´ï¼Œå†³ç­–æ ‘æ–¹æ³•è¢«æå‡ºï¼Œå¾ˆå¿«ID3ï¼ŒID4ï¼ŒCARTç­‰æ”¹è¿›çš„å†³ç­–æ ‘æ–¹æ³•ç›¸ç»§å‡ºç°ï¼Œåˆ°ç›®å‰ä»ç„¶æ˜¯éå¸¸å¸¸ç”¨çš„ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¹Ÿæ˜¯ç¬¦å·å­¦ä¹ æ–¹æ³•çš„ä»£è¡¨ã€‚
	1995å¹´ï¼Œçº¿æ€§SVMè¢«ç»Ÿè®¡å­¦å®¶Vapnikæå‡ºã€‚è¯¥æ–¹æ³•çš„ç‰¹ç‚¹æœ‰ä¸¤ä¸ªï¼šç”±éå¸¸å®Œç¾çš„æ•°å­¦ç†è®ºæ¨å¯¼è€Œæ¥ï¼ˆç»Ÿè®¡å­¦ä¸å‡¸ä¼˜åŒ–ç­‰ï¼‰ï¼Œç¬¦åˆäººçš„ç›´è§‚æ„Ÿå—ï¼ˆæœ€å¤§é—´éš”ï¼‰ã€‚
	1997å¹´ï¼ŒAdaBoostè¢«æå‡ºï¼Œè¯¥æ–¹æ³•æ˜¯PACç†è®ºåœ¨æœºå™¨å­¦ä¹ å®è·µä¸Šçš„ä»£è¡¨ï¼Œä¹Ÿå‚¬ç”Ÿäº†é›†æˆæ–¹æ³•è¿™ä¸€ç±»ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸€ç³»åˆ—çš„å¼±åˆ†ç±»å™¨é›†æˆï¼Œè¾¾åˆ°å¼ºåˆ†ç±»å™¨çš„æ•ˆæœã€‚
	2000å¹´ï¼ŒKernelSVMè¢«æå‡ºï¼Œæ ¸åŒ–çš„SVMé€šè¿‡ä¸€ç§å·§å¦™çš„æ–¹å¼å°†åŸç©ºé—´çº¿æ€§ä¸å¯åˆ†çš„é—®é¢˜ï¼Œé€šè¿‡Kernelæ˜ å°„æˆé«˜ç»´ç©ºé—´çš„çº¿æ€§å¯åˆ†é—®é¢˜ï¼ŒæˆåŠŸè§£å†³äº†éçº¿æ€§åˆ†ç±»çš„é—®é¢˜ï¼Œä¸”åˆ†ç±»æ•ˆæœéå¸¸å¥½ã€‚è‡³æ­¤ä¹Ÿæ›´åŠ ç»ˆç»“äº†NNæ—¶ä»£ã€‚
	2001å¹´ï¼Œéšæœºæ£®æ—è¢«æå‡ºï¼Œè¿™æ˜¯é›†æˆæ–¹æ³•çš„å¦ä¸€ä»£è¡¨ï¼Œè¯¥æ–¹æ³•çš„ç†è®ºæ‰å®ï¼Œæ¯”AdaBoostæ›´å¥½çš„æŠ‘åˆ¶è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå®é™…æ•ˆæœä¹Ÿéå¸¸ä¸é”™ã€‚
	2006å¹´ï¼ŒDLå…ƒå¹´ï¼Œè¿™ä¸€å¹´ï¼ŒHintonæå‡ºäº†æ·±å±‚ç½‘ç»œè®­ç»ƒä¸­æ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼šæ— ç›‘ç£é¢„è®­ç»ƒå¯¹æƒå€¼è¿›è¡Œåˆå§‹åŒ–+æœ‰ç›‘ç£è®­ç»ƒå¾®è°ƒã€‚å…¶ä¸»è¦æ€æƒ³æ˜¯å…ˆé€šè¿‡è‡ªå­¦ä¹ çš„æ–¹æ³•å­¦ä¹ åˆ°è®­ç»ƒæ•°æ®çš„ç»“æ„ï¼ˆè‡ªåŠ¨ç¼–ç å™¨ï¼‰ï¼Œç„¶ååœ¨è¯¥ç»“æ„ä¸Šè¿›è¡Œæœ‰ç›‘ç£è®­ç»ƒå¾®è°ƒã€‚
	2011å¹´ï¼ŒReLUæ¿€æ´»å‡½æ•°è¢«æå‡ºï¼Œè¯¥æ¿€æ´»å‡½æ•°èƒ½å¤Ÿæœ‰æ•ˆçš„æŠ‘åˆ¶æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚
	2011å¹´ï¼Œå¾®è½¯é¦–æ¬¡å°†DLåº”ç”¨åœ¨è¯­éŸ³è¯†åˆ«ä¸Šï¼Œå–å¾—äº†é‡å¤§çªç ´ã€‚
	2012å¹´ï¼ŒHintonè¯¾é¢˜ç»„ä¸ºäº†è¯æ˜æ·±åº¦å­¦ä¹ çš„æ½œåŠ›ï¼Œé¦–æ¬¡å‚åŠ ImageNetå›¾åƒè¯†åˆ«æ¯”èµ›ï¼Œå…¶é€šè¿‡æ„å»ºçš„CNNç½‘ç»œAlexNetä¸€ä¸¾å¤ºå¾—å† å†›ï¼Œä¸”ç¢¾å‹ç¬¬äºŒåï¼ˆSVMæ–¹æ³•ï¼‰çš„åˆ†ç±»æ€§èƒ½ã€‚ä¹Ÿæ­£æ˜¯ç”±äºè¯¥æ¯”èµ›ï¼ŒCNNå¸å¼•åˆ°äº†ä¼—å¤šç ”ç©¶è€…çš„æ³¨æ„ã€‚
	2015ï¼ŒDeepResidualNetå‘æ˜ã€‚

**äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ çš„åŒºåˆ«ä¸è”ç³»**
	äººå·¥æ™ºèƒ½æœ€åˆæ˜¯ä¸€ç§æ¦‚å¿µï¼Œå³äººä»¬æ¢¦æƒ³ä½¿ç”¨è®¡ç®—æœºåˆ›é€ å‡ºä¸€ç§ä¸äººç±»æœ‰åŒæ ·æ™ºæ…§ç‰¹æ€§çš„æœºå™¨ã€‚ç°åœ¨äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚
	æœºå™¨å­¦ä¹ æ˜¯å®ç°äººå·¥æ™ºèƒ½çš„ä¸€ç§æ–¹æ³•ï¼Œæ˜¯ä¸€é—¨å¤šé¢†åŸŸäº¤å‰å­¦ç§‘ï¼Œæ¶‰åŠæ¦‚ç‡è®ºã€ç»Ÿè®¡å­¦ã€é€¼è¿‘è®ºã€å‡¸åˆ†æã€ç®—æ³•å¤æ‚åº¦ç†è®ºç­‰å¤šé—¨å­¦ç§‘ã€‚ä¸“é—¨ç ”ç©¶è®¡ç®—æœºæ€æ ·æ¨¡æ‹Ÿæˆ–å®ç°äººç±»çš„å­¦ä¹ è¡Œä¸ºï¼Œä»¥è·å–æ–°çš„çŸ¥è¯†æˆ–æŠ€èƒ½ï¼Œé‡æ–°ç»„ç»‡å·²æœ‰çš„çŸ¥è¯†ç»“æ„ä½¿ä¹‹ä¸æ–­æ”¹å–„è‡ªèº«çš„æ€§èƒ½ã€‚ï¼Œä¹Ÿæ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒã€‚
	æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ é‡è¦çš„ä¸€ä¸ªç ”ç©¶æ–¹å‘ï¼Œä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥è§£å†³å„ç§æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„é—®é¢˜ã€‚
	ä»–ä»¬ä¸‰è€…æ˜¯åŒ…å«ä¸è¢«åŒ…å«çš„å…³ç³»ï¼Œä»å¤§åˆ°å°æ¥çœ‹ï¼šäººå·¥æ™ºèƒ½â€”â€”>æœºå™¨å­¦ä¹ â€”â€”>æ·±åº¦å­¦ä¹ ã€‚

**ç¥ç»å…ƒ**
	![](https://ai-studio-static-online.cdn.bcebos.com/0648951cf1a843cb956b2154436ac4a9e2a98d7e06cf4c0bb8d5f9e28e6bb382)
	ä¸€ä¸ªç¥ç»å…ƒé€šå¸¸å…·æœ‰å¤šä¸ªæ ‘çªï¼Œä¸»è¦ç”¨æ¥æ¥å—ä¼ å…¥ä¿¡æ¯ï¼›è€Œè½´çªåªæœ‰ä¸€æ¡ï¼Œè½´çªå°¾ç«¯æœ‰è®¸å¤šè½´çªæœ«æ¢¢å¯ä»¥ç»™å…¶ä»–å¤šä¸ªç¥ç»å…ƒä¼ é€’ä¿¡æ¯ã€‚è½´çªæœ«æ¢¢è·Ÿå…¶ä»–ç¥ç»å…ƒçš„æ ‘çªäº§ç”Ÿè¿æ¥ï¼Œä»è€Œä¼ é€’ä¿¡å·ã€‚è¿™ä¸ªè¿æ¥çš„ä½ç½®åœ¨ç”Ÿç‰©å­¦ä¸Šå«åšâ€œçªè§¦â€ã€‚çªè§¦ä¹‹é—´çš„äº¤æµé€šè¿‡ç¥ç»é€’è´¨å®ç°ã€‚
	å¯¹ç¥ç»å…ƒè¿›è¡ŒæŠ½è±¡å¤„ç†å¾—åˆ°ä»–çš„æ•°å­¦æ¨¡å‹ï¼Œå°†ç¥ç»å…ƒæ¥å—çš„è¾“å…¥ä¿¡æ¯å½“åšç¥ç»å…ƒæ¨¡å‹çš„å¤šä¸ªè¾“å…¥ï¼Œå¯¹å„ä¸ªè¾“å…¥è¿›è¡Œå„è‡ªæƒå€¼å¤„ç†å¹¶æ±‚å’Œï¼Œå³çº¿æ€§åŠ æƒè¿‡ç¨‹é€šè¿‡éçº¿æ€§å‡½æ•°ä¹‹åå°†ç»“æœè¿›è¡Œè¾“å‡ºã€‚
   ![](https://ai-studio-static-online.cdn.bcebos.com/8b2b7be5147d4f1c998697f9f0f35dc2da0737dbfd454182a4ed6344daf3bfa4)
**å•å±‚æ„ŸçŸ¥æœº**
	ä¸Šé¢ç¥ç»å…ƒçš„åŸºæœ¬æ¨¡å‹å…¶å®å°±æ˜¯ä¸€ä¸ªæ„ŸçŸ¥æœºçš„æ¨¡å‹ã€‚1958å¹´ï¼Œè®¡ç®—ç§‘å­¦å®¶Rosenblattæå‡ºçš„ç”±ä¸¤å±‚ç¥ç»å…ƒç»„æˆçš„ç¥ç»ç½‘ç»œã€‚
   åœ¨â€œæ„ŸçŸ¥å™¨â€ä¸­ï¼Œæœ‰ä¸¤ä¸ªå±‚æ¬¡ã€‚åˆ†åˆ«æ˜¯è¾“å…¥å±‚å’Œè¾“å‡ºå±‚ã€‚è¾“å…¥å±‚é‡Œçš„â€œè¾“å…¥å•å…ƒâ€åªè´Ÿè´£ä¼ è¾“æ•°æ®ï¼Œä¸åšè®¡ç®—ã€‚è¾“å‡ºå±‚é‡Œçš„â€œè¾“å‡ºå•å…ƒâ€åˆ™éœ€è¦å¯¹å‰é¢ä¸€å±‚çš„è¾“å…¥è¿›è¡Œè®¡ç®—ã€‚
ã€€ã€€æˆ‘ä»¬æŠŠéœ€è¦è®¡ç®—çš„å±‚æ¬¡ç§°ä¹‹ä¸ºâ€œè®¡ç®—å±‚â€ï¼Œå¹¶æŠŠæ‹¥æœ‰ä¸€ä¸ªè®¡ç®—å±‚çš„ç½‘ç»œç§°ä¹‹ä¸ºâ€œå•å±‚ç¥ç»ç½‘ç»œâ€ã€‚æœ‰ä¸€äº›æ–‡çŒ®ä¼šæŒ‰ç…§ç½‘ç»œæ‹¥æœ‰çš„å±‚æ•°æ¥å‘½åï¼Œä¾‹å¦‚æŠŠâ€œæ„ŸçŸ¥å™¨â€ç§°ä¸ºä¸¤å±‚ç¥ç»ç½‘ç»œã€‚
ã€€ã€€![](https://ai-studio-static-online.cdn.bcebos.com/4d9941e12672428bbd677a418bacdf16f12a35def6354ed5ac3e119e3e653811)
**å¤šå±‚æ„ŸçŸ¥æœº**
   å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼ŒMultilayer Perceptronï¼‰ä¹Ÿå«äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼ŒArtificial Neural Networkï¼‰ï¼Œé™¤äº†è¾“å…¥è¾“å‡ºå±‚ï¼Œå®ƒä¸­é—´å¯ä»¥æœ‰å¤šä¸ªéšå±‚ï¼Œæœ€ç®€å•çš„MLPåªå«ä¸€ä¸ªéšå±‚ï¼Œå³ä¸‰å±‚çš„ç»“æ„ã€‚
   å¤šå±‚æ„ŸçŸ¥æœºå±‚ä¸å±‚ä¹‹é—´æ˜¯å…¨è¿æ¥çš„ã€‚å¤šå±‚æ„ŸçŸ¥æœºæœ€åº•å±‚æ˜¯è¾“å…¥å±‚ï¼Œä¸­é—´æ˜¯éšè—å±‚ï¼Œæœ€åæ˜¯è¾“å‡ºå±‚ã€‚ 

**å‰å‘ä¼ æ’­**
	è®¾æ¿€æ´»å‡½æ•°æ˜¯Ïƒ ( z )ï¼Œéšè—å±‚å’Œè¾“å‡ºå±‚çš„è¾“å‡ºå€¼ä¸ºaï¼Œåˆ™å¯¹äºä¸‹å›¾çš„ä¸‰å±‚DNNï¼Œåˆ©ç”¨å’Œæ„ŸçŸ¥æœºä¸€æ ·çš„æ€è·¯ï¼Œå¯ä»¥åˆ©ç”¨ä¸Šä¸€å±‚çš„è¾“å‡ºè®¡ç®—ä¸‹ä¸€å±‚çš„è¾“å‡ºï¼Œä¹Ÿå°±æ˜¯æ‰€è°“çš„DNNå‰å‘ä¼ æ’­ç®—æ³•ã€‚
   DNNçš„å‰å‘ä¼ æ’­ç®—æ³•ä¹Ÿå°±æ˜¯åˆ©ç”¨æˆ‘ä»¬çš„è‹¥å¹²ä¸ªæƒé‡ç³»æ•°çŸ©é˜µWå’Œåå€šå‘é‡bæ¥å’Œè¾“å…¥å€¼å‘é‡xè¿›è¡Œä¸€ç³»åˆ—çº¿æ€§è¿ç®—å’Œæ¿€æ´»è¿ç®—ï¼Œä»è¾“å…¥å±‚å¼€å§‹ï¼Œä¸€å±‚å±‚çš„å‘åè®¡ç®—ï¼Œä¸€ç›´åˆ°è¿ç®—åˆ°è¾“å‡ºå±‚ï¼Œå¾—åˆ°è¾“å‡ºç»“æœä¸ºæ­¢ã€‚
   è¾“å…¥: æ€»å±‚æ•°Lï¼Œæ‰€æœ‰éšè—å±‚å’Œè¾“å‡ºå±‚å¯¹åº”çš„çŸ©é˜µWï¼Œåå€šå‘é‡bï¼Œè¾“å…¥å€¼å‘é‡x
è¾“å‡ºï¼šè¾“å‡ºå±‚çš„è¾“å‡ºal
![](https://ai-studio-static-online.cdn.bcebos.com/de60222eec73497ebf05e757a1213477d02e1f248b894833a551f5c01457376d)

**åå‘ä¼ æ’­**
ä»¥åˆ†ç±»ä¸ºä¾‹ï¼Œæœ€ç»ˆæ€»æ˜¯æœ‰è¯¯å·®çš„ï¼Œé‚£ä¹ˆæ€ä¹ˆå‡å°‘è¯¯å·®å‘¢ï¼Œå½“å‰åº”ç”¨å¹¿æ³›çš„ä¸€ä¸ªç®—æ³•å°±æ˜¯æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œä½†æ˜¯æ±‚æ¢¯åº¦å°±è¦æ±‚åå¯¼æ•°ï¼Œä¸‹é¢ä»¥å›¾ä¸­å­—æ¯ä¸ºä¾‹è®²è§£ä¸€ä¸‹ã€‚
è®¾æœ€ç»ˆæ€»è¯¯å·®ä¸ºEEï¼Œå¯¹äºè¾“å‡ºé‚£ä¹ˆEE#å¯¹äºè¾“å‡ºç»“ç‚¹ylçš„åå¯¼æ•°æ˜¯yl - tlï¼Œå…¶ä¸­tlæ˜¯çœŸå®å€¼âˆ‚ylâˆ‚zlâˆ‚ylâˆ‚zlæ˜¯æŒ‡ä¸Šé¢æåˆ°çš„æ¿€æ´»å‡½æ•°ï¼Œzlzlæ˜¯ä¸Šé¢æåˆ°çš„åŠ æƒå’Œï¼Œé‚£ä¹ˆè¿™ä¸€å±‚çš„EEå¯¹zlzlçš„åå¯¼æ•°ä¸ºâˆ‚Eâˆ‚zl=âˆ‚Eâˆ‚ylâˆ‚ylâˆ‚zlâˆ‚Eâˆ‚zl=âˆ‚Eâˆ‚ylâˆ‚ylâˆ‚zlã€‚åŒç†ï¼Œä¸‹ä¸€å±‚ä¹Ÿæ˜¯è¿™ä¹ˆè®¡ç®—ï¼Œï¼ˆåªä¸è¿‡âˆ‚Eâˆ‚ykâˆ‚Eâˆ‚ykè®¡ç®—æ–¹æ³•å˜äº†ï¼‰ï¼Œä¸€ç›´åå‘ä¼ æ’­åˆ°è¾“å…¥å±‚ï¼Œæœ€åæœ‰âˆ‚Eâˆ‚xi=âˆ‚Eâˆ‚yjâˆ‚yjâˆ‚zjâˆ‚Eâˆ‚xi=âˆ‚Eâˆ‚yjâˆ‚yjâˆ‚zj ä¸” âˆ‚zjâˆ‚xi=wijâˆ‚zjâˆ‚xi=wij
ç„¶åè°ƒæ•´è¿™äº›è¿‡ç¨‹ä¸­çš„æƒå€¼ï¼Œå†ä¸æ–­è¿›è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„è¿‡ç¨‹ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæ¯”è¾ƒå¥½çš„ç»“æœã€‚
![](https://ai-studio-static-online.cdn.bcebos.com/b02f62969ea4430595236544000bfd652dc48c116879476ba6162384ea6a7018)



```python
# æŸ¥çœ‹å½“å‰æŒ‚è½½çš„æ•°æ®é›†ç›®å½•, è¯¥ç›®å½•ä¸‹çš„å˜æ›´é‡å¯ç¯å¢ƒåä¼šè‡ªåŠ¨è¿˜åŸ
# View dataset directory. 
# This directory will be recovered automatically after resetting environment. 
!ls /home/aistudio/data
```

    data269



```python
# æŸ¥çœ‹å·¥ä½œåŒºæ–‡ä»¶, è¯¥ç›®å½•ä¸‹çš„å˜æ›´å°†ä¼šæŒä¹…ä¿å­˜. è¯·åŠæ—¶æ¸…ç†ä¸å¿…è¦çš„æ–‡ä»¶, é¿å…åŠ è½½è¿‡æ…¢.
# View personal work directory. 
# All changes under this directory will be kept even after reset. 
# Please clean unnecessary files in time to speed up environment loading. 
!ls /home/aistudio/work
```

    housing.data



```python
# å¦‚æœéœ€è¦è¿›è¡ŒæŒä¹…åŒ–å®‰è£…, éœ€è¦ä½¿ç”¨æŒä¹…åŒ–è·¯å¾„, å¦‚ä¸‹æ–¹ä»£ç ç¤ºä¾‹:
# If a persistence installation is required, 
# you need to use the persistence path as the following: 
!mkdir /home/aistudio/external-libraries
!pip install beautifulsoup4 -t /home/aistudio/external-libraries
```

    mkdir: cannot create directory â€˜/home/aistudio/external-librariesâ€™: File exists
    Looking in indexes: https://mirror.baidu.com/pypi/simple/
    Collecting beautifulsoup4
    [?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122kB 20.2MB/s eta 0:00:01
    [?25hCollecting soupsieve>1.2; python_version >= "3.0" (from beautifulsoup4)
      Downloading https://mirror.baidu.com/pypi/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl
    Installing collected packages: soupsieve, beautifulsoup4
    Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1
    [33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve already exists. Specify --upgrade to force replacement.[0m
    [33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve-2.2.1.dist-info already exists. Specify --upgrade to force replacement.[0m
    [33mWARNING: Target directory /home/aistudio/external-libraries/beautifulsoup4-4.9.3.dist-info already exists. Specify --upgrade to force replacement.[0m
    [33mWARNING: Target directory /home/aistudio/external-libraries/bs4 already exists. Specify --upgrade to force replacement.[0m



```python
# åŒæ—¶æ·»åŠ å¦‚ä¸‹ä»£ç , è¿™æ ·æ¯æ¬¡ç¯å¢ƒ(kernel)å¯åŠ¨çš„æ—¶å€™åªè¦è¿è¡Œä¸‹æ–¹ä»£ç å³å¯: 
# Also add the following code, 
# so that every time the environment (kernel) starts, 
# just run the following code: 
import sys 
sys.path.append('/home/aistudio/external-libraries')
```

è¯·ç‚¹å‡»[æ­¤å¤„](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)æŸ¥çœ‹æœ¬ç¯å¢ƒåŸºæœ¬ç”¨æ³•.  <br>
Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. 


```python
#python numpyå®ç°åŒå±‚ç¥ç»ç½‘ç»œæˆ¿ä»·é¢„æµ‹
import numpy as np
import random
import json
import matplotlib
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
def load_data():
    # ä»æ–‡ä»¶å¯¼å…¥æ•°æ®
    datafile = './work/housing.data'
    data = np.fromfile(datafile, sep=' ')

    # æ¯æ¡æ•°æ®åŒ…æ‹¬14é¡¹ï¼Œå…¶ä¸­å‰é¢13é¡¹æ˜¯å½±å“å› ç´ ï¼Œç¬¬14é¡¹æ˜¯ç›¸åº”çš„æˆ¿å±‹ä»·æ ¼ä¸­ä½æ•°
    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \
                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]
    feature_num = len(feature_names)

    # å°†åŸå§‹æ•°æ®è¿›è¡ŒReshapeï¼Œå˜æˆ[N, 14]è¿™æ ·çš„å½¢çŠ¶
    data = data.reshape([data.shape[0] // feature_num, feature_num])

    # å°†åŸæ•°æ®é›†æ‹†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # è¿™é‡Œä½¿ç”¨80%çš„æ•°æ®åšè®­ç»ƒï¼Œ20%çš„æ•°æ®åšæµ‹è¯•
    # æµ‹è¯•é›†å’Œè®­ç»ƒé›†å¿…é¡»æ˜¯æ²¡æœ‰äº¤é›†çš„
    ratio = 0.8
    offset = int(data.shape[0] * ratio)
    training_data = data[:offset]

    # è®¡ç®—è®­ç»ƒé›†çš„æœ€å¤§å€¼ï¼Œæœ€å°å€¼ï¼Œå¹³å‡å€¼
    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \
                                 training_data.sum(axis=0) / training_data.shape[0]

    # å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
    for i in range(feature_num):
        #print(maximums[i], minimums[i], avgs[i])
        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])

    # è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ’åˆ†æ¯”ä¾‹
    training_data = data[:offset]
    test_data = data[offset:]
    return training_data, test_data
# è·å–æ•°æ®
training_data, test_data = load_data()
x = training_data[:, :-1]
y = training_data[:, -1:]
# æŸ¥çœ‹æ•°æ®
#print(x[0])
#print(y[0])
class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œæ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        np.random.seed(0)
        self.w1 = np.random.randn(num_of_weights, 1)
        self.b1 = 0.2
        np.random.seed(1)
        self.w2 = np.random.randn(num_of_weights, 1)
        self.b2 = 0.
        
        
    def forward(self, x):
        z1 = np.dot(x, self.w1) + self.b1
        z2 = np.dot(x, self.w2) + self.b1
        z =z1 + z2 + self.b2
        return z
    
    def loss(self, z, y):
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    def gradient(self, x, y):
        z = self.forward(x)
        z1 = self.forward(x)
        z2 = self.forward(x)
        gradient_b2 = (z - y)
        gradient_b2 = np.mean(gradient_b2)
        gradient_w1 = (z-y)*x
        gradient_w1 = np.mean(gradient_w1, axis=0)
        gradient_w1 = gradient_w1[:, np.newaxis]
        gradient_w2 = (z-y)*x
        gradient_w2 = np.mean(gradient_w2, axis=0)
        gradient_w2 = gradient_w2[:, np.newaxis]
        gradient_b1 = (z - y)
        gradient_b1 = np.mean(gradient_b1)        
        return gradient_b2,gradient_w1,gradient_w2,gradient_b1
    
    def update(self, gradient_b2,gradient_w1,gradient_w2,gradient_b1, eta = 0.01):
        self.w1 = self.w1 - eta * gradient_w1
        self.w2 = self.w2 - eta * gradient_w2
        self.b1 = self.b1 - eta * gradient_b1
        self.b2 = self.b2 - eta * gradient_b2
        
    def train(self, x, y, iterations=100, eta=0.01):
        losses = []
        for i in range(iterations):
            z = self.forward(x)
            L = self.loss(z, y)
            gradient_b2,gradient_w1,gradient_w2,gradient_b1 = self.gradient(x, y)
            self.update(gradient_b2,gradient_w1,gradient_w2,gradient_b1, eta)
            losses.append(L)
            if (i+1) % 10 == 0:
                print('iter {}, loss {}'.format(i, L))
        return losses

# è·å–æ•°æ®
train_data, test_data = load_data()
x = train_data[:, :-1]
y = train_data[:, -1:]
# åˆ›å»ºç½‘ç»œ
net = Network(13)
num_iterations=1000
# å¯åŠ¨è®­ç»ƒ
losses = net.train(x,y, iterations=num_iterations, eta=0.01)

# ç”»å‡ºæŸå¤±å‡½æ•°çš„å˜åŒ–è¶‹åŠ¿
plot_x = np.arange(num_iterations)
plot_y = np.array(losses)
plt.plot(plot_x, plot_y)
plt.show()
```

    iter 9, loss 5.008962647338311
    iter 19, loss 4.113722853029151
    iter 29, loss 3.6423425680297323
    iter 39, loss 3.281732016539723
    iter 49, loss 2.9704776158578627
    iter 59, loss 2.6941275885361007
    iter 69, loss 2.447254104231053
    iter 79, loss 2.22639458799293
    iter 89, loss 2.02871339893448
    iter 99, loss 1.8517258880978686
    iter 109, loss 1.6932221509418974
    iter 119, loss 1.551230444372803
    iter 129, loss 1.4239903535743328
    iter 139, loss 1.3099299127577537
    iter 149, loss 1.2076453766401147
    iter 159, loss 1.1158831958420816
    iter 169, loss 1.033523929957471
    iter 179, loss 0.9595678851350226
    iter 189, loss 0.8931222903963989
    iter 199, loss 0.8333898476583954
    iter 209, loss 0.7796585082630183
    iter 219, loss 0.7312923446169984
    iter 229, loss 0.6877233996242835
    iter 239, loss 0.6484444091640602
    iter 249, loss 0.6130023040885555
    iter 259, loss 0.5809924082342142
    iter 269, loss 0.5520532578858193
    iter 279, loss 0.5258619761206873
    iter 289, loss 0.5021301425919316
    iter 299, loss 0.48060010567762235
    iter 309, loss 0.4610416896083046
    iter 319, loss 0.44324925426188266
    iter 329, loss 0.4270390698475693
    iter 339, loss 0.41224697274770256
    iter 349, loss 0.3987262723997758
    iter 359, loss 0.3863458823274528
    iter 369, loss 0.37498865131011194
    iter 379, loss 0.36454987325263155
    iter 389, loss 0.3549359566137372
    iter 399, loss 0.3460632363018002
    iter 409, loss 0.3378569127778838
    iter 419, loss 0.33025010474058225
    iter 429, loss 0.32318300322682586
    iter 439, loss 0.3166021162660877
    iter 449, loss 0.31045959438906234
    iter 459, loss 0.30471262833087837
    iter 469, loss 0.2993229111965752
    iter 479, loss 0.2942561581848829
    iter 489, loss 0.2894816777058967
    iter 499, loss 0.28497198838858634
    iter 509, loss 0.2807024770636631
    iter 519, loss 0.2766510933337741
    iter 529, loss 0.2727980768130233
    iter 539, loss 0.26912571353750653
    iter 549, loss 0.26561811842327715
    iter 559, loss 0.26226104098273595
    iter 569, loss 0.25904169180918313
    iter 579, loss 0.25594858760600736
    iter 589, loss 0.25297141277514895
    iter 599, loss 0.25010089579212713
    iter 609, loss 0.24732869878478797
    iter 619, loss 0.24464731890246638
    iter 629, loss 0.2420500002136177
    iter 639, loss 0.23953065500513776
    iter 649, loss 0.23708379347726094
    iter 659, loss 0.2347044609356805
    iter 669, loss 0.23238818167873979
    iter 679, loss 0.23013090886344756
    iter 689, loss 0.22792897971076972
    iter 699, loss 0.22577907547913542
    iter 709, loss 0.22367818569624284
    iter 719, loss 0.22162357619385117
    iter 729, loss 0.21961276053899179
    iter 739, loss 0.21764347449856625
    iter 749, loss 0.21571365321315888
    iter 759, loss 0.21382141079060013
    iter 769, loss 0.21196502206079967
    iter 779, loss 0.21014290626103904
    iter 789, loss 0.2083536124456158
    iter 799, loss 0.20659580643578987
    iter 809, loss 0.2048682591456799
    iter 819, loss 0.20316983613734363
    iter 829, loss 0.2014994882739784
    iter 839, loss 0.1998562433542019
    iter 849, loss 0.1982391986228914
    iter 859, loss 0.19664751406523948
    iter 869, loss 0.19508040640066812
    iter 879, loss 0.19353714370215305
    iter 889, loss 0.19201704057447475
    iter 899, loss 0.19051945383201294
    iter 909, loss 0.18904377862305408
    iter 919, loss 0.18758944495324206
    iter 929, loss 0.1861559145658676
    iter 939, loss 0.1847426781412065
    iter 949, loss 0.18334925278115446
    iter 959, loss 0.1819751797490073
    iter 969, loss 0.18062002243745293
    iter 979, loss 0.17928336454071822
    iter 989, loss 0.1779648084093726
    iter 999, loss 0.17666397356858898



![png](output_9_1.png)



```python
#paddleå®ç°åŒå±‚ç¥ç»ç½‘ç»œæˆ¿ä»·é¢„æµ‹
import paddle
from paddle.nn import Linear
import paddle.nn.functional as F
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import os
import random
def load_data():
    # ä»æ–‡ä»¶å¯¼å…¥æ•°æ®
    datafile = './work/housing.data'
    data = np.fromfile(datafile, sep=' ', dtype=np.float32)

    # æ¯æ¡æ•°æ®åŒ…æ‹¬14é¡¹ï¼Œå…¶ä¸­å‰é¢13é¡¹æ˜¯å½±å“å› ç´ ï¼Œç¬¬14é¡¹æ˜¯ç›¸åº”çš„æˆ¿å±‹ä»·æ ¼ä¸­ä½æ•°
    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \
                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]
    feature_num = len(feature_names)

    # å°†åŸå§‹æ•°æ®è¿›è¡ŒReshapeï¼Œå˜æˆ[N, 14]è¿™æ ·çš„å½¢çŠ¶
    data = data.reshape([data.shape[0] // feature_num, feature_num])

    # å°†åŸæ•°æ®é›†æ‹†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # è¿™é‡Œä½¿ç”¨80%çš„æ•°æ®åšè®­ç»ƒï¼Œ20%çš„æ•°æ®åšæµ‹è¯•
    # æµ‹è¯•é›†å’Œè®­ç»ƒé›†å¿…é¡»æ˜¯æ²¡æœ‰äº¤é›†çš„
    ratio = 0.8
    offset = int(data.shape[0] * ratio)
    training_data = data[:offset]

    # è®¡ç®—trainæ•°æ®é›†çš„æœ€å¤§å€¼ï¼Œæœ€å°å€¼ï¼Œå¹³å‡å€¼
    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \
                                 training_data.sum(axis=0) / training_data.shape[0]
    
    # è®°å½•æ•°æ®çš„å½’ä¸€åŒ–å‚æ•°ï¼Œåœ¨é¢„æµ‹æ—¶å¯¹æ•°æ®åšå½’ä¸€åŒ–
    global max_values
    global min_values
    global avg_values
    max_values = maximums
    min_values = minimums
    avg_values = avgs

    # å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
    for i in range(feature_num):
        data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])

    # è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ’åˆ†æ¯”ä¾‹
    training_data = data[:offset]
    test_data = data[offset:]
    return training_data, test_data
class Regressor(paddle.nn.Layer):

    # selfä»£è¡¨ç±»çš„å®ä¾‹è‡ªèº«
    def __init__(self):
        # åˆå§‹åŒ–çˆ¶ç±»ä¸­çš„ä¸€äº›å‚æ•°
        super(Regressor, self).__init__()
        
        # å®šä¹‰ä¸€å±‚å…¨è¿æ¥å±‚ï¼Œè¾“å…¥ç»´åº¦æ˜¯13ï¼Œè¾“å‡ºç»´åº¦æ˜¯1
        self.fc = Linear(in_features=13, out_features=1)
        self.act = paddle.nn.Sigmoid()
        self.fc2 = paddle.nn.Linear(in_features=10, out_features=1)
    
    # ç½‘ç»œçš„å‰å‘è®¡ç®—
    def forward(self, inputs):
        x = self.fc(inputs)
        return x
# å£°æ˜å®šä¹‰å¥½çš„çº¿æ€§å›å½’æ¨¡å‹
model = Regressor()
# å¼€å¯æ¨¡å‹è®­ç»ƒæ¨¡å¼
model.train()
# åŠ è½½æ•°æ®
training_data, test_data = load_data()
# å®šä¹‰ä¼˜åŒ–ç®—æ³•ï¼Œä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™SGD
# å­¦ä¹ ç‡è®¾ç½®ä¸º0.01
opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())
EPOCH_NUM = 10   # è®¾ç½®å¤–å±‚å¾ªç¯æ¬¡æ•°
BATCH_SIZE = 10  # è®¾ç½®batchå¤§å°

# å®šä¹‰å¤–å±‚å¾ªç¯
for epoch_id in range(EPOCH_NUM):
    # åœ¨æ¯è½®è¿­ä»£å¼€å§‹ä¹‹å‰ï¼Œå°†è®­ç»ƒæ•°æ®çš„é¡ºåºéšæœºçš„æ‰“ä¹±
    np.random.shuffle(training_data)
    # å°†è®­ç»ƒæ•°æ®è¿›è¡Œæ‹†åˆ†ï¼Œæ¯ä¸ªbatchåŒ…å«10æ¡æ•°æ®
    mini_batches = [training_data[k:k+BATCH_SIZE] for k in range(0, len(training_data), BATCH_SIZE)]
    # å®šä¹‰å†…å±‚å¾ªç¯
    for iter_id, mini_batch in enumerate(mini_batches):
        x = np.array(mini_batch[:, :-1]) # è·å¾—å½“å‰æ‰¹æ¬¡è®­ç»ƒæ•°æ®
        y = np.array(mini_batch[:, -1:]) # è·å¾—å½“å‰æ‰¹æ¬¡è®­ç»ƒæ ‡ç­¾ï¼ˆçœŸå®æˆ¿ä»·ï¼‰
        # å°†numpyæ•°æ®è½¬ä¸ºé£æ¡¨åŠ¨æ€å›¾tensorå½¢å¼
        house_features = paddle.to_tensor(x)
        prices = paddle.to_tensor(y)
        
        # å‰å‘è®¡ç®—
        predicts = model(house_features)
        
        # è®¡ç®—æŸå¤±
        loss = F.square_error_cost(predicts, label=prices)
        avg_loss = paddle.mean(loss)
        if iter_id%20==0:
            print("epoch: {}, iter: {}, loss is: {}".format(epoch_id, iter_id, avg_loss.numpy()))
        
        # åå‘ä¼ æ’­
        avg_loss.backward()
        # æœ€å°åŒ–loss,æ›´æ–°å‚æ•°
        opt.step()
        # æ¸…é™¤æ¢¯åº¦
        opt.clear_grad()

# ä¿å­˜æ¨¡å‹å‚æ•°ï¼Œæ–‡ä»¶åä¸ºLR_model.pdparams
paddle.save(model.state_dict(), 'LR_model.pdparams')
print("æ¨¡å‹ä¿å­˜æˆåŠŸï¼Œæ¨¡å‹å‚æ•°ä¿å­˜åœ¨LR_model.pdparamsä¸­")
def load_one_example():
    # ä»ä¸Šè¾¹å·²åŠ è½½çš„æµ‹è¯•é›†ä¸­ï¼Œéšæœºé€‰æ‹©ä¸€æ¡ä½œä¸ºæµ‹è¯•æ•°æ®
    idx = np.random.randint(0, test_data.shape[0])
    idx = -10
    one_data, label = test_data[idx, :-1], test_data[idx, -1]
    # ä¿®æ”¹è¯¥æ¡æ•°æ®shapeä¸º[1,13]
    one_data =  one_data.reshape([1,-1])

    return one_data, label
# å‚æ•°ä¸ºä¿å­˜æ¨¡å‹å‚æ•°çš„æ–‡ä»¶åœ°å€
model_dict = paddle.load('LR_model.pdparams')
model.load_dict(model_dict)
model.eval()

# å‚æ•°ä¸ºæ•°æ®é›†çš„æ–‡ä»¶åœ°å€
one_data, label = load_one_example()
# å°†æ•°æ®è½¬ä¸ºåŠ¨æ€å›¾çš„variableæ ¼å¼ 
one_data = paddle.to_tensor(one_data)
predict = model(one_data)

# å¯¹ç»“æœåšåå½’ä¸€åŒ–å¤„ç†
predict = predict * (max_values[-1] - min_values[-1]) + avg_values[-1]
# å¯¹labelæ•°æ®åšåå½’ä¸€åŒ–å¤„ç†
label = label * (max_values[-1] - min_values[-1]) + avg_values[-1]

print("Inference result is {}, the corresponding label is {}".format(predict.numpy(), label))
```

    epoch: 0, iter: 0, loss is: [0.05365043]
    epoch: 0, iter: 20, loss is: [0.09618521]
    epoch: 0, iter: 40, loss is: [0.0267882]
    epoch: 1, iter: 0, loss is: [0.02718393]
    epoch: 1, iter: 20, loss is: [0.04753923]
    epoch: 1, iter: 40, loss is: [0.10560463]
    epoch: 2, iter: 0, loss is: [0.11071634]
    epoch: 2, iter: 20, loss is: [0.0349807]
    epoch: 2, iter: 40, loss is: [0.03577711]
    epoch: 3, iter: 0, loss is: [0.04978322]
    epoch: 3, iter: 20, loss is: [0.04606231]
    epoch: 3, iter: 40, loss is: [0.15120089]
    epoch: 4, iter: 0, loss is: [0.02655281]
    epoch: 4, iter: 20, loss is: [0.06915595]
    epoch: 4, iter: 40, loss is: [0.02645713]
    epoch: 5, iter: 0, loss is: [0.07028259]
    epoch: 5, iter: 20, loss is: [0.02720477]
    epoch: 5, iter: 40, loss is: [0.05303243]
    epoch: 6, iter: 0, loss is: [0.03828095]
    epoch: 6, iter: 20, loss is: [0.03983556]
    epoch: 6, iter: 40, loss is: [0.05357898]
    epoch: 7, iter: 0, loss is: [0.00382383]
    epoch: 7, iter: 20, loss is: [0.03967518]
    epoch: 7, iter: 40, loss is: [0.01905488]
    epoch: 8, iter: 0, loss is: [0.05705719]
    epoch: 8, iter: 20, loss is: [0.02869581]
    epoch: 8, iter: 40, loss is: [0.02749251]
    epoch: 9, iter: 0, loss is: [0.00814354]
    epoch: 9, iter: 20, loss is: [0.02823475]
    epoch: 9, iter: 40, loss is: [0.01072248]
    æ¨¡å‹ä¿å­˜æˆåŠŸï¼Œæ¨¡å‹å‚æ•°ä¿å­˜åœ¨LR_model.pdparamsä¸­
    Inference result is [[19.665342]], the corresponding label is 19.700000762939453

