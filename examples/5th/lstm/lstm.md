# 循环神经网络RNN和长短时记忆网络LSTM

## RNN和LSTM网络的设计思考

与读者熟悉的卷积神经网络（Convolutional Neural Networks, CNN）一样，各种形态的神经网络在设计之初，均有针对特定场景的奇思妙想。卷积神经网络的设计具备适合视觉任务“局部视野”特点，是因为视觉信息是局部有效的。例如在一张图片的1/4区域上有一只小猫，如果将图片3/4的内容遮挡，人类依然可以判断这是一只猫。

与此类似，RNN和LSTM的设计初衷是部分场景神经网络需要有“记忆”能力才能解决的任务。在自然语言处理任务中，往往一段文字中某个词的语义可能与前一段句子的语义相关，只有记住了上下文的神经网络才能很好的处理句子的语义关系。例如：
> 我一边吃着苹果，一边玩着苹果手机。

网络只有正确的记忆两个“苹果”的上下文“吃着”和“玩着…手机”，才能正确的识别两个苹果的语义，分别是水果和手机品牌。如果网络没有记忆功能，那么两个“苹果”只能归结到更高概率出现的语义上，得到一个相同的语义输出，这显然是不合理的。

如何设计神经网络的记忆功能呢？我们先了解下RNN网络是如何实现具备记忆功能的。RNN相当于将神经网络单元进行了横向连接，处理前一部分输入的RNN单元不仅有正常的模型输出，还会输出“记忆”传递到下一个RNN单元。而处于后一部分的RNN单元，不仅仅有来自于任务数据的输入，同时会接收从前一个RNN单元传递过来的记忆输入，这样就使得整个神经网络具备了“记忆”能力。

但是RNN网络只是初步实现了“记忆”功能，在此基础上科学家们又发明了一些RNN的变体，来加强网络的记忆能力。但RNN对“记忆”能力的设计是比较粗糙的，当网络处理的序列数据过长时，累积的内部信息就会越来越复杂，直到超过网络的承载能力，通俗的说“事无巨细的记录，总有一天大脑会崩溃”。为了解决这个问题，科学家巧妙的设计了一种记忆单元，称之为“长短时记忆网络（Long Short-Term Memory，LSTM）”。在每个处理单元内部，加入了输入门、输出门和遗忘门的设计，三者有明确的任务分工：

* 输入门：控制有多少输入信号会被融合；
* 遗忘门：控制有多少过去的记忆会被遗忘；
* 输出门：控制多少处理后的信息会被输出；

三者的作用与人类的记忆方式有异曲同工之处，即：

* 与当前任务无关的信息会直接过滤掉，如非常专注的开车时，人们几乎不注意沿途的风景；
* 过去记录的事情不一定都要永远记住，如令人伤心或者不重要的事，通常会很快被淡忘；
* 根据记忆和现实观察进行决策，如开车时会结合记忆中的路线和当前看到的路标，决策转弯或不做任何动作。

了解了这些关于网络设计的本质，下面进入实现方案的细节。

## RNN网络结构

RNN是一个非常经典的面向序列的模型，可以对自然语言句子或是其它时序信号进行建模，网络结构如 **图5** 所示。
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/3f4d413393824d208177a020dcfa68205fa192ebecf04b42a5a17cbe7a146abb" width="600" ></center>
<br><center>图5：RNN网络结构</center></br>

不同于其他常见的神经网络结构，循环神经网络的输入是一个序列信息。假设给定任意一句话$[x_0, x_1, ..., x_n]$，其中每个$x_i$都代表了一个词，如“我，爱，人工，智能”。循环神经网络从左到右逐词阅读这个句子，并不断调用一个相同的RNN Cell来处理时序信息。每阅读一个单词，循环神经网络会先将本次输入的单词通过embedding lookup转换为一个向量表示。再把这个单词的向量表示和这个模型内部记忆的向量$h$融合起来，形成一个更新的记忆。最后将这个融合后的表示输出出来，作为它当前阅读到的所有内容的语义表示。当循环神经网络阅读过整个句子之后，我们就可以认为它的最后一个输出状态表示了整个句子的语义信息。

听上去很复杂，下面我们以一个简单地例子来说明，假设输入的句子为：

> “我，爱，人工，智能”

循环神经网络开始从左到右阅读这个句子，在未经过任何阅读之前，循环神经网络中的记忆向量是空白的。其处理逻辑如下：
1. 网络阅读单词“我”，并把单词“我”的向量表示和空白记忆相融合，输出一个向量$h_1$，用于表示“空白+我”的语义。
1. 网络开始阅读单词“爱”，这时循环神经网络内部存在“空白+我”的记忆。循环神经网络会将“空白+我”和“爱”的向量表示相融合，并输出“空白+我+爱”的向量表示$h_2$，用于表示“我爱”这个短语的语义信息。
1. 网络开始阅读单词“人工”，同样经过融合之后，输出“空白+我+爱+人工”的向量表示$h_3$，用于表示“空白+我+爱+人工”语义信息。
1. 最终在网络阅读了“智能”单词后，便可以输出“我爱人工智能”这一句子的整体语义信息。

------
**说明：**

在实现当前输入$x_t$和已有记忆$h_{t-1}$融合的时候，循环神经网络采用相加并通过一个激活函数tanh的方式实现：

$h_t = tanh(WX_{t}+VH_{t-1}+b)$

tanh函数是一个值域为（-1,1）的函数，其作用是长期维持内部记忆在一个固定的数值范围内，防止因多次迭代更新导致数值爆炸。同时tanh的导数是一个平滑的函数，会让神经网络的训练变得更加简单。

-----

## LSTM网络结构

上述方法听上去很有效（事实上在有些任务上效果还不错），但是存在一个明显的缺陷，就是当阅读很长的序列时，网络内部的信息会变得越来越复杂，甚至会超过网络的记忆能力，使得最终的输出信息变得混乱无用。长短时记忆网络（Long Short-Term Memory，LSTM）内部的复杂结构正是为处理这类问题而设计的，其网络结构如 **图6** 所示。
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/d7d112d458ff401ba7fb089cf0521b284c795798dcfb4b88bd2dd6b65854193a" width="600" ></center>
<br><center>图6：LSTM网络结构</center></br>

长短时记忆网络的结构和循环神经网络非常类似，都是通过不断调用同一个cell来逐次处理时序信息。每阅读一个新单词$x_t$，就会输出一个新的输出信号$h_t$，用来表示当前阅读到所有内容的整体向量表示。不过二者又有一个明显区别，长短时记忆网络在不同cell之间传递的是两个记忆信息，而不像循环神经网络一样只有一个记忆信息，此外长短时记忆网络的内部结构也更加复杂，如 **图7** 所示。

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/024b611db4ad4b709a4e1bd403ac41897c73792b895d4557a2dd561d625fcbf1" width="400" ></center>
<br><center>图7：LSTM网络内部结构示意图</center></br>

区别于循环神经网络RNN，长短时记忆网络最大的特点是在更新内部记忆时，引入了遗忘机制。即容许网络忘记过去阅读过程中看到的一些无关紧要的信息，只保留有用的历史信息。通过这种方式延长了记忆长度。举个例子：

> 我觉得这家餐馆的菜品很不错，烤鸭非常正宗，包子也不错，酱牛肉很有嚼劲。但是服务员态度太恶劣了，我们在门口等了50分钟都没有能成功进去，好不容易进去了，桌子也半天没人打扫。整个环境非常吵闹，我的孩子都被吓哭了，我下次不会带朋友来。

当我们阅读上面这段话的时候，可能会记住一些关键词，如烤鸭好吃、牛肉有嚼劲、环境吵等，但也会忽略一些不重要的内容，如“我觉得”、“好不容易”等，长短时记忆网络正是受这个启发而设计的。

长短时记忆网络的Cell有三个输入：

- 这个网络新看到的输入信号，如下一个单词，记为$x_{t}$， 其中$x_{t}$是一个向量，$t$代表了当前时刻。
- 这个网络在上一步的输出信号，记为$h_{t-1}$，这是一个向量，维度同$x_{t}$相同。
- 这个网络在上一步的记忆信号，记为$c_{t-1}$，这是一个向量，维度同$x_{t}$相同。

得到这两个信号之后，长短时记忆网络没有立即去融合这两个向量，而是计算了权重。

- 输入门：$i_{t}=sigmoid(W_{i}X_{t}+V_{i}H_{t-1}+b_i)$，控制有多少输入信号会被融合。
- 遗忘门：$f_{t}=sigmoid(W_{f}X_{t}+V_{f}H_{t-1}+b_f)$，控制有多少过去的记忆会被遗忘。
- 输出门：$o_{t}=sigmoid(W_{o}X_{t}+V_{o}H_{t-1}+b_o)$，控制最终输出多少融合了记忆的信息。
- 单元状态：$g_{t}=tanh(W_{g}X_{t}+V_{g}H_{t-1}+b_g)$，输入信号和过去的输入信号做一个信息融合。

通过学习这些门的权重设置，长短时记忆网络可以根据当前的输入信号和记忆信息，有选择性地忽略或者强化当前的记忆或是输入信号，帮助网络更好地学习长句子的语义信息：

- 记忆信号：$c_{t} = f_{t} \cdot c_{t-1} + i_{t} \cdot g_{t}$

- 输出信号：$h_{t} = o_{t} \cdot tanh(c_{t})$


