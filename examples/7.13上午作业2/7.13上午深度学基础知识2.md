# 可变形卷积方法详解

## 背景

在计算机视觉领域，视觉识别中的一个关键挑战是如何在目标尺度，姿态，视点和部件变形中适应几何变化或建模几何变换。同一物体在不同场景，角度中未知的几何变换是检测和识别的一大难题，通常来说我们有两种做法:

(1)通过充足的数据增强，扩充足够多的样本去增强模型适应尺度变换的能力。

(2)设置一些针对几何变换不变的特征或者算法，比如SIFT和sliding windows。

两种方法都有缺陷，第一种方法*因为样本的局限性显然模型的泛化能力比较低，无法泛化到一般场景中*，第二种方法则*因为手工设计的不变特征和算法对于过于复杂的变换是很难的而无法设计*。所以作者提出了Deformable Conv（可变形卷积）和 Deformable Pooling（可变形池化）来解决这个问题。

## 1、DCN v1

传统的卷积采用固定尺寸的卷积核，不能很好地适应几何形变。可变形卷积顾名思义就是卷积的位置是可变形的，并非在传统的N × N的网格上做卷积，这样的好处就是更准确地提取到我们想要的特征（传统的卷积仅仅只能提取到矩形框的特征），通过图3我们可以更直观地了解：

![m3](C:\Users\spade-卿\Documents\Tencent Files\2606984901\FileRecv\images\7.13上午图片\m3.png)

在上面这张图里面，左边传统的卷积显然没有提取到完整绵羊的特征，而右边的可变形卷积则提取到了完整的不规则绵羊的特征。
那可变卷积实际上是怎么做的呢？其实就是在每一个卷积采样点加上了一个偏移量，如下图所示：

![m4](C:\Users\spade-卿\Documents\Tencent Files\2606984901\FileRecv\images\7.13上午图片\m4.png)

(a) 所示的正常卷积规律的采样 9 个点（绿点），(b）（c）(d) 为可变形卷积，在正常的采样坐标上加上一个位移量（蓝色箭头），其中 （c）(d) 作为 (b) 的特殊情况，展示了可变形卷积可以作为尺度变换，比例变换和旋转变换等特殊情况。
以一个普通的卷积为例，以3x3卷积为例，对于每个输出y(p0)，都要从x上采样9个位置，这9个位置都在中心位置x(p0)向四周扩散，(-1,-1)代表x(p0)的左上角，(1,1)代表x(p0)的右下角。$$ R = \lbrace (-1,-1),(-1,0),…(1,1) \rbrace $$
所以传统的卷积输出为（其中$P_n$就是网格中的n个点）：
$$ y(p_0) = \sum_{p_n \in R }w(p_n)*x(p_o+p_n)  \tag{公式6} $$
正如我们上面阐述的可变形卷积，他就是在传统的卷积操作上加入了一个偏移量，正是这个偏移量才让卷积变形为不规则的卷积，这里要注意这个偏移量可以是小数，所以下面的式子的特征值需要通过双线性插值的方法来计算。：
$$ y(p_0) = \sum_{p_n \in R }w(p_n)*x(p_o+p_n+ \Delta p_n)  \tag{公式7} $$

## DCN v2

DCNv1引入了可变形卷积，能更好的适应目标的几何变换。但是v1可视化结果显示其感受野对应位置超出了目标范围，导致特征不受图像内容影响，降低算法表现（理想情况是所有的对应位置分布在目标范围以内）。
为了解决该问题：提出v2, 主要有：  
1、使用更多变形卷积；  
2、在DCNv1基础上添加每个采样点的权重；  
3、模拟RNN的feature。

### 使用更多变形卷积
在DCN v1中只在conv 5中使用了三个可变形卷积，在DCN v2中把conv3到conv5都换成了可变形卷积，提高算法对几何形变的建模能力。
### 在DCNv1基础上添加每个采样点的权重
DCV v1中的卷积就只是添加了一个偏移量$\Delta p_n $如公式7所示。  
为了解决引入了一些无关区域的问题，在DCN v2中我们不只添加每一个采样点的偏移，还添加了一个权重系数$\Delta m_k$来区分我们引入的区域是否为我们感兴趣的区域，假如这个采样点的区域我们不感兴趣，则把权重学习为0即可：
$$ y(p) = \sum_{k=1}^{K}w_k \cdot x(p+p_k+ \Delta p_k) \cdot \Delta m_k  \tag{公式8} $$  
总的来说，DCN v1中引入的offset是要寻找有效信息的区域位置，DCN v2中引入权重系数是要给找到的这个位置赋予权重，这两方面保证了有效信息的准确提取。

### 模拟RNN的feature
把R-CNN和Faster RCNN的classification score结合起来可以提升performance，说明R-CNN学到的focus在物体上的feature可以解决无关上下文的问题。但是增加额外的R-CNN会使inference速度变慢很多。DCNV2里的解决方法是把R-CNN当做teacher network，让DCN V2的ROIPooling之后的feature去模拟R-CNN的feature，类似知识蒸馏的做法，具体展开如下图：

![m5](C:\Users\spade-卿\Documents\Tencent Files\2606984901\FileRecv\images\7.13上午图片\m5.png)

左边的网络为主网络（Faster RCNN），右边的网络为子网络（RCNN）。实现上大致是用主网络训练过程中得到的RoI去裁剪原图，然后将裁剪到的图resize到224×224大小作为子网络的输入，这部分最后提取的特征和主网络输出的1024维特征作为feature mimicking loss的输入，用来约束这2个特征的差异（通过一个余弦相似度计算，如公式 9 所示），同时子网络通过一个分类损失进行监督学习，因为并不需要回归坐标，所以没有回归损失。在inference阶段仅有主网络部分，因此这个操作不会在inference阶段增加计算成本。
$$ L_{mimic} = \sum _{b \in \Omega}[1- \cos (f_{RCNN}(b),f_{FRCNN}(b)) ] \tag{公式9}$$
更加具体解释说，就是因为RCNN这个子网络的输入就是RoI在原输入图像上裁剪出来的图像，因此不存在RoI以外区域信息的干扰，这就使得RCNN这个网络训练得到的分类结果更加可靠，以此通过一个损失函数监督主网络Faster RCNN的分类支路训练就能够使网络提取到更多RoI内部特征，而不是自己引入的外部特征。
总的loss由三部分组成：mimic loss + R-CNN classification loss + Faster-RCNN loss.
