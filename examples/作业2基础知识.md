损失函数方法补充
损失函数用于描述模型预测值与真实值的差距大小。一般有有两种常见的算法——均值平方差（MSE）和交叉熵。下面来分别介绍每个算法的具体内容。
1 均值平方差
均值平方差（Mean Squared Error，MSE），也称“均方误差”，在神经网络中主要是表达预测值和真实值之间的差异，在数理统计中，均方误差是指参数估计值与参数真值之差平方的预期值。
公式如下：主要是对每一个真实值与预期值相减的平方取平均值：
![](https://ai-studio-static-online.cdn.bcebos.com/3bb3f70bca1e4a189985e50810decdf46d6d3f572f4a41dfb60c42c9895cf427)

均方误差的值越小，表明模型越好。
类似的损失算法还有均方根误差RMSE（将MSE开平方）：
![](https://ai-studio-static-online.cdn.bcebos.com/ae40da57f01f4967acf9343c1d6f14e67b6d1197ab6b4d6eaee73c8807bf256c)

平均绝对值误差MAD（对一个真实值与预测值相减的绝对值取平均值）
![](https://ai-studio-static-online.cdn.bcebos.com/f5cb3ffaf7f74eb29d2f7413ae57073dec7b03259f1b452787071ef686d205b5)

2 交叉熵
交叉熵（crossentropy）也是loss算法的一种，一般用在分类问题上，表达意思为预测输入样本属于哪一类的概率。其表达式如下，其中y代表真实值分类（0或1），a代表预测值。
二分类情况下的公式：
![](https://ai-studio-static-online.cdn.bcebos.com/854858ca201942978ae35ac4a2b81cca99b5c293decb4bd496780f974d8d8293)

交叉熵也是值越小，代表预测结果越准。
3 损失算法的选取
损失函数的选取取决于输入标签数据的类型：
如果输入的实数、无界的值，损失函数使用平方差。
如果输入标签是位矢量（分类标志），使用交叉熵会更适合。



池化方法补充
1.  一般池化（General Pooling）
池化作用于图像中不重合的区域（这与卷积操作不同），过程如下图。
我们定义池化窗口的大小为sizeX，即下图中红色正方形的边长，定义两个相邻池化窗口的水平位移/竖直位移为stride。一般池化由于每一池化窗口都是不重复的，所以sizeX=stride。
![](https://ai-studio-static-online.cdn.bcebos.com/96e7e20166dd4ca399dca95078844ba7c42fae2fd9724936bbaa02270d741858)

最常见的池化操作为平均池化mean pooling和最大池化max pooling：
平均池化：计算图像区域的平均值作为该区域池化后的值。
最大池化：选图像区域的最大值作为该区域池化后的值。

2. 重叠池化（OverlappingPooling）[2]
重叠池化正如其名字所说的，相邻池化窗口之间会有重叠区域，此时sizeX>stride。

3. 空金字塔池化（Spatial Pyramid Pooling）[3] 
空间金字塔池化可以把任何尺度的图像的卷积特征转化成相同维度，这不仅可以让CNN处理任意尺度的图像，还能避免cropping和warping操作，导致一些信息的丢失，具有非常重要的意义。
一般的CNN都需要输入图像的大小是固定的，这是因为全连接层的输入需要固定输入维度，但在卷积操作是没有对图像尺度有限制，所有作者提出了空间金字塔池化，先让图像进行卷积操作，然后转化成维度相同的特征输入到全连接层，这个可以把CNN扩展到任意大小的图像。
![](https://ai-studio-static-online.cdn.bcebos.com/05f5cb4d1d9442a6a794f79b0d44fa045cd31f18910b45159f8a39cb3d8a1d10)

空间金字塔池化的思想来自于Spatial Pyramid Model，它一个pooling变成了多个scale的pooling。用不同大小池化窗口作用于卷积特征，我们可以得到1X1,2X2,4X4的池化结果，由于conv5中共有256个过滤器，所以得到1个256维的特征，4个256个特征，以及16个256维的特征，然后把这21个256维特征链接起来输入全连接层，通过这种方式把不同大小的图像转化成相同维度的特征。
![](https://ai-studio-static-online.cdn.bcebos.com/cdaf5ecf763345d1b0e28dc61be46b9fd1109a1feff24b0e85f7334c74fc66f5)

对于不同的图像要得到相同大小的pooling结果，就需要根据图像的大小动态的计算池化窗口的大小和步长。假设conv5输出的大小为a*a，需要得到n*n大小的池化结果，可以让窗口大小sizeX为，步长为 。下图以conv5输出的大小为13*13为例。
![](https://ai-studio-static-online.cdn.bcebos.com/fe3a43f7961543d59114da4095ef1450d518e549a64645d5814d42422cfd3092)

数据增强方法
1、什么是数据增强？
数据增强也叫数据扩增，意思是在不实质性的增加数据的情况下，让有限的数据产生等价于更多数据的价值。

每张图对于网络来说都是不同的输入，加上原图就将数据扩充到原来的10倍。假如我们输入网络的图片的分辨率大小是256×256，若采用随机裁剪成224×224的方式，那么一张图最多可以产生32×32张不同的图，数据量扩充将近1000倍。虽然许多的图相似度太高，实际的效果并不等价，但仅仅是这样简单的一个操作，效果已经非凡了。

如果再辅助其他的数据增强方法，将获得更好的多样性，这就是数据增强的本质。

数据增强可以分为，有监督的数据增强和无监督的数据增强方法。其中有监督的数据增强又可以分为单样本数据增强和多样本数据增强方法，无监督的数据增强分为生成新的数据和学习增强策略两个方向。



2、有监督的数据增强
有监督数据增强，即采用预设的数据变换规则，在已有数据的基础上进行数据的扩增，包含单样本数据增强和多样本数据增强，其中单样本又包括几何操作类，颜色变换类。

2.1. 单样本数据增强

所谓单样本数据增强，即增强一个样本的时候，全部围绕着该样本本身进行操作，包括几何变换类，颜色变换类等。

(1) 几何变换类

几何变换类即对图像进行几何变换，包括翻转，旋转，裁剪，变形，缩放等各类操作
翻转操作和旋转操作，对于那些对方向不敏感的任务，比如图像分类，都是很常见的操作，在caffe等框架中翻转对应的就是mirror操作。
翻转和旋转不改变图像的大小，而裁剪会改变图像的大小。通常在训练的时候会采用随机裁剪的方法，在测试的时候选择裁剪中间部分或者不裁剪。值得注意的是，在一些竞赛中进行模型测试时，一般都是裁剪输入的多个版本然后将结果进行融合，对预测的改进效果非常明显。
以上操作都不会产生失真，而缩放变形则是失真的。
很多的时候，网络的训练输入大小是固定的，但是数据集中的图像却大小不一，此时就可以选择上面的裁剪成固定大小输入或者缩放到网络的输入大小的方案，后者就会产生失真，通常效果比前者差。

(2) 颜色变换类
上面的几何变换类操作，没有改变图像本身的内容，它可能是选择了图像的一部分或者对像素进行了重分布。如果要改变图像本身的内容，就属于颜色变换类的数据增强了，常见的包括噪声、模糊、颜色变换、擦除、填充等等。
基于噪声的数据增强就是在原来的图片的基础上，随机叠加一些噪声，最常见的做法就是高斯噪声。更复杂一点的就是在面积大小可选定、位置随机的矩形区域上丢弃像素产生黑色矩形块，从而产生一些彩色噪声，以Coarse Dropout方法为代表，甚至还可以对图片上随机选取一块区域并擦除图像信息。



2.2. 多样本数据增强

不同于单样本数据增强，多样本数据增强方法利用多个样本来产生新的样本，下面介绍几种方法。

(1) SMOTE

SMOTE即Synthetic Minority Over-sampling Technique方法，它是通过人工合成新样本来处理样本不平衡问题，从而提升分类器性能。

类不平衡现象是很常见的，它指的是数据集中各类别数量不近似相等。如果样本类别之间相差很大，会影响分类器的分类效果。假设小样本数据数量极少，如仅占总体的1%，则即使小样本被错误地全部识别为大样本，在经验风险最小化策略下的分类器识别准确率仍能达到99%，但由于没有学习到小样本的特征，实际分类效果就会很差。

SMOTE方法是基于插值的方法，它可以为小样本类合成新的样本，主要流程为：
第一步，定义好特征空间，将每个样本对应到特征空间中的某一点，根据样本不平衡比例确定好一个采样倍率N；
第二步，对每一个小样本类样本(x,y)，按欧氏距离找出K个最近邻样本，从中随机选取一个样本点，假设选择的近邻点为(xn,yn)。在特征空间中样本点与最近邻样本点的连线段上随机选取一点作为新样本点
第三步，重复以上的步骤，直到大、小样本数量平衡。


(2) SamplePairing

SamplePairing方法的原理非常简单，从训练集中随机抽取两张图片分别经过基础数据增强操作(如随机翻转等)处理后经像素以取平均值的形式叠加合成一个新的样本，标签为原样本标签中的一种。这两张图片甚至不限制为同一类别，这种方法对于医学图像比较有效。
经SamplePairing处理后可使训练集的规模从N扩增到N×N。实验结果表明，因SamplePairing数据增强操作可能引入不同标签的训练样本，导致在各数据集上使用SamplePairing训练的误差明显增加，而在验证集上误差则有较大幅度降低。
尽管SamplePairing思路简单，性能上提升效果可观，符合奥卡姆剃刀原理，但遗憾的是可解释性不强。

(3) mixup

mixup是Facebook人工智能研究院和MIT在“Beyond Empirical Risk Minimization”中提出的基于邻域风险最小化原则的数据增强方法，它使用线性插值得到新样本数据。

令(xn,yn)是插值生成的新数据，(xi,yi)和(xj,yj)是训练集随机选取的两个数据
λ的取指范围介于0到1。提出mixup方法的作者们做了丰富的实验，实验结果表明可以改进深度学习模型在ImageNet数据集、CIFAR数据集、语音数据集和表格数据集中的泛化误差，降低模型对已损坏标签的记忆，增强模型对对抗样本的鲁棒性和训练生成对抗网络的稳定性。
SMOTE，SamplePairing，mixup三者思路上有相同之处，都是试图将离散样本点连续化来拟合真实样本分布，不过所增加的样本点在特征空间中仍位于已知小样本点所围成的区域内。如果能够在给定范围之外适当插值，也许能实现更好的数据增强效果。

3、无监督的数据增强
无监督的数据增强方法包括两类：

(1) 通过模型学习数据的分布，随机生成与训练数据集分布一致的图片，代表方法GAN。

(2) 通过模型，学习出适合当前任务的数据增强方法，代表方法AutoAugment。

3.1 GAN

关于GAN(generative adversarial networks)，我们已经说的太多了。它包含两个网络，一个是生成网络，一个是对抗网络，基本原理如下：

(1) G是一个生成图片的网络，它接收随机的噪声z，通过噪声生成图片，记做G(z) 。

(2) D是一个判别网络，判别一张图片是不是“真实的”，即是真实的图片，还是由G生成的图片。
。

2 Autoaugmentation

AutoAugment是Google提出的自动选择最优数据增强方案的研究，这是无监督数据增强的重要研究方向。它的基本思路是使用增强学习从数据本身寻找最佳图像变换策略，对于不同的任务学习不同的增强方法，流程如下：

(1) 准备16个常用的数据增强操作。

(2) 从16个中选择5个操作，随机产生使用该操作的概率和相应的幅度，将其称为一个sub-policy，一共产生5个sub-polices。

(3) 对训练过程中每一个batch的图片，随机采用5个sub-polices操作中的一种。

(4) 通过模型在验证集上的泛化能力来反馈，使用的优化方法是增强学习方法。

(5) 经过80~100个epoch后网络开始学习到有效的sub-policies。

(6) 之后串接这5个sub-policies，然后再进行最后的训练。

总的来说，就是学习已有数据增强的组合策略，对于门牌数字识别等任务，研究表明剪切和平移等几何变换能够获得最佳效果。




图像分类方法
图像分类是根据图像的语义信息将不同类别图像区分开来，是计算机视觉中重要的基本问题，也是图像检测、图像分割、物体跟踪、行为分析等其他高层视觉任务的基础。图像分类的主要过程包括图像预处理、特征提取和分类器设计。
（1）传统方法：
传统图像分类通过手工提取特征或特征学习方法对整个图像进行全部描述，然后使用分类器判别物体类别，因此如何提取图像的特征至关重要。
在特征提取方面，主要包括纹理、颜色、形状等底层视觉特征，尺度不变特征变换、局部二值模式、方向梯度直方图等局部不变性特征。
在分类器方面，主要包括k NN(k-nearest neighbor,k最近邻）决策树、SVM(support vector machine，支持向量机）、人工神经网络等方法。
（2）深度学习方法：
传统的图像分类方法往往需要对目标图像进行人工特征描述和提取，对于大数量的复杂数据很难取得低成本的有效结果。然而，深度学习方法通过神经网络自主地从训练样本中学习特征，提取出更高维、抽象的特征，并且这些特征与分类器关系紧密，很好地解决了人工提取特征和分类器选择的难题，是一种端到端的模型。
对于深度学习标准网络模型使用已经非常广泛，在这里对轻量化进行简单介绍：
常用的标准网络模型：Lenet、Alxnet、Vgg系列、Resnet系列、Inception系列、Densenet系列、Googlenet、Nasnet、Xception、Senet(state of art)；
轻量化网络模型：Mobilenet v1,v2、Shufflenet v1,v2,Squeezenet
目前轻量化模型在具体项目应用时反响很好，它主要存在的优缺点如下：
优点：（1）参数模型小，方便部署（2）计算量小，速度快
缺点：（1）轻量化模型在精度上没有Resnet系列、Inception系列、Densenet系列、Senet的accuracy高

请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>
Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. 
