{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "\n",
    "数据处理包含五个部分：数据导入、数据形状变换、数据集划分、数据归一化处理和封装`load data`函数。数据预处理后，才能被模型调用。\n",
    "\n",
    "### 读入数据\n",
    "\n",
    "通过如下代码读入数据，了解下波士顿房价的数据集结构，数据存放在本地目录下housing.data文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, ..., 3.969e+02, 7.880e+00,\n",
       "       1.190e+01])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入需要用到的package\n",
    "import numpy as np\n",
    "import json\n",
    "# 读入训练数据\n",
    "datafile = './work/housing.data'\n",
    "data = np.fromfile(datafile, sep=' ')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据形状变换\n",
    "\n",
    "由于读入的原始数据是1维的，所有数据都连在一起。因此需要我们将数据的形状进行变换，形成一个2维的矩阵，每行为一个数据样本（14个值），每个数据样本包含13个$X$（影响房价的特征）和一个$Y$（该类型房屋的均价）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入之后的数据被转化成1维array，其中array的第0-13项是第一条数据，第14-27项是第二条数据，以此类推.... \n",
    "# 这里对原始数据做reshape，变成N x 14的形式\n",
    "feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS', \n",
    "                 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "feature_num = len(feature_names)\n",
    "data = data.reshape([data.shape[0] // feature_num, feature_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14,)\n",
      "[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00 2.400e+01]\n"
     ]
    }
   ],
   "source": [
    "# 查看数据\n",
    "x = data[0]\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集划分\n",
    "\n",
    "将数据集划分成训练集和测试集，其中训练集用于确定模型的参数，测试集用于评判模型的效果。\n",
    "\n",
    "我们将80%的数据用作训练集，20%用作测试集，实现代码如下。通过打印训练集的形状，可以发现共有404个样本，每个样本含有13个特征和1个预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = 0.8\n",
    "offset = int(data.shape[0] * ratio)\n",
    "training_data = data[:offset]\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据归一化处理\n",
    "\n",
    "对每个特征进行归一化处理，使得每个特征的取值缩放到0~1之间。这样做有两个好处：一是模型训练更高效；二是特征前的权重大小可以代表该变量对预测结果的贡献度（因为每个特征值本身的范围相同）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算train数据集的最大值，最小值，平均值\n",
    "maximums, minimums, avgs = \\\n",
    "                     training_data.max(axis=0), \\\n",
    "                     training_data.min(axis=0), \\\n",
    "     training_data.sum(axis=0) / training_data.shape[0]\n",
    "# 对数据进行归一化处理\n",
    "for i in range(feature_num):\n",
    "    #print(maximums[i], minimums[i], avgs[i])\n",
    "    data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 封装成load data函数\n",
    "\n",
    "将上述几个数据处理操作封装成`load data`函数，以便下一步模型的调用，实现方法如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # 从文件导入数据\n",
    "    datafile = './work/housing.data'\n",
    "    data = np.fromfile(datafile, sep=' ')\n",
    "\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "    feature_num = len(feature_names)\n",
    "\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
    "\n",
    "    # 将原数据集拆分成训练集和测试集\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\n",
    "    # 测试集和训练集必须是没有交集的\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0] * ratio)\n",
    "    training_data = data[:offset]\n",
    "\n",
    "    # 计算训练集的最大值，最小值，平均值\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\n",
    "\n",
    "    # 对数据进行归一化处理\n",
    "    for i in range(feature_num):\n",
    "        #print(maximums[i], minimums[i], avgs[i])\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\n",
    "\n",
    "    # 训练集和测试集的划分比例\n",
    "    training_data = data[:offset]\n",
    "    test_data = data[offset:]\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "training_data, test_data = load_data()\n",
    "x = training_data[:, :-1]\n",
    "y = training_data[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.18       0.07344184 0.         0.31481481 0.57750527\n",
      " 0.64160659 0.26920314 0.         0.22755741 0.28723404 1.\n",
      " 0.08967991]\n",
      "[0.42222222]\n"
     ]
    }
   ],
   "source": [
    "# 查看数据\n",
    "print(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型设计\n",
    "\n",
    "模型设计是深度学习模型关键要素之一，也称为网络结构设计，相当于模型的假设空间，即实现模型“前向计算”（从输入到输出）的过程。\n",
    "\n",
    "如果将输入特征和输出预测值均以向量表示，输入特征$x$有13个分量，$y$有1个分量，那么参数权重的形状（shape）是$13\\times1$。假设我们以如下任意数字赋值参数做初始化：\n",
    "$$w=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, -0.1, -0.2, -0.3, -0.4, 0.0]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, -0.1, -0.2, -0.3, -0.4, 0.0]\n",
    "w = np.array(w).reshape([13, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取出第1条样本数据，观察样本的特征向量与参数向量相乘的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69474855]\n"
     ]
    }
   ],
   "source": [
    "x1=x[0]\n",
    "t = np.dot(x1, w)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整的线性回归公式，还需要初始化偏移量$b$，同样随意赋初值-0.2。那么，线性回归模型的完整输出是$z=t+b$，这个从特征和参数计算输出值的过程称为“前向计算”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49474855]\n"
     ]
    }
   ],
   "source": [
    "b = -0.2\n",
    "z = t + b\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上述计算预测输出的过程以“类和对象”的方式来描述，类成员变量有参数$w$和$b$。通过写一个`forward`函数（代表“前向计算”）完成上述从特征和参数到输出预测值的计算过程，代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，\n",
    "        # 此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于Network类的定义，模型的计算过程如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.39362982]\n"
     ]
    }
   ],
   "source": [
    "net = Network(13)\n",
    "x1 = x[0]\n",
    "y1 = y[0]\n",
    "z = net.forward(x1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练配置\n",
    "\n",
    "模型设计完成后，需要通过训练配置寻找模型的最优值，即通过损失函数来衡量模型的好坏。训练配置也是深度学习模型关键要素之一。\n",
    "\n",
    "通过模型计算$x_1$表示的影响因素所对应的房价应该是$z$, 但实际数据告诉我们房价是$y$。这时我们需要有某种指标来衡量预测值$z$跟真实值$y$之间的差距。对于回归问题，最常采用的衡量方法是使用均方误差作为评价模型好坏的指标，具体定义如下：\n",
    "\n",
    "$$Loss = (y - z)^2$$\n",
    "\n",
    "上式中的$Loss$（简记为: $L$）通常也被称作损失函数，它是衡量模型好坏的指标。在回归问题中，均方误差是一种比较常见的形式，分类问题中通常会采用交叉熵作为损失函数，在后续的章节中会更详细的介绍。对一个样本计算损失函数值的实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.88644793]\n"
     ]
    }
   ],
   "source": [
    "Loss = (y1 - z)*(y1 - z)\n",
    "print(Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为计算损失函数时需要把每个样本的损失函数值都考虑到，所以我们需要对单个样本的损失函数进行求和，并除以样本总数$N$。\n",
    "$$Loss= \\frac{1}{N}\\sum_{i=1}^N{(y_i - z_i)^2}$$\n",
    "在Network类下面添加损失函数的计算过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        cost = error * error\n",
    "        cost = np.mean(cost)\n",
    "        return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用定义的Network类，可以方便的计算预测值和损失函数。需要注意的是，类中的变量$x$, $w$，$b$, $z$, $error$等均是向量。以变量$x$为例，共有两个维度，一个代表特征数量（值为13），一个代表样本数量，代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict:  [[2.39362982]\n",
      " [2.46752393]\n",
      " [2.02483479]]\n",
      "loss: 3.384496992612791\n"
     ]
    }
   ],
   "source": [
    "net = Network(13)\n",
    "# 此处可以一次性计算多个样本的预测值和损失函数\n",
    "x1 = x[0:3]\n",
    "y1 = y[0:3]\n",
    "z = net.forward(x1)\n",
    "print('predict: ', z)\n",
    "loss = net.loss(z, y1)\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程\n",
    "\n",
    "上述计算过程描述了如何构建神经网络，通过神经网络完成预测值和损失函数的计算。接下来介绍如何求解参数$w$和$b$的数值，这个过程也称为模型训练过程。训练过程是深度学习模型的关键要素之一，其目标是让定义的损失函数$Loss$尽可能的小，也就是说找到一个参数解$w$和$b$，使得损失函数取得极小值。\n",
    "\n",
    "处于曲线极值点时的斜率为0，即函数在极值点的导数为0。那么，让损失函数取极小值的$w$和$b$应该是下述方程组的解：\n",
    "$$\\frac{\\partial{L}}{\\partial{w}}=0$$\n",
    "$$\\frac{\\partial{L}}{\\partial{b}}=0$$\n",
    "\n",
    "将样本数据$(x, y)$带入上面的方程组中即可求解出$w$和$b$的值，但是这种方法只对线性回归这样简单的任务有效。如果模型中含有非线性变换，或者损失函数不是均方差这种简单的形式，则很难通过上式求解。为了解决这个问题，下面我们将引入更加普适的数值求解方法：梯度下降法。\n",
    "\n",
    "### 梯度下降法\n",
    "\n",
    "在现实中存在大量的函数正向求解容易，但反向求解较难，被称为单向函数。\n",
    "\n",
    "训练的关键是找到一组$(w, b)$，使得损失函数$L$取极小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算梯度\n",
    "\n",
    "为了使梯度计算更加简洁，引入因子$\\frac{1}{2}$，定义损失函数如下：\n",
    "\n",
    "$$L= \\frac{1}{2N}\\sum_{i=1}^N{(y_i - z_i)^2}$$\n",
    "\n",
    "其中$z_i$是网络对第$i$个样本的预测值：\n",
    "\n",
    "$$z_i = \\sum_{j=0}^{12}{x_i^{j}\\cdot w_j} + b$$\n",
    "\n",
    "梯度的定义：\n",
    "\n",
    "$$𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡 = (\\frac{\\partial{L}}{\\partial{w_0}},\\frac{\\partial{L}}{\\partial{w_1}}, ... ,\\frac{\\partial{L}}{\\partial{w_{12}}} ,\\frac{\\partial{L}}{\\partial{b}})$$\n",
    "\n",
    "可以计算出$L$对$w$和$b$的偏导数：\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)\\frac{\\partial{z_i}}{\\partial{w_j}}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)x_i^{j}}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)\\frac{\\partial{z_i}}{\\partial{b}}} = \\frac{1}{N}\\sum_{i=1}^N{(z_i - y_i)}$$\n",
    "\n",
    "从导数的计算过程可以看出，因子$\\frac{1}{2}$被消掉了，这是因为二次函数求导的时候会产生因子$2$，这也是我们将损失函数改写的原因。\n",
    "\n",
    "下面我们考虑只有一个样本的情况下，计算梯度：\n",
    "\n",
    "$$L= \\frac{1}{2}{(y_i - z_i)^2}$$\n",
    "\n",
    "$$z_1 = {x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b$$\n",
    "\n",
    "可以计算出：\n",
    "\n",
    "$$L= \\frac{1}{2}{({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b - y_1)^2}$$\n",
    "\n",
    "可以计算出$L$对$w$和$b$的偏导数：\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_0}} = ({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_12} + b - y_1)\\cdot x_1^{0}=({z_1} - {y_1})\\cdot x_1^{0}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = ({x_1^{0}\\cdot w_0} + {x_1^{1}\\cdot w_1} + ...  + {x_1^{12}\\cdot w_{12}} + b - y_1)\\cdot 1 = ({z_1} - {y_1})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 使用Numpy进行梯度计算\n",
    "\n",
    "基于Numpy广播机制（对向量和矩阵计算如同对1个单一变量计算一样），可以更快速的实现梯度计算。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综合上面的剖析，计算梯度的代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.6555403 ],\n",
       "       [ 19.35268996],\n",
       "       [ 55.88081118],\n",
       "       [ 14.00266972],\n",
       "       [ 47.98588869],\n",
       "       [ 76.87210821],\n",
       "       [ 94.8555119 ],\n",
       "       [ 36.07579608],\n",
       "       [ 45.44575958],\n",
       "       [ 59.65733292],\n",
       "       [ 83.65114918],\n",
       "       [134.80387478],\n",
       "       [ 38.93998153]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = net.forward(x)\n",
    "gradient_w = (z - y) * x\n",
    "gradient_w = np.mean(gradient_w, axis=0)\n",
    "gradient_w = gradient_w[:, np.newaxis]\n",
    "gradient_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码非常简洁地完成了$w$的梯度计算。同样，计算$b$的梯度的代码也是类似的原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142.50289323156107"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_b = (z - y)\n",
    "gradient_b = np.mean(gradient_b)\n",
    "# 此处b是一个数值，所以可以直接用np.mean得到一个标量\n",
    "gradient_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将上面计算$w$和$b$的梯度的过程，写成Network类的`gradient`函数，实现方法如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)\n",
    "        \n",
    "        return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point [-100.0, -100.0], loss 7873.345739941161\n",
      "gradient [-45.87968288123223, -35.50236884482904]\n"
     ]
    }
   ],
   "source": [
    "# 调用上面定义的gradient函数，计算梯度\n",
    "# 初始化网络\n",
    "net = Network(13)\n",
    "# 设置[w5, w9] = [-100., -100.]\n",
    "net.w[5] = -100.0\n",
    "net.w[9] = -100.0\n",
    "\n",
    "z = net.forward(x)\n",
    "loss = net.loss(z, y)\n",
    "gradient_w, gradient_b = net.gradient(x, y)\n",
    "gradient_w5 = gradient_w[5][0]\n",
    "gradient_w9 = gradient_w[9][0]\n",
    "print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))\n",
    "print('gradient {}'.format([gradient_w5, gradient_w9]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 确定损失函数更小的点\n",
    "\n",
    "下面我们开始研究更新梯度的方法。首先沿着梯度的反方向移动一小步，找到下一个点P1，观察损失函数的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point [-95.41203171187678, -96.4497631155171], loss 7214.694816482369\n",
      "gradient [-43.883932999069096, -34.019273908495926]\n"
     ]
    }
   ],
   "source": [
    "# 在[w5, w9]平面上，沿着梯度的反方向移动到下一个点P1\n",
    "# 定义移动步长 eta\n",
    "eta = 0.1\n",
    "# 更新参数w5和w9\n",
    "net.w[5] = net.w[5] - eta * gradient_w5\n",
    "net.w[9] = net.w[9] - eta * gradient_w9\n",
    "# 重新计算z和loss\n",
    "z = net.forward(x)\n",
    "loss = net.loss(z, y)\n",
    "gradient_w, gradient_b = net.gradient(x, y)\n",
    "gradient_w5 = gradient_w[5][0]\n",
    "gradient_w9 = gradient_w[9][0]\n",
    "print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))\n",
    "print('gradient {}'.format([gradient_w5, gradient_w9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上面的代码，可以发现沿着梯度反方向走一小步，下一个点的损失函数的确减少了。感兴趣的话，大家可以尝试不停的点击上面的代码块，观察损失函数是否一直在变小。\n",
    "\n",
    "在上述代码中，每次更新参数使用的语句：\n",
    "`net.w[5] = net.w[5] - eta * gradient_w5`\n",
    "\n",
    "* 相减：参数需要向梯度的反方向移动。\n",
    "* eta：控制每次参数值沿着梯度反方向变动的大小，即每次移动的步长，又称为学习率。\n",
    "\n",
    "大家可以思考下，为什么之前我们要做输入特征的归一化，保持尺度一致？这是为了让统一的步长更加合适。\n",
    "\n",
    "\n",
    "###  代码封装Train函数\n",
    "\n",
    "将上面的循环计算过程封装在`train`和`update`函数中，实现方法如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, point [-99.54120317118768, -99.64497631155172], loss 7873.345739941161\n",
      "iter 50, point [-78.9761810944732, -83.65939206734069], loss 5131.480704109405\n",
      "iter 100, point [-62.4493631356931, -70.67918223434114], loss 3346.754494352463\n",
      "iter 150, point [-49.17799206644332, -60.12620415441553], loss 2184.906016270654\n",
      "iter 200, point [-38.53070194231174, -51.533984751788346], loss 1428.4172504483342\n",
      "iter 250, point [-29.998249130283174, -44.52613603923428], loss 935.7392894242679\n",
      "iter 300, point [-23.169901624519575, -38.79894318028118], loss 614.7592258739251\n",
      "iter 350, point [-17.71439280083778, -34.10731848231335], loss 405.53408184471505\n",
      "iter 400, point [-13.364557220746388, -30.253470630210863], loss 269.0551396220099\n",
      "iter 450, point [-9.904936677384967, -27.077764259976597], loss 179.9364750604248\n",
      "iter 500, point [-7.161782280775628, -24.451346444229817], loss 121.65711285489998\n",
      "iter 550, point [-4.994989383373879, -22.270198517465555], loss 83.46491706360901\n",
      "iter 600, point [-3.2915916915280783, -20.450337700789422], loss 58.36183370758033\n",
      "iter 650, point [-1.9605131425212885, -18.923946252536773], loss 41.792808952534\n",
      "iter 700, point [-0.9283343968114077, -17.636248840494844], loss 30.792614998570482\n",
      "iter 750, point [-0.13587780041668718, -16.542993494033716], loss 23.43065354742935\n",
      "iter 800, point [0.4645474092373408, -15.60841945615185], loss 18.449664464381506\n",
      "iter 850, point [0.9113672926170796, -14.803617811655524], loss 15.030615923519784\n",
      "iter 900, point [1.2355357562745004, -14.105208963393421], loss 12.639705730905764\n",
      "iter 950, point [1.4619805189121953, -13.494275706622066], loss 10.928795653764196\n",
      "iter 1000, point [1.6107694974712377, -12.955502492189021], loss 9.670616807081698\n",
      "iter 1050, point [1.6980516626374353, -12.476481020835202], loss 8.716602071285436\n",
      "iter 1100, point [1.7368159644039771, -12.04715001603925], loss 7.969442965176621\n",
      "iter 1150, point [1.7375034995020395, -11.659343238414994], loss 7.365228465612388\n",
      "iter 1200, point [1.7085012931271857, -11.306424818680442], loss 6.861819342703047\n",
      "iter 1250, point [1.6565405824483015, -10.982995030930885], loss 6.431280353078019\n",
      "iter 1300, point [1.5870180647823104, -10.684652890749808], loss 6.054953198278096\n",
      "iter 1350, point [1.5042550040699705, -10.407804594738165], loss 5.720248083137862\n",
      "iter 1400, point [1.4117062100403601, -10.14950894127009], loss 5.418553777303124\n",
      "iter 1450, point [1.3121285818148223, -9.907352585055445], loss 5.143875665274019\n",
      "iter 1500, point [1.2077170340724794, -9.67934935975478], loss 4.891947653805328\n",
      "iter 1550, point [1.1002141124777076, -9.463859017459276], loss 4.659652555766873\n",
      "iter 1600, point [0.990998385834045, -9.259521632951046], loss 4.444643323159747\n",
      "iter 1650, point [0.8811557188942747, -9.065204645952335], loss 4.245095084874306\n",
      "iter 1700, point [0.7715367363576023, -8.87996009965401], loss 4.059542401818773\n",
      "iter 1750, point [0.662803148565214, -8.702990105791185], loss 3.88677206759292\n",
      "iter 1800, point [0.5554650931141796, -8.533618947271485], loss 3.7257521401326525\n",
      "iter 1850, point [0.4499112301277286, -8.371270536496699], loss 3.5755846299900256\n",
      "iter 1900, point [0.3464329929523944, -8.215450195281456], loss 3.4354736574042524\n",
      "iter 1950, point [0.24524412503452966, -8.065729922139326], loss 3.3047037451160453\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3X10XPV95/H3V8+yZD3ZsixLBtlgICaEJwVMnjYJiTEkjWnzcMjmFJd6121Dd5Nmuy3ZdJdu0uxJ2m2S0gd6nODE9CQhhJLizbIhjiFtEoJBBmPAYCSMjS1sS7b8bMt6+u4f85MZC401Y83MleZ+XufMmXt/85s737mS5qN77+/ONXdHRETipyjqAkREJBoKABGRmFIAiIjElAJARCSmFAAiIjGlABARiSkFgIhITCkARERiSgEgIhJTJVEXcDazZ8/2tra2qMsQEZlWNm3atN/dGyfqN6UDoK2tjY6OjqjLEBGZVsxsZzr9tAtIRCSm0goAM/sjM3vBzJ43s++bWYWZLTCzjWbWZWY/MLOy0Lc8zHeFx9uSlvP50L7NzG7IzVsSEZF0TBgAZtYC/Geg3d3fChQDtwBfBb7u7hcCB4GV4SkrgYOh/euhH2a2ODzvUmAZ8A9mVpzdtyMiIulKdxdQCVBpZiXADGAP8H7ggfD4WuDmML08zBMev97MLLTf5+6n3P1VoAu4ZvJvQUREzsWEAeDu3cD/Bl4j8cF/GNgEHHL3odBtN9ASpluAXeG5Q6H/rOT2cZ4jIiJ5ls4uoHoS/70vAOYBVSR24eSEma0ysw4z6+jt7c3Vy4iIxF46u4A+ALzq7r3uPgg8CLwTqAu7hABage4w3Q3MBwiP1wIHktvHec5p7r7a3dvdvb2xccJhrCIico7SCYDXgCVmNiPsy78e2Ao8Bnws9FkBPBSm14V5wuOPeuK6k+uAW8IooQXAIuDJ7LyNM3UfOsnXfrqNHfuP52LxIiIFIZ1jABtJHMx9GnguPGc18KfA58ysi8Q+/nvCU+4BZoX2zwF3hOW8ANxPIjx+Atzu7sNZfTfBoRMD3PVoFy/tPZKLxYuIFIS0zgR29zuBO8c0b2ecUTzu3g98PMVyvgx8OcMaM9Y4sxyA3qOncv1SIiLTVkGeCTyrqpwiUwCIiJxNQQZAcZExq7qcHgWAiEhKBRkAAI3V5doCEBE5i4INgDk15fQeUwCIiKRSsAHQWF1OzxEFgIhIKoUbADPL2X/sFCMjHnUpIiJTUsEGwJyZ5QyNOIdODkZdiojIlFSwAdA4swKAnqP9EVciIjI1FXAA6GQwEZGzKdgAmKMAEBE5q4INgNEtAJ0MJiIyvoINgKryEmaUFWsLQEQkhYINAEhsBSgARETGV9ABMGdmuUYBiYikUNABoC0AEZHUCjsA9IVwIiIpFXQAzKmp4Ej/EP2DObnwmIjItDZhAJjZxWa2Oel2xMw+a2YNZrbezDrDfX3ob2Z2l5l1mdkWM7sqaVkrQv9OM1uR+lWzo7Fa5wKIiKSSzjWBt7n7Fe5+BXA1cAL4EYlr/W5w90XAhjAPcCOJC74vAlYBdwOYWQOJy0peS+JSkneOhkaunD4bWF8LLSLyJpnuAroeeMXddwLLgbWhfS1wc5heDtzrCU8AdWbWDNwArHf3Pnc/CKwHlk36HZzF6ZPBjmgkkIjIWJkGwC3A98N0k7vvCdN7gaYw3QLsSnrO7tCWqv0MZrbKzDrMrKO3tzfD8s7UXJv4Qri9hxUAIiJjpR0AZlYGfAT44djH3N2BrHzxvruvdvd2d29vbGyc1LIaqsooKy5ij7YARETeJJMtgBuBp919X5jfF3btEO57Qns3MD/pea2hLVV7zpgZc2rK2actABGRN8kkAD7JG7t/ANYBoyN5VgAPJbXfGkYDLQEOh11FjwBLzaw+HPxdGtpyam5NBXu1BSAi8iYl6XQysyrgg8DvJTV/BbjfzFYCO4FPhPaHgZuALhIjhm4DcPc+M/sS8FTo90V375v0O5hAU20FW18/kuuXERGZdtIKAHc/Dswa03aAxKigsX0duD3FctYAazIv89w111Tw6Is9uDtmls+XFhGZ0gr6TGCAubUVnBwc5kj/UNSliIhMKQUfAE01GgoqIjKegg+AuaPnAuhAsIjIGQo/AMIWgIaCioicqeADYE5N4usgtAUgInKmgg+A8pJiZlWVKQBERMYo+ACAxIFg7QISETlTLAJgbm0FexQAIiJniEUANNVUsE+7gEREzhCLAJhbU8GB4wOcGtKlIUVERsUiAEavC9BzRFcGExEZFYsAaNLJYCIibxKLAJgXAuD1QycjrkREZOqIRwDUVQLw+iFtAYiIjIpFAFSVl1A3o5TuQyeiLkVEZMqIRQAAtNRVagtARCRJWgFgZnVm9oCZvWRmL5rZdWbWYGbrzawz3NeHvmZmd5lZl5ltMbOrkpazIvTvNLMVqV8x++bVVdJ9UMcARERGpbsF8DfAT9z9EuBy4EXgDmCDuy8CNoR5SFw8flG4rQLuBjCzBuBO4FrgGuDO0dDIh8QWgAJARGTUhAFgZrXAe4B7ANx9wN0PAcuBtaHbWuDmML0cuNcTngDqzKwZuAFY7+597n4QWA8sy+q7OYuWukqOnhri8MnBfL2kiMiUls4WwAKgF/i2mT1jZt8KF4lvcvc9oc9eoClMtwC7kp6/O7Slaj+Dma0ysw4z6+jt7c3s3ZzFGyOBtBUgIgLpBUAJcBVwt7tfCRznjd09wOkLwXs2CnL31e7e7u7tjY2N2VgkAC31CgARkWTpBMBuYLe7bwzzD5AIhH1h1w7hvic83g3MT3p+a2hL1Z4X8+oSJ4N1KwBERIA0AsDd9wK7zOzi0HQ9sBVYB4yO5FkBPBSm1wG3htFAS4DDYVfRI8BSM6sPB3+Xhra8mF1VTllxkQJARCQoSbPffwK+a2ZlwHbgNhLhcb+ZrQR2Ap8IfR8GbgK6gBOhL+7eZ2ZfAp4K/b7o7n1ZeRdpKCoy5tVVaCioiEiQVgC4+2agfZyHrh+nrwO3p1jOGmBNJgVm0zwNBRUROS02ZwJDYiiodgGJiCTEKgDm1VXSc/QUA0MjUZciIhK5WAVAS10l7ujykCIixC0AwrkAuw7qW0FFRGIVAOc1zABgd5+OA4iIxCoAmmsrKC4yXuvTFoCISKwCoKS4iJa6SgWAiAgxCwBI7AZSAIiIxDAA5jfMYJcCQEQkfgFwXsMMDhwf4NipoahLERGJVCwDANBWgIjEXmwDQMcBRCTuYhsA2gIQkbiLXQDUziilpqJEWwAiEnuxCwCA82ZpKKiISDwDQOcCiIikFwBmtsPMnjOzzWbWEdoazGy9mXWG+/rQbmZ2l5l1mdkWM7sqaTkrQv9OM1uR6vVybX7DDHb3nWRkJCvXsRcRmZYy2QJ4n7tf4e6jVwa7A9jg7ouADWEe4EZgUbitAu6GRGAAdwLXAtcAd46GRr6d1zCDgeER9h3V10KLSHxNZhfQcmBtmF4L3JzUfq8nPAHUmVkzcAOw3t373P0gsB5YNonXP2ejI4F27NduIBGJr3QDwIGfmtkmM1sV2prcfU+Y3gs0hekWYFfSc3eHtlTtZzCzVWbWYWYdvb29aZaXmQWzqwDYceB4TpYvIjIdpHVReOBd7t5tZnOA9Wb2UvKD7u5mlpUd6u6+GlgN0N7enpOd9PNqKykrKeLV/QoAEYmvtLYA3L073PcAPyKxD39f2LVDuO8J3buB+UlPbw1tqdrzrqjIWDCriu29CgARia8JA8DMqsxs5ug0sBR4HlgHjI7kWQE8FKbXAbeG0UBLgMNhV9EjwFIzqw8Hf5eGtkgsmF3Fq/uPRfXyIiKRS2cXUBPwIzMb7f89d/+JmT0F3G9mK4GdwCdC/4eBm4Au4ARwG4C795nZl4CnQr8vuntf1t5JhhY0VrHhpX0MDY9QUhzL0yFEJOYmDAB33w5cPk77AeD6cdoduD3FstYAazIvM/sWzK5icNjpPnSS82dVRV2OiEjexfZf34VhJNB2HQgWkZiKbQCMDgV9VQeCRSSmYhsADVVl1FSUaCioiMRWbAPAzFjQWK0AEJHYim0AQOI4gAJAROIq1gGwYHYV3YdO0j84HHUpIiJ5F/sAALQVICKxFOsAuHBONQCdPTojWETiJ9YBsLCxiiKDrn1Hoy5FRCTvYh0A5SXFtM2q4uV92gIQkfiJdQAALGqqprNHWwAiEj8KgDkz2XHgBKeGNBJIROJFAdBUzfCI6/KQIhI7CoA5MwF4WQeCRSRmYh8AoyOBNBRUROIm9gFQUVrM+bOq6NKBYBGJmbQDwMyKzewZM/txmF9gZhvNrMvMfmBmZaG9PMx3hcfbkpbx+dC+zcxuyPabOVeL5lRrKKiIxE4mWwCfAV5Mmv8q8HV3vxA4CKwM7SuBg6H966EfZrYYuAW4FFgG/IOZFU+u/OxY1FTNjv3HGRgaiboUEZG8SSsAzKwV+BDwrTBvwPuBB0KXtcDNYXp5mCc8fn3ovxy4z91PufurJK4ZfE023sRkXdQ0k6ER13cCiUispLsF8A3gT4DRf5FnAYfcfSjM7wZawnQLsAsgPH449D/dPs5zInXJ3BoAXtxzJOJKRETyZ8IAMLMPAz3uvikP9WBmq8ysw8w6ent78/GSLGysoqykSAEgIrGSzhbAO4GPmNkO4D4Su37+Bqgzs5LQpxXoDtPdwHyA8HgtcCC5fZznnObuq9293d3bGxsbM35D56K0uIiLmqrZqgAQkRiZMADc/fPu3urubSQO4j7q7p8CHgM+FrqtAB4K0+vCPOHxR93dQ/stYZTQAmAR8GTW3skkLW6uYevrR0iUKiJS+CZzHsCfAp8zsy4S+/jvCe33ALNC++eAOwDc/QXgfmAr8BPgdnefMl/A85bmGg4cH6D36KmoSxERyYuSibu8wd1/Dvw8TG9nnFE87t4PfDzF878MfDnTIvNhcXPiQPDWPUeYU1MRcTUiIrkX+zOBR71l3hsBICISBwqAoKailNb6Sl7co6+EEJF4UAAkSRwIPhx1GSIieaEASPKW5hpe3X+ckwNT5ti0iEjOKACSXDqvhhHXcQARiQcFQJK3tdYBsGX3oYgrERHJPQVAkrm1FcyZWc6W3ToOICKFTwEwxtta63hWWwAiEgMKgDEub61le+9xjvQPRl2KiEhOKQDGeNv8xHGA57UbSEQKnAJgjMtbawF4VgEgIgVOATBG3Ywyzp81QyOBRKTgKQDG8bbWOp7dpQAQkcKmABjH5a21vH64n56j/VGXIiKSMwqAcVx5Xj0AT+/UVoCIFC4FwDje2lJDWUkRHTv6oi5FRCRn0rkofIWZPWlmz5rZC2b2P0P7AjPbaGZdZvYDMysL7eVhvis83pa0rM+H9m1mdkOu3tRklZcUc3lrLR07D0ZdiohIzqSzBXAKeL+7Xw5cASwzsyXAV4Gvu/uFwEFgZei/EjgY2r8e+mFmi0lcU/hSYBnwD2ZWnM03k03tbQ08331Y3wwqIgUrnYvCu7sfC7Ol4ebA+4EHQvta4OYwvTzMEx6/3swstN/n7qfc/VWgi3EuKTlVtJ9fz9CI62shRKRgpXUMwMyKzWwz0AOsB14BDrn7UOiyG2gJ0y3ALoDw+GESF40/3T7Oc6acq89PHAjWcQARKVRpBYC7D7v7FUArif/aL8lVQWa2ysw6zKyjt7c3Vy8zoboZZSyaU63jACJSsDIaBeTuh4DHgOuAOjMrCQ+1At1huhuYDxAerwUOJLeP85zk11jt7u3u3t7Y2JhJeVnX3tbApp0HGRnxSOsQEcmFdEYBNZpZXZiuBD4IvEgiCD4Wuq0AHgrT68I84fFH3d1D+y1hlNACYBHwZLbeSC68va2eo/1DvLhXVwgTkcJTMnEXmoG1YcROEXC/u//YzLYC95nZXwDPAPeE/vcA/2RmXUAfiZE/uPsLZnY/sBUYAm539yk9xOa6C2YB8OtXDnDpvNqIqxERya4JA8DdtwBXjtO+nXFG8bh7P/DxFMv6MvDlzMuMRnNtJQtnV/H4Kwf4D+9eGHU5IiJZpTOBJ/COC2excfsBBodHoi5FRCSrFAATeMcFszk+MKzrBItIwVEATGDJwtHjAPsjrkREJLsUABNoqCpjcXMNv+o6EHUpIiJZpQBIwzsumMWm1w7SPzilBy2JiGREAZCGd1/UyMDQCL/erq0AESkcCoA0XLuggYrSIn7+Uk/UpYiIZI0CIA0VpcW884LZPLqth8RJzSIi058CIE3vu2QOu/pO8krv8ahLERHJCgVAmt53yRwAHtNuIBEpEAqANLXUVXJx00we26YAEJHCoADIwHsvaeTJV/s42j8YdSkiIpOmAMjA9Zc0MTTiPLYtugvViIhkiwIgA1efX0/jzHJ+8vyeqEsREZk0BUAGiouMZZfO5bGXejkxMDTxE0REpjAFQIZuvGwuJweH+VftBhKRaS6dS0LON7PHzGyrmb1gZp8J7Q1mtt7MOsN9fWg3M7vLzLrMbIuZXZW0rBWhf6eZrUj1mlPZNW0NzKoq4/8+p91AIjK9pbMFMAT8F3dfDCwBbjezxcAdwAZ3XwRsCPMAN5K43u8iYBVwNyQCA7gTuJbElcTuHA2N6aSkuIill87l0Zd69OVwIjKtTRgA7r7H3Z8O00dJXBC+BVgOrA3d1gI3h+nlwL2e8ARQZ2bNwA3Aenfvc/eDwHpgWVbfTZ7cdNlcTgwM83OdEyAi01hGxwDMrI3E9YE3Ak3uProfZC/QFKZbgF1JT9sd2lK1TzvXLZzF7OpyHny6O+pSRETOWdoBYGbVwD8Dn3X3I8mPeeIb0rLyLWlmtsrMOsyso7d3ah5oLSku4uYr5vHYth76jg9EXY6IyDlJKwDMrJTEh/933f3B0Lwv7Noh3I/uD+kG5ic9vTW0pWo/g7uvdvd2d29vbGzM5L3k1UevbmVw2Fm3WVsBIjI9pTMKyIB7gBfd/WtJD60DRkfyrAAeSmq/NYwGWgIcDruKHgGWmll9OPi7NLRNS29prmFxcw0PPqMAEJHpKZ0tgHcCvw2838w2h9tNwFeAD5pZJ/CBMA/wMLAd6AK+CXwawN37gC8BT4XbF0PbtPXRq1vZsvswnfuORl2KiEjGbCpf4KS9vd07OjqiLiOl/cdOseR/beB33tHGn314cdTliIgAYGab3L19on46E3gSZleXc8Nb5/LDTbs5OaBzAkRkelEATNKtS87n8MlB/s+zr0ddiohIRhQAk3TNggYuaqrm3id26HrBIjKtKAAmycz47evaeL77CM/sOhR1OSIiaVMAZMFvXtlCdXkJ3/nVjqhLERFJmwIgC6rLS/j3157Hj7e8zmsHTkRdjohIWhQAWbLyXQsoKSpi9S9eiboUEZG0KACypKmmgo9e3cL9HbvpOdofdTkiIhNSAGTR773nAoaGR1jzyx1RlyIiMiEFQBa1za7iw2+bx9rHd9B79FTU5YiInJUCIMs+98GLGBwe4e8e7Yy6FBGRs1IAZFnb7Co+8fb5fO/J1zQiSESmNAVADnzm+kUUmfG19duiLkVEJCUFQA401VSw8l0L+JfNr7Np57T+xmsRKWAKgBy5/X0X0lxbwX//lxcYHtF3BInI1KMAyJGq8hK+8KG3sHXPEb67cWfU5YiIvEk6l4RcY2Y9ZvZ8UluDma03s85wXx/azczuMrMuM9tiZlclPWdF6N9pZivGe61C86HLmnnnhbP4q0e2sfewTg4TkaklnS2A7wDLxrTdAWxw90XAhjAPcCOwKNxWAXdDIjCAO4FrgWuAO0dDo5CZGX9x82UMDo9wx4Nb9HXRIjKlTBgA7v5vwNgjmcuBtWF6LXBzUvu9nvAEUGdmzcANwHp373P3g8B63hwqBWnB7CruWHYJP9/Wy/0du6IuR0TktHM9BtDk7nvC9F6gKUy3AMmfcrtDW6r2WLj1ujauWziLL/34RXb16dwAEZkaJn0Q2BP7NbK2b8PMVplZh5l19Pb2ZmuxkSoqMv7yY2/DDD793afpH9T1g0UkeucaAPvCrh3CfU9o7wbmJ/VrDW2p2t/E3Ve7e7u7tzc2Np5jeVPP/IYZ/PXHL+e57sN88cdboy5HROScA2AdMDqSZwXwUFL7rWE00BLgcNhV9Aiw1Mzqw8HfpaEtVpZeOpff/3cX8L2Nr+l4gIhErmSiDmb2feC9wGwz201iNM9XgPvNbCWwE/hE6P4wcBPQBZwAbgNw9z4z+xLwVOj3RXeP5Smyf7z0Ip7rPsR/e/A5WuoqeeeFs6MuSURiyqby0MT29nbv6OiIuoysO9I/yMfv/jWvHzrJD//gOi6ZWxN1SSJSQMxsk7u3T9RPZwJHoKailG/f9nZmlBezYs2TbO89FnVJIhJDCoCIzKur5N7fvZahYeeT33xCISAieacAiNDFc2fyvf+4hKFh55bVT/DyvqNRlyQiMaIAiNhoCDjw0bsf5/FX9kddkojEhAJgCrh47kx+9Ol3MLemghVrnuSHGiIqInmgAJgiWutn8MAfvIO3tzXwXx/Ywh3/vEVnDItITikAppDaylLu/d1r+PR7L+C+p3Zx89//im17dVxARHJDATDFlBQX8SfLLuHbt72dnqOn+PDf/oJv/OxlBoZGoi5NRAqMAmCKet/Fc1j/R+/hxrc2842fdfIbf/tLftFZGF+OJyJTgwJgCptVXc5dn7ySb93azvGBIX77nif5nW8/qd1CIpIV+iqIaeLU0DD3Pr6Tv320kyP9Qyxd3MSn33chV8yvi7o0EZli0v0qCAXANHPw+ADf/tWrfOfxHRzpH2LJwgY+de35LL20ifKS4qjLE5EpQAFQ4I6dGuJ7G3ey9vGddB86Sf2MUn7rqlZ+4/J5XN5ai5lFXaKIREQBEBPDI84vu/Zz35OvsX7rPoZGnJa6Sm5861w+sLiJq86rp6xEh3pE4kQBEEOHTgywfus+fvL8Xn7RuZ+B4REqS4u5dmED77pwNm9va+AtzTUKBJECpwCIuaP9g/z6lQP8sms/v+zaz/be4wCUlRRx6bwaLm+t47KWWhY1VXPhnGpmlE14bSARmSambACY2TLgb4Bi4Fvu/pVUfRUA2bPn8Emeee0Qm3clbs/tPszJpK+aaK2vZNGcatpmV9FaP4PW+spwm0FtZWmElYtIptINgLz+22dmxcDfAx8EdgNPmdk6d9dV0nOsubaS5ssquemyZgCGhkfY2XeCzn3H6Nx3lM6eY7y87ygbX+3jxMCZ30E0s7yExpnlzKouY3Z1ObOr35huqCpjZkUJMytKqQn3MytKqCjViCSRqS7f2/3XAF3uvh3AzO4DlgMKgDwrKS7igsZqLmisZtlb555ud3cOnRhk98GT7D54gt0HT9J96CS9x05x4NgpOnuO8evtBzh0YvCsyy8rKaKmooSq8hIqS4spLy2msrSIitJiKkqKqSwrpmJ0PrSVFBulxUZJUVHivriIkiKjtLiIkjHtpUWJ++Iio7jIKDIoMsOMMJ9oM0tMF4fHipL6jvYZnbYiKB6dDoOoTt9jp+dHx1eZWdI0Gnkl006+A6AFSP6u493AtXmuQc7CzKivKqO+qozLWmtT9hscHqHv+AB9xwc42j/E0f5BjvYPcSTp/sjJIY6fGqJ/cJj+oRH6B4bpOz7AyYFh+oeG6R9MtPUPDTM4PHWPRZ2rs4YFp5Pl9F1y2NiZD5/xfMYud5y+b6olRX3jtKZ8L+e6TJv0MtMP1nGXmWZNqV4m3ZoyWJ1pLfO9FzXyZx9ePP4CsmTKHfkzs1XAKoDzzjsv4mokldLiIppqKmiqqcjK8oZHnMHhEYZGnKHhEQaHnaGREYaG32gfHE7MD42Ex8O0O4y4MzzijHhiK2YktJ2+jSTmT/f1pL4jY/u/0TeZJ7U5JE2f2R46n552T/R5Y/qN9tHnc8Zyfczj478WyX3HLPOMunlz4/j9xjf+YcI0l5lioZOpKZNlptlEqmOh6b7+ZJc5XmNzXeW4z8+mfAdANzA/ab41tJ3m7quB1ZA4CJy/0iRKiV05Om4gkk/5HhD+FLDIzBaYWRlwC7AuzzWIiAh53gJw9yEz+0PgERLDQNe4+wv5rEFERBLyfgzA3R8GHs7364qIyJn0nQAiIjGlABARiSkFgIhITCkARERiSgEgIhJTU/rroM2sF9g5iUXMBvZnqZxsUl2ZUV2ZUV2ZKcS6znf3xok6TekAmCwz60jnK1HzTXVlRnVlRnVlJs51aReQiEhMKQBERGKq0ANgddQFpKC6MqO6MqO6MhPbugr6GICIiKRW6FsAIiKSQkEGgJktM7NtZtZlZnfk+bXnm9ljZrbVzF4ws8+E9j83s24z2xxuNyU95/Oh1m1mdkMOa9thZs+F1+8IbQ1mtt7MOsN9fWg3M7sr1LXFzK7KUU0XJ62TzWZ2xMw+G8X6MrM1ZtZjZs8ntWW8fsxsRejfaWYrclTXX5nZS+G1f2RmdaG9zcxOJq23f0x6ztXh598Vap/UNSxT1JXxzy3bf68p6vpBUk07zGxzaM/n+kr12RDd71jiKkeFcyPxNdOvAAuBMuBZYHEeX78ZuCpMzwReBhYDfw788Tj9F4cay4EFofbiHNW2A5g9pu0vgTvC9B3AV8P0TcD/I3H1uiXAxjz97PYC50exvoD3AFcBz5/r+gEagO3hvj5M1+egrqVASZj+alJdbcn9xiznyVCrhdpvzEFdGf3ccvH3Ol5dYx7/a+B/RLC+Un02RPY7VohbAKcvPO/uA8Dohefzwt33uPvTYfoo8CKJayGnshy4z91PufurQBeJ95Avy4G1YXotcHNS+72e8ARQZ2bNOa7leuAVdz/byX85W1/u/m9A3zivl8n6uQFY7+597n4QWA8sy3Zd7v5Tdx8Ks0+QuLpeSqG2Gnd/whOfIvcmvZes1XUWqX5uWf97PVtd4b/4TwDfP9sycrS+Un02RPY7VogBMN6F58/2AZwzZtYGXAlsDE1/GDbl1oxu5pHfeh34qZltssS1lwGa3H1PmN4LNEVQ16hbOPMPM+r1BZmvnyjW2++S+E9x1AIze8bM/tXM3h3aWkIt+agrk59bvtfXu4F97t6Z1Jb39TXmsyGy37FCDIApwcwKwUOnAAACaElEQVSqgX8GPuvuR4C7gQuAK4A9JDZD8+1d7n4VcCNwu5m9J/nB8J9OJMPCLHGJ0I8APwxNU2F9nSHK9ZOKmX0BGAK+G5r2AOe5+5XA54DvmVlNHkuacj+3MT7Jmf9k5H19jfPZcFq+f8cKMQAmvPB8rplZKYkf8Hfd/UEAd9/n7sPuPgJ8kzd2W+StXnfvDvc9wI9CDftGd+2E+5581xXcCDzt7vtCjZGvryDT9ZO3+szsd4APA58KHxyEXSwHwvQmEvvXLwo1JO8mykld5/Bzy+f6KgF+C/hBUr15XV/jfTYQ4e9YIQZApBeeD/sY7wFedPevJbUn7z//TWB0hMI64BYzKzezBcAiEgefsl1XlZnNHJ0mcRDx+fD6o6MIVgAPJdV1axiJsAQ4nLSZmgtn/GcW9fpKkun6eQRYamb1YffH0tCWVWa2DPgT4CPufiKpvdHMisP0QhLrZ3uo7YiZLQm/o7cmvZds1pXpzy2ff68fAF5y99O7dvK5vlJ9NhDl79hkjmpP1RuJo+cvk0jzL+T5td9FYhNuC7A53G4C/gl4LrSvA5qTnvOFUOs2JjnS4Cx1LSQxwuJZ4IXR9QLMAjYAncDPgIbQbsDfh7qeA9pzuM6qgANAbVJb3tcXiQDaAwyS2K+68lzWD4l98l3hdluO6uoisR949HfsH0Pfj4af72bgaeA3kpbTTuID+RXg7wgngma5rox/btn+ex2vrtD+HeD3x/TN5/pK9dkQ2e+YzgQWEYmpQtwFJCIiaVAAiIjElAJARCSmFAAiIjGlABARiSkFgIhITCkARERiSgEgIhJT/x8gg9FOjzGARQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights,1)\n",
    "        self.w[5] = -100.\n",
    "        self.w[9] = -100.\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)        \n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, gradient_w5, gradient_w9, eta=0.01):\n",
    "        net.w[5] = net.w[5] - eta * gradient_w5\n",
    "        net.w[9] = net.w[9] - eta * gradient_w9\n",
    "        \n",
    "    def train(self, x, y, iterations=100, eta=0.01):\n",
    "        points = []\n",
    "        losses = []\n",
    "        for i in range(iterations):\n",
    "            points.append([net.w[5][0], net.w[9][0]])\n",
    "            z = self.forward(x)\n",
    "            L = self.loss(z, y)\n",
    "            gradient_w, gradient_b = self.gradient(x, y)\n",
    "            gradient_w5 = gradient_w[5][0]\n",
    "            gradient_w9 = gradient_w[9][0]\n",
    "            self.update(gradient_w5, gradient_w9, eta)\n",
    "            losses.append(L)\n",
    "            if i % 50 == 0:\n",
    "                print('iter {}, point {}, loss {}'.format(i, [net.w[5][0], net.w[9][0]], L))\n",
    "        return points, losses\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "x = train_data[:, :-1]\n",
    "y = train_data[:, -1:]\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "num_iterations=2000\n",
    "# 启动训练\n",
    "points, losses = net.train(x, y, iterations=num_iterations, eta=0.01)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(num_iterations)\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练扩展到全部参数\n",
    "\n",
    "为了能给读者直观的感受，上面演示的梯度下降的过程仅包含$w_5$和$w_9$两个参数，但房价预测的完整模型，必须要对所有参数$w$和$b$进行求解。这需要将Network中的`update`和`train`函数进行修改。由于不再限定参与计算的参数（所有参数均参与计算），修改之后的代码反而更加简洁。实现逻辑：“前向计算输出、根据输出和真实值计算Loss、基于Loss和输入计算梯度、根据梯度更新参数值”四个部分反复执行，直到到损失函数最小。具体代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 9, loss 5.143394325795511\n",
      "iter 19, loss 3.097924194225988\n",
      "iter 29, loss 2.082241020617026\n",
      "iter 39, loss 1.5673801618157397\n",
      "iter 49, loss 1.2966204735077431\n",
      "iter 59, loss 1.1453399043319765\n",
      "iter 69, loss 1.0530155717435201\n",
      "iter 79, loss 0.9902292156463155\n",
      "iter 89, loss 0.9426576903842504\n",
      "iter 99, loss 0.9033048096880774\n",
      "iter 109, loss 0.868732003041364\n",
      "iter 119, loss 0.837229250968144\n",
      "iter 129, loss 0.807927474161227\n",
      "iter 139, loss 0.7803677341465797\n",
      "iter 149, loss 0.7542920908532763\n",
      "iter 159, loss 0.7295420168915829\n",
      "iter 169, loss 0.7060090054240882\n",
      "iter 179, loss 0.6836105084697767\n",
      "iter 189, loss 0.6622781710179412\n",
      "iter 199, loss 0.6419520361168637\n",
      "iter 209, loss 0.622577651786949\n",
      "iter 219, loss 0.6041045903195837\n",
      "iter 229, loss 0.5864856570315078\n",
      "iter 239, loss 0.5696764374763879\n",
      "iter 249, loss 0.5536350125932015\n",
      "iter 259, loss 0.5383217588525027\n",
      "iter 269, loss 0.5236991929680567\n",
      "iter 279, loss 0.509731841376165\n",
      "iter 289, loss 0.4963861247069634\n",
      "iter 299, loss 0.48363025234390233\n",
      "iter 309, loss 0.47143412454019784\n",
      "iter 319, loss 0.45976924072044867\n",
      "iter 329, loss 0.44860861316590983\n",
      "iter 339, loss 0.4379266855659793\n",
      "iter 349, loss 0.4276992560632111\n",
      "iter 359, loss 0.4179034044959738\n",
      "iter 369, loss 0.4085174235863553\n",
      "iter 379, loss 0.39952075384787633\n",
      "iter 389, loss 0.39089392200622347\n",
      "iter 399, loss 0.382618482740513\n",
      "iter 409, loss 0.3746769635645124\n",
      "iter 419, loss 0.36705281267772816\n",
      "iter 429, loss 0.35973034962581096\n",
      "iter 439, loss 0.35269471861856694\n",
      "iter 449, loss 0.3459318443621334\n",
      "iter 459, loss 0.33942839026966587\n",
      "iter 469, loss 0.33317171892221653\n",
      "iter 479, loss 0.3271498546584252\n",
      "iter 489, loss 0.3213514481781961\n",
      "iter 499, loss 0.31576574305173283\n",
      "iter 509, loss 0.3103825440311682\n",
      "iter 519, loss 0.30519218706757245\n",
      "iter 529, loss 0.30018551094136725\n",
      "iter 539, loss 0.29535383041913843\n",
      "iter 549, loss 0.29068891085453674\n",
      "iter 559, loss 0.28618294415539336\n",
      "iter 569, loss 0.28182852604338504\n",
      "iter 579, loss 0.27761863453655344\n",
      "iter 589, loss 0.27354660958874766\n",
      "iter 599, loss 0.2696061338236152\n",
      "iter 609, loss 0.26579121430413205\n",
      "iter 619, loss 0.26209616528184804\n",
      "iter 629, loss 0.25851559187303397\n",
      "iter 639, loss 0.25504437461176843\n",
      "iter 649, loss 0.2516776548326958\n",
      "iter 659, loss 0.2484108208387405\n",
      "iter 669, loss 0.24523949481147198\n",
      "iter 679, loss 0.24215952042409844\n",
      "iter 689, loss 0.2391669511192288\n",
      "iter 699, loss 0.2362580390155805\n",
      "iter 709, loss 0.2334292244097483\n",
      "iter 719, loss 0.2306771258409729\n",
      "iter 729, loss 0.22799853068858245\n",
      "iter 739, loss 0.22539038627340982\n",
      "iter 749, loss 0.22284979143604464\n",
      "iter 759, loss 0.22037398856623477\n",
      "iter 769, loss 0.21796035605914357\n",
      "iter 779, loss 0.2156064011754777\n",
      "iter 789, loss 0.21330975328373866\n",
      "iter 799, loss 0.2110681574640261\n",
      "iter 809, loss 0.20887946845393043\n",
      "iter 819, loss 0.20674164491810018\n",
      "iter 829, loss 0.2046527440240648\n",
      "iter 839, loss 0.20261091630783168\n",
      "iter 849, loss 0.20061440081366638\n",
      "iter 859, loss 0.1986615204933024\n",
      "iter 869, loss 0.19675067785062839\n",
      "iter 879, loss 0.19488035081864621\n",
      "iter 889, loss 0.19304908885621125\n",
      "iter 899, loss 0.1912555092527351\n",
      "iter 909, loss 0.1894982936296714\n",
      "iter 919, loss 0.18777618462820625\n",
      "iter 929, loss 0.18608798277314595\n",
      "iter 939, loss 0.18443254350353405\n",
      "iter 949, loss 0.18280877436103968\n",
      "iter 959, loss 0.18121563232764165\n",
      "iter 969, loss 0.17965212130459232\n",
      "iter 979, loss 0.1781172897250724\n",
      "iter 989, loss 0.1766102282933619\n",
      "iter 999, loss 0.17513006784373505\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        np.random.seed(0)\n",
    "        self.w = np.random.randn(num_of_weights, 1)\n",
    "        self.b = 0.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.w) + self.b\n",
    "        return z\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        z = self.forward(x)\n",
    "        gradient_w = (z-y)*x\n",
    "        gradient_w = np.mean(gradient_w, axis=0)\n",
    "        gradient_w = gradient_w[:, np.newaxis]\n",
    "        gradient_b = (z - y)\n",
    "        gradient_b = np.mean(gradient_b)        \n",
    "        return gradient_w, gradient_b\n",
    "    \n",
    "    def update(self, gradient_w, gradient_b, eta = 0.01):\n",
    "        self.w = self.w - eta * gradient_w\n",
    "        self.b = self.b - eta * gradient_b\n",
    "        \n",
    "    def train(self, x, y, iterations=100, eta=0.01):\n",
    "        losses = []\n",
    "        for i in range(iterations):\n",
    "            z = self.forward(x)\n",
    "            L = self.loss(z, y)\n",
    "            gradient_w, gradient_b = self.gradient(x, y)\n",
    "            self.update(gradient_w, gradient_b, eta)\n",
    "            losses.append(L)\n",
    "            if (i+1) % 10 == 0:\n",
    "                print('iter {}, loss {}'.format(i, L))\n",
    "        return losses\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "x = train_data[:, :-1]\n",
    "y = train_data[:, -1:]\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "num_iterations=1000\n",
    "# 启动训练\n",
    "losses = net.train(x,y, iterations=num_iterations, eta=0.01)\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(num_iterations)\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}