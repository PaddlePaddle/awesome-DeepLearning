{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导入需要用到的package\n",
    "import numpy as np\n",
    "import json\n",
    "# 读入训练数据\n",
    "datafile = './work/housing.data'\n",
    "data = np.fromfile(datafile, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # 从文件导入数据\n",
    "    datafile = './work/housing.data'\n",
    "    data = np.fromfile(datafile, sep=' ')\n",
    "\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "    feature_num = len(feature_names)\n",
    "\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
    "\n",
    "    # 将原数据集拆分成训练集和测试集\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\n",
    "    # 测试集和训练集必须是没有交集的\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0] * ratio)\n",
    "    training_data = data[:offset]\n",
    "\n",
    "    # 计算训练集的最大值，最小值，平均值\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\n",
    "    # 记录数据的归一化参数，在预测时对数据做归一化\n",
    "    global max_values\n",
    "    global min_values\n",
    "    global avg_values\n",
    "    max_values = maximums\n",
    "    min_values = minimums\n",
    "    avg_values = avgs\n",
    "\n",
    "    # 对数据进行归一化处理\n",
    "    for i in range(feature_num):\n",
    "        #print(maximums[i], minimums[i], avgs[i])\n",
    "        data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])\n",
    "\n",
    "    # 训练集和测试集的划分比例\n",
    "    training_data = data[:offset]\n",
    "    test_data = data[offset:]\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.30055004 -0.14232673  0.34131295 -0.08663366  0.13429644 -0.22573461\n",
      "  0.36634938 -0.2350493   0.74827809  0.65363071  0.23125132 -0.51825133\n",
      "  0.24075744 -0.1750165 ]\n",
      "Epoch   0 / iter   0, loss = 3.4996\n",
      "Epoch   0 / iter  20, loss = 0.2816\n",
      "Epoch   0 / iter  40, loss = 0.7048\n",
      "Epoch   1 / iter   0, loss = 0.9865\n",
      "Epoch   1 / iter  20, loss = 0.0307\n",
      "Epoch   1 / iter  40, loss = 0.0149\n",
      "Epoch   2 / iter   0, loss = 0.1088\n",
      "Epoch   2 / iter  20, loss = 0.2628\n",
      "Epoch   2 / iter  40, loss = 0.2409\n",
      "Epoch   3 / iter   0, loss = 0.0763\n",
      "Epoch   3 / iter  20, loss = 0.0618\n",
      "Epoch   3 / iter  40, loss = 0.0058\n",
      "Epoch   4 / iter   0, loss = 0.1007\n",
      "Epoch   4 / iter  20, loss = 0.3342\n",
      "Epoch   4 / iter  40, loss = 0.0620\n",
      "Epoch   5 / iter   0, loss = 0.1001\n",
      "Epoch   5 / iter  20, loss = 0.0513\n",
      "Epoch   5 / iter  40, loss = 0.0306\n",
      "Epoch   6 / iter   0, loss = 0.1403\n",
      "Epoch   6 / iter  20, loss = 0.0093\n",
      "Epoch   6 / iter  40, loss = 0.1849\n",
      "Epoch   7 / iter   0, loss = 0.0835\n",
      "Epoch   7 / iter  20, loss = 0.1384\n",
      "Epoch   7 / iter  40, loss = 0.0662\n",
      "Epoch   8 / iter   0, loss = 0.0918\n",
      "Epoch   8 / iter  20, loss = 0.1145\n",
      "Epoch   8 / iter  40, loss = 0.1988\n",
      "Epoch   9 / iter   0, loss = 0.0760\n",
      "Epoch   9 / iter  20, loss = 0.1212\n",
      "Epoch   9 / iter  40, loss = 0.1395\n",
      "Epoch  10 / iter   0, loss = 0.0158\n",
      "Epoch  10 / iter  20, loss = 0.0351\n",
      "Epoch  10 / iter  40, loss = 0.1888\n",
      "Epoch  11 / iter   0, loss = 0.1707\n",
      "Epoch  11 / iter  20, loss = 0.1048\n",
      "Epoch  11 / iter  40, loss = 0.0069\n",
      "Epoch  12 / iter   0, loss = 0.2002\n",
      "Epoch  12 / iter  20, loss = 0.1474\n",
      "Epoch  12 / iter  40, loss = 0.1872\n",
      "Epoch  13 / iter   0, loss = 0.0335\n",
      "Epoch  13 / iter  20, loss = 0.0945\n",
      "Epoch  13 / iter  40, loss = 0.0190\n",
      "Epoch  14 / iter   0, loss = 0.1220\n",
      "Epoch  14 / iter  20, loss = 0.2548\n",
      "Epoch  14 / iter  40, loss = 0.1847\n",
      "Epoch  15 / iter   0, loss = 0.1886\n",
      "Epoch  15 / iter  20, loss = 0.1119\n",
      "Epoch  15 / iter  40, loss = 0.1511\n",
      "Epoch  16 / iter   0, loss = 0.3456\n",
      "Epoch  16 / iter  20, loss = 0.1699\n",
      "Epoch  16 / iter  40, loss = 0.3836\n",
      "Epoch  17 / iter   0, loss = 0.0102\n",
      "Epoch  17 / iter  20, loss = 0.0277\n",
      "Epoch  17 / iter  40, loss = 0.0890\n",
      "Epoch  18 / iter   0, loss = 0.1302\n",
      "Epoch  18 / iter  20, loss = 0.2172\n",
      "Epoch  18 / iter  40, loss = 0.0415\n",
      "Epoch  19 / iter   0, loss = 0.1452\n",
      "Epoch  19 / iter  20, loss = 0.0593\n",
      "Epoch  19 / iter  40, loss = 0.6670\n",
      "Epoch  20 / iter   0, loss = 0.1207\n",
      "Epoch  20 / iter  20, loss = 0.0672\n",
      "Epoch  20 / iter  40, loss = 0.3579\n",
      "Epoch  21 / iter   0, loss = 0.0327\n",
      "Epoch  21 / iter  20, loss = 0.1752\n",
      "Epoch  21 / iter  40, loss = 0.1309\n",
      "Epoch  22 / iter   0, loss = 0.1935\n",
      "Epoch  22 / iter  20, loss = 0.0465\n",
      "Epoch  22 / iter  40, loss = 0.0047\n",
      "Epoch  23 / iter   0, loss = 0.2521\n",
      "Epoch  23 / iter  20, loss = 0.1070\n",
      "Epoch  23 / iter  40, loss = 0.3033\n",
      "Epoch  24 / iter   0, loss = 0.1891\n",
      "Epoch  24 / iter  20, loss = 0.1301\n",
      "Epoch  24 / iter  40, loss = 0.0956\n",
      "Epoch  25 / iter   0, loss = 0.2050\n",
      "Epoch  25 / iter  20, loss = 0.2617\n",
      "Epoch  25 / iter  40, loss = 0.0253\n",
      "Epoch  26 / iter   0, loss = 0.1268\n",
      "Epoch  26 / iter  20, loss = 0.2685\n",
      "Epoch  26 / iter  40, loss = 0.3688\n",
      "Epoch  27 / iter   0, loss = 0.1264\n",
      "Epoch  27 / iter  20, loss = 0.0631\n",
      "Epoch  27 / iter  40, loss = 0.0043\n",
      "Epoch  28 / iter   0, loss = 0.1881\n",
      "Epoch  28 / iter  20, loss = 0.0148\n",
      "Epoch  28 / iter  40, loss = 0.3999\n",
      "Epoch  29 / iter   0, loss = 0.1688\n",
      "Epoch  29 / iter  20, loss = 0.1324\n",
      "Epoch  29 / iter  40, loss = 0.3274\n",
      "Epoch  30 / iter   0, loss = 0.2130\n",
      "Epoch  30 / iter  20, loss = 0.0414\n",
      "Epoch  30 / iter  40, loss = 0.1104\n",
      "Epoch  31 / iter   0, loss = 0.0303\n",
      "Epoch  31 / iter  20, loss = 0.1922\n",
      "Epoch  31 / iter  40, loss = 0.2286\n",
      "Epoch  32 / iter   0, loss = 0.1072\n",
      "Epoch  32 / iter  20, loss = 0.1695\n",
      "Epoch  32 / iter  40, loss = 0.0208\n",
      "Epoch  33 / iter   0, loss = 0.0247\n",
      "Epoch  33 / iter  20, loss = 0.1797\n",
      "Epoch  33 / iter  40, loss = 0.1902\n",
      "Epoch  34 / iter   0, loss = 0.1062\n",
      "Epoch  34 / iter  20, loss = 0.1927\n",
      "Epoch  34 / iter  40, loss = 0.0341\n",
      "Epoch  35 / iter   0, loss = 0.0062\n",
      "Epoch  35 / iter  20, loss = 0.0966\n",
      "Epoch  35 / iter  40, loss = 0.1334\n",
      "Epoch  36 / iter   0, loss = 0.0541\n",
      "Epoch  36 / iter  20, loss = 0.0581\n",
      "Epoch  36 / iter  40, loss = 0.0088\n",
      "Epoch  37 / iter   0, loss = 0.2315\n",
      "Epoch  37 / iter  20, loss = 0.0912\n",
      "Epoch  37 / iter  40, loss = 0.0158\n",
      "Epoch  38 / iter   0, loss = 0.1754\n",
      "Epoch  38 / iter  20, loss = 0.0258\n",
      "Epoch  38 / iter  40, loss = 0.0403\n",
      "Epoch  39 / iter   0, loss = 0.0126\n",
      "Epoch  39 / iter  20, loss = 0.0969\n",
      "Epoch  39 / iter  40, loss = 0.0248\n",
      "Epoch  40 / iter   0, loss = 0.0266\n",
      "Epoch  40 / iter  20, loss = 0.0522\n",
      "Epoch  40 / iter  40, loss = 0.0820\n",
      "Epoch  41 / iter   0, loss = 0.0417\n",
      "Epoch  41 / iter  20, loss = 0.0560\n",
      "Epoch  41 / iter  40, loss = 0.0168\n",
      "Epoch  42 / iter   0, loss = 0.2427\n",
      "Epoch  42 / iter  20, loss = 0.0392\n",
      "Epoch  42 / iter  40, loss = 0.0215\n",
      "Epoch  43 / iter   0, loss = 0.0609\n",
      "Epoch  43 / iter  20, loss = 0.0400\n",
      "Epoch  43 / iter  40, loss = 0.0107\n",
      "Epoch  44 / iter   0, loss = 0.0525\n",
      "Epoch  44 / iter  20, loss = 0.0476\n",
      "Epoch  44 / iter  40, loss = 0.1928\n",
      "Epoch  45 / iter   0, loss = 0.0616\n",
      "Epoch  45 / iter  20, loss = 0.1170\n",
      "Epoch  45 / iter  40, loss = 0.2290\n",
      "Epoch  46 / iter   0, loss = 0.0371\n",
      "Epoch  46 / iter  20, loss = 0.0931\n",
      "Epoch  46 / iter  40, loss = 0.0605\n",
      "Epoch  47 / iter   0, loss = 0.0892\n",
      "Epoch  47 / iter  20, loss = 0.0141\n",
      "Epoch  47 / iter  40, loss = 0.0212\n",
      "Epoch  48 / iter   0, loss = 0.0631\n",
      "Epoch  48 / iter  20, loss = 0.0263\n",
      "Epoch  48 / iter  40, loss = 0.1641\n",
      "Epoch  49 / iter   0, loss = 0.0686\n",
      "Epoch  49 / iter  20, loss = 0.0779\n",
      "Epoch  49 / iter  40, loss = 0.1417\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, num_of_weights):\n",
    "        # 随机产生w的初始值\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\n",
    "        #np.random.seed(0)\n",
    "        self.w1 = np.random.randn(num_of_weights, 128)\n",
    "        self.b1 = 0.\n",
    "        self.w2 = np.random.randn(128, 1)\n",
    "        self.b2 = 0. \n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = np.dot(x, self.w1) + self.b1\n",
    "        o_sig = sigmoid(o)\n",
    "        z = np.dot(o_sig, self.w2) + self.b2\n",
    "        return z, o_sig\n",
    "    \n",
    "    def loss(self, z, y):\n",
    "        error = z - y\n",
    "        num_samples = error.shape[0]\n",
    "        cost = error * error\n",
    "        cost = np.sum(cost) / num_samples\n",
    "        return cost\n",
    "\n",
    "    def gradient(self, x, y):\n",
    "        z, o_sig = self.forward(x)\n",
    "        N = x.shape[0]\n",
    "        gradient_w2 = 1. / N * np.sum((z-y)*o_sig, axis=0)\n",
    "        gradient_w2 = gradient_w2[:, np.newaxis]\n",
    "        gradient_b2 = 1. / N * np.sum(z-y)\n",
    "\n",
    "        gradient_w1 = 1. / N * np.sum(np.dot(self.w2.T,np.dot(o_sig.T,(z-y)))*np.dot(x.T,(1-o_sig)), axis=0)\n",
    "        gradient_w1 = gradient_w1[:, np.newaxis]\n",
    "        gradient_b1 = 1. / N *  np.sum(np.dot(self.w2.T,np.dot(o_sig.T,(z-y)))*(1-o_sig), axis=0)\n",
    "\n",
    "        return gradient_w1, gradient_b1, gradient_w2, gradient_b2\n",
    "    \n",
    "    def update(self, gradient_w1, gradient_b1, gradient_w2, gradient_b2, eta = 0.01):\n",
    "        self.w1 = self.w1 - eta * gradient_w1.T\n",
    "        self.b1 = self.b1 - eta * gradient_b1\n",
    "        self.w2 = self.w2 - eta * gradient_w2\n",
    "        self.b2 = self.b2 - eta * gradient_b2\n",
    "            \n",
    "                \n",
    "    def train(self, training_data, num_epochs, batch_size=10, eta=0.01):\n",
    "        n = len(training_data)\n",
    "        losses = []\n",
    "        for epoch_id in range(num_epochs):\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\n",
    "            # 然后再按每次取batch_size条数据的方式取出\n",
    "            np.random.shuffle(training_data)\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\n",
    "                #print(self.w.shape)\n",
    "                #print(self.b)\n",
    "                x = mini_batch[:, :-1]\n",
    "                y = mini_batch[:, -1:]\n",
    "                a,o_sig = self.forward(x)\n",
    "                loss = self.loss(a, y)\n",
    "                gradient_w1, gradient_b1, gradient_w2, gradient_b2 = self.gradient(x, y,)\n",
    "                self.update(gradient_w1, gradient_b1, gradient_w2, gradient_b2, eta)\n",
    "                losses.append(loss)\n",
    "                if iter_id%20 == 0:\n",
    "                    print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\n",
    "                                    format(epoch_id, iter_id, loss))\n",
    "        \n",
    "        return losses\n",
    "\n",
    "    def predict(self, x):\n",
    "        a,o_sig = self.forward(x)\n",
    "        return a\n",
    "\n",
    "\n",
    "# 获取数据\n",
    "train_data, test_data = load_data()\n",
    "print(test_data[:10][-1])\n",
    "\n",
    "# 创建网络\n",
    "net = Network(13)\n",
    "# 启动训练\n",
    "losses = net.train(train_data, num_epochs=50, batch_size=10, eta=0.01)\n",
    "\n",
    "\n",
    "# 画出损失函数的变化趋势\n",
    "plot_x = np.arange(len(losses))\n",
    "plot_y = np.array(losses)\n",
    "plt.plot(plot_x, plot_y)\n",
    "plt.savefig('curve.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference result is [[22.64035121]], the corresponding label is 22.4\n"
     ]
    }
   ],
   "source": [
    "def load_one_example():\r\n",
    "    # 从上边已加载的测试集中，随机选择一条作为测试数据\r\n",
    "    # idx = np.random.randint(0, test_data.shape[0])\r\n",
    "    idx = -5\r\n",
    "    one_data, label = test_data[idx, :-1], test_data[idx, -1]\r\n",
    "    # 修改该条数据shape为[1,13]\r\n",
    "    one_data =  one_data.reshape([1,-1])\r\n",
    "\r\n",
    "    return one_data, label\r\n",
    "\r\n",
    "\r\n",
    "# 参数为数据集的文件地址\r\n",
    "one_data, label = load_one_example()\r\n",
    "predict = net.predict(one_data)\r\n",
    "\r\n",
    "# 对结果做反归一化处理\r\n",
    "predict = predict * (maximums[-1] - minimums[-1]) + avgs[-1]\r\n",
    "# 对label数据做反归一化处理\r\n",
    "label = label * (maximums[-1] - minimums[-1]) + avgs[-1]\r\n",
    "\r\n",
    "print(\"Inference result is {}, the corresponding label is {}\".format(predict, label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
