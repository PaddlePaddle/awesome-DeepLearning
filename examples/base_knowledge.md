# 1.深度学习基础知识

## （1）深度学习发展历史

**萌芽期（20世纪40年代）：**

- 1943年由神经科学家麦卡洛克(W.S.McCilloch) 和数学家皮兹（W.Pitts）在《数学生物物理学公告》上发表论文《神经活动中内在思想的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity）。建立了神经网络和数学模型，称为MCP模型。所谓MCP模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，也就诞生了所谓的“模拟大脑”，人工神经网络的大门由此开启。
- 1949年，心理学家D. O. Hebb提出神经元之间突触联系是可变的假说——Hebb学习律。

**第一次高潮期（1950~1968）:**

+ 以MarvinMinsky，FrankRosenblatt，BernardWidrow等为代表人物，代表作是单级感知器（Perceptron）。
+ 可用电子线路模拟。
+ 人们乐观地认为几乎已经找到了智能的关键。许多部门都开始大批地投入此项研究，希望尽快占领制高点。

**反思期（1969~1982）:**

+ M. L. Minsky和S. Papert，《Perceptron》，MIT Press，1969年
+ “异或”运算不可表示
+ 二十世纪70年代和80年代早期的研究结果
+ 认识规律：认识——实践——再认识

**第二高潮期（1983~1990）:**

+ 循环网络，J.Hopfield，1982年
+ Boltzmann机，Hinton、Rumelhart，1985年
+ BP算法，Rumelhart等，1986年
+ 1988年，RBF神经网络
+ 90年代早期，Vapnik提出SVM

**再认识与应用研究期（1991~2011）：**

+ 存在应用面还不够宽、结果不够精确等问题
+ 改进现有模型、算法，以提高网络的训练速度和运行的准确度。
+ 算法的集成
+ 希望在理论上寻找新的突破，建立新的专用/通用模型和算法
+ 进一步对生物神经系统进行研究，不断地丰富对人脑的认识。

**爆发期（2012~......）：**

+ 深度置信网络的提出
+ 进入深度学习时代：深度神经网络(DNN)、卷积神经网络(CNN)、深度置信网络(DBN)、递归神经网络(RNN)、长短期记忆网络(LSTM)和GANs等。
+ 深度学习开发框架

## （2）人工智能、机器学习、深度学习有什么区别和联系？

![1](https://github.com/827184100/awesome-DeepLearning/blob/work2/examples/1.png)

（图来源于网上）

人工智能中包含机器学习，而机器学习中包含着深度学习。

人工智能：自从1956 年计算机科学家们在达特茅斯会议（Dartmouth Conferences）上确认人工智能这个术语以来，人们就不乏关于人工智能奇思妙想，研究人员也在不遗余力地研究。在此后的几十年间，人工智能先是被捧为人类文明光明未来的钥匙。人工智能先驱们在达特茅斯开会时，心中的梦想是希望通过当时新兴的计算机，打造拥有相当于人类智能的复杂机器。这就是我们所说的“通用人工智能”（General AI）概念，拥有人类五感（甚至更多）、推理能力以及人类思维方式的神奇机器。

机器学习：机器学习是实现人工智能的一种方法。机器学习的概念来自早期的人工智能研究者，已经研究出的算法包括决策树学习、归纳逻辑编程、增强学习和贝叶斯网络等。简单来说，机器学习就是使用算法分析数据，从中学习并做出推断或预测。与传统的使用特定指令集手写软件不同，我们使用大量数据和算法来“训练”机器，由此带来机器学习如何完成任务。

深度学习： 深度学习是实现机器学习的一种技术。早期机器学习研究者中受人类大脑的启发而开发了一种叫人工神经网络的算法。神经网络中神经元之间有相互连接关系。随着计算机性能的巨大提高及几乎无限的存储空间和海量数据的出现（大数据）：图像、文本、交易数据、地图数据等，神经网络的层数也越来越深。

## （3）神经元、单层感知机、多层感知机

**神经元：**

+ 生物神经元：神经细胞被称为生物神经元。神经元主要由三个部分组成：细胞体、轴突。
+ 人工神经元：神经元是构成神经网络的最基本单元（构件）。人工神经元模型应该具有生物神经元的基本特性

![image-20210709122359550](https://github.com/827184100/awesome-DeepLearning/blob/work2/examples/2)

（图片来源于网上）

**单层感知机：**

感知器(Perceptron)也可称为感知机,是 Frank Rosenblatt 在1957年发明的一种人工神经网络。它可以视为一种形式最简单的前馈式人工神经网络，是一种二元线性分类器。感知器由两层神经元组成，作为一种线性分类器，感知器是最简单的前向人工神经网络形式。尽管结构简单，但能够学习并解决比较复杂的问题。在人工神经网络领域中,感知器也指单层的人工神经网络，以区别于较复杂的多层感知器(Multilayer Perceptron，MLP)。感知器是一种双层神经网络模型，第一层为输人层，第二层为隐层，其中包括计算单元。感知器可以通过监督学习来逐步增强模式划分的能力，从而达到学习的目的。单一的感知器可以解决简单的逻辑运算，但是对于“异或”问题却无法实现。

**多层感知机：**

多层感知器克服了单层感知器的许多缺点，一些单层感知器无法解决的问题，在多层感知器中就可以解决。其算法思想是将单输出感知器的处理逐个地用于多输出感知器输出层的每一个神经元的处理。

## （4）什么是前向传播

**前项传播：**

将上一层的输出作为下一层的输入，并计算下一层的输出，一直到运算到输出层为止。

![img](https://github.com/827184100/awesome-DeepLearning/blob/work2/examples/3.png)

（图片来源于网上）

## （5）什么是反向传播

**反向传播：**

反向传播（back propagation, BP）算法是 "误差反向传播" 的简称，也称为backprop，允许来自代价函数的信息通过网络向后流动，以便计算梯度。

反向传播是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。

