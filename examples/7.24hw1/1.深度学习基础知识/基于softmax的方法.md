# 基于softmax的方法
#### 分层Softmax
Hierarchical softmax （H-Softmax）是由Morin和Bengio[3]受到二叉树的启发而提出。H-Softmax本质上是用层级关系替代了扁平化的softmax层，如图1所示，每个叶子节点表示一个词语。于是，计算单个词语概率值的计算过程被拆解为一系列的概率计算，这样可以避免对所有词语进行标准化计算。用H-Softmax替换softmax层之后，词语的预测速度可以提升至少50倍，速度的提升对于低延时要求的实时系统至关重要，比如谷歌新推出的消息应用Allo。



我们可以把原来的softmax看做深度为1的树，词表V中的每一个词语表示一个叶子节点。计算一个词语的softmax概率需要对|V|个节点的概率值做标准化。如果把softmax改为二叉树结构，每个word表示叶子节点，那么只需要沿着通向该词语的叶子节点的路径搜索，而不需要考虑其它的节点。

平衡二叉树的深度是log2(|V|)，因此，最多只需要计算log2(|V|)个节点就能得到目标词语的概率值。注意，得到的概率值已经经过了标准化，因为二叉树所有叶子节点组成一个概率分布，所有叶子节点的概率值总和等于1。我们可以简单地验证一下，在图1的根节点（Node o）处，两个分枝的概率和必须为1。之后的每个节点，它的两个子节点的概率值之和等于节点本身的概率值。因为整条搜索路径没有概率值的损失，所以最底层所有叶子节点的概率值之和必定等于1，hierarchical softmax定义了词表V中所有词语的标准化概率分布。

具体说来，当遍历树的时候，我们需要能够计算左侧分枝或是右侧分枝的概率值。为此，给每个节点分配一个向量表示。与常规的softmax做法不同，这里不是给每个输出词语w生成词向量v’w，而是给每个节点n计算一个向量v’n。总共有|V|-1个节点，每个节点都有自己独一无二的向量表示，H-Softmax方法用到的参数与常规的softmax几乎一样。于是，在给定上下文c时，就能够计算节点n左右两个分枝的概率：
![Alt text](./1626922006615.png)



上式与常规的softmax大致相同。现在需要计算h与树的每个节点的向量v’n的内积，而不是与每个输出词语的向量计算。而且，现在只需要计算一个概率值，这里就是偏向n节点右枝的概率值。相反的，偏向左枝的概率值是1−p(right|n,c)



如图2所示，假设已知出现了词语“the”、“dog”、“and”、“the”，则出现词语“cat”的概率值就是在节点1向左偏的概率值、在节点2向右偏的概率以及在节点5向右偏的概率值的乘积。Hugo Lachorelle在他的视频教程中给了更详细的解释。Rong[7]的文章也详细地解释了这些概念，并推导了H-Softmax。

显然，树形的结构非常重要。若我们让模型在各个节点的预测更方便，比如路径相近的节点概率值也相近，那么凭直觉系统的性能肯定还会提升。沿着这个思路，Morin和Bengio使用WordNet的同义词集作为树簇。然而性能依旧不如常规的softmax方法。Mnih和Hinton[8]将聚类算法融入到树形结构的学习过程，递归地将词集分为两个集合，效果终于和softmax方法持平，计算量有所减小。

值得注意的是，此方法只是加速了训练过程，因为我们可以提前知道将要预测的词语（以及其搜索路径）。在测试过程中，被预测词语是未知的，仍然无法避免计算所有词语的概率值。

在实践中，一般不用“左节点”和“右节点”，而是给每个节点赋一个索引向量，这个向量表示该节点的搜索路径。如图2所示，如果约定该位为0表示向左搜索，该位为1表示向右搜索，那词语“cat”的向量就是011。

上文中提到平衡二叉树的深度不超过log2(|V|)。若词表的大小是|V|=10000，那么搜索路径的平均长度就是13.3。因此，词表中的每个词语都能表示为一个平均长度为13.3比特的向量，即信息量为13.3比特。

关于信息量

在信息论中，人们习惯于将词语w概率值的负对数定义为信息量I(w)：

I(w)=−log2p(w)

而熵H则是词表中所有词语的信息量的期望值：

H=∑i∈Vp(wi)I(wi)

熵也代表着根据信息的概率分布对信息编码所需要的最短平均编码长度。 抛硬币事件需要用1比特来编码正反两个时间，对于永恒不变的事件则只需0比特。若用平衡二叉树的节点来表示词表中的词语，还是假设词表的大小|V|=10000，词表中词语的概率值均相等，那么熵H与平均搜索路径的长度恰好相等：
![Alt text](./1626922032779.png)



之前我们一再强调了树结构的重要性，因为利用好树结构不仅能提升系统的性能，还能加快运算速度。若我们给树加入额外的信息，就能缩短某些携带信息量少的词语的搜索路径。Morin和Bengio就是利用了词表中各个词语出现概率不相等这一信息。他们认为词表中的一些词语出现的概率总是大于其它词语，那这些词语就应该用更短的向量编码。他们所用的文档集（|V|=10000）的熵大约是9.16。

于是，考虑词频之后，文档集中每个词语的平均编码长度从13.3比特减为9.16比特，运算速度也提升了31%。Mikolov等人在他们关于hierarchical softmax的论文[1]里就用到了霍夫曼树，即词频越高的词语编码长度越短。比如，“the”是英语中最常见的词语，那“the”在霍夫曼树中的编码长度最短，词频第二高的词语编码长度仅次于“the”，以此类推。整篇文档的平均编码长度因此降低。

霍夫曼编码通常也被称作熵编码，因为每个词语的编码长度与它的熵几乎成正比。香农通过实验[5]得出英语字母的信息量通常在0.6~1.3之间。假设单词的平均长度是4.5个字母，那么所携带的信息量就是2.7~5.85比特。

再回到语言模型：衡量语言模型好坏的指标perplexity是2H，H表示熵。熵为9.16的unigram模型的perplexity达到29.16=572.0。我们可以直观的理解为，平均情况下用该unigram模型预测下一个词语时，有572个词语是等可能的候选词语。目前，Jozefowicz在2016年的论文中提到最好的模型perplexity=24.2。因为24.6=24.2，所以这个模型平均只需要4.6比特来表示一个词语，已经非常接近香农记录的实验下限值了。这个模型是否能用于改进网络的hierarchical softmax层，仍需要人们进一步探索。

#### 分片Softmax
Chen等人在论文中介绍了一种传统softmax层的变换形式，称作Differentiated Softmax (D-Softmax)。D-Softmax基于的假设是并不是所有词语都需要相同数量的参数：多次出现的高频词语需要更多的参数去拟合，而较少见的词语就可以用较少的参数。

传统的softmax层用到了dx|V|的稠密矩阵来存放输出的词向量表示v′w∈ℝd，论文中采用了稀疏矩阵。他们将词向量v′w按照词频分块，每块区域的向量维度各不相同。分块数量和对应的维度是超参数，可以根据需要调整。




图3中，A区域的词向量维度是dA（这个分块是高频词语，向量的维度较高），B和C区域的词向量维度分别是dB和dC。其余空白区域的值为0。

隐藏层h的输出被视为是各个分块的级联，比如图3中h层的输出是由三个长度分别为dA、dB、dC的向量级联而成。D-Softmax只需计算各个向量段与h对应位置的内积，而不需整个矩阵和向量参与计算。

由于大多数的词语只需要相对较少的参数，计算softmax的复杂度得到降低，训练速度因此提升。相对于H-Softmax方法，D-Softmax的优化方法在测试阶段仍然有效。Chen在2015年的论文中提到D-Softmax是测试阶段最快的方法，同时也是准确率最高的之一。但是，由于低频词语的参数较少，D-Softmax对这部分数据的建模能力较弱。

#### CNN-Softmax
传统softmax层的另一种改进是受到Kim[3]的论文启发，Kim对输入词向量vw采用了字符级别的CNN模型。相反，Jozefowicz在2016年将同样的方法用于输出词向量v′w，并将这种方法称为CNN-Softmax。如图4所示，如果我们在输入端和输出端加上CNN模型，输出端CNN生成的向量v′w与输入端CNN生成的向量必然不相同，因为输入和输出的词向量矩阵就不一样。



尽管这个方法仍需要计算常规softmax的标准化，但模型的参数却大大的减少：只需要保留CNN模型的参数，不需要保存整个词向量矩阵dx|V|。在测试阶段，输出词向量v′w可以提前计算，所以性能损失不大。

但是，由于字符串是在连续空间内表示，而且模型倾向于用平滑函数将字符串映射到词向量，因此基于字符的模型往往无法区分拼写相似但是含义不同的词语。为了消除上述影响，论文的作者增加了一个矫正因数，显著地缩小了CNN-Softmax与传统方法的性能差距。通过调整矫正因数的维度，作者可以方便地取舍模型的大小与计算性能。



# LSTM实现NLP任务
#### 机器翻译

深度学习在机器翻译领域的应用
一、基本的深度学习翻译模型
最简单的翻译模型是直接用RNN，模型分为两部分encoder和decoder，encoder将源语言句子信息编码成一个向量，decoder根据向量解码出目标语言，模型图如下：
![Alt text](./1626923065562.png)

可以看出encoder的最后一个向量需要捕获整个句子的信息，而不幸的是，通常一个周期只能捕获5-6个词，前面的信息都丢失掉了。模型公式如下：
![Alt text](./1626923072594.png)

二、循环神经网络的扩展

1.在训练encoding和decoding时使用不同的权重，每个输入都有一个权重矩阵W与之关联

2.decoder的输入除了包括前一层的隐含层输出外还包括encoder的最后一次输出以及上一次预测输出的单词，模型图和公式如下：
![Alt text](./1626923083149.png)
![Alt text](./1626923088934.png)


其中$D表示sigmod(Wh,Uc,Vy)。将y_(t-1)输入模型可以帮助模型防止多次重复预测同一单词。

3.用多层RNN

4.训练双向的encoder

5.可以将原来的输入反向输入，这样源语言的第一个单词(输入的最后一个单词)与目标语言的第一个距离最近，因此至少在开始处，那些会较好的被翻译出来。

GRU门控循环单元
之前也提到过普通的循环神经网络有一个缺陷就是一般只能记住前面5-6个单词，也就是存在遗忘问题。GRU应运而生，它的思想是我们希望保存一些记忆能捕获一些较长依赖的关系。顾名思义，GRU中有2种门分别是更新门update gate和重置门reset gate，两者都是根据当前输入词和隐含层状态计算的，与计算当前隐含层状态的公式类似：更新门(update gate)
![Alt text](./1626923099029.png)

重置门(reset gate)
![Alt text](./1626923104118.png)

GRU在每个时间点引入了一个临时的状态ht’，用来表示当前的新的内容，公式如下：
![Alt text](./1626923111213.png)

因为计算重置门的时候外面加的是sigmod函数，所以rt是一个0-1之间的值，当重置门的值是0的时候，它会忽略之前的内存只表示当前的输入单词。

Question:那么为什么要有重置门呢？
让我们拿情感分析任务来举一个例子，假设打算讨论一个谈论剧情的电源评论，会先说一端比较长的电影剧情，最后评论说“这个电影真的很无聊”。虽然评论是围绕着整个情节，但是这不重要，“无聊”是非常强烈的负面词，并且基本上会让模型忽略之前的剧情总结，因为着与情感分析任务不相关，这就是重置门做的事情。
接下来就是更新门登场了，它是在计算当前时间步最终的状态时出现的，公式如下：
![Alt text](./1626923118484.png)

从公式可以看出，如果zt是1的话，模型就可以完全复制上一个时间点的数据。

Question:那么为什么要有更新门呢？

假如还是那个情感分析的例子，在评论刚开始时，作者说“这个故事真好，我真喜欢这个故事，之后开始说故事情节”，这里对于情感分析任务真正有作用的不是爱情故事本身，而是开头的那句话，并且你需要确保不会丢失这条信息，这个模型可以解决这个问题，模型会自动学习何时重置何时更新。
为了让读者可以更清楚的理解，下面给出模型图：
![Alt text](./1626923124902.png)


LSTM长短时记忆网络
LSTM比GRU更加复杂，LSTM中有输入门，遗忘门，输出门和记忆单元，最终内存以及最终隐状态，公式如下：
![Alt text](./1626923131302.png)

1.输入门决定了当前单元或当前输入的词的作用有多大

2.遗忘门是一种分离机制，告诉单元需要遗忘多少内容

3.输出门：当有这些输出门时将分离那些与预测相关的内容和在当前单元需要保留在循环神经网络中的内容。也就是说在当前节点，这个特定的节点并不重要，不过随后会变得重要，因此现在不会输出到最终的softmax上。这是一个分离机制，会去学习何时输出。
![Alt text](./1626923144433.png)

