#!/usr/bin/env python
# coding: utf-8

# # æ„å»ºæ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹ä»»åŠ¡çš„ç¥ç»ç½‘ç»œæ¨¡å‹
# æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹æ˜¯ä¸€ä¸ªç»å…¸çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œç±»ä¼¼äºç¨‹åºå‘˜ä¸–ç•Œçš„â€œHello Worldâ€ã€‚å’Œå¤§å®¶å¯¹æˆ¿ä»·çš„æ™®éè®¤çŸ¥ç›¸åŒï¼Œæ³¢å£«é¡¿åœ°åŒºçš„æˆ¿ä»·å—è¯¸å¤šå› ç´ å½±å“ã€‚è¯¥æ•°æ®é›†ç»Ÿè®¡äº†13ç§å¯èƒ½å½±å“æˆ¿ä»·çš„å› ç´ å’Œè¯¥ç±»å‹æˆ¿å±‹çš„å‡ä»·ï¼ŒæœŸæœ›æ„å»ºä¸€ä¸ªåŸºäº13ä¸ªå› ç´ è¿›è¡Œæˆ¿ä»·é¢„æµ‹çš„æ¨¡å‹ï¼Œå¦‚ **å›¾1** æ‰€ç¤ºã€‚
# <br></br>
# <center><img src="https://ai-studio-static-online.cdn.bcebos.com/abce0cb2a92f4e679c6855cfa520491597171533a0b0447e8d51d904446e213e" width="500" hegiht="" ></center>
# <center><br>å›¾1ï¼šæ³¢å£«é¡¿æˆ¿ä»·å½±å“å› ç´ ç¤ºæ„å›¾</br></center>
# <br></br>
# å¯¹äºé¢„æµ‹é—®é¢˜ï¼Œå¯ä»¥æ ¹æ®é¢„æµ‹è¾“å‡ºçš„ç±»å‹æ˜¯è¿ç»­çš„å®æ•°å€¼ï¼Œè¿˜æ˜¯ç¦»æ•£çš„æ ‡ç­¾ï¼ŒåŒºåˆ†ä¸ºå›å½’ä»»åŠ¡å’Œåˆ†ç±»ä»»åŠ¡ã€‚å› ä¸ºæˆ¿ä»·æ˜¯ä¸€ä¸ªè¿ç»­å€¼ï¼Œæ‰€ä»¥æˆ¿ä»·é¢„æµ‹æ˜¾ç„¶æ˜¯ä¸€ä¸ªå›å½’ä»»åŠ¡ã€‚é¦–å…ˆæˆ‘ä»¬å°è¯•ç”¨æœ€ç®€å•çš„**çº¿æ€§å›å½’æ¨¡å‹**è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶ç”¨ç¥ç»ç½‘ç»œæ¥å®ç°è¿™ä¸ªæ¨¡å‹ï¼ˆpython + Numpyï¼‰ã€‚
# 
# ## çº¿æ€§å›å½’æ¨¡å‹
# 
# å‡è®¾æˆ¿ä»·å’Œå„å½±å“å› ç´ ä¹‹é—´èƒ½å¤Ÿç”¨çº¿æ€§å…³ç³»æ¥æè¿°ï¼š
# 
# $$y = {\sum_{j=1}^Mx_j w_j} + b$$
# 
# æ¨¡å‹çš„æ±‚è§£å³æ˜¯é€šè¿‡æ•°æ®æ‹Ÿåˆå‡ºæ¯ä¸ª$w_j$å’Œ$b$ã€‚å…¶ä¸­ï¼Œ$w_j$å’Œ$b$åˆ†åˆ«è¡¨ç¤ºè¯¥çº¿æ€§æ¨¡å‹çš„æƒé‡å’Œåç½®ã€‚ä¸€ç»´æƒ…å†µä¸‹ï¼Œ$w_j$ å’Œ $b$ æ˜¯ç›´çº¿çš„æ–œç‡å’Œæˆªè·ã€‚
# 
# çº¿æ€§å›å½’æ¨¡å‹ä½¿ç”¨å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°ï¼ˆLossï¼‰ï¼Œç”¨ä»¥è¡¡é‡é¢„æµ‹æˆ¿ä»·å’ŒçœŸå®æˆ¿ä»·çš„å·®å¼‚ï¼Œå…¬å¼å¦‚ä¸‹ï¼š
# 
# $$MSE = \frac{1}{n} \sum_{i=1}^n(\hat{Y_i} - {Y_i})^{2}$$
# 

# # æ•°æ®å¤„ç†
# æ•°æ®å¤„ç†åŒ…å«äº”ä¸ªéƒ¨åˆ†ï¼šæ•°æ®å¯¼å…¥ã€æ•°æ®å½¢çŠ¶å˜æ¢ã€æ•°æ®é›†åˆ’åˆ†ã€æ•°æ®å½’ä¸€åŒ–å¤„ç†å’Œå°è£…`load data`å‡½æ•°ã€‚æ•°æ®é¢„å¤„ç†åï¼Œæ‰èƒ½è¢«æ¨¡å‹è°ƒç”¨ã€‚
# ## è¯»å…¥æ•°æ®
# é€šè¿‡å¦‚ä¸‹ä»£ç è¯»å…¥æ•°æ®ï¼Œäº†è§£ä¸‹æ³¢å£«é¡¿æˆ¿ä»·çš„æ•°æ®é›†ç»“æ„ï¼Œæ•°æ®å­˜æ”¾åœ¨æœ¬åœ°ç›®å½•ä¸‹housing.dataæ–‡ä»¶ä¸­ã€‚
# ```
# # å¯¼å…¥éœ€è¦ç”¨åˆ°çš„package
# import numpy as np
# import json
# # è¯»å…¥è®­ç»ƒæ•°æ®
# datafile = './work/housing.data'
# data = np.fromfile(datafile, sep=' ')
# ```
# ## æ•°æ®å½¢çŠ¶å˜æ¢
# ç”±äºè¯»å…¥çš„åŸå§‹æ•°æ®æ˜¯1ç»´çš„ï¼Œæ‰€æœ‰æ•°æ®éƒ½è¿åœ¨ä¸€èµ·ã€‚å› æ­¤éœ€è¦æˆ‘ä»¬å°†æ•°æ®çš„å½¢çŠ¶è¿›è¡Œå˜æ¢ï¼Œå½¢æˆä¸€ä¸ª2ç»´çš„çŸ©é˜µï¼Œæ¯è¡Œä¸ºä¸€ä¸ªæ•°æ®æ ·æœ¬ï¼ˆ14ä¸ªå€¼ï¼‰ï¼Œæ¯ä¸ªæ•°æ®æ ·æœ¬åŒ…å«13ä¸ªXXXï¼ˆå½±å“æˆ¿ä»·çš„ç‰¹å¾ï¼‰å’Œä¸€ä¸ªYYYï¼ˆè¯¥ç±»å‹æˆ¿å±‹çš„å‡ä»·ï¼‰ã€‚
# ```
# # è¯»å…¥ä¹‹åçš„æ•°æ®è¢«è½¬åŒ–æˆ1ç»´arrayï¼Œå…¶ä¸­arrayçš„ç¬¬0-13é¡¹æ˜¯ç¬¬ä¸€æ¡æ•°æ®ï¼Œç¬¬14-27é¡¹æ˜¯ç¬¬äºŒæ¡æ•°æ®ï¼Œä»¥æ­¤ç±»æ¨.... 
# # è¿™é‡Œå¯¹åŸå§‹æ•°æ®åšreshapeï¼Œå˜æˆN x 14çš„å½¢å¼
# feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS', 
#                  'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]
# feature_num = len(feature_names)
# data = data.reshape([data.shape[0] // feature_num, feature_num])
# ```
# ## æ•°æ®é›†åˆ’åˆ†
# å°†æ•°æ®é›†åˆ’åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå…¶ä¸­è®­ç»ƒé›†ç”¨äºç¡®å®šæ¨¡å‹çš„å‚æ•°ï¼Œæµ‹è¯•é›†ç”¨äºè¯„åˆ¤æ¨¡å‹çš„æ•ˆæœã€‚
# åœ¨æœ¬æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†80%çš„æ•°æ®ç”¨ä½œè®­ç»ƒé›†ï¼Œ20%ç”¨ä½œæµ‹è¯•é›†ï¼Œå¯ä»¥å‘ç°å…±æœ‰404ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬å«æœ‰13ä¸ªç‰¹å¾å’Œ1ä¸ªé¢„æµ‹å€¼ã€‚
# ```
# ratio = 0.8
# offset = int(data.shape[0] * ratio)
# training_data = data[:offset]
# training_data.shape
# ```
# ## æ•°æ®å½’ä¸€åŒ–å¤„ç†
# å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œä½¿å¾—æ¯ä¸ªç‰¹å¾çš„å–å€¼ç¼©æ”¾åˆ°0~1ä¹‹é—´ã€‚è¿™æ ·åšæœ‰ä¸¤ä¸ªå¥½å¤„ï¼šä¸€æ˜¯æ¨¡å‹è®­ç»ƒæ›´é«˜æ•ˆï¼›äºŒæ˜¯ç‰¹å¾å‰çš„æƒé‡å¤§å°å¯ä»¥ä»£è¡¨è¯¥å˜é‡å¯¹é¢„æµ‹ç»“æœçš„è´¡çŒ®åº¦ï¼ˆå› ä¸ºæ¯ä¸ªç‰¹å¾å€¼æœ¬èº«çš„èŒƒå›´ç›¸åŒï¼‰ã€‚
# ```
# # è®¡ç®—trainæ•°æ®é›†çš„æœ€å¤§å€¼ï¼Œæœ€å°å€¼ï¼Œå¹³å‡å€¼
# maximums, minimums, avgs = \
#                      training_data.max(axis=0), \
#                      training_data.min(axis=0), \
#      training_data.sum(axis=0) / training_data.shape[0]
# # å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
# for i in range(feature_num):
#     #print(maximums[i], minimums[i], avgs[i])
#     data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])
# ```
# ## å°è£…æˆ`load data`å‡½æ•°
# å°†ä¸Šè¿°å‡ ä¸ªæ•°æ®å¤„ç†æ“ä½œå°è£…æˆ`load data`å‡½æ•°ï¼Œä»¥ä¾¿ä¸‹ä¸€æ­¥æ¨¡å‹çš„è°ƒç”¨ï¼Œå®ç°æ–¹æ³•å¦‚ä¸‹ã€‚

# In[14]:



def load_data():
    # ä»æ–‡ä»¶å¯¼å…¥æ•°æ®
    datafile = './work/housing.data'
    data = np.fromfile(datafile, sep=' ')

    # æ¯æ¡æ•°æ®åŒ…æ‹¬14é¡¹ï¼Œå…¶ä¸­å‰é¢13é¡¹æ˜¯å½±å“å› ç´ ï¼Œç¬¬14é¡¹æ˜¯ç›¸åº”çš„æˆ¿å±‹ä»·æ ¼ä¸­ä½æ•°
    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',                       'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]
    feature_num = len(feature_names)

    # å°†åŸå§‹æ•°æ®è¿›è¡ŒReshapeï¼Œå˜æˆ[N, 14]è¿™æ ·çš„å½¢çŠ¶
    data = data.reshape([data.shape[0] // feature_num, feature_num])

    # å°†åŸæ•°æ®é›†æ‹†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # è¿™é‡Œä½¿ç”¨80%çš„æ•°æ®åšè®­ç»ƒï¼Œ20%çš„æ•°æ®åšæµ‹è¯•
    # æµ‹è¯•é›†å’Œè®­ç»ƒé›†å¿…é¡»æ˜¯æ²¡æœ‰äº¤é›†çš„
    ratio = 0.8
    offset = int(data.shape[0] * ratio)
    training_data = data[:offset]

    # è®¡ç®—è®­ç»ƒé›†çš„æœ€å¤§å€¼ï¼Œæœ€å°å€¼ï¼Œå¹³å‡å€¼
    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0),                                  training_data.sum(axis=0) / training_data.shape[0]

    # å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
    for i in range(feature_num):
        #print(maximums[i], minimums[i], avgs[i])
        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])

    # è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ’åˆ†æ¯”ä¾‹
    training_data = data[:offset]
    test_data = data[offset:]
    return training_data, test_data


# In[15]:


import matplotlib.pyplot as plt
import numpy as np
import json
import paddle # ç”¨æ¥ä¿å­˜æ¨¡å‹


# ## è®­ç»ƒè¿‡ç¨‹
# 
# ä¸Šè¿°è®¡ç®—è¿‡ç¨‹æè¿°äº†å¦‚ä½•æ„å»ºç¥ç»ç½‘ç»œï¼Œé€šè¿‡ç¥ç»ç½‘ç»œå®Œæˆé¢„æµ‹å€¼å’ŒæŸå¤±å‡½æ•°çš„è®¡ç®—ã€‚æ¥ä¸‹æ¥ä»‹ç»å¦‚ä½•æ±‚è§£å‚æ•°$w$å’Œ$b$çš„æ•°å€¼ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¹Ÿç§°ä¸ºæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚è®­ç»ƒè¿‡ç¨‹æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å…³é”®è¦ç´ ä¹‹ä¸€ï¼Œå…¶ç›®æ ‡æ˜¯è®©å®šä¹‰çš„æŸå¤±å‡½æ•°$Loss$å°½å¯èƒ½çš„å°ï¼Œä¹Ÿå°±æ˜¯è¯´æ‰¾åˆ°ä¸€ä¸ªå‚æ•°è§£$w$å’Œ$b$ï¼Œä½¿å¾—æŸå¤±å‡½æ•°å–å¾—æå°å€¼ã€‚
# 
# ### æ±‚è§£æ–¹æ³•ï¼šæ¢¯åº¦ä¸‹é™æ³•
# 
# #### **è®¡ç®—æ¢¯åº¦**
# 
# ä¸Šé¢æˆ‘ä»¬è®²è¿‡äº†æŸå¤±å‡½æ•°çš„è®¡ç®—æ–¹æ³•ï¼Œè¿™é‡Œç¨å¾®æ”¹å†™ï¼Œä¸ºäº†ä½¿æ¢¯åº¦è®¡ç®—æ›´åŠ ç®€æ´ï¼Œå¼•å…¥å› å­$\frac{1}{2}$ï¼Œå®šä¹‰æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š
# 
# $$L= \frac{1}{2N}\sum_{i=1}^N{(y_i - z_i)^2}$$
# 
# å…¶ä¸­$z_i$æ˜¯ç½‘ç»œå¯¹ç¬¬$i$ä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼ï¼š
# 
# $$z_i = \sum_{j=0}^{12}{x_i^{j}\cdot w_j} + b$$
# 
# æ¢¯åº¦çš„å®šä¹‰ï¼š
# 
# $$ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡ = (\frac{\partial{L}}{\partial{w_0}},\frac{\partial{L}}{\partial{w_1}}, ... ,\frac{\partial{L}}{\partial{w_{12}}} ,\frac{\partial{L}}{\partial{b}})$$
# 
# å¯ä»¥è®¡ç®—å‡º$L$å¯¹$w$å’Œ$b$çš„åå¯¼æ•°ï¼š
# 
# $$\frac{\partial{L}}{\partial{w_j}} = \frac{1}{N}\sum_{i=1}^N{(z_i - y_i)\frac{\partial{z_i}}{\partial{w_j}}} = \frac{1}{N}\sum_{i=1}^N{(z_i - y_i)x_i^{j}}$$
# 
# $$\frac{\partial{L}}{\partial{b}} = \frac{1}{N}\sum_{i=1}^N{(z_i - y_i)\frac{\partial{z_i}}{\partial{b}}} = \frac{1}{N}\sum_{i=1}^N{(z_i - y_i)}$$
# 
# ä»å¯¼æ•°çš„è®¡ç®—è¿‡ç¨‹å¯ä»¥çœ‹å‡ºï¼Œå› å­$\frac{1}{2}$è¢«æ¶ˆæ‰äº†ï¼Œè¿™æ˜¯å› ä¸ºäºŒæ¬¡å‡½æ•°æ±‚å¯¼çš„æ—¶å€™ä¼šäº§ç”Ÿå› å­$2$ï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬å°†æŸå¤±å‡½æ•°æ”¹å†™çš„åŸå› ã€‚
# 
# ä¸‹é¢æˆ‘ä»¬è€ƒè™‘åªæœ‰ä¸€ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—æ¢¯åº¦ï¼š
# 
# $$L= \frac{1}{2}{(y_i - z_i)^2}$$
# 
# $$z_1 = {x_1^{0}\cdot w_0} + {x_1^{1}\cdot w_1} + ...  + {x_1^{12}\cdot w_{12}} + b$$
# 
# å¯ä»¥è®¡ç®—å‡ºï¼š
# 
# $$L= \frac{1}{2}{({x_1^{0}\cdot w_0} + {x_1^{1}\cdot w_1} + ...  + {x_1^{12}\cdot w_{12}} + b - y_1)^2}$$
# 
# å¯ä»¥è®¡ç®—å‡º$L$å¯¹$w$å’Œ$b$çš„åå¯¼æ•°ï¼š
# 
# $$\frac{\partial{L}}{\partial{w_0}} = ({x_1^{0}\cdot w_0} + {x_1^{1}\cdot w_1} + ...  + {x_1^{12}\cdot w_12} + b - y_1)\cdot x_1^{0}=({z_1} - {y_1})\cdot x_1^{0}$$
# 
# $$\frac{\partial{L}}{\partial{b}} = ({x_1^{0}\cdot w_0} + {x_1^{1}\cdot w_1} + ...  + {x_1^{12}\cdot w_{12}} + b - y_1)\cdot 1 = ({z_1} - {y_1})$$

# In[16]:


class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œæ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    def gradient(self, x, y):
        z = self.forward(x)
        gradient_w = (z-y)*x
        gradient_w = np.mean(gradient_w, axis=0)
        gradient_w = gradient_w[:, np.newaxis]
        gradient_b = (z - y)
        gradient_b = np.mean(gradient_b)        
        return gradient_w, gradient_b
    
    def update(self, gradient_w, gradient_b, eta = 0.01):
        self.w = self.w - eta * gradient_w
        self.b = self.b - eta * gradient_b
        
    def train(self, x, y, iterations=100, eta=0.01):
        losses = []
        for i in range(iterations):
            z = self.forward(x)
            L = self.loss(z, y)
            gradient_w, gradient_b = self.gradient(x, y)
            self.update(gradient_w, gradient_b, eta)
            losses.append(L)
            if (i+1) % 10 == 0:
                print('iter {}, loss {}'.format(i, L))
        return losses

# è·å–æ•°æ®
train_data, test_data = load_data()
x = train_data[:, :-1]
y = train_data[:, -1:]
# åˆ›å»ºç½‘ç»œ
net = Network(13)
num_iterations=1000
# å¯åŠ¨è®­ç»ƒ
losses = net.train(x,y, iterations=num_iterations, eta=0.01)

# ç”»å‡ºæŸå¤±å‡½æ•°çš„å˜åŒ–è¶‹åŠ¿
plot_x = np.arange(num_iterations)
plot_y = np.array(losses)
plt.plot(plot_x, plot_y)
plt.show()


# # ã€æ”¹è¿›ã€‘éšæœºæ¢¯åº¦ä¸‹é™
# åœ¨ä¸Šè¿°ç¨‹åºä¸­ï¼Œæ¯æ¬¡æŸå¤±å‡½æ•°å’Œæ¢¯åº¦è®¡ç®—éƒ½æ˜¯åŸºäºæ•°æ®é›†ä¸­çš„å…¨é‡æ•°æ®ã€‚å¯¹äºæ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹ä»»åŠ¡æ•°æ®é›†è€Œè¨€ï¼Œæ ·æœ¬æ•°æ¯”è¾ƒå°‘ï¼Œåªæœ‰404ä¸ªã€‚ä½†åœ¨å®é™…é—®é¢˜ä¸­ï¼Œæ•°æ®é›†å¾€å¾€éå¸¸å¤§ï¼Œå¦‚æœæ¯æ¬¡éƒ½ä½¿ç”¨å…¨é‡æ•°æ®è¿›è¡Œè®¡ç®—ï¼Œæ•ˆç‡éå¸¸ä½ï¼Œé€šä¿—åœ°è¯´å°±æ˜¯â€œæ€é¸¡ç„‰ç”¨ç‰›åˆ€â€ã€‚ç”±äºå‚æ•°æ¯æ¬¡åªæ²¿ç€æ¢¯åº¦åæ–¹å‘æ›´æ–°ä¸€ç‚¹ç‚¹ï¼Œå› æ­¤æ–¹å‘å¹¶ä¸éœ€è¦é‚£ä¹ˆç²¾ç¡®ã€‚ä¸€ä¸ªåˆç†çš„è§£å†³æ–¹æ¡ˆæ˜¯æ¯æ¬¡ä»æ€»çš„æ•°æ®é›†ä¸­éšæœºæŠ½å–å‡ºå°éƒ¨åˆ†æ•°æ®æ¥ä»£è¡¨æ•´ä½“ï¼ŒåŸºäºè¿™éƒ¨åˆ†æ•°æ®è®¡ç®—æ¢¯åº¦å’ŒæŸå¤±æ¥æ›´æ–°å‚æ•°ï¼Œè¿™ç§æ–¹æ³•è¢«ç§°ä½œéšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆStochastic Gradient Descentï¼ŒSGDï¼‰ï¼Œæ ¸å¿ƒæ¦‚å¿µå¦‚ä¸‹ï¼š
# 
# * mini-batchï¼šæ¯æ¬¡è¿­ä»£æ—¶æŠ½å–å‡ºæ¥çš„ä¸€æ‰¹æ•°æ®è¢«ç§°ä¸ºä¸€ä¸ªmini-batchã€‚
# * batch_sizeï¼šä¸€ä¸ªmini-batchæ‰€åŒ…å«çš„æ ·æœ¬æ•°ç›®ç§°ä¸ºbatch_sizeã€‚
# * epochï¼šå½“ç¨‹åºè¿­ä»£çš„æ—¶å€™ï¼ŒæŒ‰mini-batché€æ¸æŠ½å–å‡ºæ ·æœ¬ï¼Œå½“æŠŠæ•´ä¸ªæ•°æ®é›†éƒ½éå†åˆ°äº†çš„æ—¶å€™ï¼Œåˆ™å®Œæˆäº†ä¸€è½®è®­ç»ƒï¼Œä¹Ÿå«ä¸€ä¸ªepochã€‚å¯åŠ¨è®­ç»ƒæ—¶ï¼Œå¯ä»¥å°†è®­ç»ƒçš„è½®æ•°num_epochså’Œbatch_sizeä½œä¸ºå‚æ•°ä¼ å…¥ã€‚
# 
# ä¸‹é¢ç»“åˆç¨‹åºä»‹ç»å…·ä½“çš„å®ç°è¿‡ç¨‹ï¼Œæ¶‰åŠåˆ°æ•°æ®å¤„ç†å’Œè®­ç»ƒè¿‡ç¨‹ä¸¤éƒ¨åˆ†ä»£ç çš„ä¿®æ”¹ã€‚
# 
# #### **æ•°æ®å¤„ç†ä»£ç ä¿®æ”¹**
# 
# æ•°æ®å¤„ç†éœ€è¦å®ç°æ‹†åˆ†æ•°æ®æ‰¹æ¬¡å’Œæ ·æœ¬ä¹±åºï¼ˆä¸ºäº†å®ç°éšæœºæŠ½æ ·çš„æ•ˆæœï¼‰ä¸¤ä¸ªåŠŸèƒ½ã€‚
# #### **è®­ç»ƒè¿‡ç¨‹ä»£ç ä¿®æ”¹**
# 
# å°†æ¯ä¸ªéšæœºæŠ½å–çš„mini-batchæ•°æ®è¾“å…¥åˆ°æ¨¡å‹ä¸­ç”¨äºå‚æ•°è®­ç»ƒã€‚è®­ç»ƒè¿‡ç¨‹çš„æ ¸å¿ƒæ˜¯ä¸¤å±‚å¾ªç¯ï¼š
# 
# 1. ç¬¬ä¸€å±‚å¾ªç¯ï¼Œä»£è¡¨æ ·æœ¬é›†åˆè¦è¢«è®­ç»ƒéå†å‡ æ¬¡ï¼Œç§°ä¸ºâ€œepochâ€ï¼Œä»£ç å¦‚ä¸‹ï¼š
# 
# `for epoch_id in range(num_epochs):`
# 
# 2. ç¬¬äºŒå±‚å¾ªç¯ï¼Œä»£è¡¨æ¯æ¬¡éå†æ—¶ï¼Œæ ·æœ¬é›†åˆè¢«æ‹†åˆ†æˆçš„å¤šä¸ªæ‰¹æ¬¡ï¼Œéœ€è¦å…¨éƒ¨æ‰§è¡Œè®­ç»ƒï¼Œç§°ä¸ºâ€œiter (iteration)â€ï¼Œä»£ç å¦‚ä¸‹ï¼š
# 
# `for iter_id,mini_batch in emumerate(mini_batches):`
# 
# åœ¨ä¸¤å±‚å¾ªç¯çš„å†…éƒ¨æ˜¯ç»å…¸çš„å››æ­¥è®­ç»ƒæµç¨‹ï¼šå‰å‘è®¡ç®—->è®¡ç®—æŸå¤±->è®¡ç®—æ¢¯åº¦->æ›´æ–°å‚æ•°ï¼Œè¿™ä¸å¤§å®¶ä¹‹å‰æ‰€å­¦æ˜¯ä¸€è‡´çš„ï¼Œä»£ç å¦‚ä¸‹ï¼š
# 
#                 x = mini_batch[:, :-1]
#                 y = mini_batch[:, -1:]
#                 a = self.forward(x)  #å‰å‘è®¡ç®—
#                 loss = self.loss(a, y)  #è®¡ç®—æŸå¤±
#                 gradient_w, gradient_b = self.gradient(x, y)  #è®¡ç®—æ¢¯åº¦
#                 self.update(gradient_w, gradient_b, eta)  #æ›´æ–°å‚æ•°
# 
# 
# å°†ä¸¤éƒ¨åˆ†æ”¹å†™çš„ä»£ç é›†æˆåˆ°Networkç±»ä¸­çš„`train`å‡½æ•°ä¸­ï¼Œæœ€ç»ˆçš„å®ç°å¦‚ä¸‹ã€‚

# In[17]:


# è·å–æ•°æ®
train_data, test_data = load_data()

# æ‰“ä¹±æ ·æœ¬é¡ºåº
np.random.shuffle(train_data)

# å°†train_dataåˆ†æˆå¤šä¸ªmini_batch
batch_size = 10
n = len(train_data)
mini_batches = [train_data[k:k+batch_size] for k in range(0, n, batch_size)]

# åˆ›å»ºç½‘ç»œ
net = Network(13)

# ä¾æ¬¡ä½¿ç”¨æ¯ä¸ªmini_batchçš„æ•°æ®
for mini_batch in mini_batches:
    x = mini_batch[:, :-1]
    y = mini_batch[:, -1:]
    loss = net.train(x, y, iterations=1)


class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œæ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        #np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    def gradient(self, x, y):
        z = self.forward(x)
        N = x.shape[0]
        gradient_w = 1. / N * np.sum((z-y) * x, axis=0)
        gradient_w = gradient_w[:, np.newaxis]
        gradient_b = 1. / N * np.sum(z-y)
        return gradient_w, gradient_b
    
    def update(self, gradient_w, gradient_b, eta = 0.01):
        self.w = self.w - eta * gradient_w
        self.b = self.b - eta * gradient_b
            
                
    def train(self, training_data, num_epochs, batch_size=10, eta=0.01):
        n = len(training_data)
        losses = []
        for epoch_id in range(num_epochs):
            # åœ¨æ¯è½®è¿­ä»£å¼€å§‹ä¹‹å‰ï¼Œå°†è®­ç»ƒæ•°æ®çš„é¡ºåºéšæœºæ‰“ä¹±
            # ç„¶åå†æŒ‰æ¯æ¬¡å–batch_sizeæ¡æ•°æ®çš„æ–¹å¼å–å‡º
            np.random.shuffle(training_data)
            # å°†è®­ç»ƒæ•°æ®è¿›è¡Œæ‹†åˆ†ï¼Œæ¯ä¸ªmini_batchåŒ…å«batch_sizeæ¡çš„æ•°æ®
            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]
            for iter_id, mini_batch in enumerate(mini_batches):
                #print(self.w.shape)
                #print(self.b)
                x = mini_batch[:, :-1]
                y = mini_batch[:, -1:]
                a = self.forward(x)
                loss = self.loss(a, y)
                gradient_w, gradient_b = self.gradient(x, y)
                self.update(gradient_w, gradient_b, eta)
                losses.append(loss)
                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.
                                 format(epoch_id, iter_id, loss))
        
        return losses

# è·å–æ•°æ®
train_data, test_data = load_data()

# åˆ›å»ºç½‘ç»œ
net = Network(13)
# å¯åŠ¨è®­ç»ƒ
losses = net.train(train_data, num_epochs=50, batch_size=100, eta=0.1)

# ç”»å‡ºæŸå¤±å‡½æ•°çš„å˜åŒ–è¶‹åŠ¿
plot_x = np.arange(len(losses))
plot_y = np.array(losses)
plt.plot(plot_x, plot_y)
plt.show()

