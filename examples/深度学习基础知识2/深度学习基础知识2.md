# 深度学习基础知识

## CNN-DSSM知识点补充

### 概念

针对 DSSM 词袋模型丢失上下文信息的缺点，CLSM（convolutional latent semantic model）应运而生，又叫 CNN-DSSM。CNN-DSSM 与 DSSM 的区别主要在于输入层和表示层。

### 模型

输入层

英文处理方式：

![6](C:\Users\1\Desktop\3\6.png)

表示层 由一个卷积神经网络组成：

![7](C:\Users\1\Desktop\3\7.png)

卷积层的作用是提取滑动窗口下的上下文特征；池化层的作用是为句子找到全局的上下文特征；最后通过全连接层转为低维语义向量。

### 匹配层

可以用两个语义向量的cosine距离来表示语义相似性，通过softmax函数把语义相似性转化为一个后验概率。

### 作用

解决DSSM词袋模型丢失上下文信息的缺点

### 优缺点

优点：CNN-DSSM 通过卷积层提取了滑动窗口下的上下文信息，又通过池化层提取了全局的上下文信息，上下文信息得到较为有效的保留。

缺点：相隔较远的上下文信息难以捕捉

## LSTM-DSSM知识点补充

### 概念

LSTM（Long-Short-Term Memory）是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM-DSSM 其实用的是 LSTM 的一个变种——加入了peep hole的 LSTM。

### 模型

![8](C:\Users\1\Desktop\3\8.png)

### 作用

针对 CNN-DSSM 无法捕获较远距离上下文特征的缺点

### 优缺点

优点：解决了 CNN-DSSM 无法捕获较远距离上下文特征的缺点

缺点：仍需要有海量的训练样本

## MMoE多任务学习知识点补充

### 概念

是相对于shared-bottom结构不明显增加模型参数的要求下捕捉任务的不同。其核心思想是将shared-bottom网络中的函数f替换成MoE层

### 模型

![9](C:\Users\1\Desktop\3\9.webp)

### 作用

用一个模型来同时学习多个目标和任务

### 优缺点

优点：表现要好于Shared-bottom；多个任务共享一个模型，占用内存量减少；关联任务通过共享信息，相互补充，可以提升彼此的表现。

## ShareBottom多任务学习知识点补充

### 概念

这种模型结构的特点是所有目标共享同一个输入。

### 模型

![11](C:\Users\1\Desktop\3\11.jpg)

### 作用

降低了过拟合风险

### 优缺点

多个任务共享一个模型，占用内存量减少；多个任务一次前向计算得出结果，推理速度增加；关联任务通过共享信息，相互补充，可以提升彼此的表现；通过多任务之间不相关的部分的相互作用，有助于逃离局部极小值点；而多任务之前相关的部分则有利于底部共享层对通用特征表示的学习。

## YouTube深度学习视频推荐系统知识点

### 概念

YouTube平台的视频推荐系统

### 模型

推荐系统架构：

![9](C:\Users\1\Desktop\3\9.png)

候选集生成模型：

![10](C:\Users\1\Desktop\3\10.png)

排序模型：

![10](C:\Users\1\Desktop\3\10.png)

### 作用

对海量的视频进行快速、准确的排序

### 场景

作为全球最大的视频分享网站，YouTube 平台中几乎所有的视频都来自 UGC（User Generated Content，用户原创内容），这样的内容产生模式有两个特点：

一是其商业模式不同于 Netflix，以及国内的腾讯视频、爱奇艺这样的流媒体，这些流媒体的大部分内容都是采购或自制的电影、剧集等头部内容，而 YouTube 的内容都是用户上传的自制视频，种类风格繁多，头部效应没那么明显；二是由于 YouTube 的视频基数巨大，用户难以发现喜欢的内容。

### 优缺点

候选集生成模型把推荐问题转换成多分类问题，在预测下一次观看（next watch）的场景下，每一个备选视频都会是一个分类，因此总共的分类有数百万之巨，YouTube 采用了 Word2vec 中常用的负采样训练方法减少了每次预测的分类数量，从而加快了整个模型的收敛速度，在实践中选择了更为简便的负采样方法。

YouTube 对每个用户提取等数量的训练样本，目的是减少高度活跃用户对模型损失的过度影响，使模型过于偏向活跃用户的行为模式，而忽略数量更广大的长尾用户的体验。



