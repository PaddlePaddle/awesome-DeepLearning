#Instance Normalization
#概念
Instance Normalization和Batch Normalization一样，也是Normalization的一种方法，只是IN是作用于单张图片，但是BN作用于一个Batch。
BN对Batch中的每一张图片的同一个通道一起进行Normalization操作，而IN是指单张图片的单个通道单独进行Normalization操作。如下图所示，其中C代表通道数，N代表图片数量（Batch）。
（image1）

 
#算法流程
·沿着通道计算每张图的均值u
·沿着通道计算每张图的方差σ^2
·对x做归一化，x’=(x-u)/开根号(σ^2+ε)
·加入缩放和平移变量γ和β ,归一化后的值，y=γx’+β
对于（image2） ，IN 对每个样本的 H、W 维度的数据求均值和标准差，保留 N 、C 维度，也就是说，它只在 channel 内部求均值和标准差，其公式为：
（image3）
 
IN 求均值时，相当于把一页书中所有字加起来，再除以该页的总字数：H×W，即求每页书的“平均字”，求标准差时也是同理。
#作用
IN的作用，一言以蔽之，就是消除全局信息。比如对一段语音进行IN，就会把全局信息滤掉，比如音色就是全局信息，因为从头到尾它都存在，而语音的内容是局部信息，因为不同时间说的话不同。
所以在做风格迁移的时候，对于提取内容的那一路，可以用IN来滤除音色信息。
但对于音色来说，它是一个全局信息，提取过程中很可能有内容信息的残留，我该怎么办呢？
如下图所示，我们一般会用AdaIN，它并不是喂给decoder，而是利用AdaIN加入到decoder当中去，用来控制decoder的全局信息比如音色。
（image4）
 
#应用场景
BN适用于判别模型中，比如图片分类模型。因为BN注重对每个batch进行归一化，从而保证数据分布的一致性，而判别模型的结果正是取决于数据整体分布。但是BN对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；
IN适用于生成模型中，比如图片风格迁移。因为图片生成的结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，在风格迁移中使用Instance Normalization不仅可以加速模型收敛，并且可以保持每个图像实例之间的独立。
IN 在训练和测试阶段都用，BN 只在训练阶段用，测试阶段用训练时通过指数衰减滑动平均保存的均值和方差



