批归一化Batch Normalization（简称BN）是由Google于2015年提出，这是一个深度神经网络训练的技巧，它不仅可以加快了模型的收敛速度，而且更重要的是在一定程度缓解了深层网络中“梯度弥散”的问题，从而使得训练深层网络模型更加容易和稳定。所以目前BN已经成为几乎所有卷积神经网络的标配技巧了。

从字面意思看来，BN就是对每一批数据进行归一化，对于训练中某一个batch的数据{x1,x2,…,xn}，注意这个数据是可以输入也可以是网络中间的某一层输出。在BN出现之前，我们的归一化操作一般都在数据输入层，对输入的数据进行求均值以及求方差做归一化，但是BN的出现打破了这一个规定，我们可以在网络中任意一层进行归一化处理，因为我们现在所用的优化方法大多都是min-batch SGD，所以我们的归一化操作就成为Batch Normalization。

批归一化算法主要分为四个步骤：

1.求每一个训练批次数据的均值

2.求每一个训练批次数据的方差

2.求每一个训练批次数据的方差

3.使用求得的均值和方差对该批次的训练数据做归一化，获得0-1分布。其中ε是为了避免除数为0时所使用的微小正数。

4.尺度变换和偏移：
将xi乘以γ调整数值大小，再加上β增加偏移后得到yi，这里的γ是尺度因子，β是平移因子。这一步是BN的精髓，由于归一化后的xi基本会被限制在正态分布下，使得网络的表达能力下降。为解决该问题，我们引入两个新的参数：γ,β。γ和β是在训练时网络自己学习得到的。



批归一化的作用：

1.改善流经网络的梯度；

2.允许更大的学习率，大幅提高训练速度：
你可以选择比较大的初始学习率，让你的训练速度飙涨。以前还需要慢慢调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性；

3.减少对初始化的强烈依赖；

4.改善正则化策略：
作为正则化的一种形式，轻微减少了对dropout的需求，你再也不用去理会过拟合中drop out、L2正则项参数的选择问题，采用BN算法后，你可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；

5.再也不需要使用使用局部响应归一化层了，因为BN本身就是一个归一化网络层；

6.可以把训练数据彻底打乱（防止每批训练的时候，某一个样本都经常被挑选到，文献说这个可以提高1%的精度）。



批量归一化已被证实为深度学习中非常有效的组成部分，在很大程度上推动了计算机视觉领域的发展。许多实践都证明了这一点，BN使用（小）批计算的均值和方差对特征进行归一化，以简化优化使非常深的网络能够融合。批量统计的随机不确定性也可以作为一个正则化器，它可以适用于泛化。BN一直是许多最先进的计算机视觉算法的基础。尽管BN取得了巨大的成功，但其存在的弊端也是由于其独特的归一化行为造成的。特别是，BN要求有足够大的批量才能工作。小批量会导致批量统计数据的估算不准确，并且减少BN的批量大小会显著增加模型误差。因此，最近的许多模型都是用较大的批量来进行训练的，这些大批量都是很耗费内存的。反过来，训练模型时对BN有效性的高度依赖性阻碍了人们用有限内存探索更高容量的模型。

计算机视觉任务（包括检测、分割、视频识别和其他基于此的高级系统）对批量大小的限制要求更高。例如，Fast/er和Mask R-CNN 框架使用批量为1或2的图像，为了更高的分辨率，其中 BN 通过变换为线性层而被固定；在3D卷积视频分类中，时空特征的出现导致在时间长度和批大小之间需要作出权衡。BN的使用通常要求这些系统在模型设计和批大小之间作出妥协。

Group Normalization(简称GN)作为BN的简单替代方法。GN将通道分成若干组，并在每组内计算均值和方差进行归一化。GN的计算不受batch sizes的影响，在很大的batch sizes范围内，GN的精度都很稳定。在ImageNet中训练的ResNet-50上，当使用batch sizes大小为2时，GN的误差比BN低10.6%；当使用典型的batch sizes时，GN与BN相当，并优于其他归一化变种。此外，GN可以自然地从预训练转移到微调。在COCO中，GN在目标检测和分割方面的表现优于BN，在Kinetics中，GN在视频分类方面的表现也优于BN，这表明GN可以在各种任务中有效地替代强大的BN。GN在现代库中只需几行代码就能轻松实现。


```python
def GroupNorm(x, gamma, beta, G, eps=1e-5):
    # x: input features with shape [N, C, H, W]
    # gamma, beta: scale and offset, with shape [1, C, 1, 1]
    # G: number of groups for GN

    N, C, H, W = x.shape
    x = tf.reshape(x, [N, G, C // G, H, W])

    mean, var = tf.nn.moments(x, [2, 3, 4], keep_dims=True)
    x = (x - mean) / tf.sqrt(var + eps)

    x = tf.reshape(x, [N, C, H, W])

    return x * gamma + beta
```

GN对batch_size不敏感，BN的误差随着小批量的大小而变得相当高。GN的行为比较稳定，对批量大小不敏感。实际上，GN在32到2的大批量范围内有非常相似的曲线（受随机变化的影响），在批量大小为2的情况下，GN的误差率比BN的误差率低。

这些结果表明，批均值和方差估计可能是过度随机和不准确的，特别是当它们是在4或2张图像上计算的。然而，如果从1张图像中计算统计数据，这种随机性就会消失，在这种情况下，BN在训练时变得与IN相似。我们看到，在批次大小为2的情况下，IN的结果比BN更好批均值和方差估计可能是过度随机和不准确的，特别是当它们是在4或2张图像上计算的。然而，如果从1张图像中计算统计数据，这种随机性就会消失，在这种情况下，BN在训练时变得与IN相似。我们看到，在批次大小为2的情况下，IN的结果比BN更好。GN的稳健结果显示了GN的优势。它允许消除BN所施加的批量大小的约束，可以给出相当多的内存（如16个或更多）。这将使训练更大容量的模型成为可能，否则就会受到内存限制的瓶颈。

GN对于图像识别、目标检测等许多场景所用的神经网络都非常有效。
