# 深度学习基础知识



## ①深度学习发展历史

- **1943年**

由神经科学家**麦卡洛克(W.S.McCilloch)** 和**数学家皮兹（W.Pitts）**在《数学生物物理学公告》上发表论文《神经活动中内在思想的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity）。建立了神经网络和数学模型，称为MCP模型。所谓**MCP**模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，也就诞生了所谓的“模拟大脑”，人工神经网络的大门由此开启。

- **1958年**

计算机科学家**罗森布拉特（ Rosenblatt）**提出了两层神经元组成的神经网络，称之为**“感知器”(Perceptrons)**。第一次将MCP用于**机器学习（machine learning）分类(classification)**。“感知器”算法算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年,该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。

- **1969年**

纵观科学发展史，无疑都是充满曲折的，深度学习也毫不例外。 1969年，美国数学家及人工智能先驱 **Marvin Minsky** 在其著作中证明了感知器本质上是一种**线性模型（linear model）**，只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了将近20年的停滞。

- **1986年**

由神经网络之父 **Geoffrey Hinton** 在1986年发明了适用于多层感知器（MLP）的**BP（Backpropagation）**算法，并采用**Sigmoid**进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。

- **90年代时期**

1991年BP算法被指出存在梯度消失问题，也就是说在误差梯度后项传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该问题直接阻碍了深度学习的进一步发展。

此外90年代中期，支持向量机算法诞生（SVM算法）等各种浅层机器学习模型被提出，SVM也是一种有监督的学习模型，应用于模式识别，分类以及回归分析等。支持向量机以统计学为基础，和神经网络有明显的差异，支持向量机等算法的提出再次阻碍了深度学习的发展。

- **发展期 2006年 - 2012年**

2006年，加拿大多伦多大学教授、机器学习领域泰斗、神经网络之父—— Geoffrey Hinton 和他的学生 Ruslan Salakhutdinov 在顶尖学术刊物《科学》上发表了一篇文章，该文章提出了深层网络训练中梯度消失问题的解决方案：**无监督预训练对权值进行初始化+有监督训练微调**。斯坦福大学、纽约大学、加拿大蒙特利尔大学等成为研究深度学习的重镇，至此开启了深度学习在学术界和工业界的浪潮。

2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。2011年以来，微软首次将DL应用在语音识别上，取得了重大突破。微软研究院和Google的语音识别研究人员先后采用DNN技术降低语音识别错误率20％~30％，是语音识别领域十多年来最大的突破性进展。2012年，DNN技术在图像识别领域取得惊人的效果，在ImageNet评测上将错误率从26％降低到15％。在这一年，DNN还被应用于制药公司的DrugeActivity预测问题，并获得世界最好成绩。

- **爆发期 2012 - 2017**

2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。

2013、2014、2015、2016年，通过ImageNet图像识别比赛，DL的网络结构，训练方法，GPU硬件的不断进步，促使其在其他领域也在不断的征服战场。

2016年3月，由谷歌（Google）旗下DeepMind公司开发的AlphaGo(基于深度学习)与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜；2016年末2017年初，该程序在中国棋类网站上以“大师”（Master）为注册帐号与中日韩数十位围棋高手进行快棋对决，连续60局无一败绩；2017年5月，在中国乌镇围棋峰会上，它与排名世界第一的世界围棋冠军柯洁对战，以3比0的总比分获胜。围棋界公认阿尔法围棋的棋力已经超过人类职业围棋顶尖水平。

## ②人工智能、机器学习、深度学习有什么区别和联系

人工智能是最早出现的，其次是机器学习，最后是深度学习。

五十年代，人工智能曾一度被极为看好。之后，人工智能的一些较小的子集发展了起来。先是机器学习，然后是深度学习。深度学习又是机器学习的子集。深度学习造成了前所未有的巨大的影响。

人工智能——为机器赋予人的智能

 机器学习—— 一种实现人工智能的方法

深度学习——一种实现机器学习的技术

## ③神经元、单层感知机、多层感知机

正如其名字所表明，神经网络的灵感来源于人类大脑的神经结构，像在一个人类大脑中，最基本的构件就叫做神经元。它的功能和人的神经元很相似，换句话说，它有一些输入，然后给一个输出。在数学上，在机器学习中的神经元就是一个数学函数的占位符，它仅有的工作就是对输入使用一个函数，然后给一个输出。

单层感知器由一个线性组合器和一个二值阈值元件组成。

![](C:\Users\13073\Desktop\第一次\单层感知机.png)

输入向量为x，权重向量为w，w0为偏执。

简单的理解可以解释为：将x0,x1······xn的变量输入，经过组合器的整合，输出1或者-1，也就是通过组合器对输入变量判断其正确与否。

而这个判断的依据就是权重w0,w1······wn。

因为线性组合器是实现加法的方式，根据向量的运算法则，所以以上公式的输入值可以理解为：
 w0+x1w1+······+xnwn

多层感知机（MLP，Multilayer Perceptron）也叫人工神经网络（ANN，Artificial Neural Network），除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构，如下图

![多层感知机](C:\Users\13073\Desktop\第一次\多层感知机.png)

 从上图可以看到，多层感知机层与层之间是全连接的。多层感知机最底层是输入层，中间是隐藏层，最后是输出层。 

## ④什么是前向传播

![前向传播](C:\Users\13073\Desktop\第一次\前向传播.png)

假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。
最终不断的通过这种方法一层层的运算，得到输出层结果。对于前向传播来说，不管维度多高，其过程都可以用如下公式表示：

​                                                   α²=σ(z²)=σ(α✳W²+b²)

其中，上标代表层数，星号表示卷积，b表示偏置项bias，σ 表示激活函数。

## ⑤反向传播算法

反向传播算法是多层神经网络的训练中举足轻重的算法。简单的理解，它的确就是复合函数的链式法则。

![反向传播1](C:\Users\13073\Desktop\第一次\反向传播1.jpg)

![反向传播2](C:\Users\13073\Desktop\第一次\反向传播2.jpg)

上面式中的Wij就是相邻两层神经元之间的权值，它们就是深度学习需要学习的参数，也就相当于直线拟合y=k*x+b中的待求参数k和b。

