# 下一个词预测

## 数据集介绍

使用paddle.text.datasets中的Imikolov数据集，选取出现频率不小于200次的单词作为实验数据（共有585种单词），以NGRAM方式采样选取长度为5的句子，用前4个单词来预测第5个单词。

使用内置的方法分为训练集和测试集，其中训练集共有803522个数据，测试集有71152个数据。

## 模型搭建与训练

对于训练数据，先使用一层词嵌入层将每个单词嵌入为20维的词向量，再使用3层参数相同的LSTM层，其中每一个隐层的大小为200，LSTM输出层的大小为4*200，展平后接一层全连接层（大小为800 * 585）进行分类。

使用交叉熵损失，优化方法使用Adam，设置学习率为0.001，dropout_rate为0.2，批次大小为32，一共训练5轮，观察训练损失变化并在每轮结束后在测试集上评估分类准确率，召回率，平均召回率等。

## 结果表现

模型经过多次参数调整后仍表现不佳，具有将下一个词预测为索引最大的那一类即'<unk>'的偏好，这可能与'<unk>'在数据集中的占比较大有关（整体分类准确度达到了26.28%）。模型架构和训练方法还有待改进。


