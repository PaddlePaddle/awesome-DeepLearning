```python
# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原
# View dataset directory. 
# This directory will be recovered automatically after resetting environment. 
!ls /home/aistudio/data
```

    data65


### **1.损失函数方法补充**

#### **①MSE均方方差**

&emsp;&emsp;表达式如下（ y 与 -y 代表模型的预测值与标签值）：

![](https://ai-studio-static-online.cdn.bcebos.com/e328591796ad417abde384ae1557386e85ec2e7cc2804eb6b74692bed63e43e1)


#### **代码实现**


```python
import numpy as np

def MSE(y, t):
    return (1/len(y)) * np.sum((y - t)**2)

t = [0, 1, 0, 0]
y = [0.1, 0.05, 0.05, 0.8]
print(MSE(np.array(y), np.array(t)))
```

    0.38875000000000004


### **2.池化方法补充**

#### **①最大池化**

&emsp;&emsp;max pooling的前向传播是把patch中最大的值传递给后一层，而其他像素的值直接被舍弃掉。那么反向传播也就是把梯度直接传给前一层某一个像素，而其他像素不接受梯度，也就是为0。所以max pooling操作和mean pooling操作不同点在于需要记录下池化操作时到底哪个像素的值是最大。

![](https://ai-studio-static-online.cdn.bcebos.com/0c811cc92ca345fba3ab698fdfdf59840294b9adb9af49dfb99f0999c28c07b8)

#### **②平均池化**

&emsp;&emsp;avg pooling的前向传播是把patch中所有值的均值传递给后一层。

![](https://ai-studio-static-online.cdn.bcebos.com/8d0200c3ccac42ee8f8b5accd059b5319c5a96d8ca34448ba5a48e8ae6170745)


#### **代码实现**


```python
import numpy as np
from numpy.lib.stride_tricks import as_strided

def pool2d(A, kernel_size, stride, padding, pool_mode='max'):
    '''
    2D Pooling

    Parameters:
        A: input 2D array
        kernel_size: int, the size of the window
        stride: int, the stride of the window
        padding: int, implicit zero paddings on both sides of the input
        pool_mode: string, 'max' or 'avg'
    '''
    # Padding
    A = np.pad(A, padding, mode='constant')

    # Window view of A
    output_shape = ((A.shape[0] - kernel_size)//stride + 1,
                    (A.shape[1] - kernel_size)//stride + 1)
    kernel_size = (kernel_size, kernel_size)
    A_w = as_strided(A, shape = output_shape + kernel_size, 
                        strides = (stride*A.strides[0],
                                   stride*A.strides[1]) + A.strides)
    A_w = A_w.reshape(-1, *kernel_size)

    # Return the result of pooling
    if pool_mode == 'max':
        return A_w.max(axis=(1,2)).reshape(output_shape)
    elif pool_mode == 'avg':
        return A_w.mean(axis=(1,2)).reshape(output_shape)

```

#### **max pooling**


```python
A = np.array([[1, 1, 2, 4],
              [5, 6, 7, 8],
              [3, 2, 1, 0],
              [1, 2, 3, 4]])

print(pool2d(A, kernel_size=2, stride=2, padding=0, pool_mode='max'))

```

    [[6 8]
     [3 4]]


#### **avg pooling**


```python
A = np.array([[1, 1, 2, 4],
              [5, 6, 7, 8],
              [3, 2, 1, 0],
              [1, 2, 3, 4]])

print(pool2d(A, kernel_size=2, stride=2, padding=0, pool_mode='avg'))

```

    [[3.25 5.25]
     [2.   2.  ]]


### **3.数据增强方法补充**

#### **常用的数据增强方法有：**

①旋转 | 反射变换(Rotation/reflection): 随机旋转图像一定角度; 改变图像内容的朝向;

②翻转变换(flip): 沿着水平或者垂直方向翻转图像;

③缩放变换(zoom): 按照一定的比例放大或者缩小图像;

④平移变换(shift): 在图像平面上对图像以一定方式进行平移;

⑤可以采用随机或人为定义的方式指定平移范围和平移步长, 沿水平或竖直方向进行平移. 改变图像内容的位置;

⑥尺度变换(scale): 对图像按照指定的尺度因子, 进行放大或缩小; 或者参照SIFT特征提取思想, 利用指定的尺度因子对图像滤波构造尺度空间. 改变图像内容的大小或模糊程度;

⑦对比度变换(contrast): 在图像的HSV颜色空间，改变饱和度S和V亮度分量，保持色调H不变. 对每个像素的S和V分量进行指数运算(指数因子在0.25到4之间), 增加光照变化;

⑧噪声扰动(noise): 对图像的每个像素RGB进行随机扰动, 常用的噪声模式是椒盐噪声和高斯噪声;

⑨颜色变化：在图像通道上添加随机扰动，输入图像随机选择一块区域涂黑。


### **4.图像分类方法综述**

#### **1.传统图像分类方法**

&emsp;&emsp;通常完整建立图像识别模型一般包括底层特征学习、特征编码、空间约束、分类器设计、模型融合等几个阶段，如下图所示。

![](https://ai-studio-static-online.cdn.bcebos.com/de9d5f58df364256893f8cae730e84160f6e83cd7be84f5e8920d7584a455ec7)

#### **2.深度学习方法**

##### **①CNN**

&emsp;&emsp;Alex Krizhevsky在2012年ILSVRC提出的CNN模型取得了历史性的突破，效果大幅度超越传统方法，获得了ILSVRC2012冠军，该模型被称作AlexNet。这也是首次将深度学习用于大规模图像分类中。

&emsp;&emsp;传统CNN包含卷积层、全连接层等组件，并采用softmax多类别分类器和多类交叉熵损失函数，一个典型的卷积神经网络如图6所示，我们先介绍用来构造CNN的常见组件。

![](https://ai-studio-static-online.cdn.bcebos.com/a8f98a42d0fe46bcb9510eb93aab9502f46cd7da3731481b9f31f7bc3a79f60d)

&emsp;&emsp;卷积层(convolution layer): 执行卷积操作提取底层到高层的特征，发掘出图片局部关联性质和空间不变性质。

&emsp;&emsp;池化层(pooling layer): 执行降采样操作。通过取卷积输出特征图中局部区块的最大值(max-pooling)或者均值(avg-pooling)。降采样也是图像处理中常见的一种操作，可以过滤掉一些不重要的高频信息。

&emsp;&emsp;全连接层(fully-connected layer，或者fc layer): 输入层到隐藏层的神经元是全部连接的。

&emsp;&emsp;非线性变化: 卷积层、全连接层后面一般都会接非线性变化函数，例如Sigmoid、Tanh、ReLu等来增强网络的表达能力，在CNN里最常使用的为ReLu激活函数。

&emsp;&emsp;Dropout: 在模型训练阶段随机让一些隐层节点权重不工作，提高网络的泛化能力，一定程度上防止过拟合。

##### **②VGG**

&emsp;&emsp;牛津大学VGG(Visual Geometry Group)组在2014年ILSVRC提出的模型被称作VGG模型。该模型相比以往模型进一步加宽和加深了网络结构，它的核心是五组卷积操作，每两组之间做Max-Pooling空间降维。同一组内采用多次连续的3X3卷积，卷积核的数目由较浅组的64增多到最深组的512，同一组内的卷积核数目是一样的。卷积之后接两层全连接层，之后是分类层。

&emsp;&emsp;由于每组内卷积层的不同，有11、13、16、19层这几种模型，下图展示一个16层的网络结构。VGG模型结构相对简洁，提出之后也有很多文章基于此模型进行研究，如在ImageNet上首次公开超过人眼识别的模型就是借鉴VGG模型的结构。

![](https://ai-studio-static-online.cdn.bcebos.com/28c949e171ea46d68bd87693627e4a235b3983b89e3049a8810f2d7fd6727924)

#### **③GoogleNet**

&emsp;&emsp;GoogLeNet 在2014年ILSVRC的获得了冠军，在介绍该模型之前我们先来了解NIN(Network in Network)模型和Inception模块，因为GoogLeNet模型由多组Inception模块组成，模型设计借鉴了NIN的一些思想。

![](https://ai-studio-static-online.cdn.bcebos.com/5c7ea9febee94d8a98685b222772f96e0315a06d2864443cbb2940ce29ce00b4)

&emsp;&emsp;GoogLeNet由多组Inception模块堆积而成。另外，在网络最后也没有采用传统的多层全连接层，而是像NIN网络一样采用了均值池化层；但与NIN不同的是，GoogLeNet在池化层后加了一个全连接层来映射类别数。

&emsp;&emsp;除了这两个特点之外，由于网络中间层特征也很有判别性，GoogLeNet在中间层添加了两个辅助分类器，在后向传播中增强梯度并且增强正则化，而整个网络的损失函数是这个三个分类器的损失加权求和。

&emsp;&emsp;GoogLeNet整体网络结构总共22层网络：开始由3层普通的卷积组成；接下来由三组子网络组成，第一组子网络包含2个Inception模块，第二组包含5个Inception模块，第三组包含2个Inception模块；然后接均值池化层、全连接层。

#### **④ResNet**

&emsp;&emsp;ResNet(Residual Network) 是2015年ImageNet图像分类、图像物体定位和图像物体检测比赛的冠军。针对随着网络训练加深导致准确度下降的问题，ResNet提出了残差学习方法来减轻训练深层网络的困难。

&emsp;&emsp;在已有设计思路(BN, 小卷积核，全卷积网络)的基础上，引入了残差模块。每个残差模块包含两条路径，其中一条路径是输入特征的直连通路，另一条路径对该特征做两到三次卷积操作得到该特征的残差，最后再将两条路径上的特征相加。

&emsp;&emsp;残差模块如下图所示，左边是基本模块连接方式，由两个输出通道数相同的3x3卷积组成。右边是瓶颈模块(Bottleneck)连接方式，之所以称为瓶颈，是因为上面的1x1卷积用来降维(图示例即256->64)，下面的1x1卷积用来升维(图示例即64->256)，这样中间3x3卷积的输入和输出通道数都较小(图示例即64->64)。

![](https://ai-studio-static-online.cdn.bcebos.com/c5ef21ebacca494bbe84266e23e58f75b747e2382a8d493a80607e52c3561c22)

