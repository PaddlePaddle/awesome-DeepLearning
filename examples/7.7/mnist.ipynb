{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 手写数字识别\n",
    "\n",
    "* 导入环境\n",
    "* 数据准备\t\n",
    "* 模型设计——基于ResNet50\n",
    "* 训练配置\n",
    "* 训练过程\n",
    "* 模型保存\n",
    "\n",
    "## 概要\n",
    "\n",
    "* 任务输入：一系列手写数字图片，其中每张图片都是28x28的像素矩阵。\n",
    "* 任务输出：经过了大小归一化和居中处理，输出对应的0~9的数字标签。\n",
    "\n",
    "## 导入环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import json\r\n",
    "import gzip\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import paddle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据准备\n",
    "一般涉及如下五个环节：\n",
    "* 读入数据\n",
    "* 数据划分\n",
    "* 数据乱序\n",
    "\n",
    "### 读入数据并划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mnist dataset from ./work/datasets/mnist.json.gz ......\n",
      "mnist dataset load done\n",
      "训练数据集数量:  50000\n",
      "验证数据集数量:  10000\n",
      "测试数据集数量:  10000\n"
     ]
    }
   ],
   "source": [
    "# 声明数据集文件位置\r\n",
    "datafile = './work/datasets/mnist.json.gz'\r\n",
    "print('loading mnist dataset from {} ......'.format(datafile))\r\n",
    "# 加载json数据文件\r\n",
    "data = json.load(gzip.open(datafile))\r\n",
    "print('mnist dataset load done')\r\n",
    "# 读取到的数据区分训练集，验证集，测试集\r\n",
    "train_set, val_set, eval_set = data\r\n",
    "\r\n",
    "# 观察训练集数据\r\n",
    "imgs, labels = train_set[0], train_set[1]\r\n",
    "print(\"训练数据集数量: \", len(imgs))\r\n",
    "\r\n",
    "# 观察验证集数量\r\n",
    "imgs, labels = val_set[0], val_set[1]\r\n",
    "print(\"验证数据集数量: \", len(imgs))\r\n",
    "\r\n",
    "# 观察测试集数量\r\n",
    "imgs, labels = val= eval_set[0], eval_set[1]\r\n",
    "print(\"测试数据集数量: \", len(imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据乱序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgs_length = len(imgs)\r\n",
    "# 定义数据集每个数据的序号，根据序号读取数据\r\n",
    "index_list = list(range(imgs_length))\r\n",
    "# 读入数据时用到的批次大小\r\n",
    "BATCHSIZE = 100\r\n",
    "\r\n",
    "# 随机打乱训练数据的索引序号\r\n",
    "random.shuffle(index_list)\r\n",
    "\r\n",
    "# 定义数据生成器，返回批次数据\r\n",
    "def data_generator():\r\n",
    "    imgs_list = []\r\n",
    "    labels_list = []\r\n",
    "    for i in index_list:\r\n",
    "        # 将数据处理成希望的类型\r\n",
    "        img = np.reshape(imgs[i], [1, 28, 28]).astype('float32')\r\n",
    "        label = np.reshape(labels[i], [1]).astype('int64')\r\n",
    "        imgs_list.append(img) \r\n",
    "        labels_list.append(label)\r\n",
    "        if len(imgs_list) == BATCHSIZE:\r\n",
    "            # 获得一个batchsize的数据，并返回\r\n",
    "            yield np.array(imgs_list), np.array(labels_list)\r\n",
    "            # 清空数据读取列表\r\n",
    "            imgs_list = []\r\n",
    "            labels_list = []\r\n",
    "\r\n",
    "    # 如果剩余数据的数目小于BATCHSIZE，\r\n",
    "    # 则剩余数据一起构成一个大小为len(imgs_list)的mini-batch\r\n",
    "    if len(imgs_list) > 0:\r\n",
    "        yield np.array(imgs_list), np.array(labels_list)\r\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印第一个batch数据的维度:\n",
      "图像维度: (100, 1, 28, 28), 标签维度: (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# 声明数据读取函数，从训练集中读取数据\r\n",
    "train_loader = data_generator\r\n",
    "# 以迭代的形式读取数据\r\n",
    "for batch_id, data in enumerate(train_loader()):\r\n",
    "    image_data, label_data = data\r\n",
    "    if batch_id == 0:\r\n",
    "        # 打印数据shape和类型\r\n",
    "        print(\"打印第一个batch数据的维度:\")\r\n",
    "        print(\"图像维度: {}, 标签维度: {}\".format(image_data.shape, label_data.shape))\r\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 封装数据准备函数\n",
    "上文，我们从读取数据、划分数据集、到打乱训练数据、构建数据读取器，完成了一整套一般性的数据处理流程，下面将这些步骤放在一个函数中实现，方便在神经网络训练时直接调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(mode='train'):\r\n",
    "    datafile = './work/datasets/mnist.json.gz'\r\n",
    "    print('loading mnist dataset from {} ......'.format(datafile))\r\n",
    "    # 加载json数据文件\r\n",
    "    data = json.load(gzip.open(datafile))\r\n",
    "    print('mnist dataset load done')\r\n",
    "   \r\n",
    "    # 读取到的数据区分训练集，验证集，测试集\r\n",
    "    train_set, val_set, eval_set = data\r\n",
    "    if mode=='train':\r\n",
    "        # 获得训练数据集\r\n",
    "        imgs, labels = train_set[0], train_set[1]\r\n",
    "    elif mode=='valid':\r\n",
    "        # 获得验证数据集\r\n",
    "        imgs, labels = val_set[0], val_set[1]\r\n",
    "    elif mode=='eval':\r\n",
    "        # 获得测试数据集\r\n",
    "        imgs, labels = eval_set[0], eval_set[1]\r\n",
    "    else:\r\n",
    "        raise Exception(\"mode can only be one of ['train', 'valid', 'eval']\")\r\n",
    "    print(\"训练数据集数量: \", len(imgs))\r\n",
    "    \r\n",
    "    # 获得数据集长度\r\n",
    "    imgs_length = len(imgs)\r\n",
    "\r\n",
    "    # 定义数据集每个数据的序号，根据序号读取数据\r\n",
    "    index_list = list(range(imgs_length))\r\n",
    "    # 读入数据时用到的批次大小\r\n",
    "    BATCHSIZE = 100\r\n",
    "    \r\n",
    "    # 定义数据生成器\r\n",
    "    def data_generator():\r\n",
    "        if mode == 'train':\r\n",
    "            # 训练模式下打乱数据\r\n",
    "            random.shuffle(index_list)\r\n",
    "        imgs_list = []\r\n",
    "        labels_list = []\r\n",
    "        for i in index_list:\r\n",
    "            # 将数据处理成希望的类型\r\n",
    "            img = np.reshape(imgs[i], [1, 28, 28]).astype('float32')\r\n",
    "            label = np.reshape(labels[i], [1]).astype('int64')\r\n",
    "            imgs_list.append(img) \r\n",
    "            labels_list.append(label)\r\n",
    "            if len(imgs_list) == BATCHSIZE:\r\n",
    "                # 获得一个batchsize的数据，并返回\r\n",
    "                yield np.array(imgs_list), np.array(labels_list)\r\n",
    "                # 清空数据读取列表\r\n",
    "                imgs_list = []\r\n",
    "                labels_list = []\r\n",
    "    \r\n",
    "        # 如果剩余数据的数目小于BATCHSIZE，\r\n",
    "        # 则剩余数据一起构成一个大小为len(imgs_list)的mini-batch\r\n",
    "        if len(imgs_list) > 0:\r\n",
    "            yield np.array(imgs_list), np.array(labels_list)\r\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 基于ResNet50实现MNIST任务\n",
    "\n",
    "#### 1. 简述：\n",
    "* Resnet是残差网络(Residual Network)的缩写,该系列网络广泛用于目标分类等领域以及作为计算机视觉任务主干经典神经网络的一部分，典型的网络有resnet50, resnet101等。Resnet网络的证明网络能够向更深（包含更多隐藏层）的方向发展。\n",
    "* 论文：[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "#### 2. ResNet50网络结构\n",
    "首先对输入做了卷积操作，之后包含4个残差快（ResidualBlock), 最后进行全连接操作以便于进行分类任务，网络构成示意图如下所示, Resnet50则包含50个conv2d操作。\n",
    "\n",
    "<center>\n",
    "    <img \n",
    "    src=\"https://img-blog.csdnimg.cn/20190529172052544.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMjc4Nzkx,size_16,color_FFFFFF,t_70\"\n",
    "    width=512 height=\n",
    "    >\n",
    "    <br>\n",
    "    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">ResNet50网络结构</div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resnet50 = paddle.vision.models.resnet50(num_classes = 10)\r\n",
    "resnet50.conv1 = paddle.nn.Conv2D(1, 64, kernel_size=7, stride=2, padding=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型训练、评估与保存\r\n",
    "这里我们创建一个 Trainer 类，该类用于执行训练过程，计算模型分类效果以及保存模型。\r\n",
    "#### 训练\r\n",
    "train()用于执行整个训练过程；train_epoch()用于执行一个epoch的训练过程；train_step()用于执行一个批次的训练过程；\r\n",
    "\r\n",
    "#### 评估\r\n",
    "准确率是一个直观衡量分类模型效果的指标，为了能够充分的反应不同方法之间的优略，这里我们实现了 val_epoch() 函数来计算模型的分类准确率。\r\n",
    "\r\n",
    "#### 保存\r\n",
    "save()用于保存模型参数，这里调用了paddle的 save() API；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn.functional as F\r\n",
    "\r\n",
    "class Trainer(object):\r\n",
    "    def __init__(self, model_path, model, optimizer):\r\n",
    "        self.model_path = model_path   # 模型存放路径 \r\n",
    "        self.model = model             # 定义的模型\r\n",
    "        self.optimizer = optimizer     # 优化器\r\n",
    "\r\n",
    "    def save(self):\r\n",
    "        # 保存模型\r\n",
    "        paddle.save(self.model.state_dict(), self.model_path)\r\n",
    "\r\n",
    "    def val_epoch(self, datasets):\r\n",
    "        self.model.eval()  # 将模型设置为评估状态\r\n",
    "        acc = list()\r\n",
    "        for batch_id, data in enumerate(datasets()):\r\n",
    "            images, labels = data\r\n",
    "            images = paddle.to_tensor(images)\r\n",
    "            labels = paddle.to_tensor(labels)\r\n",
    "            pred = self.model(images)   # 获取预测值\r\n",
    "            # 取 pred 中得分最高的索引作为分类结果\r\n",
    "            pred = paddle.argmax(pred, axis=-1)  \r\n",
    "            res = paddle.equal(pred, labels)\r\n",
    "            res = paddle.cast(res, dtype='float32')\r\n",
    "            acc.extend(res.numpy())  # 追加\r\n",
    "        acc = np.array(acc).mean()\r\n",
    "        return acc\r\n",
    "\r\n",
    "    def train_step(self, data):\r\n",
    "        images, labels = data\r\n",
    "        images = paddle.to_tensor(images)\r\n",
    "        labels = paddle.to_tensor(labels)\r\n",
    "        # 前向计算的过程\r\n",
    "        predicts = self.model(images)\r\n",
    "        # 计算损失\r\n",
    "        loss = F.cross_entropy(predicts, labels)\r\n",
    "        avg_loss = paddle.mean(loss)\r\n",
    "        # 后向传播，更新参数的过程\r\n",
    "        avg_loss.backward()\r\n",
    "        self.optimizer.step()\r\n",
    "        self.optimizer.clear_grad()\r\n",
    "        return avg_loss\r\n",
    "\r\n",
    "    def train_epoch(self, datasets, epoch):\r\n",
    "        self.model.train()\r\n",
    "        for batch_id, data in enumerate(datasets()):\r\n",
    "            loss = self.train_step(data)\r\n",
    "            # 每训练了100批次的数据，打印下当前Loss的情况\r\n",
    "            if batch_id % 100 == 0:\r\n",
    "                print(\"epoch_id: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, loss.numpy()))\r\n",
    "\r\n",
    "    def train(self, train_datasets, val_datasets, epochs):\r\n",
    "        for i in range(epochs):\r\n",
    "            self.train_epoch(train_datasets, i)\r\n",
    "            train_acc = self.val_epoch(train_datasets)\r\n",
    "            val_acc = self.val_epoch(val_datasets)\r\n",
    "            print(\"epoch_id: {}, train acc is: {}, val acc is {}\".format(i, train_acc, val_acc))\r\n",
    "        self.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 训练配置\n",
    "\n",
    "* optimizer: SGD\n",
    "\n",
    "* learning_rate = 0.01\n",
    "\n",
    "* batch_size = 100\n",
    "\n",
    "* epoch = 10\n",
    "\n",
    "* 资源配置：GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mnist dataset from ./work/datasets/mnist.json.gz ......\n",
      "mnist dataset load done\n",
      "训练数据集数量:  50000\n",
      "loading mnist dataset from ./work/datasets/mnist.json.gz ......\n",
      "mnist dataset load done\n",
      "训练数据集数量:  10000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\r\n",
    "lr = 0.01\r\n",
    "model_path = './mnist.pdparams'\r\n",
    "\r\n",
    "train_loader = load_data(mode='train')\r\n",
    "\r\n",
    "val_loader = load_data(mode='eval')\r\n",
    "\r\n",
    "model = resnet50\r\n",
    "opt = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters())\r\n",
    "\r\n",
    "trainer = Trainer(\r\n",
    "    model_path=model_path,\r\n",
    "    model=model,\r\n",
    "    optimizer=opt\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_id: 0, batch_id: 0, loss is: [4.66148]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:648: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_id: 0, batch_id: 100, loss is: [0.6221934]\n",
      "epoch_id: 0, batch_id: 200, loss is: [0.41913813]\n",
      "epoch_id: 0, batch_id: 300, loss is: [0.3925648]\n",
      "epoch_id: 0, batch_id: 400, loss is: [0.1667978]\n",
      "epoch_id: 0, train acc is: 0.10873360186815262, val acc is 0.1057019978761673\n",
      "epoch_id: 1, batch_id: 0, loss is: [0.1806835]\n",
      "epoch_id: 1, batch_id: 100, loss is: [0.1847708]\n",
      "epoch_id: 1, batch_id: 200, loss is: [0.04671999]\n",
      "epoch_id: 1, batch_id: 300, loss is: [0.13059853]\n",
      "epoch_id: 1, batch_id: 400, loss is: [0.18486111]\n",
      "epoch_id: 1, train acc is: 0.10925640165805817, val acc is 0.10611099749803543\n",
      "epoch_id: 2, batch_id: 0, loss is: [0.09198789]\n",
      "epoch_id: 2, batch_id: 100, loss is: [0.10301861]\n",
      "epoch_id: 2, batch_id: 200, loss is: [0.04523353]\n",
      "epoch_id: 2, batch_id: 300, loss is: [0.00639199]\n",
      "epoch_id: 2, batch_id: 400, loss is: [0.03405045]\n",
      "epoch_id: 2, train acc is: 0.10922800004482269, val acc is 0.10610699653625488\n",
      "epoch_id: 3, batch_id: 0, loss is: [0.01870867]\n",
      "epoch_id: 3, batch_id: 100, loss is: [0.05855814]\n",
      "epoch_id: 3, batch_id: 200, loss is: [0.04524782]\n",
      "epoch_id: 3, batch_id: 300, loss is: [0.02252109]\n",
      "epoch_id: 3, batch_id: 400, loss is: [0.0387023]\n",
      "epoch_id: 3, train acc is: 0.10890679806470871, val acc is 0.10605999827384949\n",
      "epoch_id: 4, batch_id: 0, loss is: [0.02594046]\n",
      "epoch_id: 4, batch_id: 100, loss is: [0.06403918]\n",
      "epoch_id: 4, batch_id: 200, loss is: [0.00496634]\n",
      "epoch_id: 4, batch_id: 300, loss is: [0.00697721]\n",
      "epoch_id: 4, batch_id: 400, loss is: [0.00414439]\n",
      "epoch_id: 4, train acc is: 0.1094290018081665, val acc is 0.10610000044107437\n",
      "epoch_id: 5, batch_id: 0, loss is: [0.00843224]\n",
      "epoch_id: 5, batch_id: 100, loss is: [0.00834099]\n",
      "epoch_id: 5, batch_id: 200, loss is: [0.00732643]\n",
      "epoch_id: 5, batch_id: 300, loss is: [0.01014869]\n",
      "epoch_id: 5, batch_id: 400, loss is: [0.00246353]\n",
      "epoch_id: 5, train acc is: 0.10934960097074509, val acc is 0.10606899857521057\n",
      "epoch_id: 6, batch_id: 0, loss is: [0.00336655]\n",
      "epoch_id: 6, batch_id: 100, loss is: [0.00326408]\n",
      "epoch_id: 6, batch_id: 200, loss is: [0.00875772]\n",
      "epoch_id: 6, batch_id: 300, loss is: [0.00418]\n",
      "epoch_id: 6, batch_id: 400, loss is: [0.05893454]\n",
      "epoch_id: 6, train acc is: 0.10890799760818481, val acc is 0.1061059981584549\n",
      "epoch_id: 7, batch_id: 0, loss is: [0.03115534]\n",
      "epoch_id: 7, batch_id: 100, loss is: [0.00268216]\n",
      "epoch_id: 7, batch_id: 200, loss is: [0.00467543]\n",
      "epoch_id: 7, batch_id: 300, loss is: [0.00238595]\n",
      "epoch_id: 7, batch_id: 400, loss is: [0.01005261]\n",
      "epoch_id: 7, train acc is: 0.10915359854698181, val acc is 0.1060359999537468\n",
      "epoch_id: 8, batch_id: 0, loss is: [0.00655347]\n",
      "epoch_id: 8, batch_id: 100, loss is: [0.01317948]\n",
      "epoch_id: 8, batch_id: 200, loss is: [0.00064187]\n",
      "epoch_id: 8, batch_id: 300, loss is: [0.00302099]\n",
      "epoch_id: 8, batch_id: 400, loss is: [0.00104737]\n",
      "epoch_id: 8, train acc is: 0.10890399664640427, val acc is 0.10600399971008301\n",
      "epoch_id: 9, batch_id: 0, loss is: [0.00045348]\n",
      "epoch_id: 9, batch_id: 100, loss is: [0.00032724]\n",
      "epoch_id: 9, batch_id: 200, loss is: [0.00058778]\n",
      "epoch_id: 9, batch_id: 300, loss is: [0.00095531]\n",
      "epoch_id: 9, batch_id: 400, loss is: [0.0004282]\n",
      "epoch_id: 9, train acc is: 0.10923200100660324, val acc is 0.10602100193500519\n"
     ]
    }
   ],
   "source": [
    "use_gpu = True\r\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')\r\n",
    "\r\n",
    "trainer.train(train_datasets=train_loader, val_datasets=val_loader, epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
