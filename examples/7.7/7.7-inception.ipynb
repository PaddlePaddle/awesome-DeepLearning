{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist.json.gz\r\n"
     ]
    }
   ],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/external-libraries’: File exists\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 15.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve-2.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/bs4 already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/beautifulsoup4-4.9.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\r\n",
    "import paddle.nn.functional as F\r\n",
    "import os\r\n",
    "import gzip\r\n",
    "import json\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import paddle.fluid as fluid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "data包含三个元素的列表：train_set、val_set、 test_set，包括50 000条训练样本、10 000条验证样本、10 000条测试样本。每个样本包含手写数字图片和对应的标签。\n",
    "\n",
    "train_set（训练集）：用于确定模型参数。 val_set（验证集）：用于调节模型超参数（如多个网络结构、正则化权重的最优选择）。 test_set（测试集）：用于估计应用效果（没有在模型中应用过的数据，更贴近模型在真实场景应用的效果）。 train_set包含两个元素的列表：train_images、train_labels。\n",
    "\n",
    "train_images：[50000,784]的二维列表，包含50 000张图片。每张图片用一个长度为784的向量表示，内容是28*28尺寸的像素灰度值（黑白图片）。 train_labels：[50 000, ]的列表，表示这些图片对应的分类标签，即0~9之间的一个数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义数据集读取器\r\n",
    "def load_data(mode='train'):\r\n",
    "\r\n",
    "    # 读取数据文件\r\n",
    "    datafile = './work/mnist.json.gz'\r\n",
    "    print('loading mnist dataset from {} ......'.format(datafile))\r\n",
    "    data = json.load(gzip.open(datafile))\r\n",
    "    # 读取数据集中的训练集，验证集和测试集\r\n",
    "    train_set, val_set, eval_set = data\r\n",
    "\r\n",
    "    # 数据集相关参数，图片高度IMG_ROWS, 图片宽度IMG_COLS\r\n",
    "    IMG_ROWS = 28\r\n",
    "    IMG_COLS = 28\r\n",
    "    # 根据输入mode参数决定使用训练集，验证集还是测试\r\n",
    "    if mode == 'train':\r\n",
    "        imgs = train_set[0]\r\n",
    "        labels = train_set[1]\r\n",
    "    elif mode == 'valid':\r\n",
    "        imgs = val_set[0]\r\n",
    "        labels = val_set[1]\r\n",
    "    elif mode == 'eval':\r\n",
    "        imgs = eval_set[0]\r\n",
    "        labels = eval_set[1]\r\n",
    "    # 获得所有图像的数量\r\n",
    "    imgs_length = len(imgs)\r\n",
    "    # 验证图像数量和标签数量是否一致\r\n",
    "    assert len(imgs) == len(labels), \\\r\n",
    "          \"length of train_imgs({}) should be the same as train_labels({})\".format(\r\n",
    "                  len(imgs), len(labels))\r\n",
    "\r\n",
    "    index_list = list(range(imgs_length))\r\n",
    "\r\n",
    "    # 读入数据时用到的batchsize\r\n",
    "    BATCHSIZE = 100\r\n",
    "\r\n",
    "    # 定义数据生成器\r\n",
    "    def data_generator():\r\n",
    "        # 训练模式下，打乱训练数据\r\n",
    "        if mode == 'train':\r\n",
    "            random.shuffle(index_list)\r\n",
    "        imgs_list = []\r\n",
    "        labels_list = []\r\n",
    "        # 按照索引读取数据\r\n",
    "        for i in index_list:\r\n",
    "            # 读取图像和标签，转换其尺寸和类型\r\n",
    "            img = np.reshape(imgs[i], [1, IMG_ROWS, IMG_COLS]).astype('float32')\r\n",
    "            label = np.reshape(labels[i], [1]).astype('int64')\r\n",
    "            imgs_list.append(img) \r\n",
    "            labels_list.append(label)\r\n",
    "            # 如果当前数据缓存达到了batch size，就返回一个批次数据\r\n",
    "            if len(imgs_list) == BATCHSIZE:\r\n",
    "                yield np.array(imgs_list), np.array(labels_list)\r\n",
    "                # 清空数据缓存列表\r\n",
    "                imgs_list = []\r\n",
    "                labels_list = []\r\n",
    "\r\n",
    "        # 如果剩余数据的数目小于BATCHSIZE，\r\n",
    "        # 则剩余数据一起构成一个大小为len(imgs_list)的mini-batch\r\n",
    "        if len(imgs_list) > 0:\r\n",
    "            yield np.array(imgs_list), np.array(labels_list)\r\n",
    "\r\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "卷积神经网络的结构多种多样，可以在网络的深度上进行延仲，也可在网络的宽度上进行拓展.GoogleNet采用了多个Inception模块来提升网络的深度和宽度，从而达到提高分类准确率，本实验所用的网络是GoogLeNet的简化版。\n",
    "\n",
    "网络中的 Inception模块由4个分支组成，其具体结构如图所示，输入数据分别由4个分支进行处理（处理前后图像尺寸一样)，然后将4个分支的输出堆叠在一起作为下一层的输入。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/2af18e11e26744329bdc2a441e0edbeddbbd5fcd7f3f4ea89ecac4f1b629e0e8)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7c44b3fff817434dac21524b9020e4f4b9bf019bdc2d4932b2c96b933547474e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class InceptionA(paddle.nn.Layer):\r\n",
    "    def __init__(self, in_channels):\r\n",
    "        super(InceptionA, self).__init__()\r\n",
    "        self.branch1x1 = Conv2D(in_channels, 16, kernel_size=1)  \r\n",
    "\r\n",
    "        self.branch5x5_1 = Conv2D(in_channels, 16, kernel_size=1)  \r\n",
    "        self.branch5x5_2 = Conv2D(16, 24, kernel_size=5, padding=2)  \r\n",
    "\r\n",
    "        self.branch3x3_1 = Conv2D(in_channels, 16, kernel_size=1)\r\n",
    "        self.branch3x3_2 = Conv2D(16, 24, kernel_size=3, padding=1)\r\n",
    "        self.branch3x3_3 = Conv2D(24, 24, kernel_size=3, padding=1)\r\n",
    "\r\n",
    "        self.branch_pool = Conv2D(in_channels, 24, kernel_size=1)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        branch1x1 = self.branch1x1(x)\r\n",
    "\r\n",
    "        branch5x5 = self.branch5x5_1(x)\r\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\r\n",
    "\r\n",
    "        branch3x3 = self.branch3x3_1(x)\r\n",
    "        branch3x3 = self.branch3x3_2(branch3x3)\r\n",
    "        branch3x3 = self.branch3x3_3(branch3x3)\r\n",
    "\r\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\r\n",
    "        branch_pool = self.branch_pool(branch_pool)\r\n",
    "\r\n",
    "        outputs = [branch1x1, branch5x5, branch3x3, branch_pool]  \r\n",
    "        cat = fluid.layers.concat(outputs, axis=1)\r\n",
    "        cat = fluid.layers.relu(cat)\r\n",
    "        return cat\r\n",
    "\r\n",
    "class Net(paddle.nn.Layer):  \r\n",
    "    def __init__(self, name_scope):\r\n",
    "        super(Net, self).__init__(name_scope)\r\n",
    "        name_scope = self.full_name()\r\n",
    "        self.conv1 = Conv2D(1, 10, kernel_size=5, stride=1, padding=0)\r\n",
    "        self.conv2 = Conv2D(88, 20, kernel_size=5,stride=1, padding=0)\r\n",
    "        \r\n",
    "        self.incep1 = InceptionA(in_channels=10)  \r\n",
    "        self.incep2 = InceptionA(in_channels=20) \r\n",
    "\r\n",
    "        self.maxpool = MaxPool2D(kernel_size=2)\r\n",
    "\r\n",
    "        self.fc = Linear(1408, 10)  \r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        x = self.conv1(inputs)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.maxpool(x)\r\n",
    "        x = self.incep1(x) \r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.maxpool(x)\r\n",
    "        x = self.incep2(x)  \r\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\r\n",
    "        x = self.fc(x)\r\n",
    "        x = F.softmax(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mnist dataset from ./work/mnist.json.gz ......\n",
      "epoch: 0, batch: 0, loss is: [5.5705004]\n",
      "epoch: 0, batch: 200, loss is: [0.28762066]\n",
      "epoch: 0, batch: 400, loss is: [0.17652251]\n",
      "0.85864\n",
      "epoch: 1, batch: 0, loss is: [0.08533961]\n",
      "epoch: 1, batch: 200, loss is: [0.16160418]\n",
      "epoch: 1, batch: 400, loss is: [0.2550488]\n",
      "0.96344\n",
      "epoch: 2, batch: 0, loss is: [0.09920743]\n",
      "epoch: 2, batch: 200, loss is: [0.12028863]\n",
      "epoch: 2, batch: 400, loss is: [0.10722609]\n",
      "0.97378\n",
      "epoch: 3, batch: 0, loss is: [0.08541002]\n",
      "epoch: 3, batch: 200, loss is: [0.02128511]\n",
      "epoch: 3, batch: 400, loss is: [0.07881244]\n",
      "0.97872\n",
      "epoch: 4, batch: 0, loss is: [0.03711385]\n",
      "epoch: 4, batch: 200, loss is: [0.02530745]\n",
      "epoch: 4, batch: 400, loss is: [0.06978733]\n",
      "0.98146\n"
     ]
    }
   ],
   "source": [
    "model = Net(\"mnist\")\r\n",
    "\r\n",
    "with fluid.dygraph.guard():\r\n",
    "    \r\n",
    "    \r\n",
    "    #调用加载数据的函数\r\n",
    "    train_loader = load_data('train')\r\n",
    "    #选择优化算法\r\n",
    "    optimizer = fluid.optimizer.SGDOptimizer(learning_rate=0.01, parameter_list=model.parameters())\r\n",
    "    EPOCH_NUM = 5\r\n",
    "    for epoch_id in range(EPOCH_NUM):\r\n",
    "        correct = 0\r\n",
    "        total = 0\r\n",
    "        for batch_id, data in enumerate(train_loader()):\r\n",
    "            #准备数据，变得更加简洁\r\n",
    "            image_data, label_data = data\r\n",
    "            image = fluid.dygraph.to_variable(image_data)\r\n",
    "            label = fluid.dygraph.to_variable(label_data)\r\n",
    "            \r\n",
    "            #前向计算的过程\r\n",
    "            predict = model(image)\r\n",
    "            \r\n",
    "            #计算损失，使用交叉熵损失函数，取一个批次样本损失的平均值\r\n",
    "            loss = fluid.layers.cross_entropy(predict, label)\r\n",
    "            avg_loss = fluid.layers.mean(loss)\r\n",
    "            \r\n",
    "            \r\n",
    "            \r\n",
    "            total += label.shape[0]\r\n",
    "            pred = predict.argmax(1)\r\n",
    "            for i in range(len(pred)):\r\n",
    "                if(pred[i] == label[i]):\r\n",
    "                    correct += 1\r\n",
    "            \r\n",
    "            #每训练了200批次的数据，打印下当前Loss的情况\r\n",
    "            if batch_id % 200 == 0:\r\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}\".format(epoch_id, batch_id, avg_loss.numpy()))\r\n",
    "            \r\n",
    "            #后向传播，更新参数的过程\r\n",
    "            avg_loss.backward()\r\n",
    "            optimizer.minimize(avg_loss)\r\n",
    "            model.clear_gradients()\r\n",
    "        print(correct/total)\r\n",
    "\r\n",
    "    #保存模型参数\r\n",
    "    paddle.save(model.state_dict(), 'mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mnist dataset from ./work/mnist.json.gz ......\n",
      "0.9818\n"
     ]
    }
   ],
   "source": [
    "valid_loader = load_data('eval')\r\n",
    "\r\n",
    "correct = 0\r\n",
    "total = 0\r\n",
    "for batch_id, data in enumerate(valid_loader()):\r\n",
    "     #准备数据，变得更加简洁\r\n",
    "    image_data, label_data = data\r\n",
    "    image = fluid.dygraph.to_variable(image_data)\r\n",
    "    label = fluid.dygraph.to_variable(label_data)\r\n",
    "               \r\n",
    "    predict = model(image)\r\n",
    "                        \r\n",
    "    total += label.shape[0]\r\n",
    "    pred = predict.argmax(1)\r\n",
    "    for i in range(len(pred)):\r\n",
    "        if(pred[i] == label[i]):\r\n",
    "            correct += 1   \r\n",
    "print(correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
