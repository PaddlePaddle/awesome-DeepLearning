# 损失函数

**1.1 回归问题**

回归问题中有众多损失函数，而目前机器学习主流的损失函数还是均方误差函数，平均绝对误差也使用较多，在此基础上发展出了很多其他函数

- **平均绝对误差――L1损失函数**

平均绝对误差（MAE）是另一种常用的回归损失函数，它是目标值与预测值之差绝对值的和，表示了预测值的平均误差幅度，而不需要考虑误差的方向，范围是0到∞，其公式如下所示：

![](https://ai-studio-static-online.cdn.bcebos.com/5890992d2a744b6889663cda2b656b9eed4715d09bab45e296b698df648cbe22)

**python版本**

```
import numpy as np
#定义L1损失函数
def L1_loss(y_true,y_pre): 
    return np.sum(np.abs(y_true-y_pre))
print('L1 loss is {}'.format(L1_loss(y_true,y_pre)))
```

- **均方误差――L2损失函数**

均方误差（MSE）是回归损失函数中最常用的误差，它是预测值与目标值之间差值的平方和，其公式如下所示：

![](https://ai-studio-static-online.cdn.bcebos.com/1ce4b99335114e55844d714b350303e9ae607380227744d8a62ea5983ccce3dc)

**python版本**
```
import numpy as np
def L2_loss(y_true,y_pre):
    return np.sum(np.square(y_true-y_pre))
print('L2 loss is {}'.format(L2_loss(y_true,y_pre)))
```

- **Huber损失――平滑平均绝对误差**

Huber损失相比于平方损失来说对于异常值不敏感，但它同样保持了可微的特性。它基于绝对误差，但在误差很小的时候变成了平方误差。我们可以使用超参数δ来调节这一误差的阈值。当δ趋向于0时它就退化成了MAE，而当δ趋向于无穷时则退化为了MSE，其表达式如下，是一个连续可微的分段函数：

![](https://ai-studio-static-online.cdn.bcebos.com/b3b532c55f424babb8bb9f9010b894d49f0327d0899d43588ecd1a53266acbf9)

**python版本**

```
def Huber(true, pred, delta):
    loss = np.where(np.abs(true-pred) < delta , 0.5*((true-pred)**2), delta*np.abs(true - pred) - 0.5*(delta**2))
    return np.sum(loss)
```

**1.2 分类问题**

分类问题相对回归问题更为具体，目标量只存在于一个有限集合，并且是离散的。分类问题往往比回归问题多出了一步，用于判断类别。回归问题的损失函数就是性能度量函数，而分类问题的损失函数不能直接用于性能度量，其最终评估的标准不是离目标的距离，而是类别判断的准确率。为了最大地提升类别判断准确率，我们需要为分类问题定义不同的损失函数。

- **0-1损失函数**

以二分类问题为例，错误率=1-正确率，也就是0-1损失函数，可以定义为：

![](https://ai-studio-static-online.cdn.bcebos.com/b05c2495c5bd4563808b27f8985e7d3a55a7084f7f804dabbb61df2b7042de33)

- **交叉熵损失函数（Logistic回归）**

Logistic回归主要用于二分类问题，包含激活函数和损失函数两部分。激活函数是logistic函数（又叫sigmoid函数），损失函数是交叉熵函数。

logistic函数作用在分类算法的最后，它使得对任意的输入值，对应的输出都在区间(0,1)内，可以将输入从实数域映射到(0,1)区间，刚好满足概率的范围。函数公式为：

![](https://ai-studio-static-online.cdn.bcebos.com/f9773b379dad4a609d377b286144ab47141fcdb689c7429098f8f0eceb162b71)

结合交叉熵函数，通过推导可以得到logistic的损失函数表达式：

![](https://ai-studio-static-online.cdn.bcebos.com/c32445d264db49f385858d79e6807566f64db6415d2e44c48e8465a3a8bc0f69)

**python版本**
```
import numpy as np
def cross_entropy(a, y):
    return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))
```
    
- **交叉熵损失函数（Softmax激活）**

交叉熵用到了log函数的特性，它不改变凸函数的凹凸性，同时能简化计算，是常用的技巧。这里主要讲多分类问题，激活函数是softmax，作用在神经网络的最后一层，损失函数是对数似然函数，也可以看成是交叉熵损失函数。

softmax激活函数表达式为：

![](https://ai-studio-static-online.cdn.bcebos.com/9757ecc51dde4a8d927cf4e6672a2ec66f9b8d41e21d4b3eb82e5b8e0a34e3f1)

当类别数C=2时，softmax函数就等于sigmoid函数，可见两者只是分量个数的差异。激活后的是长度为C的向量，每个分量都在（0，1）区间内，并且和为1，而最大分量所对应的类别就是我们所预测的结果。

**python版本**
```
def softmax(x):
    shift_x = x - np.max(x)    # 防止输入增大时输出为nan
    exp_x = np.exp(shift_x)
    return exp_x / np.sum(exp_x)
```

# 池化方法

池化（Pooling）是卷积神经网络中的一个重要的概念，它实际上是一种形式的降采样。在图像处理中，由于图像中存在较多冗余信息，可用某一区域子块的统计信息（如最大值或均值等）来刻画该区域中所有像素点呈现的空间分布模式，以替代区域子块中所有像素点取值，这就是卷积神经网络中池化(pooling)操作。

- **一般池化（General Pooling）**

池化作用于图像中不重合的区域（这与卷积操作不同），过程如下图。

![](https://ai-studio-static-online.cdn.bcebos.com/1c61737a647943348d40c69869933753ac60176efe2045e9a22417a3f61af493)

我们定义池化窗口的大小为sizeX，即下图中红色正方形的边长，定义两个相邻池化窗口的水平位移/竖直位移为stride。一般池化由于每一池化窗口都是不重复的，所以sizeX=stride。

最常见的池化操作为平均池化mean pooling和最大池化max pooling：

平均池化：计算图像区域的平均值作为该区域池化后的值。

最大池化：选图像区域的最大值作为该区域池化后的值。

- **重叠池化（OverlappingPooling）**

重叠池化，即相邻池化窗口之间会有重叠区域。如果定义池化窗口的大小为sizeX，定义两个相邻池化窗口的水平位移 / 竖直位移为stride，此时sizeX>stride。

- **空间金字塔池化（Spatial Pyramid Pooling）**

空间金字塔池化可以把任何尺度的图像的卷积特征转化成相同维度，这不仅可以让CNN处理任意尺度的图像，还能避免cropping和warping操作，导致一些信息的丢失，具有非常重要的意义。

一般的CNN都需要输入图像的大小是固定的，这是因为全连接层的输入需要固定输入维度，但在卷积操作是没有对图像尺度有限制，所有作者提出了空间金字塔池化，先让图像进行卷积操作，然后转化成维度相同的特征输入到全连接层，这个可以把CNN扩展到任意大小的图像。

![](https://ai-studio-static-online.cdn.bcebos.com/4271e7cd6775453a86a326deec24ab41fb6aad0d6cb94a2f997db93dde869ac3)

SPP其实就是一种多个scale的pooling，可以获取图像中的多尺度信息；在CNN中加入SPP后，可以让CNN处理任意大小的输入，这让模型变得更加的flexible。

- **全局池化(Global Pooling)**

Global Pooling就是池化窗口的大小 = 整张特征图的大小。这样，每个 W×H×C 的特征图输入就会被转化为 1×1×C 的输出，也等同于每个位置权重都为 1/(W×H) 的全连接层操作。

- **随机池化(Stochastic Pooling)**

随机池化是一种简单有效的正则化CNN的方法，能够降低max pooling的过拟合现象，提高泛化能力。对于pooling层的输入，根据输入的多项式分布随机选择一个值作为输出。

随机池化可以看作在一个池化窗口内对特征图数值进行归一化， 按照特征图归一化后的概率值大小随机采样选择，即元素值大的被选中的概率也大。

# 数据增强方法

数据增强也叫数据扩增，意思是在不实质性的增加数据的情况下，让有限的数据产生等价于更多数据的价值。数据增强可以分为，有监督的数据增强和无监督的数据增强方法。其中有监督的数据增强又可以分为单样本数据增强和多样本数据增强方法，无监督的数据增强分为生成新的数据和学习增强策略两个方向。
**3.1 有监督的数据增强**

有监督数据增强，即采用预设的数据变换规则，在已有数据的基础上进行数据的扩增，包含单样本数据增强和多样本数据增强，其中单样本又包括几何操作类，颜色变换类。

- **单样本数据增强**

所谓单样本数据增强，即增强一个样本的时候，全部围绕着该样本本身进行操作，包括几何变换类，颜色变换类等。

几何变换类：对图像进行几何变换，包括翻转，旋转，裁剪，变形，缩放等各类操作

颜色变换类：对图像内容进行改变，常见的包括噪声、模糊、颜色变换、擦除、填充等。

- **多样本数据增强**

不同于单样本数据增强，多样本数据增强方法利用多个样本来产生新的样本，下面介绍几种方法。

**SMOTE**

SMOTE即Synthetic Minority Over-sampling Technique方法，它是通过人工合成新样本来处理样本不平衡问题，从而提升分类器性能。

SMOTE方法是基于插值的方法，它可以为小样本类合成新的样本，主要流程为：

第一步，定义好特征空间，将每个样本对应到特征空间中的某一点，根据样本不平衡比例确定好一个采样倍率N；

第二步，对每一个小样本类样本(x,y)，按欧氏距离找出K个最近邻样本，从中随机选取一个样本点，假设选择的近邻点为(xn,yn)。在特征空间中样本点与最近邻样本点的连线段上随机选取一点作为新样本点，满足以下公式：

![](https://ai-studio-static-online.cdn.bcebos.com/491656c1ac1e425eb32a7a2b3fa1884bafec94eb71304903a73e54f9e481ad4b)

第三步，重复以上的步骤，直到大、小样本数量平衡。

**SamplePairing**

SamplePairing方法是从训练集中随机抽取两张图片分别经过基础数据增强操作(如随机翻转等)处理后经像素以取平均值的形式叠加合成一个新的样本，标签为原样本标签中的一种。这两张图片甚至不限制为同一类别，这种方法对于医学图像比较有效。

![](https://ai-studio-static-online.cdn.bcebos.com/0d789c7ffe9d4498a4b15ae4d97857e969cfb16dbabb4b6f8d4a0004ec3b3c89)

**mixup**

mixup是Facebook人工智能研究院和MIT在“Beyond Empirical Risk Minimization”中提出的基于邻域风险最小化原则的数据增强方法，它使用线性插值得到新样本数据。令(xn,yn)是插值生成的新数据，(xi,yi)和(xj,yj)是训练集随机选取的两个数据，则数据生成方式如下：

![](https://ai-studio-static-online.cdn.bcebos.com/c63c204f309b4d32a7a2c9531867f0217a21325b8eba40e1802d939e3406c1b2)

**3.2 无监督的数据增强**

无监督的数据增强方法包括两类：

（1）通过模型学习数据的分布，随机生成与训练数据集分布一致的图片，代表方法GAN。

- **GAN**

GAN包含两个网络，一个是生成网络，一个是对抗网络，基本原理如下：

![](https://ai-studio-static-online.cdn.bcebos.com/ae9aa492bed842b1a4aa924e232c75a04284d39025fb4738aad41be9422e15db)

G是一个生成图片的网络，它接收随机的噪声z，通过噪声生成图片，记做G(z)；

D是一个判别网络，判别一张图片是不是“真实的”，即是真实的图片，还是由G生成的图片。

（2）通过模型，学习出适合当前任务的数据增强方法，代表方法AutoAugment。

- **Autoaugmentation**

AutoAugment是Google提出的自动选择最优数据增强方案的研究，这是无监督数据增强的重要研究方向。它的基本思路是使用增强学习从数据本身寻找最佳图像变换策略，对于不同的任务学习不同的增强方法，流程如下：

(1) 准备16个常用的数据增强操作。

(2) 从16个中选择5个操作，随机产生使用该操作的概率和相应的幅度，将其称为一个sub-policy，一共产生5个sub-polices。

(3) 对训练过程中每一个batch的图片，随机采用5个sub-polices操作中的一种。

(4) 通过模型在验证集上的泛化能力来反馈，使用的优化方法是增强学习方法。

(5) 经过80~100个epoch后网络开始学习到有效的sub-policies。

(6) 之后串接这5个sub-policies，然后再进行最后的训练。

# 图像分类方法综述

![](https://ai-studio-static-online.cdn.bcebos.com/d8113e86b9874104afba5fb3ecdfc34bd266a12415ea42b0b981e2ea58223e17)

图像分类是根据图像的语义信息将不同类别图像区分开来，是计算机视觉中重要的基本问题，也是图像检测、图像分割、物体跟踪、行为分析等其他高层视觉任务的基础。图像分类的主要过程包括图像预处理、特征提取和分类器设计。

- **传统方法**

传统图像分类通过手工提取特征或特征学习方法对整个图像进行全部描述，然后使用分类器判别物体类别，因此如何提取图像的特征至关重要。

![](https://ai-studio-static-online.cdn.bcebos.com/979b8bd4f3734f1c998722ba642b8c0a5098777b55814e269ab01c413ddfac45)

在特征提取方面，主要包括纹理、颜色、形状等底层视觉特征，尺度不变特征变换、局部二值模式、方向梯度直方图等局部不变性特征。

在分类器方面，主要包括k NN(k-nearest neighbor,k最近邻）决策树、SVM(support vector machine，支持向量机）、人工神经网络等方法。

1.基于色彩特征的索引技术

色彩是物体表面的一种视觉特性,每种物体都有其特有的色彩特征,譬如人们说到绿色往往是和树木或草原相关,谈到蓝色往往是和大海或蓝天相关,同一类物体往拍几有着相似的色彩特征,因此我们可以根据色彩特征来区分物体.用色彩特特征进行图像分类一可以追溯到Swain和Ballard提出的色彩直方图的方法.由于色彩直方图具有简单且随图像的大小、旋转变化不敏感等特点,得到了研究人员的厂泛关注,目前几乎所有基于内容分类的图像数据库系统都把色彩分类方法作为分类的一个重要手段,并提出了许多改进方法,归纳起主要可以分为两类：全局色彩特征索引和局部色彩特征索引。

2.基于纹理的图像分类技术

纹理特征也是图像的重要特征之一,其本质是刻画象素的邻域灰度空间分布规律由于它在模式识别和计算机视觉等领域已经取得了丰富的研究成果,因此可以借用到图像分类中。
在70年代早期,Haralick等人提出纹理特征的灰度共生矩阵表示法(eo一oeeurrenee matrix representation),这个方法提取的是纹理的灰度级空间相关性(gray level Spatial dependenee),它首先基于象素之间的距离和方向建立灰度共生矩阵,再由这个矩阵提取有意义的统计量作为纹理特征向量。基于一项人眼对纹理的视觉感知的心理研究,Tamuar等人提出可以模拟纹理视觉模型的6个纹理属性,分别是粒度,对比度,方向性,线型,均匀性和粗糙度。QBIC系统和MARS系统就采用的是这种纹理表示方法。
在90年代初期,当小波变换的理论结构建一认起来之后,许多研究者开始研究
如何用小波变换表示纹理特征。smiht和chang利用从小波子带中提取的统计量(平均值和方差)作为纹理特征。这个算法在112幅Brodatz纹理图像中达到了90%的准确率。为了利用中间带的特征,Chang和Kuo开发出一种树型结构的小波变化来进一步提高分类的准确性。还有一些研究者将小波变换和其他的变换结合起来以得到更好的性能,如Thygaarajna等人结合小波变换和共生矩阵,以兼顾基于统计的和基于变换的纹理分析算法的优点。

3.基于形状的图像分类技术

形状是图像的重要可视化内容之一在二维图像空间中,形状通常被认为是一条封闭的轮廓曲线所包围的区域,所以对形状的描述涉及到对轮廓边界的描述以及对这个边界所包围区域的描述.目前的基于形状分类方法大多围绕着从形状的轮廓特征和形状的区域特征建立图像索引。关于对形状轮廓特征的描述主要有:直线段描述、样条拟合曲线、傅立叶描述子以及高斯参数曲线等等。
实际上更常用的办法是采用区域特征和边界特征相结合来进行形状的相似分类.如Eakins等人提出了一组重画规则并对形状轮廓用线段和圆弧进行简化表达,然后定义形状的邻接族和形族两种分族函数对形状进行分类.邻接分族主要采用了形状的边界信息,而形状形族主要采用了形状区域信息.在形状进行匹配时,除了每个族中形状差异外,还比较每个族中质心和周长的差异,以及整个形状的位置特征矢量的差异,查询判别距离是这些差异的加权和。

4.基于空间关系的图像分类技术

在图像信息系统中,依据图像中对象及对象间的空间位置关系来区别图像库中的不同图像是一个非常重要的方法。因此,如何存贮图像对象及其中对象位置关系以方便图像的分类,是图像数据库系统设计的一个重要问题。而且利用图像中对象间的空间关系来区别图像,符合人们识别图像的习惯,所以许多研究人员从图像中对象空间位置关系出发,着手对基于对象空间位置关系的分类方法进行了研究。早在1976年,Tanimoto提出了用像元方法来表示图像中的实体,并提出了用像元来作为图像对象索引。随后被美国匹兹堡大学chang采纳并提出用二维符号串(2D一String)的表示方法来进行图像空间关系的分类,由于该方法简单,并且对于部分图像来说可以从ZD一String重构它们的符号图,因此被许多人采用和改进,该方法的缺点是仅用对象的质心表示空间位置;其次是对于一些图像来
说我们不能根据其ZD一string完个重构其符号图;再则是上述的空间关系太简单,实际中的空间关系要复杂得多。,针对这些问题许多人提出了改进力一法。Jungert根据图像对象的最小包围盒分别在:x轴方向和y轴上的投影区间之间的交叠关系来表示对象之间的空间关系,随后Cllallg和Jungert等人又提出了广义ZD一string(ZDG一String)的方法,将图像对象进一步切分为更小的子对象来表示对象的空间关系,但是该方法不足之处是当图像对象数日比较多且空间关系比较复杂时,需要切分的子对象的数目很多,存储的开销太大,针对此Lee和Hsu等人提出了ZDC一string的方一法,它们采用Anell提出的13种时态间隔关系并应用到空间投影区问上来表达空间关系。在x轴方向和y轴方向的组合关系共有169种,他提出了5种基本关系转换法则,在此基础上又提出了新的对象切分方法。采用
ZDC一string的方法比ZDG一string切分子对象的数目明显减少。为了在空间关系中保留两个对象的相对空间距离和对象的大小,Huang等人提出了ZDC书string的方法提高符号图的重构精度,并使得对包含对象相对大小、距离的符号图的推理成为可能。上述方法都涉及到将图像对象进行划分为子对象,且在用符号串重构对象时处理时间的开销都比较大,为解决这些方法的不足,Lee等人又提出了ZDB一String的方法,它不要求对象进一步划分,用对象的名称来表示对象的起点和终点边界。为了解决符号图的重构问题,Chin一ChenCllang等人提出了面向相对坐标解决符号图的重构问题,Chin一ChenChang等人提出了面向相对坐标符号串表示(RCOS串),它们用对象最小外接包围盒的左下角坐标和右上角坐标来表示对象之间的空间关系.
对于对象之间的空间关系采用Allen提出的13种区间表示方法。实际上上述所有方法都不是和对象的方位无关,为此Huang等人又提出了RSString表示方法。虽然上述各种方法在对图像对象空间信息的分类起到过一定作用,由于它们都是采用对象的最小外接矩形来表示一个对象空间位置,这对于矩形对象来说是比较合适的,但是当两个对象是不规则形状,且它们在空间关系上是分离时,它们的外接矩形却存在着某种包含和交叠,结果出现对这些对象空间关系的错误表示。用上述空间关系进行图像分类都是定性的分类方一法,将图像的空间关系转换为图像相似性的定量度量是一个较为困难的事情。Nabil综合ZD一String方法和二维平面中对象之间的点集拓扑关系。提出了ZD一PIR分类方法,两个对象之间的相似与否就转换为两个图像的ZD一PIR图之间是否同构。ZD一PIR中只有图像对象之间的空间拓扑关系具有旋转不变性,在进行图像分类的时候没有考虑对象之间的相对距离。


- **深度学习方法**

传统的图像分类方法往往需要对目标图像进行人工特征描述和提取，对于大数量的复杂数据很难取得低成本的有效结果。然而，深度学习方法通过神经网络自主地从训练样本中学习特征，提取出更高维、抽象的特征，并且这些特征与分类器关系紧密，很好地解决了人工提取特征和分类器选择的难题，是一种端到端的模型。

对于深度学习标准网络模型使用已经非常广泛，在这里对轻量化进行简单介绍：

常用的标准网络模型：Lenet、Alxnet、Vgg系列、Resnet系列、Inception系列、Densenet系列、Googlenet、Nasnet、Xception、Senet(state of art)；

轻量化网络模型：Mobilenet v1,v2、Shufflenet v1,v2,Squeezenet

目前轻量化模型在具体项目应用时反响很好，它主要存在的优缺点如下：

优点：（1）参数模型小，方便部署（2）计算量小，速度快

缺点：（1）轻量化模型在精度上没有Resnet系列、Inception系列、Densenet系列、Senet的accuracy高