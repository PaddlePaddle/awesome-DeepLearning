{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data10595  data65  log\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/external-libraries’: File exists\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 14.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/bs4 already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/beautifulsoup4-4.9.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve-2.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-27-ad2c4c8814b0>, line 85)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-ad2c4c8814b0>\"\u001b[0;36m, line \u001b[0;32m85\u001b[0m\n\u001b[0;31m    datafile = './work/mnist.json.gz'\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "from paddle.nn import Linear\r\n",
    "import paddle.nn.functional as F\r\n",
    "import os\r\n",
    "import gzip\r\n",
    "import json\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "# 声明数据集文件位置\r\n",
    "# 声明数据集文件位置\r\n",
    "datafile = '/home/aistudio/data/data65/t10k-images-idx3-ubyte.gz'\r\n",
    "print('loading mnist dataset from {} ......'.format(datafile))\r\n",
    "# 加载json数据文件\r\n",
    "data = json.load(gzip.open(datafile))\r\n",
    "print('mnist dataset load done')\r\n",
    "# 读取到的数据区分训练集，验证集，测试集\r\n",
    "train_set, val_set, eval_set = data\r\n",
    "\r\n",
    "# 观察训练集数据\r\n",
    "imgs, labels = train_set[0], train_set[1]\r\n",
    "print(\"训练数据集数量: \", len(imgs))\r\n",
    "\r\n",
    "# 观察验证集数量\r\n",
    "imgs, labels = val_set[0], val_set[1]\r\n",
    "print(\"验证数据集数量: \", len(imgs))\r\n",
    "\r\n",
    "# 观察测试集数量\r\n",
    "imgs, labels = val= eval_set[0], eval_set[1]\r\n",
    "print(\"测试数据集数量: \", len(imgs))\r\n",
    "imgs, labels = train_set[0], train_set[1]\r\n",
    "print(\"训练数据集数量: \", len(imgs))\r\n",
    "# 获得数据集长度\r\n",
    "imgs_length = len(imgs)\r\n",
    "# 定义数据集每个数据的序号，根据序号读取数据\r\n",
    "index_list = list(range(imgs_length))\r\n",
    "# 读入数据时用到的批次大小\r\n",
    "BATCHSIZE = 100\r\n",
    "\r\n",
    "# 随机打乱训练数据的索引序号\r\n",
    "random.shuffle(index_list)\r\n",
    "\r\n",
    "# 定义数据生成器，返回批次数据\r\n",
    "def data_generator():\r\n",
    "    imgs_list = []\r\n",
    "    labels_list = []\r\n",
    "    for i in index_list:\r\n",
    "        # 将数据处理成希望的类型\r\n",
    "        img = np.array(imgs[i]).astype('float32')\r\n",
    "        label = np.array(labels[i]).astype('float32')\r\n",
    "        imgs_list.append(img) \r\n",
    "        labels_list.append(label)\r\n",
    "        if len(imgs_list) == BATCHSIZE:\r\n",
    "            # 获得一个batchsize的数据，并返回\r\n",
    "            yield np.array(imgs_list), np.array(labels_list)\r\n",
    "            # 清空数据读取列表\r\n",
    "            imgs_list = []\r\n",
    "            labels_list = []\r\n",
    "\r\n",
    "    # 如果剩余数据的数目小于BATCHSIZE，\r\n",
    "    # 则剩余数据一起构成一个大小为len(imgs_list)的mini-batch\r\n",
    "    if len(imgs_list) > 0:\r\n",
    "        yield np.array(imgs_list), np.array(labels_list)\r\n",
    "    return data_generator\r\n",
    "    # 声明数据读取函数，从训练集中读取数据\r\n",
    "train_loader = data_generator\r\n",
    "# 以迭代的形式读取数据\r\n",
    "for batch_id, data in enumerate(train_loader()):\r\n",
    "    image_data, label_data = data\r\n",
    "    if batch_id == 0:\r\n",
    "        # 打印数据shape和类型\r\n",
    "        print(\"打印第一个batch数据的维度:\")\r\n",
    "        print(\"图像维度: {}, 标签维度: {}\".format(image_data.shape, label_data.shape))\r\n",
    "    break\r\n",
    "    # 声明数据读取函数，从训练集中读取数据\r\n",
    "train_loader = data_generator\r\n",
    "# 以迭代的形式读取数据\r\n",
    "for batch_id, data in enumerate(train_loader()):\r\n",
    "    image_data, label_data = data\r\n",
    "    if batch_id == 0:\r\n",
    "        # 打印数据shape和类型\r\n",
    "        print(\"打印第一个batch数据的维度，以及数据的类型:\")\r\n",
    "        print(\"图像维度: {}, 标签维度: {}, 图像数据类型: {}, 标签数据类型: {}\".format(image_data.shape, label_data.shape, type(image_data), type(label_data)))\r\n",
    "    break\r\n",
    "    def load_data(mode='train'):\r\n",
    "    datafile = './work/mnist.json.gz'\r\n",
    "    print('loading mnist dataset from {} ......'.format(datafile))\r\n",
    "    # 加载json数据文件\r\n",
    "    data = json.load(gzip.open(datafile))\r\n",
    "    print('mnist dataset load done')\r\n",
    "   \r\n",
    "    # 读取到的数据区分训练集，验证集，测试集\r\n",
    "    train_set, val_set, eval_set = data\r\n",
    "    if mode=='train':\r\n",
    "        # 获得训练数据集\r\n",
    "        imgs, labels = train_set[0], train_set[1]\r\n",
    "    elif mode=='valid':\r\n",
    "        # 获得验证数据集\r\n",
    "        imgs, labels = val_set[0], val_set[1]\r\n",
    "    elif mode=='eval':\r\n",
    "        # 获得测试数据集\r\n",
    "        imgs, labels = eval_set[0], eval_set[1]\r\n",
    "    else:\r\n",
    "        raise Exception(\"mode can only be one of ['train', 'valid', 'eval']\")\r\n",
    "    print(\"训练数据集数量: \", len(imgs))\r\n",
    "    \r\n",
    "    # 校验数据\r\n",
    "    imgs_length = len(imgs)\r\n",
    "\r\n",
    "    assert len(imgs) == len(labels), \\\r\n",
    "          \"length of train_imgs({}) should be the same as train_labels({})\".format(len(imgs), len(labels))\r\n",
    "    \r\n",
    "    # 获得数据集长度\r\n",
    "    imgs_length = len(imgs)\r\n",
    "    \r\n",
    "    # 定义数据集每个数据的序号，根据序号读取数据\r\n",
    "    index_list = list(range(imgs_length))\r\n",
    "    # 读入数据时用到的批次大小\r\n",
    "    BATCHSIZE = 100\r\n",
    "    \r\n",
    "    # 定义数据生成器\r\n",
    "    def data_generator():\r\n",
    "        if mode == 'train':\r\n",
    "            # 训练模式下打乱数据\r\n",
    "            random.shuffle(index_list)\r\n",
    "        imgs_list = []\r\n",
    "        labels_list = []\r\n",
    "        for i in index_list:\r\n",
    "            # 将数据处理成希望的类型\r\n",
    "            img = np.array(imgs[i]).astype('float32')\r\n",
    "            label = np.array(labels[i]).astype('float32')\r\n",
    "            imgs_list.append(img) \r\n",
    "            labels_list.append(label)\r\n",
    "            if len(imgs_list) == BATCHSIZE:\r\n",
    "                # 获得一个batchsize的数据，并返回\r\n",
    "                yield np.array(imgs_list), np.array(labels_list)\r\n",
    "                # 清空数据读取列表\r\n",
    "                imgs_list = []\r\n",
    "                labels_list = []\r\n",
    "    \r\n",
    "        # 如果剩余数据的数目小于BATCHSIZE，\r\n",
    "        # 则剩余数据一起构成一个大小为len(imgs_list)的mini-batch\r\n",
    "        if len(imgs_list) > 0:\r\n",
    "            yield np.array(imgs_list), np.array(labels_list)\r\n",
    "    return data_generator\r\n",
    "    #数据处理部分之后的代码，数据读取的部分调用Load_data函数\r\n",
    "# 定义网络结构，同上一节所使用的网络结构\r\n",
    "class MNIST(paddle.nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(MNIST, self).__init__()\r\n",
    "        # 定义一层全连接层，输出维度是1\r\n",
    "        self.fc = paddle.nn.Linear(in_features=784, out_features=1)\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        outputs = self.fc(inputs)\r\n",
    "        return outputs\r\n",
    "\r\n",
    "# 训练配置，并启动训练过程\r\n",
    "def train(model):\r\n",
    "    model = MNIST()\r\n",
    "    model.train()\r\n",
    "    #调用加载数据的函数\r\n",
    "    train_loader = load_data('train')\r\n",
    "    opt = paddle.optimizer.SGD(learning_rate=0.001, parameters=model.parameters())\r\n",
    "    EPOCH_NUM = 10\r\n",
    "    for epoch_id in range(EPOCH_NUM):\r\n",
    "        for batch_id, data in enumerate(train_loader()):\r\n",
    "            #准备数据，变得更加简洁\r\n",
    "            images, labels = data\r\n",
    "            images = paddle.to_tensor(images)\r\n",
    "            labels = paddle.to_tensor(labels) \r\n",
    "\r\n",
    "            #前向计算的过程\r\n",
    "            predits = model(images)\r\n",
    "            \r\n",
    "            #计算损失，取一个批次样本损失的平均值\r\n",
    "            loss = F.square_error_cost(predits, labels)\r\n",
    "            avg_loss = paddle.mean(loss)      \r\n",
    "            \r\n",
    "            #每训练了200批次的数据，打印下当前Loss的情况\r\n",
    "            if batch_id % 200 == 0:\r\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}\".format(epoch_id, batch_id, avg_loss.numpy()))\r\n",
    "            \r\n",
    "            #后向传播，更新参数的过程\r\n",
    "            avg_loss.backward()\r\n",
    "            opt.step()\r\n",
    "            opt.clear_grad()\r\n",
    "\r\n",
    "    # 保存模型\r\n",
    "    paddle.save(model.state_dict(), './mnist.pdparams')\r\n",
    "# 创建模型           \r\n",
    "model = MNIST()\r\n",
    "# 启动训练过程\r\n",
    "train(model)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "from paddle.io import Dataset\r\n",
    "# 构建一个类，继承paddle.io.Dataset，创建数据读取器\r\n",
    "class RandomDataset(Dataset):\r\n",
    "    def __init__(self, num_samples):\r\n",
    "        # 样本数量\r\n",
    "        self.num_samples = num_samples\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        # 随机产生数据和label\r\n",
    "        image = np.random.random([784]).astype('float32')\r\n",
    "        label = np.random.randint(0, 9, (1, )).astype('float32')\r\n",
    "        return image, label\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        # 返回样本总数量\r\n",
    "        return self.num_samples\r\n",
    "        \r\n",
    "# 测试数据读取器\r\n",
    "dataset = RandomDataset(10)\r\n",
    "for i in range(len(dataset)):\r\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loader = paddle.io.DataLoader(dataset, batch_size=3, shuffle=True, drop_last=True, num_workers=2)\r\n",
    "for i, data in enumerate(loader()):\r\n",
    "    images, labels = data[0], data[1]\r\n",
    "    print(\"batch_id: {}, 训练数据shape: {}, 标签数据shape: {}\".format(i, images.shape, labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "import json\r\n",
    "import gzip\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# 创建一个类MnistDataset，继承paddle.io.Dataset 这个类\r\n",
    "# MnistDataset的作用和上面load_data()函数的作用相同，均是构建一个迭代器\r\n",
    "class MnistDataset(paddle.io.Dataset):\r\n",
    "    def __init__(self, mode):\r\n",
    "        datafile = './work/mnist.json.gz'\r\n",
    "        data = json.load(gzip.open(datafile))\r\n",
    "        # 读取到的数据区分训练集，验证集，测试集\r\n",
    "        train_set, val_set, eval_set = data\r\n",
    "        if mode=='train':\r\n",
    "            # 获得训练数据集\r\n",
    "            imgs, labels = train_set[0], train_set[1]\r\n",
    "        elif mode=='valid':\r\n",
    "            # 获得验证数据集\r\n",
    "            imgs, labels = val_set[0], val_set[1]\r\n",
    "        elif mode=='eval':\r\n",
    "            # 获得测试数据集\r\n",
    "            imgs, labels = eval_set[0], eval_set[1]\r\n",
    "        else:\r\n",
    "            raise Exception(\"mode can only be one of ['train', 'valid', 'eval']\")\r\n",
    "        \r\n",
    "        # 校验数据\r\n",
    "        imgs_length = len(imgs)\r\n",
    "        assert len(imgs) == len(labels), \\\r\n",
    "            \"length of train_imgs({}) should be the same as train_labels({})\".format(len(imgs), len(labels))\r\n",
    "        \r\n",
    "        self.imgs = imgs\r\n",
    "        self.labels = labels\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        img = np.array(self.imgs[idx]).astype('float32')\r\n",
    "        label = np.array(self.labels[idx]).astype('float32')\r\n",
    "        \r\n",
    "        return img, label\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.imgs)\r\n",
    "        # 声明数据加载函数，使用训练模式，MnistDataset构建的迭代器每次迭代只返回batch=1的数据\r\n",
    "train_dataset = MnistDataset(mode='train')\r\n",
    "# 使用paddle.io.DataLoader 定义DataLoader对象用于加载Python生成器产生的数据，\r\n",
    "# DataLoader 返回的是一个批次数据迭代器，并且是异步的；\r\n",
    "data_loader = paddle.io.DataLoader(train_dataset, batch_size=100, shuffle=True)\r\n",
    "# 迭代的读取数据并打印数据的形状\r\n",
    "for i, data in enumerate(data_loader()):\r\n",
    "    images, labels = data\r\n",
    "    print(i, images.shape, labels.shape)\r\n",
    "    if i>=2:\r\n",
    "        break\r\n",
    "        def train(model):\r\n",
    "    model = MNIST()\r\n",
    "    model.train()\r\n",
    "    opt = paddle.optimizer.SGD(learning_rate=0.001, parameters=model.parameters())\r\n",
    "    EPOCH_NUM = 3\r\n",
    "    for epoch_id in range(EPOCH_NUM):\r\n",
    "        for batch_id, data in enumerate(data_loader()):\r\n",
    "            images, labels = data\r\n",
    "            images = paddle.to_tensor(images)\r\n",
    "            labels = paddle.to_tensor(labels).astype('float32')\r\n",
    "            \r\n",
    "            #前向计算的过程  \r\n",
    "            predicts = model(images)\r\n",
    "\r\n",
    "            #计算损失，取一个批次样本损失的平均值\r\n",
    "            loss = F.square_error_cost(predicts, labels)\r\n",
    "            avg_loss = paddle.mean(loss)       \r\n",
    "            \r\n",
    "            #每训练了200批次的数据，打印下当前Loss的情况\r\n",
    "            if batch_id % 200 == 0:\r\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}\".format(epoch_id, batch_id, avg_loss.numpy()))\r\n",
    "            \r\n",
    "            #后向传播，更新参数的过程\r\n",
    "            avg_loss.backward()\r\n",
    "            opt.step()\r\n",
    "            opt.clear_grad()\r\n",
    "\r\n",
    "    #保存模型参数\r\n",
    "    paddle.save(model.state_dict(), 'mnist')\r\n",
    "\r\n",
    "#创建模型\r\n",
    "model = MNIST()\r\n",
    "#启动训练过程\r\n",
    "train(model)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import random\r\n",
    "import paddle\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from PIL import Image\r\n",
    "\r\n",
    "import gzip\r\n",
    "import json\r\n",
    "\r\n",
    "# 定义数据集读取器\r\n",
    "def load_data(mode='train'):\r\n",
    "\r\n",
    "    # 加载数据\r\n",
    "    datafile = './work/mnist.json.gz'\r\n",
    "    print('loading mnist dataset from {} ......'.format(datafile))\r\n",
    "    data = json.load(gzip.open(datafile))\r\n",
    "    print('mnist dataset load done')\r\n",
    "\r\n",
    "    # 读取到的数据区分训练集，验证集，测试集\r\n",
    "    train_set, val_set, eval_set = data\r\n",
    "\r\n",
    "    # 数据集相关参数，图片高度IMG_ROWS, 图片宽度IMG_COLS\r\n",
    "    # IMG_ROWS = 28\r\n",
    "    # IMG_COLS = 28\r\n",
    "\r\n",
    "    if mode == 'train':\r\n",
    "        # 获得训练数据集\r\n",
    "        imgs, labels = train_set[0], train_set[1]\r\n",
    "    elif mode == 'valid':\r\n",
    "        # 获得验证数据集\r\n",
    "        imgs, labels = val_set[0], val_set[1]\r\n",
    "    elif mode == 'eval':\r\n",
    "        # 获得测试数据集\r\n",
    "        imgs, labels = eval_set[0], eval_set[1]\r\n",
    "    else:\r\n",
    "        raise Exception(\"mode can only be one of ['train', 'valid', 'eval']\")\r\n",
    "\r\n",
    "    #校验数据\r\n",
    "    imgs_length = len(imgs)\r\n",
    "    assert len(imgs) == len(labels), \\\r\n",
    "          \"length of train_imgs({}) should be the same as train_labels({})\".format(\r\n",
    "                  len(imgs), len(labels))\r\n",
    "\r\n",
    "    # 定义数据集每个数据的序号， 根据序号读取数据\r\n",
    "    index_list = list(range(imgs_length))\r\n",
    "    # 读入数据时用到的batchsize\r\n",
    "    BATCHSIZE = 100\r\n",
    "\r\n",
    "    # 定义数据生成器\r\n",
    "    def data_generator():\r\n",
    "        if mode == 'train':\r\n",
    "            random.shuffle(index_list)\r\n",
    "        imgs_list = []\r\n",
    "        labels_list = []\r\n",
    "        for i in index_list:\r\n",
    "            img = np.array(imgs[i]).astype('float32')\r\n",
    "            label = np.array(labels[i]).astype('float32')\r\n",
    "            # img = np.reshape(imgs[i], [1, IMG_ROWS, IMG_COLS]).astype('float32')\r\n",
    "            # label = np.reshape(labels[i], [1]).astype('float32')\r\n",
    "            imgs_list.append(img) \r\n",
    "            labels_list.append(label)\r\n",
    "            if len(imgs_list) == BATCHSIZE:\r\n",
    "                yield np.array(imgs_list), np.array(labels_list)\r\n",
    "                imgs_list = []\r\n",
    "                labels_list = []\r\n",
    "\r\n",
    "        # 如果剩余数据的数目小于BATCHSIZE，\r\n",
    "        # 则剩余数据一起构成一个大小为len(imgs_list)的mini-batch\r\n",
    "        if len(imgs_list) > 0:\r\n",
    "            yield np.array(imgs_list), np.array(labels_list)\r\n",
    "\r\n",
    "    return data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\r\n",
    "    # 直接返回sigmoid函数\r\n",
    "    return 1. / (1. + np.exp(-x))\r\n",
    " \r\n",
    "# param:起点，终点，间距\r\n",
    "x = np.arange(-8, 8, 0.2)\r\n",
    "y = sigmoid(x)\r\n",
    "plt.plot(x, y)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn.functional as F\r\n",
    "from paddle.nn import Linear\r\n",
    "\r\n",
    "# 定义多层全连接神经网络\r\n",
    "class MNIST(paddle.nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(MNIST, self).__init__()\r\n",
    "        # 定义两层全连接隐含层，输出维度是10，当前设定隐含节点数为10，可根据任务调整\r\n",
    "        self.fc1 = Linear(in_features=784, out_features=10)\r\n",
    "        self.fc2 = Linear(in_features=10, out_features=10)\r\n",
    "        # 定义一层全连接输出层，输出维度是1\r\n",
    "        self.fc3 = Linear(in_features=10, out_features=1)\r\n",
    "    \r\n",
    "    # 定义网络的前向计算，隐含层激活函数为sigmoid，输出层不使用激活函数\r\n",
    "    def forward(self, inputs):\r\n",
    "        # inputs = paddle.reshape(inputs, [inputs.shape[0], 784])\r\n",
    "        outputs1 = self.fc1(inputs)\r\n",
    "        outputs1 = F.sigmoid(outputs1)\r\n",
    "        outputs2 = self.fc2(outputs1)\r\n",
    "        outputs2 = F.sigmoid(outputs2)\r\n",
    "        outputs_final = self.fc3(outputs2)\r\n",
    "        return outputs_final\r\n",
    "#网络结构部分之后的代码，保持不变\r\n",
    "def train(model):\r\n",
    "    model.train()\r\n",
    "    #调用加载数据的函数，获得MNIST训练数据集\r\n",
    "    train_loader = load_data('train')\r\n",
    "    # 使用SGD优化器，learning_rate设置为0.01\r\n",
    "    opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())\r\n",
    "    # 训练5轮\r\n",
    "    EPOCH_NUM = 5\r\n",
    "    for epoch_id in range(EPOCH_NUM):\r\n",
    "        for batch_id, data in enumerate(train_loader()):\r\n",
    "            #准备数据\r\n",
    "            images, labels = data\r\n",
    "            images = paddle.to_tensor(images)\r\n",
    "            labels = paddle.to_tensor(labels)\r\n",
    "            \r\n",
    "            #前向计算的过程\r\n",
    "            predicts = model(images)\r\n",
    "            \r\n",
    "            #计算损失，取一个批次样本损失的平均值\r\n",
    "            loss = F.square_error_cost(predicts, labels)\r\n",
    "            avg_loss = paddle.mean(loss)\r\n",
    "\r\n",
    "            #每训练200批次的数据，打印下当前Loss的情况\r\n",
    "            if batch_id % 200 == 0:\r\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}\".format(epoch_id, batch_id, avg_loss.numpy()))\r\n",
    "            \r\n",
    "            #后向传播，更新参数的过程\r\n",
    "            avg_loss.backward()\r\n",
    "            # 最小化loss,更新参数\r\n",
    "            opt.step()\r\n",
    "            # 清除梯度\r\n",
    "            opt.clear_grad()\r\n",
    "\r\n",
    "    #保存模型参数\r\n",
    "    paddle.save(model.state_dict(), 'mnist.pdparams')\r\n",
    "\r\n",
    "model = MNIST()\r\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
