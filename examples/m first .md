**层归一化：**
批量归一化是对一个中间层的单个神经元进行归一化操作，因此要求小批量样本的数量不能太小，否则难以计算单个神经元的统计信息。

Layer Normalizaiton

1.LN的计算就是把每个CHW单独拿出来归一化处理，不受batchsize 的影响

2.常用在RNN网络，但如果输入的特征区别很大，那么就不建议使用它做归一化处理

对于样本的一个小批量集合，层归一化是矩阵对每一列进行归一化，而批量归一化是对每一行进行归一化。一般而言，批归一化是一种更好的选择，当小批量样本数量比较小时，可以选择层归一化。
权重归一化 (Weight Normalization)是对神经网络的连接权重进行归一化，通过再参数化
(Reparameterization)方法，将连接权重分解为长度和方向两种参数。假设第N层神经元
，我们将再参数化为：表示权重的第行，为神经元数量，新引入的参数为标量，维数相同。由于在神经网络中权重经常是共享的，权重数量往往比神经元数量更少，因此权重归一化的开销比较小。通常用在基于卷积的图像处理上。假设一个卷积层的输出特征映射为三维向量，其中每个切片矩阵为一个输出特征映射。

实例归一化是组归一化的特例，其中组大小与通道大小（或轴大小）相同。 实验结果表明，当替换批次归一化时，实例归一化在样式迁移方面表现良好。 最近，实例归一化也已被用来代替 GAN 中的批次归一化。 在 Conv2D 层之后应用 InstanceNormalization 并使用统一的初始化比例和偏移因子。