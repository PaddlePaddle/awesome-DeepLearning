```python
# æŸ¥çœ‹å½“å‰æŒ‚è½½çš„æ•°æ®é›†ç›®å½•, è¯¥ç›®å½•ä¸‹çš„å˜æ›´é‡å¯ç¯å¢ƒåä¼šè‡ªåŠ¨è¿˜åŸ
# View dataset directory. 
# This directory will be recovered automatically after resetting environment. 
!ls /home/aistudio/data
```

    data16317  data269



```python
!unzip -oq /home/aistudio/data/data269/æˆ¿ä»·é¢„æµ‹.zip
```


```python
# æŸ¥çœ‹å·¥ä½œåŒºæ–‡ä»¶, è¯¥ç›®å½•ä¸‹çš„å˜æ›´å°†ä¼šæŒä¹…ä¿å­˜. è¯·åŠæ—¶æ¸…ç†ä¸å¿…è¦çš„æ–‡ä»¶, é¿å…åŠ è½½è¿‡æ…¢.
# View personal work directory. 
# All changes under this directory will be kept even after reset. 
# Please clean unnecessary files in time to speed up environment loading. 
!ls /home/aistudio/work
```


```python
# å¦‚æœéœ€è¦è¿›è¡ŒæŒä¹…åŒ–å®‰è£…, éœ€è¦ä½¿ç”¨æŒä¹…åŒ–è·¯å¾„, å¦‚ä¸‹æ–¹ä»£ç ç¤ºä¾‹:
# If a persistence installation is required, 
# you need to use the persistence path as the following: 
!mkdir /home/aistudio/external-libraries
!pip install beautifulsoup4 -t /home/aistudio/external-libraries
```

    Looking in indexes: https://mirror.baidu.com/pypi/simple/
    Collecting beautifulsoup4
    [?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 122kB 17.3MB/s eta 0:00:01
    [?25hCollecting soupsieve>1.2; python_version >= "3.0" (from beautifulsoup4)
      Downloading https://mirror.baidu.com/pypi/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl
    Installing collected packages: soupsieve, beautifulsoup4
    Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1



```python
# åŒæ—¶æ·»åŠ å¦‚ä¸‹ä»£ç , è¿™æ ·æ¯æ¬¡ç¯å¢ƒ(kernel)å¯åŠ¨çš„æ—¶å€™åªè¦è¿è¡Œä¸‹æ–¹ä»£ç å³å¯: 
# Also add the following code, 
# so that every time the environment (kernel) starts, 
# just run the following code: 
import sys 
sys.path.append('/home/aistudio/external-libraries')
```

è¯·ç‚¹å‡»[æ­¤å¤„](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)æŸ¥çœ‹æœ¬ç¯å¢ƒåŸºæœ¬ç”¨æ³•.  <br>
Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. 

# **ï¼ˆä¸€ï¼‰æ·±åº¦å­¦ä¹ å‘å±•å†å²**
**1. æ·±åº¦å­¦ä¹ çš„èµ·æºé˜¶æ®µ**

	1943å¹´ï¼Œå¿ƒé‡Œå­¦å®¶éº¦å¡æ´›å…‹å’Œæ•°å­¦é€»è¾‘å­¦å®¶çš®å…¹å‘è¡¨è®ºæ–‡ã€Šç¥ç»æ´»åŠ¨ä¸­å†…åœ¨æ€æƒ³çš„é€»è¾‘æ¼”ç®—ã€‹ï¼Œæå‡ºäº†MPæ¨¡å‹ã€‚MPæ¨¡å‹æ˜¯æ¨¡ä»¿ç¥ç»å…ƒçš„ç»“æ„å’Œå·¥ä½œåŸç†ï¼Œæ„æˆå‡ºçš„ä¸€ä¸ªåŸºäºç¥ç»ç½‘ç»œçš„æ•°å­¦æ¨¡å‹ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ç§â€œæ¨¡æ‹Ÿäººç±»å¤§è„‘â€çš„ç¥ç»å…ƒæ¨¡å‹ã€‚MPæ¨¡å‹ä½œä¸ºäººå·¥ç¥ç»ç½‘ç»œçš„èµ·æºï¼Œå¼€åˆ›äº†äººå·¥ç¥ç»ç½‘ç»œçš„æ–°æ—¶ä»£ï¼Œä¹Ÿå¥ å®šäº†ç¥ç»ç½‘ç»œæ¨¡å‹çš„åŸºç¡€ã€‚
    
	1949å¹´ï¼ŒåŠ æ‹¿å¤§è‘—åå¿ƒç†å­¦å®¶å”çº³å¾·Â·èµ«å¸ƒåœ¨ã€Šè¡Œä¸ºçš„ç»„ç»‡ã€‹ä¸­æå‡ºäº†ä¸€ç§åŸºäºæ— ç›‘ç£å­¦ä¹ çš„è§„åˆ™â€”â€”æµ·å¸ƒå­¦ä¹ è§„åˆ™(Hebb Rule)ã€‚æµ·å¸ƒè§„åˆ™æ¨¡ä»¿äººç±»è®¤çŸ¥ä¸–ç•Œçš„è¿‡ç¨‹å»ºç«‹ä¸€ç§â€œç½‘ç»œæ¨¡å‹â€ï¼Œè¯¥ç½‘ç»œæ¨¡å‹é’ˆå¯¹è®­ç»ƒé›†è¿›è¡Œå¤§é‡çš„è®­ç»ƒå¹¶æå–è®­ç»ƒé›†çš„ç»Ÿè®¡ç‰¹å¾ï¼Œç„¶åæŒ‰ç…§æ ·æœ¬çš„ç›¸ä¼¼ç¨‹åº¦è¿›è¡Œåˆ†ç±»ï¼ŒæŠŠç›¸äº’ä¹‹é—´è”ç³»å¯†åˆ‡çš„æ ·æœ¬åˆ†ä¸ºä¸€ç±»ï¼Œè¿™æ ·å°±æŠŠæ ·æœ¬åˆ†æˆäº†è‹¥å¹²ç±»ã€‚æµ·å¸ƒå­¦ä¹ è§„åˆ™ä¸â€œæ¡ä»¶åå°„â€æœºç†ä¸€è‡´ï¼Œä¸ºä»¥åçš„ç¥ç»ç½‘ç»œå­¦ä¹ ç®—æ³•å¥ å®šäº†åŸºç¡€ï¼Œå…·æœ‰é‡å¤§çš„å†å²æ„ä¹‰ã€‚
    
	20ä¸–çºª50å¹´ä»£æœ«ï¼Œåœ¨MPæ¨¡å‹å’Œæµ·å¸ƒå­¦ä¹ è§„åˆ™çš„ç ”ç©¶åŸºç¡€ä¸Šï¼Œç¾å›½ç§‘å­¦å®¶ç½—æ£®å¸ƒæ‹‰ç‰¹å‘ç°äº†ä¸€ç§ç±»ä¼¼äºäººç±»å­¦ä¹ è¿‡ç¨‹çš„å­¦ä¹ ç®—æ³•â€”â€”æ„ŸçŸ¥æœºå­¦ä¹ ã€‚å¹¶äº1958å¹´ï¼Œæ­£å¼æå‡ºäº†ç”±ä¸¤å±‚ç¥ç»å…ƒç»„æˆçš„ç¥ç»ç½‘ç»œï¼Œç§°ä¹‹ä¸ºâ€œæ„ŸçŸ¥å™¨â€ã€‚æ„ŸçŸ¥å™¨æœ¬è´¨ä¸Šæ˜¯ä¸€ç§çº¿æ€§æ¨¡å‹ï¼Œå¯ä»¥å¯¹è¾“å…¥çš„è®­ç»ƒé›†æ•°æ®è¿›è¡ŒäºŒåˆ†ç±»ï¼Œä¸”èƒ½å¤Ÿåœ¨è®­ç»ƒé›†ä¸­è‡ªåŠ¨æ›´æ–°æƒå€¼ã€‚æ„ŸçŸ¥å™¨çš„æå‡ºå¸å¼•äº†å¤§é‡ç§‘å­¦å®¶å¯¹äººå·¥ç¥ç»ç½‘ç»œç ”ç©¶çš„å…´è¶£ï¼Œå¯¹ç¥ç»ç½‘ç»œçš„å‘å±•å…·æœ‰é‡Œç¨‹ç¢‘å¼çš„æ„ä¹‰ã€‚
    
	ä½†éšç€ç ”ç©¶çš„æ·±å…¥ï¼Œåœ¨1969å¹´ï¼Œâ€œAIä¹‹çˆ¶â€é©¬æ–‡Â·æ˜æ–¯åŸºå’ŒLOGOè¯­è¨€çš„åˆ›å§‹äººè¥¿è’™Â·æ´¾ç€ç‰¹å…±åŒç¼–å†™äº†ä¸€æœ¬ä¹¦ç±ã€Šæ„ŸçŸ¥å™¨ã€‹ï¼Œåœ¨ä¹¦ä¸­ä»–ä»¬è¯æ˜äº†å•å±‚æ„ŸçŸ¥å™¨æ— æ³•è§£å†³çº¿æ€§ä¸å¯åˆ†é—®é¢˜ï¼ˆä¾‹å¦‚ï¼šå¼‚æˆ–é—®é¢˜ï¼‰ã€‚ç”±äºè¿™ä¸ªè‡´å‘½çš„ç¼ºé™·ä»¥åŠæ²¡æœ‰åŠæ—¶æ¨å¹¿æ„ŸçŸ¥å™¨åˆ°å¤šå±‚ç¥ç»ç½‘ç»œä¸­ï¼Œåœ¨20ä¸–çºª70å¹´ä»£ï¼Œäººå·¥ç¥ç»ç½‘ç»œè¿›å…¥äº†ç¬¬ä¸€ä¸ªå¯’å†¬æœŸï¼Œäººä»¬å¯¹ç¥ç»ç½‘ç»œçš„ç ”ç©¶ä¹Ÿåœæ»äº†å°†è¿‘20å¹´ã€‚

**2. æ·±åº¦å­¦ä¹ çš„å‘å±•é˜¶æ®µ**

	1982å¹´ï¼Œè‘—åç‰©ç†å­¦å®¶çº¦ç¿°Â·éœæ™®è²å°”å¾·å‘æ˜äº†Hopfieldç¥ç»ç½‘ç»œã€‚Hopfieldç¥ç»ç½‘ç»œæ˜¯ä¸€ç§ç»“åˆå­˜å‚¨ç³»ç»Ÿå’ŒäºŒå…ƒç³»ç»Ÿçš„å¾ªç¯ç¥ç»ç½‘ç»œã€‚Hopfieldç½‘ç»œä¹Ÿå¯ä»¥æ¨¡æ‹Ÿäººç±»çš„è®°å¿†ï¼Œæ ¹æ®æ¿€æ´»å‡½æ•°çš„é€‰å–ä¸åŒï¼Œæœ‰è¿ç»­å‹å’Œç¦»æ•£å‹ä¸¤ç§ç±»å‹ï¼Œåˆ†åˆ«ç”¨äºä¼˜åŒ–è®¡ç®—å’Œè”æƒ³è®°å¿†ã€‚ä½†ç”±äºå®¹æ˜“é™·å…¥å±€éƒ¨æœ€å°å€¼çš„ç¼ºé™·ï¼Œè¯¥ç®—æ³•å¹¶æœªåœ¨å½“æ—¶å¼•èµ·å¾ˆå¤§çš„è½°åŠ¨ã€‚
    
	ç›´åˆ°1986å¹´ï¼Œæ·±åº¦å­¦ä¹ ä¹‹çˆ¶æ°å¼—é‡ŒÂ·è¾›é¡¿æå‡ºäº†ä¸€ç§é€‚ç”¨äºå¤šå±‚æ„ŸçŸ¥å™¨çš„åå‘ä¼ æ’­ç®—æ³•â€”â€”BPç®—æ³•ã€‚BPç®—æ³•åœ¨ä¼ ç»Ÿç¥ç»ç½‘ç»œæ­£å‘ä¼ æ’­çš„åŸºç¡€ä¸Šï¼Œå¢åŠ äº†è¯¯å·®çš„åå‘ä¼ æ’­è¿‡ç¨‹ã€‚åå‘ä¼ æ’­è¿‡ç¨‹ä¸æ–­åœ°è°ƒæ•´ç¥ç»å…ƒä¹‹é—´çš„æƒå€¼å’Œé˜ˆå€¼ï¼Œç›´åˆ°è¾“å‡ºçš„è¯¯å·®è¾¾åˆ°å‡å°åˆ°å…è®¸çš„èŒƒå›´ä¹‹å†…ï¼Œæˆ–è¾¾åˆ°é¢„å…ˆè®¾å®šçš„è®­ç»ƒæ¬¡æ•°ä¸ºæ­¢ã€‚BPç®—æ³•å®Œç¾çš„è§£å†³äº†éçº¿æ€§åˆ†ç±»é—®é¢˜ï¼Œè®©äººå·¥ç¥ç»ç½‘ç»œå†æ¬¡çš„å¼•èµ·äº†äººä»¬å¹¿æ³›çš„å…³æ³¨ã€‚
    
	ä½†æ˜¯ç”±äºå…«åå¹´ä»£è®¡ç®—æœºçš„ç¡¬ä»¶æ°´å¹³æœ‰é™ï¼Œå¦‚ï¼šè¿ç®—èƒ½åŠ›è·Ÿä¸ä¸Šï¼Œè¿™å°±å¯¼è‡´å½“ç¥ç»ç½‘ç»œçš„è§„æ¨¡å¢å¤§æ—¶ï¼Œå†ä½¿ç”¨BPç®—æ³•ä¼šå‡ºç°â€œæ¢¯åº¦æ¶ˆå¤±â€çš„é—®é¢˜ã€‚è¿™ä½¿å¾—BPç®—æ³•çš„å‘å±•å—åˆ°äº†å¾ˆå¤§çš„é™åˆ¶ã€‚å†åŠ ä¸Š90å¹´ä»£ä¸­æœŸï¼Œä»¥SVMä¸ºä»£è¡¨çš„å…¶å®ƒæµ…å±‚æœºå™¨å­¦ä¹ ç®—æ³•è¢«æå‡ºï¼Œå¹¶åœ¨åˆ†ç±»ã€å›å½’é—®é¢˜ä¸Šå‡å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœï¼Œå…¶åŸç†åˆæ˜æ˜¾ä¸åŒäºç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæ‰€ä»¥äººå·¥ç¥ç»ç½‘ç»œçš„å‘å±•å†æ¬¡è¿›å…¥äº†ç“¶é¢ˆæœŸã€‚

**3.æ·±åº¦å­¦ä¹ çš„çˆ†å‘é˜¶æ®µ**

	2006å¹´ï¼Œæ°å¼—é‡ŒÂ·è¾›é¡¿ä»¥åŠä»–çš„å­¦ç”Ÿé²æ–¯å…°Â·è¨æ‹‰èµ«ä¸è¯ºå¤«æ­£å¼æå‡ºäº†æ·±åº¦å­¦ä¹ çš„æ¦‚å¿µã€‚ä»–ä»¬åœ¨ä¸–ç•Œé¡¶çº§å­¦æœ¯æœŸåˆŠã€Šç§‘å­¦ã€‹å‘è¡¨çš„ä¸€ç¯‡æ–‡ç« ä¸­è¯¦ç»†çš„ç»™å‡ºäº†â€œæ¢¯åº¦æ¶ˆå¤±â€é—®é¢˜çš„è§£å†³æ–¹æ¡ˆâ€”â€”é€šè¿‡æ— ç›‘ç£çš„å­¦ä¹ æ–¹æ³•é€å±‚è®­ç»ƒç®—æ³•ï¼Œå†ä½¿ç”¨æœ‰ç›‘ç£çš„åå‘ä¼ æ’­ç®—æ³•è¿›è¡Œè°ƒä¼˜ã€‚è¯¥æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æå‡ºï¼Œç«‹å³åœ¨å­¦æœ¯åœˆå¼•èµ·äº†å·¨å¤§çš„åå“ï¼Œä»¥æ–¯å¦ç¦å¤§å­¦ã€å¤šä¼¦å¤šå¤§å­¦ä¸ºä»£è¡¨çš„ä¼—å¤šä¸–ç•ŒçŸ¥åé«˜æ ¡çº·çº·æŠ•å…¥å·¨å¤§çš„äººåŠ›ã€è´¢åŠ›è¿›è¡Œæ·±åº¦å­¦ä¹ é¢†åŸŸçš„ç›¸å…³ç ”ç©¶ã€‚è€Œååˆåœ¨è¿…é€Ÿè”“å»¶åˆ°å·¥ä¸šç•Œä¸­ã€‚
    
	2006å¹´ï¼Œæ°å¼—é‡ŒÂ·è¾›é¡¿ä»¥åŠä»–çš„å­¦ç”Ÿé²æ–¯å…°Â·è¨æ‹‰èµ«ä¸è¯ºå¤«æ­£å¼æå‡ºäº†æ·±åº¦å­¦ä¹ çš„æ¦‚å¿µã€‚ä»–ä»¬åœ¨ä¸–ç•Œé¡¶çº§å­¦æœ¯æœŸåˆŠã€Šç§‘å­¦ã€‹å‘è¡¨çš„ä¸€ç¯‡æ–‡ç« ä¸­è¯¦ç»†çš„ç»™å‡ºäº†â€œæ¢¯åº¦æ¶ˆå¤±â€é—®é¢˜çš„è§£å†³æ–¹æ¡ˆâ€”â€”é€šè¿‡æ— ç›‘ç£çš„å­¦ä¹ æ–¹æ³•é€å±‚è®­ç»ƒç®—æ³•ï¼Œå†ä½¿ç”¨æœ‰ç›‘ç£çš„åå‘ä¼ æ’­ç®—æ³•è¿›è¡Œè°ƒä¼˜ã€‚è¯¥æ·±åº¦å­¦ä¹ æ–¹æ³•çš„æå‡ºï¼Œç«‹å³åœ¨å­¦æœ¯åœˆå¼•èµ·äº†å·¨å¤§çš„åå“ï¼Œä»¥æ–¯å¦ç¦å¤§å­¦ã€å¤šä¼¦å¤šå¤§å­¦ä¸ºä»£è¡¨çš„ä¼—å¤šä¸–ç•ŒçŸ¥åé«˜æ ¡çº·çº·æŠ•å…¥å·¨å¤§çš„äººåŠ›ã€è´¢åŠ›è¿›è¡Œæ·±åº¦å­¦ä¹ é¢†åŸŸçš„ç›¸å…³ç ”ç©¶ã€‚è€Œååˆåœ¨è¿…é€Ÿè”“å»¶åˆ°å·¥ä¸šç•Œä¸­ã€‚
    
	2012å¹´ï¼Œåœ¨è‘—åçš„ImageNetå›¾åƒè¯†åˆ«å¤§èµ›ä¸­ï¼Œæ°å¼—é‡ŒÂ·è¾›é¡¿é¢†å¯¼çš„å°ç»„é‡‡ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹AlexNetä¸€ä¸¾å¤ºå† ã€‚AlexNeté‡‡ç”¨ReLUæ¿€æ´»å‡½æ•°ï¼Œä»æ ¹æœ¬ä¸Šè§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œå¹¶é‡‡ç”¨GPUæå¤§çš„æé«˜äº†æ¨¡å‹çš„è¿ç®—é€Ÿåº¦ã€‚åŒå¹´ï¼Œç”±æ–¯å¦ç¦å¤§å­¦è‘—åçš„å´æ©è¾¾æ•™æˆå’Œä¸–ç•Œé¡¶å°–è®¡ç®—æœºä¸“å®¶Jeff Deanå…±åŒä¸»å¯¼çš„æ·±åº¦ç¥ç»ç½‘ç»œâ€”â€”DNNæŠ€æœ¯åœ¨å›¾åƒè¯†åˆ«é¢†åŸŸå–å¾—äº†æƒŠäººçš„æˆç»©ï¼Œåœ¨ImageNetè¯„æµ‹ä¸­æˆåŠŸçš„æŠŠé”™è¯¯ç‡ä»26ï¼…é™ä½åˆ°äº†15ï¼…ã€‚æ·±åº¦å­¦ä¹ ç®—æ³•åœ¨ä¸–ç•Œå¤§èµ›çš„è„±é¢–è€Œå‡ºï¼Œä¹Ÿå†ä¸€æ¬¡å¸å¼•äº†å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œå¯¹äºæ·±åº¦å­¦ä¹ é¢†åŸŸçš„å…³æ³¨ã€‚
    
	éšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„ä¸æ–­è¿›æ­¥ä»¥åŠæ•°æ®å¤„ç†èƒ½åŠ›çš„ä¸æ–­æå‡ï¼Œ2014å¹´ï¼ŒFacebookåŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„DeepFaceé¡¹ç›®ï¼Œåœ¨äººè„¸è¯†åˆ«æ–¹é¢çš„å‡†ç¡®ç‡å·²ç»èƒ½è¾¾åˆ°97%ä»¥ä¸Šï¼Œè·Ÿäººç±»è¯†åˆ«çš„å‡†ç¡®ç‡å‡ ä¹æ²¡æœ‰å·®åˆ«ã€‚è¿™æ ·çš„ç»“æœä¹Ÿå†ä¸€æ¬¡è¯æ˜äº†æ·±åº¦å­¦ä¹ ç®—æ³•åœ¨å›¾åƒè¯†åˆ«æ–¹é¢çš„ä¸€éª‘ç»å°˜ã€‚
    
	2016å¹´ï¼Œéšç€è°·æ­Œå…¬å¸åŸºäºæ·±åº¦å­¦ä¹ å¼€å‘çš„AlphaGoä»¥4:1çš„æ¯”åˆ†æˆ˜èƒœäº†å›½é™…é¡¶å°–å›´æ£‹é«˜æ‰‹æä¸–çŸ³ï¼Œæ·±åº¦å­¦ä¹ çš„çƒ­åº¦ä¸€æ—¶æ— ä¸¤ã€‚åæ¥ï¼ŒAlphaGoåˆæ¥è¿å’Œä¼—å¤šä¸–ç•Œçº§å›´æ£‹é«˜æ‰‹è¿‡æ‹›ï¼Œå‡å–å¾—äº†å®Œèƒœã€‚è¿™ä¹Ÿè¯æ˜äº†åœ¨å›´æ£‹ç•Œï¼ŒåŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„æœºå™¨äººå·²ç»è¶…è¶Šäº†äººç±»ã€‚
    
	2017å¹´ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ ç®—æ³•çš„AlphaGoå‡çº§ç‰ˆAlphaGo Zeroæ¨ªç©ºå‡ºä¸–ã€‚å…¶é‡‡ç”¨â€œä»é›¶å¼€å§‹â€ã€â€œæ— å¸ˆè‡ªé€šâ€çš„å­¦ä¹ æ¨¡å¼ï¼Œä»¥100:0çš„æ¯”åˆ†è½»è€Œæ˜“ä¸¾æ‰“è´¥äº†ä¹‹å‰çš„AlphaGoã€‚é™¤äº†å›´æ£‹ï¼Œå®ƒè¿˜ç²¾é€šå›½é™…è±¡æ£‹ç­‰å…¶å®ƒæ£‹ç±»æ¸¸æˆï¼Œå¯ä»¥è¯´æ˜¯çœŸæ­£çš„æ£‹ç±»â€œå¤©æ‰â€ã€‚æ­¤å¤–åœ¨è¿™ä¸€å¹´ï¼Œæ·±åº¦å­¦ä¹ çš„ç›¸å…³ç®—æ³•åœ¨åŒ»ç–—ã€é‡‘èã€è‰ºæœ¯ã€æ— äººé©¾é©¶ç­‰å¤šä¸ªé¢†åŸŸå‡å–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚æ‰€ä»¥ï¼Œä¹Ÿæœ‰ä¸“å®¶æŠŠ2017å¹´çœ‹ä½œæ˜¯æ·±åº¦å­¦ä¹ ç”šè‡³æ˜¯äººå·¥æ™ºèƒ½å‘å±•æœ€ä¸ºçªé£çŒ›è¿›çš„ä¸€å¹´ã€‚

# **ï¼ˆäºŒï¼‰äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ã€å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«å’Œè”ç³»**

**åŒºåˆ«**

äººå·¥æ™ºèƒ½ï¼šä¸ºæœºå™¨èµ‹äºˆäººçš„æ™ºèƒ½

æœºå™¨å­¦ä¹ ï¼šä¸€ç§å®ç°äººå·¥æ™ºèƒ½çš„æ–¹æ³•

æ·±åº¦å­¦ä¹ ï¼šä¸€ç§å®ç°æœºå™¨å­¦ä¹ çš„æŠ€æœ¯

**è”ç³»**

![](https://ai-studio-static-online.cdn.bcebos.com/e76708ca9bed481daf43d503f66c903618d3ae973fff4929a0b437e2cecf2f4f)


# **ï¼ˆä¸‰ï¼‰ç¥ç»å…ƒã€å•å±‚æ„ŸçŸ¥æœºã€å¤šå±‚æ„ŸçŸ¥æœº**

**äººå·¥ç¥ç»å…ƒæ¨¡å‹**

ç”Ÿç‰©å­¦ä¸Šç¥ç»å…ƒé€šå¸¸ç”±ç»†èƒä½“ï¼Œç»†èƒæ ¸ï¼Œæ ‘çªå’Œè½´çªæ„æˆã€‚

æ ‘çªç”¨æ¥æ¥æ”¶å…¶ä»–ç¥ç»å…ƒä¼ å¯¼è¿‡æ¥çš„ä¿¡å·ï¼Œä¸€ä¸ªç¥ç»å…ƒæœ‰å¤šä¸ªæ ‘çªï¼›

ç»†èƒæ ¸æ˜¯ç¥ç»å…ƒä¸­çš„æ ¸å¿ƒæ¨¡å—ï¼Œç”¨æ¥å¤„ç†æ‰€æœ‰çš„ä¼ å…¥ä¿¡å·ï¼›

è½´çªæ˜¯è¾“å‡ºä¿¡å·çš„å•å…ƒï¼Œå®ƒæœ‰å¾ˆå¤šä¸ªè½´çªæœ«æ¢¢ï¼Œå¯ä»¥ç»™å…¶å®ƒç¥ç»å…ƒçš„æ ‘çªä¼ é€’ä¿¡å·ã€‚

ç”Ÿç‰©å­¦ç¥ç»å…ƒå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://ai-studio-static-online.cdn.bcebos.com/255356fddd484cb59c8505bf7fe3a59b2979dcd5899648feba97df7deb341723)

äººå·¥ç¥ç»å…ƒçš„æ¨¡å‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://ai-studio-static-online.cdn.bcebos.com/6c527f77c23f473db572ced6eebb010655c8d9217b2a4b6bbabb885f6bdc96d9)

å›¾ä¸­Xiæ˜¯ä»å…¶å®ƒç¥ç»å…ƒä¼ å…¥çš„è¾“å…¥ä¿¡å·ï¼ŒWinåˆ†åˆ«æ˜¯ä¼ å…¥ä¿¡å·çš„æƒé‡ï¼ŒÎ¸è¡¨ç¤ºä¸€ä¸ªé˜ˆå€¼ï¼Œæˆ–ç§°ä¸ºåç½®ï¼ˆbiasï¼‰ï¼Œåç½®çš„è®¾ç½®æ˜¯ä¸ºäº†æ­£ç¡®åˆ†ç±»æ ·æœ¬ï¼Œæ˜¯æ¨¡å‹ä¸­ä¸€ä¸ªé‡è¦çš„å‚æ•°ã€‚ç¥ç»å…ƒç»¼åˆçš„è¾“å…¥ä¿¡å·å’Œåç½®ç›¸åŠ ä¹‹åäº§ç”Ÿå½“å‰ç¥ç»å…ƒæœ€ç»ˆçš„å¤„ç†ä¿¡å·netï¼Œè¯¥ä¿¡å·ç§°ä¸ºå‡€æ¿€æ´»æˆ–å‡€æ¿€åŠ±ï¼ˆnet activationï¼‰ï¼Œæ¿€æ´»ä¿¡å·ä½œä¸ºä¸Šå›¾ä¸­åœ†åœˆçš„å³åŠéƒ¨åˆ†fï¼ˆ*ï¼‰å‡½æ•°çš„è¾“å…¥ï¼Œå³f(net)ï¼› fç§°ä¸ºæ¿€æ´»å‡½æ•°æˆ–æ¿€åŠ±å‡½æ•°ï¼ˆActivation Functionï¼‰ï¼Œæ¿€æ´»å‡½æ•°çš„ä¸»è¦ä½œç”¨æ˜¯åŠ å…¥éçº¿æ€§å› ç´ ï¼Œè§£å†³çº¿æ€§æ¨¡å‹çš„è¡¨è¾¾ã€åˆ†ç±»èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚ä¸Šå›¾ä¸­çš„yiæ˜¯å½“å‰ç¥ç»å…ƒçš„è¾“å‡ºã€‚

![](https://ai-studio-static-online.cdn.bcebos.com/b3340a4096754883b16c8d5fe9c6d05a7ac8012c8fb14948b8a876974f220974)

**å•å±‚æ„ŸçŸ¥æœº**

![](https://ai-studio-static-online.cdn.bcebos.com/4641b006895f4370a364aaae49d9c6910fdeaf227e214276b621abb0512b0227)


å•å±‚æ„ŸçŸ¥æœºç›®æ ‡æ˜¯å°†è¢«æ„ŸçŸ¥æ•°æ®é›†åˆ’åˆ†ä¸ºä¸¤ç±»çš„åˆ†ç¦»è¶…å¹³é¢ï¼Œå¹¶è®¡ç®—å‡ºè¯¥è¶…å¹³é¢ã€‚å•å±‚æ„ŸçŸ¥æœºæ˜¯äºŒåˆ†ç±»çš„çº¿æ€§åˆ†ç±»æ¨¡å‹ï¼Œè¾“å…¥æ˜¯è¢«æ„ŸçŸ¥æ•°æ®é›†çš„ç‰¹å¾å‘é‡ï¼Œè¾“å‡ºæ—¶æ•°æ®é›†çš„ç±»åˆ«{+1,-1}ã€‚æ„ŸçŸ¥å™¨çš„æ¨¡å‹å¯ä»¥ç®€å•è¡¨ç¤ºä¸ºï¼š

f(x)=sign(w.x+b)

è¯¥å‡½æ•°ç§°ä¸ºå•å±‚æ„ŸçŸ¥æœºï¼Œå…¶ä¸­wæ˜¯ç½‘ç»œçš„Nç»´æƒé‡å‘é‡ï¼Œbæ˜¯ç½‘ç»œçš„Nç»´åç½®å‘é‡, w.xæ˜¯wå’Œxçš„å†…ç§¯ï¼Œwå’Œbçš„Nç»´å‘é‡å–å€¼è¦æ±‚åœ¨å®æ•°åŸŸã€‚

signå‡½æ•°æ˜¯æ„ŸçŸ¥æœºçš„æ—©æœŸæ¿€æ´»å‡½æ•°ï¼Œåé¢åˆæ¼”åŒ–å‡ºä¸€ç³»åˆ—çš„æ¿€æ´»å‡½æ•°ã€‚æ¿€æ´»å‡½æ•°ä¸€èˆ¬é‡‡ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œä»¥å¢å¼ºç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ã€‚å¸¸è§çš„æ¿€æ´»å‡½æ•°æœ‰ï¼šsign, sigmoid,tanh,ReLUç­‰ã€‚

![](https://ai-studio-static-online.cdn.bcebos.com/173492fd80134f5a91e0b95cbbb22581b2a1d730ff9d4bfcaceba81bf9466e15)

**å¤šå±‚æ„ŸçŸ¥æœº**
å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼ŒMultilayer Perceptronï¼‰ä¹Ÿå«äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼ŒArtificial Neural Networkï¼‰ï¼Œé™¤äº†è¾“å…¥è¾“å‡ºå±‚ï¼Œå®ƒä¸­é—´å¯ä»¥æœ‰å¤šä¸ªéšå±‚ï¼Œæœ€ç®€å•çš„MLPåªå«ä¸€ä¸ªéšå±‚ï¼Œå³ä¸‰å±‚çš„ç»“æ„ï¼Œå¦‚ä¸‹å›¾ï¼š

![](https://ai-studio-static-online.cdn.bcebos.com/909abb78a4f1420288378799d6c0f6052f831e4dc00c4b97bc4d53dff470a134)

ä»ä¸Šå›¾å¯ä»¥çœ‹åˆ°ï¼Œå¤šå±‚æ„ŸçŸ¥æœºå±‚ä¸å±‚ä¹‹é—´æ˜¯å…¨è¿æ¥çš„ã€‚å¤šå±‚æ„ŸçŸ¥æœºæœ€åº•å±‚æ˜¯è¾“å…¥å±‚ï¼Œä¸­é—´æ˜¯éšè—å±‚ï¼Œæœ€åæ˜¯è¾“å‡ºå±‚ã€‚

éšè—å±‚çš„ç¥ç»å…ƒæ€ä¹ˆå¾—æ¥ï¼Ÿé¦–å…ˆå®ƒä¸è¾“å…¥å±‚æ˜¯å…¨è¿æ¥çš„ï¼Œå‡è®¾è¾“å…¥å±‚ç”¨å‘é‡Xè¡¨ç¤ºï¼Œåˆ™éšè—å±‚çš„è¾“å‡ºå°±æ˜¯ f ( W 1 X + b 1 )ï¼Œ Wiæ˜¯æƒé‡ï¼ˆä¹Ÿå«è¿æ¥ç³»æ•°ï¼‰ï¼Œbiæ˜¯åç½®ï¼Œå‡½æ•°få¯ä»¥æ˜¯æ¿€æ´»å‡½æ•°ï¼Œæ¯”å¦‚å¸¸ç”¨çš„sigmoidå‡½æ•°æˆ–è€…tanhå‡½æ•°ã€‚




**ï¼ˆå››ï¼‰ä»€ä¹ˆæ˜¯å‰å‘ä¼ æ’­**

æ‰€è°“çš„å‰å‘ä¼ æ’­ç®—æ³•å°±æ˜¯ï¼šå°†ä¸Šä¸€å±‚çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥ï¼Œå¹¶è®¡ç®—ä¸‹ä¸€å±‚çš„è¾“å‡ºï¼Œä¸€ç›´åˆ°è¿ç®—åˆ°è¾“å‡ºå±‚ä¸ºæ­¢ã€‚

![](https://ai-studio-static-online.cdn.bcebos.com/2c4d201a47cb41eea0e61da9130b442e84b39694e8d248938be885d83763e88f)

å…¶ä¸­layer1ä¸ºè¾“å…¥å±‚ï¼Œlayer2ä¸ºéšè—å±‚ï¼Œlayer3ä¸ºè¾“å‡ºå±‚

![](https://ai-studio-static-online.cdn.bcebos.com/a484a8d572064f5cb10948cc2d4f33875cab7723214a45f0b8e0b7998d24d9d5)



**ï¼ˆäº”ï¼‰ä»€ä¹ˆæ˜¯åå‘ä¼ æ’­**

åå‘ä¼ æ’­ï¼ˆback propagation, BPï¼‰ç®—æ³•æ˜¯ "è¯¯å·®åå‘ä¼ æ’­" çš„ç®€ç§°ï¼Œä¹Ÿç§°ä¸ºbackpropï¼Œå…è®¸æ¥è‡ªä»£ä»·å‡½æ•°çš„ä¿¡æ¯é€šè¿‡ç½‘ç»œå‘åæµåŠ¨ï¼Œä»¥ä¾¿è®¡ç®—æ¢¯åº¦ã€‚

åå‘ä¼ æ’­æ˜¯ä¸€ç§ä¸æœ€ä¼˜åŒ–æ–¹æ³•ï¼ˆå¦‚æ¢¯åº¦ä¸‹é™æ³•ï¼‰ç»“åˆä½¿ç”¨çš„ï¼Œç”¨æ¥è®­ç»ƒäººå·¥ç¥ç»ç½‘ç»œçš„å¸¸è§æ–¹æ³•ã€‚è¯¥æ–¹æ³•å¯¹ç½‘ç»œä¸­æ‰€æœ‰æƒé‡è®¡ç®—æŸå¤±å‡½æ•°çš„æ¢¯åº¦ã€‚è¿™ä¸ªæ¢¯åº¦ä¼šåé¦ˆç»™æœ€ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨æ¥æ›´æ–°æƒå€¼ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚

åå‘ä¼ æ’­è¿™ä¸ªæœ¯è¯­ç»å¸¸è¢«è¯¯è§£ä¸ºç”¨äºå¤šå±‚ç¥ç»ç½‘ç»œçš„æ•´ä¸ªå­¦ä¹ ç®—æ³•ã€‚å®é™…ä¸Šï¼Œåå‘ä¼ æ’­ä»…æŒ‡ç”¨äºè®¡ç®—æ¢¯åº¦çš„æ–¹æ³•ã€‚è€Œå¦ä¸€ç§ç®—æ³•ï¼Œä¾‹å¦‚éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼Œæ‰æ˜¯ä½¿ç”¨è¯¥æ¢¯åº¦æ¥è¿›è¡Œå­¦ä¹ ã€‚å¦å¤–ï¼Œåå‘ä¼ æ’­è¿˜ç»å¸¸è¢«è¯¯è§£ä¸ºä»…é€‚ç”¨äºå¤šå±‚ç¥ç»ç½‘ç»œï¼Œä½†æ˜¯åŸåˆ™ä¸Šå®ƒå¯ä»¥è®¡ç®—ä»»ä½•å‡½æ•°çš„åˆ°å¯¼æ•°


![](https://ai-studio-static-online.cdn.bcebos.com/8625a1c1153746d99c1f8947e020d2b12663783b9fbc4eb8bb0b4c6014e71ac5)

![](https://ai-studio-static-online.cdn.bcebos.com/69e117a489a8444497eac655dc2f688a7787b091efd44d0bb64b36078df7c5ed)

![](https://ai-studio-static-online.cdn.bcebos.com/754cd8bd65b347468e30addf2afcfe28828a02d04dbe42bf903dcb5dca6e923b)

![](https://ai-studio-static-online.cdn.bcebos.com/ce1a956b3b6445ac95e6ed8c785b3940161090d0b06d41abad008513576da413)

![](https://ai-studio-static-online.cdn.bcebos.com/aebd1cccaf5d46659bcfb30be5963427db396b68d1f440f293d2c1d241329752)







ï¼ˆå…­ï¼‰æˆ¿ä»·é¢„æµ‹

çº¿æ€§å›å½’ä¸­ï¼š

ï¼ˆ1ï¼‰å‡è®¾å‡½æ•°æ˜¯æŒ‡ï¼Œç”¨æ•°å­¦çš„æ–¹æ³•æè¿°è‡ªå˜é‡å’Œå› å˜é‡ä¹‹é—´çš„å…³ç³»ï¼Œå®ƒä»¬ä¹‹é—´å¯ä»¥æ˜¯ä¸€ä¸ªçº¿æ€§å‡½æ•°æˆ–éçº¿æ€§å‡½æ•°ã€‚ åœ¨æœ¬æ¬¡çº¿æ€§å›é¡¾æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬çš„å‡è®¾å‡½æ•°ä¸º Yâ€™= wX+b ï¼Œå…¶ä¸­ï¼ŒYâ€™è¡¨ç¤ºæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼ˆé¢„æµ‹æˆ¿ä»·ï¼‰ï¼Œç”¨æ¥å’ŒçœŸå®çš„YåŒºåˆ†ã€‚æ¨¡å‹è¦å­¦ä¹ çš„å‚æ•°å³ï¼šw,bã€‚

ï¼ˆ2ï¼‰æŸå¤±å‡½æ•°æ˜¯æŒ‡ï¼Œç”¨æ•°å­¦çš„æ–¹æ³•è¡¡é‡å‡è®¾å‡½æ•°é¢„æµ‹ç»“æœä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®ã€‚è¿™ä¸ªå·®è·è¶Šå°é¢„æµ‹è¶Šå‡†ç¡®ï¼Œè€Œç®—æ³•çš„ä»»åŠ¡å°±æ˜¯ä½¿è¿™ä¸ªå·®è·è¶Šæ¥è¶Šå°ã€‚ å»ºç«‹æ¨¡å‹åï¼Œæˆ‘ä»¬éœ€è¦ç»™æ¨¡å‹ä¸€ä¸ªä¼˜åŒ–ç›®æ ‡ï¼Œä½¿å¾—å­¦åˆ°çš„å‚æ•°èƒ½å¤Ÿè®©é¢„æµ‹å€¼Yâ€™å°½å¯èƒ½åœ°æ¥è¿‘çœŸå®å€¼Yã€‚è¿™ä¸ªå®å€¼é€šå¸¸ç”¨æ¥åæ˜ æ¨¡å‹è¯¯å·®çš„å¤§å°ã€‚ä¸åŒé—®é¢˜åœºæ™¯ä¸‹é‡‡ç”¨ä¸åŒçš„æŸå¤±å‡½æ•°ã€‚ å¯¹äºçº¿æ€§æ¨¡å‹æ¥è®²ï¼Œæœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°å°±æ˜¯å‡æ–¹è¯¯å·®ï¼ˆMean Squared Errorï¼Œ MSEï¼‰ã€‚

ï¼ˆ3ï¼‰ä¼˜åŒ–ç®—æ³•ï¼šç¥ç»ç½‘ç»œçš„è®­ç»ƒå°±æ˜¯è°ƒæ•´æƒé‡ï¼ˆå‚æ•°ï¼‰ä½¿å¾—æŸå¤±å‡½æ•°å€¼å°½å¯èƒ½å¾—å°ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå°†æŸå¤±å‡½æ•°å€¼é€æ¸æ”¶æ•›ï¼Œå¾—åˆ°ä¸€ç»„ä½¿å¾—ç¥ç»ç½‘ç»œæ‹ŸåˆçœŸå®æ¨¡å‹çš„æƒé‡ï¼ˆå‚æ•°ï¼‰ã€‚æ‰€ä»¥ï¼Œä¼˜åŒ–ç®—æ³•çš„æœ€ç»ˆç›®æ ‡æ˜¯æ‰¾åˆ°æŸå¤±å‡½æ•°çš„æœ€å°å€¼ã€‚è€Œè¿™ä¸ªå¯»æ‰¾è¿‡ç¨‹å°±æ˜¯ä¸æ–­åœ°å¾®è°ƒå˜é‡wå’Œbçš„å€¼ï¼Œä¸€æ­¥ä¸€æ­¥åœ°è¯•å‡ºè¿™ä¸ªæœ€å°å€¼ã€‚ å¸¸è§çš„ä¼˜åŒ–ç®—æ³•æœ‰éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆSGDï¼‰ã€Adamç®—æ³•ç­‰ç­‰


é¦–å…ˆå¯¼å…¥å¿…è¦çš„åŒ…ï¼Œåˆ†åˆ«æ˜¯ï¼š

paddle.fluid--->PaddlePaddleæ·±åº¦å­¦ä¹ æ¡†æ¶

numpy---------->pythonåŸºæœ¬åº“ï¼Œç”¨äºç§‘å­¦è®¡ç®—

os------------------>pythonçš„æ¨¡å—ï¼Œå¯ä½¿ç”¨è¯¥æ¨¡å—å¯¹æ“ä½œç³»ç»Ÿè¿›è¡Œæ“ä½œ

matplotlib----->pythonç»˜å›¾åº“ï¼Œå¯æ–¹ä¾¿ç»˜åˆ¶æŠ˜çº¿å›¾ã€æ•£ç‚¹å›¾ç­‰å›¾å½¢


```python
import paddle.fluid as fluid
import paddle
import numpy as np
import os
import matplotlib.pyplot as plt


def load_data():
    # ä»æ–‡ä»¶å¯¼å…¥æ•°æ®
    df =  pd.read_csv("./æˆ¿ä»·é¢„æµ‹/data/data.txt",sep=',')
    data = np.array(df)
    # æ¯æ¡æ•°æ®åŒ…æ‹¬14é¡¹ï¼Œå…¶ä¸­å‰é¢13é¡¹æ˜¯å½±å“å› ç´ ï¼Œç¬¬14é¡¹æ˜¯ç›¸åº”çš„æˆ¿å±‹ä»·æ ¼ä¸­ä½æ•°
    feature_names = [ 'mianji', 'money'] 
    feature_num = len(feature_names)
    # å°†åŸå§‹æ•°æ®è¿›è¡ŒReshapeï¼Œå˜æˆ[N, 2]è¿™æ ·çš„å½¢çŠ¶
    data = data.reshape([data.shape[0], feature_num])
    # å°†åŸæ•°æ®é›†æ‹†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # è¿™é‡Œä½¿ç”¨80%çš„æ•°æ®åšè®­ç»ƒï¼Œ20%çš„æ•°æ®åšæµ‹è¯•
    # æµ‹è¯•é›†å’Œè®­ç»ƒé›†å¿…é¡»æ˜¯æ²¡æœ‰äº¤é›†çš„
    ratio = 0.8
    offset = int(data.shape[0] * ratio) 
    training_data = data[:offset]
    # è®¡ç®—trainæ•°æ®é›†çš„æœ€å¤§å€¼ï¼Œæœ€å°å€¼ï¼Œå¹³å‡å€¼
    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \
                                 training_data.sum(axis=0) / training_data.shape[0]
    # å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
    for i in range(feature_num):
        #print(maximums[i], minimums[i], avgs[i])
        data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])
    # è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ’åˆ†æ¯”ä¾‹
    training_data = data[:offset]
    test_data = data[offset:]
    return training_data, test_data


```


```python
import numpy as np
class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œæ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        #np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.

    # å‰å‘ä¼ æ’­è¿‡ç¨‹
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    
    # å‡æ–¹å·®æŸå¤±å‡½æ•°
    def loss(self, z, y):
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    # æ¢¯åº¦ä¸‹é™æ³•
    def gradient(self, x, y):
        z = self.forward(x)
        N = x.shape[0]
        gradient_w = 1. / N * np.sum((z-y) * x, axis=0)
        gradient_w = gradient_w[:, np.newaxis]
        gradient_b = 1. / N * np.sum(z-y)
        return gradient_w, gradient_b
    
    # å‚æ•°æ›´æ–°å…¬å¼
    def update(self, gradient_w, gradient_b, eta = 0.01):
        self.w = self.w - eta * gradient_w
        self.b = self.b - eta * gradient_b
            
    # è®­ç»ƒè¿‡ç¨‹            
    def train(self, training_data, num_epoches, batch_size=10, eta=0.01):
        n = len(training_data)
        losses = []
        for epoch_id in range(num_epoches):
            # åœ¨æ¯è½®è¿­ä»£å¼€å§‹ä¹‹å‰ï¼Œå°†è®­ç»ƒæ•°æ®çš„é¡ºåºéšæœºçš„æ‰“ä¹±ï¼Œ
            # ç„¶åå†æŒ‰æ¯æ¬¡å–batch_sizeæ¡æ•°æ®çš„æ–¹å¼å–å‡º
            np.random.shuffle(training_data)
            # å°†è®­ç»ƒæ•°æ®è¿›è¡Œæ‹†åˆ†ï¼Œæ¯ä¸ªmini_batchåŒ…å«batch_sizeæ¡çš„æ•°æ®
            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]
            for iter_id, mini_batch in enumerate(mini_batches):
                #print(self.w.shape)
                #print(self.b)
                x = mini_batch[:, :-1]
                y = mini_batch[:, -1:]
                a = self.forward(x)
                loss = self.loss(a, y)
                gradient_w, gradient_b = self.gradient(x, y)
                self.update(gradient_w, gradient_b, eta)
                losses.append(loss)
                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.
                                 format(epoch_id, iter_id, loss))
        
        return losses

# è·å–æ•°æ®
train_data, test_data = load_data()

# åˆ›å»ºç½‘ç»œ
net = Network(1)
# å¯åŠ¨è®­ç»ƒ
losses = net.train(train_data, num_epoches=50, batch_size=100, eta=0.1)

# ç”»å‡ºæŸå¤±å‡½æ•°çš„å˜åŒ–è¶‹åŠ¿
plot_x = np.arange(len(losses))
plot_y = np.array(losses)
plt.plot(plot_x, plot_y)
plt.show()

```

    Epoch   0 / iter   0, loss = 0.0457
    Epoch   0 / iter   1, loss = 0.0349
    Epoch   0 / iter   2, loss = 0.0227
    Epoch   0 / iter   3, loss = 0.0205
    Epoch   0 / iter   4, loss = 0.0479
    Epoch   0 / iter   5, loss = 0.0283
    Epoch   0 / iter   6, loss = 0.0271
    Epoch   1 / iter   0, loss = 0.0258
    Epoch   1 / iter   1, loss = 0.0310
    Epoch   1 / iter   2, loss = 0.0381
    Epoch   1 / iter   3, loss = 0.0366
    Epoch   1 / iter   4, loss = 0.0269
    Epoch   1 / iter   5, loss = 0.0319
    Epoch   1 / iter   6, loss = 0.0269
    Epoch   2 / iter   0, loss = 0.0330
    Epoch   2 / iter   1, loss = 0.0396
    Epoch   2 / iter   2, loss = 0.0272
    Epoch   2 / iter   3, loss = 0.0297
    Epoch   2 / iter   4, loss = 0.0292
    Epoch   2 / iter   5, loss = 0.0209
    Epoch   2 / iter   6, loss = 0.0284
    Epoch   3 / iter   0, loss = 0.0279
    Epoch   3 / iter   1, loss = 0.0408
    Epoch   3 / iter   2, loss = 0.0338
    Epoch   3 / iter   3, loss = 0.0211
    Epoch   3 / iter   4, loss = 0.0238
    Epoch   3 / iter   5, loss = 0.0210
    Epoch   3 / iter   6, loss = 0.0308
    Epoch   4 / iter   0, loss = 0.0215
    Epoch   4 / iter   1, loss = 0.0296
    Epoch   4 / iter   2, loss = 0.0248
    Epoch   4 / iter   3, loss = 0.0350
    Epoch   4 / iter   4, loss = 0.0271
    Epoch   4 / iter   5, loss = 0.0227
    Epoch   4 / iter   6, loss = 0.0300
    Epoch   5 / iter   0, loss = 0.0190
    Epoch   5 / iter   1, loss = 0.0210
    Epoch   5 / iter   2, loss = 0.0239
    Epoch   5 / iter   3, loss = 0.0308
    Epoch   5 / iter   4, loss = 0.0259
    Epoch   5 / iter   5, loss = 0.0356
    Epoch   5 / iter   6, loss = 0.0267
    Epoch   6 / iter   0, loss = 0.0234
    Epoch   6 / iter   1, loss = 0.0218
    Epoch   6 / iter   2, loss = 0.0292
    Epoch   6 / iter   3, loss = 0.0282
    Epoch   6 / iter   4, loss = 0.0182
    Epoch   6 / iter   5, loss = 0.0220
    Epoch   6 / iter   6, loss = 0.0330
    Epoch   7 / iter   0, loss = 0.0216
    Epoch   7 / iter   1, loss = 0.0290
    Epoch   7 / iter   2, loss = 0.0230
    Epoch   7 / iter   3, loss = 0.0152
    Epoch   7 / iter   4, loss = 0.0256
    Epoch   7 / iter   5, loss = 0.0266
    Epoch   7 / iter   6, loss = 0.0275
    Epoch   8 / iter   0, loss = 0.0196
    Epoch   8 / iter   1, loss = 0.0248
    Epoch   8 / iter   2, loss = 0.0182
    Epoch   8 / iter   3, loss = 0.0380
    Epoch   8 / iter   4, loss = 0.0230
    Epoch   8 / iter   5, loss = 0.0190
    Epoch   8 / iter   6, loss = 0.0191
    Epoch   9 / iter   0, loss = 0.0143
    Epoch   9 / iter   1, loss = 0.0277
    Epoch   9 / iter   2, loss = 0.0195
    Epoch   9 / iter   3, loss = 0.0310
    Epoch   9 / iter   4, loss = 0.0241
    Epoch   9 / iter   5, loss = 0.0170
    Epoch   9 / iter   6, loss = 0.0220
    Epoch  10 / iter   0, loss = 0.0157
    Epoch  10 / iter   1, loss = 0.0232
    Epoch  10 / iter   2, loss = 0.0152
    Epoch  10 / iter   3, loss = 0.0276
    Epoch  10 / iter   4, loss = 0.0204
    Epoch  10 / iter   5, loss = 0.0255
    Epoch  10 / iter   6, loss = 0.0221
    Epoch  11 / iter   0, loss = 0.0247
    Epoch  11 / iter   1, loss = 0.0229
    Epoch  11 / iter   2, loss = 0.0133
    Epoch  11 / iter   3, loss = 0.0201
    Epoch  11 / iter   4, loss = 0.0209
    Epoch  11 / iter   5, loss = 0.0212
    Epoch  11 / iter   6, loss = 0.0212
    Epoch  12 / iter   0, loss = 0.0146
    Epoch  12 / iter   1, loss = 0.0139
    Epoch  12 / iter   2, loss = 0.0128
    Epoch  12 / iter   3, loss = 0.0214
    Epoch  12 / iter   4, loss = 0.0219
    Epoch  12 / iter   5, loss = 0.0262
    Epoch  12 / iter   6, loss = 0.0285
    Epoch  13 / iter   0, loss = 0.0310
    Epoch  13 / iter   1, loss = 0.0143
    Epoch  13 / iter   2, loss = 0.0192
    Epoch  13 / iter   3, loss = 0.0200
    Epoch  13 / iter   4, loss = 0.0188
    Epoch  13 / iter   5, loss = 0.0158
    Epoch  13 / iter   6, loss = 0.0147
    Epoch  14 / iter   0, loss = 0.0198
    Epoch  14 / iter   1, loss = 0.0191
    Epoch  14 / iter   2, loss = 0.0173
    Epoch  14 / iter   3, loss = 0.0193
    Epoch  14 / iter   4, loss = 0.0194
    Epoch  14 / iter   5, loss = 0.0148
    Epoch  14 / iter   6, loss = 0.0197
    Epoch  15 / iter   0, loss = 0.0196
    Epoch  15 / iter   1, loss = 0.0172
    Epoch  15 / iter   2, loss = 0.0227
    Epoch  15 / iter   3, loss = 0.0231
    Epoch  15 / iter   4, loss = 0.0138
    Epoch  15 / iter   5, loss = 0.0174
    Epoch  15 / iter   6, loss = 0.0109
    Epoch  16 / iter   0, loss = 0.0215
    Epoch  16 / iter   1, loss = 0.0096
    Epoch  16 / iter   2, loss = 0.0194
    Epoch  16 / iter   3, loss = 0.0177
    Epoch  16 / iter   4, loss = 0.0188
    Epoch  16 / iter   5, loss = 0.0188
    Epoch  16 / iter   6, loss = 0.0153
    Epoch  17 / iter   0, loss = 0.0141
    Epoch  17 / iter   1, loss = 0.0142
    Epoch  17 / iter   2, loss = 0.0141
    Epoch  17 / iter   3, loss = 0.0187
    Epoch  17 / iter   4, loss = 0.0160
    Epoch  17 / iter   5, loss = 0.0148
    Epoch  17 / iter   6, loss = 0.0255
    Epoch  18 / iter   0, loss = 0.0156
    Epoch  18 / iter   1, loss = 0.0152
    Epoch  18 / iter   2, loss = 0.0191
    Epoch  18 / iter   3, loss = 0.0214
    Epoch  18 / iter   4, loss = 0.0148
    Epoch  18 / iter   5, loss = 0.0167
    Epoch  18 / iter   6, loss = 0.0102
    Epoch  19 / iter   0, loss = 0.0197
    Epoch  19 / iter   1, loss = 0.0129
    Epoch  19 / iter   2, loss = 0.0154
    Epoch  19 / iter   3, loss = 0.0132
    Epoch  19 / iter   4, loss = 0.0140
    Epoch  19 / iter   5, loss = 0.0183
    Epoch  19 / iter   6, loss = 0.0165
    Epoch  20 / iter   0, loss = 0.0111
    Epoch  20 / iter   1, loss = 0.0181
    Epoch  20 / iter   2, loss = 0.0132
    Epoch  20 / iter   3, loss = 0.0167
    Epoch  20 / iter   4, loss = 0.0126
    Epoch  20 / iter   5, loss = 0.0210
    Epoch  20 / iter   6, loss = 0.0139
    Epoch  21 / iter   0, loss = 0.0126
    Epoch  21 / iter   1, loss = 0.0190
    Epoch  21 / iter   2, loss = 0.0111
    Epoch  21 / iter   3, loss = 0.0183
    Epoch  21 / iter   4, loss = 0.0113
    Epoch  21 / iter   5, loss = 0.0118
    Epoch  21 / iter   6, loss = 0.0196
    Epoch  22 / iter   0, loss = 0.0157
    Epoch  22 / iter   1, loss = 0.0151
    Epoch  22 / iter   2, loss = 0.0092
    Epoch  22 / iter   3, loss = 0.0203
    Epoch  22 / iter   4, loss = 0.0136
    Epoch  22 / iter   5, loss = 0.0123
    Epoch  22 / iter   6, loss = 0.0144
    Epoch  23 / iter   0, loss = 0.0161
    Epoch  23 / iter   1, loss = 0.0141
    Epoch  23 / iter   2, loss = 0.0149
    Epoch  23 / iter   3, loss = 0.0107
    Epoch  23 / iter   4, loss = 0.0111
    Epoch  23 / iter   5, loss = 0.0186
    Epoch  23 / iter   6, loss = 0.0123
    Epoch  24 / iter   0, loss = 0.0164
    Epoch  24 / iter   1, loss = 0.0148
    Epoch  24 / iter   2, loss = 0.0126
    Epoch  24 / iter   3, loss = 0.0111
    Epoch  24 / iter   4, loss = 0.0136
    Epoch  24 / iter   5, loss = 0.0165
    Epoch  24 / iter   6, loss = 0.0100
    Epoch  25 / iter   0, loss = 0.0178
    Epoch  25 / iter   1, loss = 0.0101
    Epoch  25 / iter   2, loss = 0.0115
    Epoch  25 / iter   3, loss = 0.0123
    Epoch  25 / iter   4, loss = 0.0119
    Epoch  25 / iter   5, loss = 0.0160
    Epoch  25 / iter   6, loss = 0.0134
    Epoch  26 / iter   0, loss = 0.0174
    Epoch  26 / iter   1, loss = 0.0112
    Epoch  26 / iter   2, loss = 0.0094
    Epoch  26 / iter   3, loss = 0.0086
    Epoch  26 / iter   4, loss = 0.0119
    Epoch  26 / iter   5, loss = 0.0186
    Epoch  26 / iter   6, loss = 0.0137
    Epoch  27 / iter   0, loss = 0.0088
    Epoch  27 / iter   1, loss = 0.0079
    Epoch  27 / iter   2, loss = 0.0176
    Epoch  27 / iter   3, loss = 0.0099
    Epoch  27 / iter   4, loss = 0.0113
    Epoch  27 / iter   5, loss = 0.0115
    Epoch  27 / iter   6, loss = 0.0220
    Epoch  28 / iter   0, loss = 0.0102
    Epoch  28 / iter   1, loss = 0.0157
    Epoch  28 / iter   2, loss = 0.0114
    Epoch  28 / iter   3, loss = 0.0102
    Epoch  28 / iter   4, loss = 0.0123
    Epoch  28 / iter   5, loss = 0.0127
    Epoch  28 / iter   6, loss = 0.0141
    Epoch  29 / iter   0, loss = 0.0112
    Epoch  29 / iter   1, loss = 0.0159
    Epoch  29 / iter   2, loss = 0.0097
    Epoch  29 / iter   3, loss = 0.0188
    Epoch  29 / iter   4, loss = 0.0106
    Epoch  29 / iter   5, loss = 0.0114
    Epoch  29 / iter   6, loss = 0.0068
    Epoch  30 / iter   0, loss = 0.0129
    Epoch  30 / iter   1, loss = 0.0085
    Epoch  30 / iter   2, loss = 0.0144
    Epoch  30 / iter   3, loss = 0.0155
    Epoch  30 / iter   4, loss = 0.0065
    Epoch  30 / iter   5, loss = 0.0104
    Epoch  30 / iter   6, loss = 0.0147
    Epoch  31 / iter   0, loss = 0.0169
    Epoch  31 / iter   1, loss = 0.0140
    Epoch  31 / iter   2, loss = 0.0094
    Epoch  31 / iter   3, loss = 0.0091
    Epoch  31 / iter   4, loss = 0.0118
    Epoch  31 / iter   5, loss = 0.0078
    Epoch  31 / iter   6, loss = 0.0122
    Epoch  32 / iter   0, loss = 0.0129
    Epoch  32 / iter   1, loss = 0.0135
    Epoch  32 / iter   2, loss = 0.0128
    Epoch  32 / iter   3, loss = 0.0083
    Epoch  32 / iter   4, loss = 0.0114
    Epoch  32 / iter   5, loss = 0.0096
    Epoch  32 / iter   6, loss = 0.0110
    Epoch  33 / iter   0, loss = 0.0131
    Epoch  33 / iter   1, loss = 0.0091
    Epoch  33 / iter   2, loss = 0.0127
    Epoch  33 / iter   3, loss = 0.0089
    Epoch  33 / iter   4, loss = 0.0108
    Epoch  33 / iter   5, loss = 0.0104
    Epoch  33 / iter   6, loss = 0.0130
    Epoch  34 / iter   0, loss = 0.0151
    Epoch  34 / iter   1, loss = 0.0070
    Epoch  34 / iter   2, loss = 0.0078
    Epoch  34 / iter   3, loss = 0.0064
    Epoch  34 / iter   4, loss = 0.0119
    Epoch  34 / iter   5, loss = 0.0124
    Epoch  34 / iter   6, loss = 0.0163
    Epoch  35 / iter   0, loss = 0.0088
    Epoch  35 / iter   1, loss = 0.0144
    Epoch  35 / iter   2, loss = 0.0102
    Epoch  35 / iter   3, loss = 0.0117
    Epoch  35 / iter   4, loss = 0.0106
    Epoch  35 / iter   5, loss = 0.0110
    Epoch  35 / iter   6, loss = 0.0084
    Epoch  36 / iter   0, loss = 0.0145
    Epoch  36 / iter   1, loss = 0.0070
    Epoch  36 / iter   2, loss = 0.0160
    Epoch  36 / iter   3, loss = 0.0096
    Epoch  36 / iter   4, loss = 0.0086
    Epoch  36 / iter   5, loss = 0.0101
    Epoch  36 / iter   6, loss = 0.0080
    Epoch  37 / iter   0, loss = 0.0086
    Epoch  37 / iter   1, loss = 0.0131
    Epoch  37 / iter   2, loss = 0.0100
    Epoch  37 / iter   3, loss = 0.0118
    Epoch  37 / iter   4, loss = 0.0093
    Epoch  37 / iter   5, loss = 0.0115
    Epoch  37 / iter   6, loss = 0.0085
    Epoch  38 / iter   0, loss = 0.0127
    Epoch  38 / iter   1, loss = 0.0088
    Epoch  38 / iter   2, loss = 0.0104
    Epoch  38 / iter   3, loss = 0.0144
    Epoch  38 / iter   4, loss = 0.0096
    Epoch  38 / iter   5, loss = 0.0074
    Epoch  38 / iter   6, loss = 0.0082
    Epoch  39 / iter   0, loss = 0.0068
    Epoch  39 / iter   1, loss = 0.0102
    Epoch  39 / iter   2, loss = 0.0116
    Epoch  39 / iter   3, loss = 0.0139
    Epoch  39 / iter   4, loss = 0.0081
    Epoch  39 / iter   5, loss = 0.0087
    Epoch  39 / iter   6, loss = 0.0113
    Epoch  40 / iter   0, loss = 0.0070
    Epoch  40 / iter   1, loss = 0.0095
    Epoch  40 / iter   2, loss = 0.0086
    Epoch  40 / iter   3, loss = 0.0093
    Epoch  40 / iter   4, loss = 0.0128
    Epoch  40 / iter   5, loss = 0.0112
    Epoch  40 / iter   6, loss = 0.0113
    Epoch  41 / iter   0, loss = 0.0078
    Epoch  41 / iter   1, loss = 0.0096
    Epoch  41 / iter   2, loss = 0.0091
    Epoch  41 / iter   3, loss = 0.0093
    Epoch  41 / iter   4, loss = 0.0125
    Epoch  41 / iter   5, loss = 0.0084
    Epoch  41 / iter   6, loss = 0.0120
    Epoch  42 / iter   0, loss = 0.0117
    Epoch  42 / iter   1, loss = 0.0087
    Epoch  42 / iter   2, loss = 0.0130
    Epoch  42 / iter   3, loss = 0.0099
    Epoch  42 / iter   4, loss = 0.0057
    Epoch  42 / iter   5, loss = 0.0065
    Epoch  42 / iter   6, loss = 0.0123
    Epoch  43 / iter   0, loss = 0.0147
    Epoch  43 / iter   1, loss = 0.0084
    Epoch  43 / iter   2, loss = 0.0072
    Epoch  43 / iter   3, loss = 0.0093
    Epoch  43 / iter   4, loss = 0.0085
    Epoch  43 / iter   5, loss = 0.0121
    Epoch  43 / iter   6, loss = 0.0067
    Epoch  44 / iter   0, loss = 0.0091
    Epoch  44 / iter   1, loss = 0.0061
    Epoch  44 / iter   2, loss = 0.0101
    Epoch  44 / iter   3, loss = 0.0124
    Epoch  44 / iter   4, loss = 0.0067
    Epoch  44 / iter   5, loss = 0.0144
    Epoch  44 / iter   6, loss = 0.0072
    Epoch  45 / iter   0, loss = 0.0144
    Epoch  45 / iter   1, loss = 0.0120
    Epoch  45 / iter   2, loss = 0.0092
    Epoch  45 / iter   3, loss = 0.0066
    Epoch  45 / iter   4, loss = 0.0065
    Epoch  45 / iter   5, loss = 0.0097
    Epoch  45 / iter   6, loss = 0.0069
    Epoch  46 / iter   0, loss = 0.0099
    Epoch  46 / iter   1, loss = 0.0084
    Epoch  46 / iter   2, loss = 0.0094
    Epoch  46 / iter   3, loss = 0.0118
    Epoch  46 / iter   4, loss = 0.0086
    Epoch  46 / iter   5, loss = 0.0080
    Epoch  46 / iter   6, loss = 0.0084
    Epoch  47 / iter   0, loss = 0.0112
    Epoch  47 / iter   1, loss = 0.0089
    Epoch  47 / iter   2, loss = 0.0070
    Epoch  47 / iter   3, loss = 0.0066
    Epoch  47 / iter   4, loss = 0.0101
    Epoch  47 / iter   5, loss = 0.0098
    Epoch  47 / iter   6, loss = 0.0104
    Epoch  48 / iter   0, loss = 0.0104
    Epoch  48 / iter   1, loss = 0.0070
    Epoch  48 / iter   2, loss = 0.0078
    Epoch  48 / iter   3, loss = 0.0053
    Epoch  48 / iter   4, loss = 0.0103
    Epoch  48 / iter   5, loss = 0.0078
    Epoch  48 / iter   6, loss = 0.0151
    Epoch  49 / iter   0, loss = 0.0069
    Epoch  49 / iter   1, loss = 0.0085
    Epoch  49 / iter   2, loss = 0.0062
    Epoch  49 / iter   3, loss = 0.0073
    Epoch  49 / iter   4, loss = 0.0119
    Epoch  49 / iter   5, loss = 0.0104
    Epoch  49 / iter   6, loss = 0.0116



![png](output_14_1.png)


é¦–å…ˆå¯¼å…¥å¿…è¦çš„åŒ…ï¼Œåˆ†åˆ«æ˜¯ï¼š

paddle.fluid--->PaddlePaddleæ·±åº¦å­¦ä¹ æ¡†æ¶

numpy---------->pythonåŸºæœ¬åº“ï¼Œç”¨äºç§‘å­¦è®¡ç®—

os------------------>pythonçš„æ¨¡å—ï¼Œå¯ä½¿ç”¨è¯¥æ¨¡å—å¯¹æ“ä½œç³»ç»Ÿè¿›è¡Œæ“ä½œ

matplotlib----->pythonç»˜å›¾åº“ï¼Œå¯æ–¹ä¾¿ç»˜åˆ¶æŠ˜çº¿å›¾ã€æ•£ç‚¹å›¾ç­‰å›¾å½¢


```python
#åŠ è½½é£æ¡¨ã€Numpyå’Œç›¸å…³ç±»åº“
import paddle
from paddle.nn import Linear
import paddle.nn.functional as F
import numpy as np
import os
import random
```

1ï¼‰uci-housingæ•°æ®é›†ä»‹ç»

æ•°æ®é›†å…±506è¡Œ,æ¯è¡Œ14åˆ—ã€‚å‰13åˆ—ç”¨æ¥æè¿°æˆ¿å±‹çš„å„ç§ä¿¡æ¯ï¼Œæœ€åä¸€åˆ—ä¸ºè¯¥ç±»æˆ¿å±‹ä»·æ ¼ä¸­ä½æ•°ã€‚

PaddlePaddleæä¾›äº†è¯»å–uci_housingè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ¥å£ï¼Œåˆ†åˆ«ä¸ºpaddle.dataset.uci_housing.train()å’Œpaddle.dataset.uci_housing.test()ã€‚

(2)train_readerå’Œtest_reader

paddle.reader.shuffle()è¡¨ç¤ºæ¯æ¬¡ç¼“å­˜BUF_SIZEä¸ªæ•°æ®é¡¹ï¼Œå¹¶è¿›è¡Œæ‰“ä¹±

paddle.batch()è¡¨ç¤ºæ¯BATCH_SIZEç»„æˆä¸€ä¸ªbatch


```python

BUF_SIZE=500
BATCH_SIZE=20

#ç”¨äºè®­ç»ƒçš„æ•°æ®æä¾›å™¨ï¼Œæ¯æ¬¡ä»ç¼“å­˜ä¸­éšæœºè¯»å–æ‰¹æ¬¡å¤§å°çš„æ•°æ®
train_reader = paddle.batch(
    paddle.reader.shuffle(paddle.dataset.uci_housing.train(), 
                          buf_size=BUF_SIZE),                    
    batch_size=BATCH_SIZE)   
#ç”¨äºæµ‹è¯•çš„æ•°æ®æä¾›å™¨ï¼Œæ¯æ¬¡ä»ç¼“å­˜ä¸­éšæœºè¯»å–æ‰¹æ¬¡å¤§å°çš„æ•°æ®
test_reader = paddle.batch(
    paddle.reader.shuffle(paddle.dataset.uci_housing.test(),
                          buf_size=BUF_SIZE),
    batch_size=BATCH_SIZE)  
    
```

    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: [93m
    Warning:
    API "paddle.dataset.uci_housing.train" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.text.datasets.UCIHousing" instead.
    reason: Please use new dataset API which supports paddle.io.DataLoader [0m
      
    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: [93m
    Warning:
    API "paddle.dataset.uci_housing.test" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.text.datasets.UCIHousing" instead.
    reason: Please use new dataset API which supports paddle.io.DataLoader [0m
      # This is added back by InteractiveShellApp.init_path()


æ¨¡å‹è®¾è®¡
æ¨¡å‹å®šä¹‰çš„å®è´¨æ˜¯å®šä¹‰çº¿æ€§å›å½’çš„ç½‘ç»œç»“æ„ï¼Œé£æ¡¨å»ºè®®é€šè¿‡åˆ›å»ºPythonç±»çš„æ–¹å¼å®Œæˆæ¨¡å‹ç½‘ç»œçš„å®šä¹‰ï¼Œè¯¥ç±»éœ€è¦ç»§æ‰¿paddle.nn.Layerçˆ¶ç±»ï¼Œå¹¶ä¸”åœ¨ç±»ä¸­å®šä¹‰initå‡½æ•°å’Œforwardå‡½æ•°ã€‚forwardå‡½æ•°æ˜¯æ¡†æ¶æŒ‡å®šå®ç°å‰å‘è®¡ç®—é€»è¾‘çš„å‡½æ•°ï¼Œç¨‹åºåœ¨è°ƒç”¨æ¨¡å‹å®ä¾‹æ—¶ä¼šè‡ªåŠ¨æ‰§è¡Œforwardæ–¹æ³•ã€‚åœ¨forwardå‡½æ•°ä¸­ä½¿ç”¨çš„ç½‘ç»œå±‚éœ€è¦åœ¨initå‡½æ•°ä¸­å£°æ˜ã€‚

å®ç°è¿‡ç¨‹åˆ†å¦‚ä¸‹ä¸¤æ­¥ï¼š

å®šä¹‰initå‡½æ•°ï¼šåœ¨ç±»çš„åˆå§‹åŒ–å‡½æ•°ä¸­å£°æ˜æ¯ä¸€å±‚ç½‘ç»œçš„å®ç°å‡½æ•°ã€‚åœ¨æˆ¿ä»·é¢„æµ‹æ¨¡å‹ä¸­ï¼Œåªéœ€è¦å®šä¹‰ä¸€å±‚å…¨è¿æ¥å±‚ï¼Œæ¨¡å‹ç»“æ„å’Œä½¿ç”¨Pythonå’ŒNumpyæ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹ã€‹ç« èŠ‚æ¨¡å‹ä¿æŒä¸€è‡´ã€‚

å®šä¹‰forwardå‡½æ•°ï¼šæ„å»ºç¥ç»ç½‘ç»œç»“æ„ï¼Œå®ç°å‰å‘è®¡ç®—è¿‡ç¨‹ï¼Œå¹¶è¿”å›é¢„æµ‹ç»“æœï¼Œåœ¨æœ¬ä»»åŠ¡ä¸­è¿”å›çš„æ˜¯æˆ¿ä»·é¢„æµ‹ç»“æœã€‚


```python
#ç”¨äºæ‰“å°ï¼ŒæŸ¥çœ‹uci_housingæ•°æ®
train_data=paddle.dataset.uci_housing.train();
sampledata=next(train_data())
print(sampledata)

```

    (array([-0.0405441 ,  0.06636364, -0.32356227, -0.06916996, -0.03435197,
            0.05563625, -0.03475696,  0.02682186, -0.37171335, -0.21419304,
           -0.33569506,  0.10143217, -0.21172912]), array([24.]))


    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: [93m
    Warning:
    API "paddle.dataset.uci_housing.train" is deprecated since 2.0.0, and will be removed in future versions. Please use "paddle.text.datasets.UCIHousing" instead.
    reason: Please use new dataset API which supports paddle.io.DataLoader [0m
      


å£°æ˜å®šä¹‰å¥½çš„å›å½’æ¨¡å‹Regressorå®ä¾‹ï¼Œå¹¶å°†æ¨¡å‹çš„çŠ¶æ€è®¾ç½®ä¸ºè®­ç»ƒã€‚
ä½¿ç”¨load_dataå‡½æ•°åŠ è½½è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ã€‚
è®¾ç½®ä¼˜åŒ–ç®—æ³•å’Œå­¦ä¹ ç‡ï¼Œä¼˜åŒ–ç®—æ³•é‡‡ç”¨éšæœºæ¢¯åº¦ä¸‹é™SGDï¼Œå­¦ä¹ ç‡è®¾ç½®ä¸º0.01ã€‚


```python
#å®šä¹‰å¼ é‡å˜é‡xï¼Œè¡¨ç¤º13ç»´çš„ç‰¹å¾å€¼
x = fluid.layers.data(name='x', shape=[13], dtype='float32')
#å®šä¹‰å¼ é‡y,è¡¨ç¤ºç›®æ ‡å€¼
y = fluid.layers.data(name='y', shape=[1], dtype='float32')
#å®šä¹‰ä¸€ä¸ªç®€å•çš„çº¿æ€§ç½‘ç»œ,è¿æ¥è¾“å…¥å’Œè¾“å‡ºçš„å…¨è¿æ¥å±‚
#input:è¾“å…¥tensor;
#size:è¯¥å±‚è¾“å‡ºå•å…ƒçš„æ•°ç›®
#act:æ¿€æ´»å‡½æ•°
y_predict=fluid.layers.fc(input=x,size=1,act=None)
```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    <ipython-input-23-eb477accf04e> in <module>
          1 #å®šä¹‰å¼ é‡å˜é‡xï¼Œè¡¨ç¤º13ç»´çš„ç‰¹å¾å€¼
    ----> 2 x = fluid.layers.data(name='x', shape=[13], dtype='float32')
          3 #å®šä¹‰å¼ é‡y,è¡¨ç¤ºç›®æ ‡å€¼
          4 y = fluid.layers.data(name='y', shape=[1], dtype='float32')
          5 #å®šä¹‰ä¸€ä¸ªç®€å•çš„çº¿æ€§ç½‘ç»œ,è¿æ¥è¾“å…¥å’Œè¾“å‡ºçš„å…¨è¿æ¥å±‚


    NameError: name 'fluid' is not defined


(2)å®šä¹‰æŸå¤±å‡½æ•°

æ­¤å¤„ä½¿ç”¨å‡æ–¹å·®æŸå¤±å‡½æ•°ã€‚

square_error_cost(input,lable):æ¥å—è¾“å…¥é¢„æµ‹å€¼å’Œç›®æ ‡å€¼ï¼Œå¹¶è¿”å›æ–¹å·®ä¼°è®¡,å³ä¸ºï¼ˆy-y_predictï¼‰çš„å¹³æ–¹


```python
cost = fluid.layers.square_error_cost(input=y_predict, label=y) #æ±‚ä¸€ä¸ªbatchçš„æŸå¤±å€¼
avg_cost = fluid.layers.mean(cost)                              #å¯¹æŸå¤±å€¼æ±‚å¹³å‡å€¼
```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    <ipython-input-24-80ffed7a17c3> in <module>
    ----> 1 cost = fluid.layers.square_error_cost(input=y_predict, label=y) #æ±‚ä¸€ä¸ªbatchçš„æŸå¤±å€¼
          2 avg_cost = fluid.layers.mean(cost)                              #å¯¹æŸå¤±å€¼æ±‚å¹³å‡å€¼


    NameError: name 'fluid' is not defined


(3)å®šä¹‰ä¼˜åŒ–å‡½æ•°

æ­¤å¤„ä½¿ç”¨çš„æ˜¯éšæœºæ¢¯åº¦ä¸‹é™ã€‚


```python
optimizer = fluid.optimizer.SGDOptimizer(learning_rate=0.001)
opts = optimizer.minimize(avg_cost)
```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    <ipython-input-25-43b0c1a55dea> in <module>
    ----> 1 optimizer = fluid.optimizer.SGDOptimizer(learning_rate=0.001)
          2 opts = optimizer.minimize(avg_cost)


    NameError: name 'fluid' is not defined



```python
test_program = fluid.default_main_program().clone(for_test=True)
```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    <ipython-input-26-0a06cd4b6937> in <module>
    ----> 1 test_program = fluid.default_main_program().clone(for_test=True)
    

    NameError: name 'fluid' is not defined


åœ¨ä¸Šè¿°æ¨¡å‹é…ç½®å®Œæ¯•åï¼Œå¾—åˆ°ä¸¤ä¸ªfluid.Programï¼šfluid.default_startup_program() ä¸fluid.default_main_program() é…ç½®å®Œæ¯•äº†ã€‚

å‚æ•°åˆå§‹åŒ–æ“ä½œä¼šè¢«å†™å…¥fluid.default_startup_program()

fluid.default_main_program()ç”¨äºè·å–é»˜è®¤æˆ–å…¨å±€main program(ä¸»ç¨‹åº)ã€‚è¯¥ä¸»ç¨‹åºç”¨äºè®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹ã€‚fluid.layers ä¸­çš„æ‰€æœ‰layerå‡½æ•°å¯ä»¥å‘ default_main_program ä¸­æ·»åŠ ç®—å­å’Œå˜é‡ã€‚default_main_program æ˜¯fluidçš„è®¸å¤šç¼–ç¨‹æ¥å£ï¼ˆAPIï¼‰çš„Programå‚æ•°çš„ç¼ºçœå€¼ã€‚ä¾‹å¦‚,å½“ç”¨æˆ·programæ²¡æœ‰ä¼ å…¥çš„æ—¶å€™ï¼Œ Executor.run() ä¼šé»˜è®¤æ‰§è¡Œ default_main_program ã€‚


```python

```


```python
# å‚æ•°ä¸ºä¿å­˜æ¨¡å‹å‚æ•°çš„æ–‡ä»¶åœ°å€
model_dict = paddle.load('LR_model.pdparams')
model.load_dict(model_dict)
model.eval()

# å‚æ•°ä¸ºæ•°æ®é›†çš„æ–‡ä»¶åœ°å€
one_data, label = load_one_example()
# å°†æ•°æ®è½¬ä¸ºåŠ¨æ€å›¾çš„variableæ ¼å¼ 
one_data = paddle.to_tensor(one_data)
predict = model(one_data)

# å¯¹ç»“æœåšåå½’ä¸€åŒ–å¤„ç†
predict = predict * (max_values[-1] - min_values[-1]) + avg_values[-1]
# å¯¹labelæ•°æ®åšåå½’ä¸€åŒ–å¤„ç†
label = label * (max_values[-1] - min_values[-1]) + avg_values[-1]

print("Inference result is {}, the corresponding label is {}".format(predict.numpy(), label))
```


    ---------------------------------------------------------------------------

    ValueError                                Traceback (most recent call last)

    <ipython-input-27-2c02b4d79248> in <module>
          1 # å‚æ•°ä¸ºä¿å­˜æ¨¡å‹å‚æ•°çš„æ–‡ä»¶åœ°å€
    ----> 2 model_dict = paddle.load('LR_model.pdparams')
          3 model.load_dict(model_dict)
          4 model.eval()
          5 


    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/framework/io.py in load(path, **configs)
        838 
        839     else:
    --> 840         load_result = _legacy_load(path, **configs)
        841 
        842     return load_result


    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/framework/io.py in _legacy_load(path, **configs)
        857     else:
        858         # file prefix and directory are compatible cases
    --> 859         model_path, config = _build_load_path_and_config(path, config)
        860         # check whether model file exists
        861         if config.model_filename is None:


    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/framework/io.py in _build_load_path_and_config(path, config)
        159                 "example, it should be written as `paddle.load('model.pdparams')` instead of " \
        160                 "`paddle.load('model')`."
    --> 161         raise ValueError(error_msg % path)
        162     else:
        163         if prefix_format_exist:


    ValueError: The ``path`` (LR_model.pdparams) to load model not exists.


é¦–å…ˆå®šä¹‰è¿ç®—åœºæ‰€ fluid.CPUPlace()å’Œ fluid.CUDAPlace(0)åˆ†åˆ«è¡¨ç¤ºè¿ç®—åœºæ‰€ä¸ºCPUå’ŒGPU

Executor:æ¥æ”¶ä¼ å…¥çš„programï¼Œé€šè¿‡run()æ–¹æ³•è¿è¡Œprogramã€‚


```python
use_cuda = False                         #use_cudaä¸ºFalse,è¡¨ç¤ºè¿ç®—åœºæ‰€ä¸ºCPU;use_cudaä¸ºTrue,è¡¨ç¤ºè¿ç®—åœºæ‰€ä¸ºGPU           
place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
exe = fluid.Executor(place)              #åˆ›å»ºä¸€ä¸ªExecutorå®ä¾‹exe
exe.run(fluid.default_startup_program()) #Executorçš„run()æ–¹æ³•æ‰§è¡Œstartup_program(),è¿›è¡Œå‚æ•°åˆå§‹åŒ–
```

DataFeederè´Ÿè´£å°†æ•°æ®æä¾›å™¨ï¼ˆtrain_reader,test_readerï¼‰è¿”å›çš„æ•°æ®è½¬æˆä¸€ç§ç‰¹æ®Šçš„æ•°æ®ç»“æ„ï¼Œä½¿å…¶å¯ä»¥è¾“å…¥åˆ°Executorä¸­ã€‚

feed_listè®¾ç½®å‘æ¨¡å‹è¾“å…¥çš„å‘å˜é‡è¡¨æˆ–è€…å˜é‡è¡¨å


```python
# å®šä¹‰è¾“å…¥æ•°æ®ç»´åº¦
feeder = fluid.DataFeeder(place=place, feed_list=[x, y])#feed_list:å‘æ¨¡å‹è¾“å…¥çš„å˜é‡è¡¨æˆ–å˜é‡è¡¨å
```


```python
iter=0;
iters=[]
train_costs=[]

def draw_train_process(iters,train_costs):
    title="training cost"
    plt.title(title, fontsize=24)
    plt.xlabel("iter", fontsize=14)
    plt.ylabel("cost", fontsize=14)
    plt.plot(iters, train_costs,color='red',label='training cost') 
    plt.grid()
    plt.show()
```

è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹

Executoræ¥æ”¶ä¼ å…¥çš„program,å¹¶æ ¹æ®feed map(è¾“å…¥æ˜ å°„è¡¨)å’Œfetch_list(ç»“æœè·å–è¡¨) å‘programä¸­æ·»åŠ feed operators(æ•°æ®è¾“å…¥ç®—å­)å’Œfetch operatorsï¼ˆç»“æœè·å–ç®—å­)ã€‚ feed mapä¸ºè¯¥programæä¾›è¾“å…¥æ•°æ®ã€‚fetch_listæä¾›programè®­ç»ƒç»“æŸåç”¨æˆ·é¢„æœŸçš„å˜é‡ã€‚

æ³¨ï¼šenumerate() å‡½æ•°ç”¨äºå°†ä¸€ä¸ªå¯éå†çš„æ•°æ®å¯¹è±¡(å¦‚åˆ—è¡¨ã€å…ƒç»„æˆ–å­—ç¬¦ä¸²)ç»„åˆä¸ºä¸€ä¸ªç´¢å¼•åºåˆ—ï¼ŒåŒæ—¶åˆ—å‡ºæ•°æ®å’Œæ•°æ®ä¸‹æ ‡ï¼Œ


```python
EPOCH_NUM=50
model_save_dir = "/home/aistudio/work/fit_a_line.inference.model"

for pass_id in range(EPOCH_NUM):                                  #è®­ç»ƒEPOCH_NUMè½®
    # å¼€å§‹è®­ç»ƒå¹¶è¾“å‡ºæœ€åä¸€ä¸ªbatchçš„æŸå¤±å€¼
    train_cost = 0
    for batch_id, data in enumerate(train_reader()):              #éå†train_readerè¿­ä»£å™¨
        train_cost = exe.run(program=fluid.default_main_program(),#è¿è¡Œä¸»ç¨‹åº
                             feed=feeder.feed(data),              #å–‚å…¥ä¸€ä¸ªbatchçš„è®­ç»ƒæ•°æ®ï¼Œæ ¹æ®feed_listå’Œdataæä¾›çš„ä¿¡æ¯ï¼Œå°†è¾“å…¥æ•°æ®è½¬æˆä¸€ç§ç‰¹æ®Šçš„æ•°æ®ç»“æ„
                             fetch_list=[avg_cost])    
        if batch_id % 40 == 0:
            print("Pass:%d, Cost:%0.5f" % (pass_id, train_cost[0][0]))    #æ‰“å°æœ€åä¸€ä¸ªbatchçš„æŸå¤±å€¼
        iter=iter+BATCH_SIZE
        iters.append(iter)
        train_costs.append(train_cost[0][0])
       
   
    # å¼€å§‹æµ‹è¯•å¹¶è¾“å‡ºæœ€åä¸€ä¸ªbatchçš„æŸå¤±å€¼
    test_cost = 0
    for batch_id, data in enumerate(test_reader()):               #éå†test_readerè¿­ä»£å™¨
        test_cost= exe.run(program=test_program, #è¿è¡Œæµ‹è¯•cheng
                            feed=feeder.feed(data),               #å–‚å…¥ä¸€ä¸ªbatchçš„æµ‹è¯•æ•°æ®
                            fetch_list=[avg_cost])                #fetchå‡æ–¹è¯¯å·®
    print('Test:%d, Cost:%0.5f' % (pass_id, test_cost[0][0]))     #æ‰“å°æœ€åä¸€ä¸ªbatchçš„æŸå¤±å€¼
    
    #ä¿å­˜æ¨¡å‹
    # å¦‚æœä¿å­˜è·¯å¾„ä¸å­˜åœ¨å°±åˆ›å»º
if not os.path.exists(model_save_dir):
    os.makedirs(model_save_dir)
print ('save models to %s' % (model_save_dir))
#ä¿å­˜è®­ç»ƒå‚æ•°åˆ°æŒ‡å®šè·¯å¾„ä¸­ï¼Œæ„å»ºä¸€ä¸ªä¸“é—¨ç”¨é¢„æµ‹çš„program
fluid.io.save_inference_model(model_save_dir,   #ä¿å­˜æ¨ç†modelçš„è·¯å¾„
                                  ['x'],            #æ¨ç†ï¼ˆinferenceï¼‰éœ€è¦ feed çš„æ•°æ®
                                  [y_predict],      #ä¿å­˜æ¨ç†ï¼ˆinferenceï¼‰ç»“æœçš„ Variables
                                  exe)              #exe ä¿å­˜ inference model
draw_train_process(iters,train_costs)
```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    <ipython-input-30-84b2a524c61a> in <module>
          6     train_cost = 0
          7     for batch_id, data in enumerate(train_reader()):              #éå†train_readerè¿­ä»£å™¨
    ----> 8         train_cost = exe.run(program=fluid.default_main_program(),#è¿è¡Œä¸»ç¨‹åº
          9                              feed=feeder.feed(data),              #å–‚å…¥ä¸€ä¸ªbatchçš„è®­ç»ƒæ•°æ®ï¼Œæ ¹æ®feed_listå’Œdataæä¾›çš„ä¿¡æ¯ï¼Œå°†è¾“å…¥æ•°æ®è½¬æˆä¸€ç§ç‰¹æ®Šçš„æ•°æ®ç»“æ„
         10                              fetch_list=[avg_cost])    


    NameError: name 'exe' is not defined


# **ç®—æ³•æ¯”è¾ƒ**

è‡ªå·±å†™çš„ç®—æ³•çš„æ•ˆæœè¾ƒå·®ï¼Œä½¿ç”¨paddleçš„æ•ˆæœæ¯”è¾ƒå¥½

è‡ªå·±å†™çš„æŸå¤±å‡½æ•°å˜åŒ–æƒ…å†µï¼š

![](https://ai-studio-static-online.cdn.bcebos.com/bd45e68f9729494f916db902010f983ba55a1cee9e754c63a7728b05d5841381)

yå€¼ç‰¹åˆ«å°çš„åŸå› æ˜¯å› ä¸ºæˆ‘å¯¹æ•°æ®ä½œäº†å½’ä¸€åŒ–å¤„ç†

paddleè‡ªå¸¦çš„å‡½æ•°å†™çš„æŸå¤±å‡½æ•°å˜åŒ–æƒ…å†µï¼š

![](https://ai-studio-static-online.cdn.bcebos.com/39cb390e870b48faaaf7487eb000dedf0f848d7d7e044fbfb41cdbe76fa19b4d)

