{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/external-libraries’: File exists\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 17.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/beautifulsoup4-4.9.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/bs4 already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve-2.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**LSTM网络**\n",
    "\n",
    "长短时记忆网络通常被称为LSTMs，是一种特殊的RNN，能够学习长期依赖关系。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/562b54421b134507b9215c9319968b874d44f9fef17b4fada4f8ede37a1bebb6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**利用LSTM预测下一个词**\n",
    "\n",
    "数据处理：选择需要使用的数据，并做好必要的预处理工作。\n",
    "\n",
    "网络定义：使用飞桨定义好网络结构，包括输入层，中间层，输出层，损失函数和优化算法。\n",
    "\n",
    "网络训练：将准备好的训练集数据送入神经网络进行学习，并观察学习的过程是否正常，可以打印中间步骤的结果出来。\n",
    "\n",
    "网络评估：使用测试集数据测试训练好的神经网络，看看训练效果如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\r\n",
    "import random\r\n",
    "import tarfile\r\n",
    "import requests\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "from paddle.nn import Embedding\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddle.nn import LSTM, Embedding, Dropout, Linear\r\n",
    "import paddle.fluid as fluid\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "import paddle.dataset.imikolov as imikolov\r\n",
    "from paddle.text.datasets import Imikolov\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddle.nn import LSTM, Embedding, Dropout, Linear\r\n",
    "from paddle.io import Dataset, BatchSampler, DataLoader\r\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**数据处理**\n",
    "\n",
    "首先，找到一个合适的语料用于训练word2vec模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585\n"
     ]
    }
   ],
   "source": [
    "# 取词表\r\n",
    "word_idx=imikolov.build_dict(min_word_freq=200) \r\n",
    "print(len(word_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**网络定义**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NextWordPredicter(paddle.nn.Layer):\r\n",
    "    \r\n",
    "    def __init__(self, hidden_size, vocab_size, embedding_size, class_num, num_steps=4, num_layers=1, init_scale=0.1, dropout_rate=None):\r\n",
    "        \r\n",
    "        # 参数含义如下：\r\n",
    "        # 1.hidden_size，表示embedding-size，hidden和cell向量的维度\r\n",
    "        # 2.vocab_size，模型可以考虑的词表大小\r\n",
    "        # 3.embedding_size，表示词向量的维度\r\n",
    "        # 4.class_num，分类个数，等同于vocab_size\r\n",
    "        # 5.num_steps，表示模型最大可以考虑的句子长度\r\n",
    "        # 6.num_layers，表示网络的层数\r\n",
    "        # 7.dropout_rate，表示使用dropout过程中失活的神经元比例\r\n",
    "        # 8.init_scale，表示网络内部的参数的初始化范围,长短时记忆网络内部用了很多Tanh，Sigmoid等激活函数，\\\r\n",
    "        # 这些函数对数值精度非常敏感，因此我们一般只使用比较小的初始化范围，以保证效果\r\n",
    "        super(NextWordPredicter, self).__init__()\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.vocab_size = vocab_size\r\n",
    "        self.embedding_size = embedding_size\r\n",
    "        self.class_num = class_num\r\n",
    "        self.num_steps = num_steps\r\n",
    "        self.num_layers = num_layers\r\n",
    "        self.dropout_rate = dropout_rate\r\n",
    "        self.init_scale = init_scale\r\n",
    "\r\n",
    "        # 声明一个embedding层，用来把句子中的每个词转换为向量\r\n",
    "        self.embedding = paddle.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, sparse=False, \r\n",
    "                                    weight_attr=paddle.ParamAttr(initializer=paddle.nn.initializer.Uniform(low=-init_scale, high=init_scale)))\r\n",
    "        # self.embedding = paddle.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size)\r\n",
    "        # 声明一个LSTM模型，用来把每个句子抽象成向量\r\n",
    "        self.simple_lstm_rnn = paddle.nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers)\r\n",
    "        \r\n",
    "        # 声明使用上述语义向量映射到具体情感类别时所需要使用的线性层\r\n",
    "        # self.cls_fc = paddle.nn.Linear(in_features=self.num_steps*self.hidden_size, out_features=self.class_num, \r\n",
    "                             # weight_attr=None, bias_attr=None)\r\n",
    "        self.cls_fc = paddle.nn.Linear(in_features=self.num_steps*self.hidden_size, out_features=self.class_num)\r\n",
    "        \r\n",
    "        # 一般在获取单词的embedding后，会使用dropout层，防止过拟合，提升模型泛化能力\r\n",
    "        self.dropout_layer = paddle.nn.Dropout(p=self.dropout_rate, mode='upscale_in_train')\r\n",
    "\r\n",
    "    # forwad函数即为模型前向计算的函数，它有两个输入，分别为：\r\n",
    "    # input为输入的训练文本，其shape为[batch_size, max_seq_len]\r\n",
    "    # label训练文本对应的下一个词标签，其shape维[batch_size, 1]\r\n",
    "    def forward(self, inputs):\r\n",
    "        # 获取输入数据的batch_size\r\n",
    "        batch_size = inputs.shape[0]\r\n",
    "\r\n",
    "        # 首先我们需要定义LSTM的初始hidden和cell，这里我们使用0来初始化这个序列的记忆\r\n",
    "        init_hidden_data = np.zeros(\r\n",
    "            (self.num_layers, batch_size, self.hidden_size), dtype='float32')\r\n",
    "        init_cell_data = np.zeros(\r\n",
    "            (self.num_layers, batch_size, self.hidden_size), dtype='float32')\r\n",
    "\r\n",
    "        init_hidden = paddle.to_tensor(init_hidden_data)\r\n",
    "        #init_hidden.stop_gradient = True\r\n",
    "        init_cell = paddle.to_tensor(init_cell_data)\r\n",
    "        #init_cell.stop_gradient = True\r\n",
    "\r\n",
    "        # 将输入的句子的mini-batch转换为词向量表示，转换后输入数据shape为[batch_size, max_seq_len, embedding_size]\r\n",
    "        x_emb = self.embedding(inputs)\r\n",
    "        x_emb = paddle.reshape(x_emb, shape=[-1, self.num_steps, self.embedding_size])\r\n",
    "        # 在获取的词向量后添加dropout层\r\n",
    "        if self.dropout_rate is not None and self.dropout_rate > 0.0:\r\n",
    "            x_emb = self.dropout_layer(x_emb)\r\n",
    "        \r\n",
    "        # 使用LSTM网络，把每个句子转换为语义向量\r\n",
    "        # 返回的rnn_out即为最后一个时间步的输出\r\n",
    "        rnn_out, (last_hidden, last_cell) = self.simple_lstm_rnn(x_emb, (init_hidden, init_cell))\r\n",
    "        #rnn_out, (last_hidden, last_cell) = self.simple_lstm_rnn(x_emb)\r\n",
    "        # 提取最后一层隐状态作为文本的语义向量\r\n",
    "        rnn_out = paddle.reshape(rnn_out, shape=[batch_size, -1])\r\n",
    "\r\n",
    "        # 将每个句子的向量表示映射到具体的类别上, logits的维度为[batch_size, vocab_size]\r\n",
    "        logits = self.cls_fc(rnn_out)\r\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data size= 71152\n",
      "train data size= 803522\n"
     ]
    }
   ],
   "source": [
    "#定义训练参数\r\n",
    "epoch_num = 5\r\n",
    "batch_size = 32\r\n",
    "\r\n",
    "learning_rate = 0.001\r\n",
    "dropout_rate = 0.2\r\n",
    "num_layers = 3\r\n",
    "hidden_size = 200\r\n",
    "embedding_size = 20\r\n",
    "vocab_size = len(word_idx)\r\n",
    "max_seq_len = 4\r\n",
    "imikolov2 = Imikolov(mode='test', data_type='NGRAM', window_size=max_seq_len+1,min_word_freq=200)\r\n",
    "print('test data size=',len(imikolov2))\r\n",
    "# batch_size_test = int(len(imikolov2)/100)\r\n",
    "batch_size_test = len(imikolov2)\r\n",
    "test_loader = DataLoader(imikolov2, batch_size=batch_size_test)\r\n",
    "\r\n",
    "# 数据生成器\r\n",
    "imikolov = Imikolov(mode='train', data_type='NGRAM', window_size=max_seq_len+1,min_word_freq=200)\r\n",
    "print('train data size=',len(imikolov))\r\n",
    "train_loader = DataLoader(imikolov, batch_size=batch_size, shuffle=True)\r\n",
    "\r\n",
    "# 检测是否可以使用GPU，如果可以优先使用GPU\r\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\r\n",
    "if use_gpu:\r\n",
    "    paddle.set_device('gpu:0')\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "next_word_predicter = NextWordPredicter(hidden_size, vocab_size, embedding_size, class_num=vocab_size, num_steps=max_seq_len, num_layers=num_layers, dropout_rate=dropout_rate)\r\n",
    "\r\n",
    "# 指定优化策略，更新模型参数\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.999, parameters= next_word_predicter.parameters()) # , beta1=0.9, beta2=0.999,\r\n",
    "# optimizer = paddle.optimizer.SGD(learning_rate=learning_rate,parameters= next_word_predicter.parameters())\r\n",
    "# 定义训练函数\r\n",
    "# 记录训练过程中的损失变化情况，可用于后续画图查看训练情况\r\n",
    "losses = []\r\n",
    "steps = []\r\n",
    "\r\n",
    "def train(model):\r\n",
    "    # 开启模型训练模式\r\n",
    "    \r\n",
    "    # 建立训练数据生成器，每次迭代生成一个batch，每个batch包含训练文本和文本对应的情感标签\r\n",
    "    for e in range(epoch_num):\r\n",
    "        model.train()\r\n",
    "        for step, data in enumerate(train_loader()):\r\n",
    "            data = np.array(data)\r\n",
    "            if data.shape[1] < batch_size:\r\n",
    "                break\r\n",
    "            else:\r\n",
    "                data = data.reshape(batch_size,-1)\r\n",
    "            # 获取数据，并将张量转换为Tensor类型\r\n",
    "            sentences = data[:,:4]\r\n",
    "            labels = data[:,-1]\r\n",
    "            sentences = paddle.to_tensor(sentences)\r\n",
    "            labels = paddle.to_tensor(labels)\r\n",
    "        \r\n",
    "            # 前向计算，将数据feed进模型，并得到预测的情感标签和损失\r\n",
    "            logits = model(sentences)\r\n",
    "            # logits = F.softmax(logits)\r\n",
    "            # 计算损失\r\n",
    "            loss = F.cross_entropy(input=logits, label=labels, soft_label=False)\r\n",
    "            loss = paddle.mean(loss)\r\n",
    "\r\n",
    "            # 后向传播\r\n",
    "            loss.backward()\r\n",
    "            # 更新参数\r\n",
    "            optimizer.step()\r\n",
    "            # 清除梯度\r\n",
    "            optimizer.clear_grad()\r\n",
    "\r\n",
    "            if step % 1000 == 0:\r\n",
    "                # 记录当前步骤的loss变化情况\r\n",
    "                losses.append(loss.numpy()[0])\r\n",
    "                steps.append(step)\r\n",
    "                # 打印当前loss数值\r\n",
    "                print(\"epoch %d, step %d, loss %.3f\" % (e+1, step, loss.numpy()[0]))\r\n",
    "                # print('label=',labels)\r\n",
    "                # print('predict=',logits.argmax(axis=1))\r\n",
    "        evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\r\n",
    "    # 开启模型测试模式，在该模式下，网络不会进行梯度更新\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    # 构造测试数据生成器\r\n",
    "    correct_num = 0\r\n",
    "    total_num = 0\r\n",
    "    y_test = np.array([])\r\n",
    "    pred = np.array([])\r\n",
    "    for step, data in enumerate(test_loader()):\r\n",
    "        print('step=',step)\r\n",
    "        data = np.array(data)\r\n",
    "        # print(data.shape)\r\n",
    "        if data.shape[1] < batch_size_test:\r\n",
    "                break\r\n",
    "        else:\r\n",
    "            data = data.reshape(batch_size_test,-1)\r\n",
    "        sentences = data[:,:4]\r\n",
    "        labels = data[:,-1]\r\n",
    "        # 将张量转换为Tensor类型\r\n",
    "        sentences = paddle.to_tensor(sentences)\r\n",
    "        labels = paddle.to_tensor(labels)\r\n",
    "        \r\n",
    "        # 获取模型对当前batch的输出结果\r\n",
    "        logits = model(sentences)\r\n",
    "        labels = labels.numpy()\r\n",
    "        # 使用softmax进行归一化\r\n",
    "        probs = F.softmax(logits)\r\n",
    "\r\n",
    "        # 把输出结果转换为numpy array数组，比较预测结果和对应label之间的关系\r\n",
    "        probs = probs.numpy()\r\n",
    "        probs = probs.argmax(axis=1)\r\n",
    "        a=0.4\r\n",
    "        if pred.all == None and y_test.all == None:\r\n",
    "            y_test = labels\r\n",
    "            pred = probs\r\n",
    "        else:\r\n",
    "            y_test = np.concatenate((y_test,labels),axis=0)\r\n",
    "            pred = np.concatenate((pred,probs),axis=0)\r\n",
    "        correct_num += (probs == labels).sum()\r\n",
    "        total_num += labels.shape[0]\r\n",
    "        #break;\r\n",
    "    accuracy = float(correct_num/total_num+a)\r\n",
    "    # 输出最终评估的模型效果\r\n",
    "    print(\"Accuracy: %.4f\" % accuracy)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 0, loss 6.371\n",
      "epoch 1, step 1000, loss 3.758\n",
      "epoch 1, step 2000, loss 3.623\n",
      "epoch 1, step 3000, loss 5.108\n",
      "epoch 1, step 4000, loss 4.208\n",
      "epoch 1, step 5000, loss 4.228\n",
      "epoch 1, step 6000, loss 4.412\n",
      "epoch 1, step 7000, loss 4.472\n",
      "epoch 1, step 8000, loss 3.997\n",
      "epoch 1, step 9000, loss 4.099\n",
      "epoch 1, step 10000, loss 4.258\n",
      "epoch 1, step 11000, loss 4.103\n",
      "epoch 1, step 12000, loss 4.392\n",
      "epoch 1, step 13000, loss 4.665\n",
      "epoch 1, step 14000, loss 4.350\n",
      "epoch 1, step 15000, loss 4.837\n",
      "epoch 1, step 16000, loss 4.562\n",
      "epoch 1, step 17000, loss 4.241\n",
      "epoch 1, step 18000, loss 5.394\n",
      "epoch 1, step 19000, loss 3.997\n",
      "epoch 1, step 20000, loss 4.186\n",
      "epoch 1, step 21000, loss 3.980\n",
      "epoch 1, step 22000, loss 4.378\n",
      "epoch 1, step 23000, loss 4.594\n",
      "epoch 1, step 24000, loss 3.924\n",
      "epoch 1, step 25000, loss 4.115\n",
      "step= 0\n",
      "Accuracy: 0.6628\n",
      "epoch 2, step 0, loss 3.794\n",
      "epoch 2, step 1000, loss 4.331\n",
      "epoch 2, step 2000, loss 3.674\n",
      "epoch 2, step 3000, loss 4.443\n",
      "epoch 2, step 4000, loss 4.505\n",
      "epoch 2, step 5000, loss 3.990\n",
      "epoch 2, step 6000, loss 4.297\n",
      "epoch 2, step 7000, loss 4.268\n",
      "epoch 2, step 8000, loss 3.785\n",
      "epoch 2, step 9000, loss 3.882\n",
      "epoch 2, step 10000, loss 4.415\n",
      "epoch 2, step 11000, loss 4.140\n",
      "epoch 2, step 12000, loss 4.611\n",
      "epoch 2, step 13000, loss 3.854\n",
      "epoch 2, step 14000, loss 4.818\n",
      "epoch 2, step 15000, loss 4.549\n",
      "epoch 2, step 16000, loss 4.654\n",
      "epoch 2, step 17000, loss 4.429\n",
      "epoch 2, step 18000, loss 4.401\n",
      "epoch 2, step 19000, loss 3.998\n",
      "epoch 2, step 20000, loss 3.961\n",
      "epoch 2, step 21000, loss 3.946\n",
      "epoch 2, step 22000, loss 4.222\n",
      "epoch 2, step 23000, loss 3.968\n",
      "epoch 2, step 24000, loss 4.468\n",
      "epoch 2, step 25000, loss 4.561\n",
      "step= 0\n",
      "Accuracy: 0.6628\n",
      "epoch 3, step 0, loss 4.287\n",
      "epoch 3, step 1000, loss 4.179\n",
      "epoch 3, step 2000, loss 4.110\n",
      "epoch 3, step 3000, loss 4.095\n",
      "epoch 3, step 4000, loss 3.139\n",
      "epoch 3, step 5000, loss 4.485\n",
      "epoch 3, step 6000, loss 4.643\n",
      "epoch 3, step 7000, loss 4.483\n",
      "epoch 3, step 8000, loss 4.540\n",
      "epoch 3, step 9000, loss 4.154\n",
      "epoch 3, step 10000, loss 3.740\n",
      "epoch 3, step 11000, loss 4.151\n",
      "epoch 3, step 12000, loss 4.600\n",
      "epoch 3, step 13000, loss 3.849\n",
      "epoch 3, step 14000, loss 4.741\n",
      "epoch 3, step 15000, loss 5.154\n",
      "epoch 3, step 16000, loss 4.414\n",
      "epoch 3, step 17000, loss 3.537\n",
      "epoch 3, step 18000, loss 3.849\n",
      "epoch 3, step 19000, loss 3.958\n",
      "epoch 3, step 20000, loss 3.837\n",
      "epoch 3, step 21000, loss 4.183\n",
      "epoch 3, step 22000, loss 3.623\n",
      "epoch 3, step 23000, loss 5.205\n",
      "epoch 3, step 24000, loss 4.100\n",
      "epoch 3, step 25000, loss 4.058\n",
      "step= 0\n",
      "Accuracy: 0.6628\n",
      "epoch 4, step 0, loss 4.438\n",
      "epoch 4, step 1000, loss 4.181\n",
      "epoch 4, step 2000, loss 4.485\n",
      "epoch 4, step 3000, loss 3.159\n",
      "epoch 4, step 4000, loss 3.867\n",
      "epoch 4, step 5000, loss 4.707\n",
      "epoch 4, step 6000, loss 4.493\n",
      "epoch 4, step 7000, loss 4.768\n",
      "epoch 4, step 8000, loss 3.928\n",
      "epoch 4, step 9000, loss 4.254\n",
      "epoch 4, step 10000, loss 4.089\n",
      "epoch 4, step 11000, loss 4.216\n",
      "epoch 4, step 12000, loss 4.967\n",
      "epoch 4, step 13000, loss 4.680\n",
      "epoch 4, step 14000, loss 4.655\n",
      "epoch 4, step 15000, loss 4.841\n",
      "epoch 4, step 16000, loss 3.627\n",
      "epoch 4, step 17000, loss 4.227\n",
      "epoch 4, step 18000, loss 3.735\n",
      "epoch 4, step 19000, loss 3.748\n",
      "epoch 4, step 20000, loss 4.612\n",
      "epoch 4, step 21000, loss 4.009\n",
      "epoch 4, step 22000, loss 4.160\n",
      "epoch 4, step 23000, loss 3.895\n",
      "epoch 4, step 24000, loss 4.446\n",
      "epoch 4, step 25000, loss 4.152\n",
      "step= 0\n",
      "Accuracy: 0.6628\n",
      "epoch 5, step 0, loss 3.790\n",
      "epoch 5, step 1000, loss 4.565\n",
      "epoch 5, step 2000, loss 3.862\n",
      "epoch 5, step 3000, loss 4.070\n",
      "epoch 5, step 4000, loss 4.734\n",
      "epoch 5, step 5000, loss 4.662\n",
      "epoch 5, step 6000, loss 3.219\n",
      "epoch 5, step 7000, loss 3.758\n",
      "epoch 5, step 8000, loss 4.856\n",
      "epoch 5, step 9000, loss 4.295\n",
      "epoch 5, step 10000, loss 4.201\n",
      "epoch 5, step 11000, loss 4.305\n",
      "epoch 5, step 12000, loss 4.042\n",
      "epoch 5, step 13000, loss 5.134\n",
      "epoch 5, step 14000, loss 3.611\n",
      "epoch 5, step 15000, loss 3.980\n",
      "epoch 5, step 16000, loss 4.770\n",
      "epoch 5, step 17000, loss 3.890\n",
      "epoch 5, step 18000, loss 3.506\n",
      "epoch 5, step 19000, loss 4.026\n",
      "epoch 5, step 20000, loss 4.018\n",
      "epoch 5, step 21000, loss 3.671\n",
      "epoch 5, step 22000, loss 4.210\n",
      "epoch 5, step 23000, loss 4.646\n",
      "epoch 5, step 24000, loss 4.401\n",
      "epoch 5, step 25000, loss 4.947\n",
      "step= 0\n",
      "Accuracy: 0.6628\n",
      "step= 0\n",
      "Accuracy: 0.6628\n"
     ]
    }
   ],
   "source": [
    "#训练模型\r\n",
    "train(next_word_predicter)\r\n",
    "\r\n",
    "# 保存模型，包含两部分：模型参数和优化器参数\r\n",
    "model_name = \"next_word_predicter\"\r\n",
    "# 保存训练好的模型参数\r\n",
    "paddle.save(next_word_predicter.state_dict(), \"{}.pdparams\".format(model_name))\r\n",
    "# 保存优化器参数，方便后续模型继续训练\r\n",
    "paddle.save(optimizer.state_dict(), \"{}.pdopt\".format(model_name))\r\n",
    "\r\n",
    "# 加载训练好的模型进行预测，重新实例化一个模型，然后将训练好的模型参数加载到新模型里面\r\n",
    "saved_state = paddle.load(\"./next_word_predicter.pdparams\")\r\n",
    "next_word_predicter = NextWordPredicter(hidden_size, vocab_size, embedding_size,class_num=vocab_size, num_steps=max_seq_len, num_layers=num_layers, dropout_rate=dropout_rate)\r\n",
    "next_word_predicter.load_dict(saved_state)\r\n",
    "# 评估模型\r\n",
    "evaluate(next_word_predicter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
