词向量训练
在自然语言处理任务中，词向量是表示自然语言里单词的一种方法，即把每个词都表示为一个N维空间内的点，即一个高维空间内的向量。通过这种方法，实现把自然语言计算转换为向量计算。
如 图1 所示的词向量计算任务中，先把每个词（如queen，king等）转换成一个高维空间的向量，这些向量在一定意义上可以代表这个词的语义信息。再通过计算这些向量之间的距离，就可以计算出词语之间的关联关系，从而达到让计算机像计算数值一样去计算自然语言的目的。

image 1 （图1：词向量计算示意图）

word2vec包含两个经典模型，CBOW（Continuous Bag-of-Words）和Skip-gram，如 图2 所示。
•	CBOW：通过上下文的词向量推理中心词。
•	Skip-gram：根据中心词推理上下文。

image 2 （图2：CBOW和Skip-gram语义学习示意图）

CBOW的算法实现
我们以这句话：“Pineapples are spiked and yellow”为例介绍CBOW算法实现。
如 图3 所示，CBOW是一个具有3层结构的神经网络，分别是：

image 3（图3：CBOW的算法实现）

•	输入层： 一个形状为C×V的one-hot张量，其中C代表上线文中词的个数，通常是一个偶数，我们假设为4；V表示词表大小，我们假设为5000，该张量的每一行都是一个上下文词的one-hot向量表示，比如“Pineapples, are, and, yellow”。
•	隐藏层： 一个形状为V×N的参数张量W1，一般称为word-embedding，N表示每个词的词向量长度，我们假设为128。输入张量和word embedding W1进行矩阵乘法，就会得到一个形状为C×N的张量。综合考虑上下文中所有词的信息去推理中心词，因此将上下文中C个词相加得一个1×N的向量，是整个上下文的一个隐含表示。
•	输出层： 创建另一个形状为N×V的参数张量，将隐藏层得到的1×N的向量乘以该N×V的参数张量，得到了一个形状为1×V的向量。最终，1×V的向量代表了使用上下文去推理中心词，每个候选词的打分，再经过softmax函数的归一化，即得到了对中心词的推理概率：
𝑠𝑜𝑓𝑡𝑚𝑎𝑥(Oi)=exp(Oi)∑jexp(Oj)𝑠𝑜𝑓𝑡𝑚𝑎𝑥({O_i})= \frac{exp({O_i})}{\sum_jexp({O_j})}softmax(Oi)=∑jexp(Oj)exp(Oi)
在实际操作中，使用一个滑动窗口（一般情况下，长度是奇数），从左到右开始扫描当前句子。每个扫描出来的片段被当成一个小句子，每个小句子中间的词被认为是中心词，其余的词被认为是这个中心词的上下文。
CBOW的实际实现
在实际中，为避免过于庞大的计算量，我们通常采用负采样的方法，来避免查询整个此表，从而将多分类问题转换为二分类问题。具体实现过程如图6：

image 4 （图4：CBOW算法的实际实现）

在实现的过程中，通常会让模型接收3个tensor输入：
•	代表上下文单词的tensor：假设我们称之为context_words VVV，一般来说，这个tensor是一个形状为[batch_size, vocab_size]的one-hot tensor，表示在一个mini-batch中每个中心词具体的ID。
•	代表目标词的tensor：假设我们称之为target_words TTT，一般来说，这个tensor同样是一个形状为[batch_size, vocab_size]的one-hot tensor，表示在一个mini-batch中每个目标词具体的ID。
•	代表目标词标签的tensor：假设我们称之为labels LLL，一般来说，这个tensor是一个形状为[batch_size, 1]的tensor，每个元素不是0就是1（0：负样本，1：正样本）。
