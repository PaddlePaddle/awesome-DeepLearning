# 7月13日下午作业

## 目标检测中正负样本不平衡问题

### 一、概念介绍

​		当前基于深度学习的目标检测主要包括：基于two-stage的目标检测和基于one-stage的目标检测。two-stage的目标检测框架一般检测精度相对较高，但检测速度慢；而one-stage的目标检测速度相对较快，但是检测精度相对较低。**one-stage的精度不如two-stage的精度，一个主要的原因是****训练过程中样本极度不均衡造成的**

​		目标检测任务中，样本包括哪些类别

- 正样本：**标签区域内**的图像区域，即目标图像块
- 负样本：**标签区域以外**的图像区域，即图像背景区域
- 易分正样本：容易正确分类的正样本，在实际训练过程中，**该类占总体样本的比重非常高，单个样本的损失函数较小**，但是累计的损失函数会主导损失函数
- 易分负样本：容易正确分类的负样本，在实际训练过程中，**该类占的比重非常高**，单个样本的损失函数较小，但是累计的损失函数会主导损失函数
- 难分正样本：错分成负样本的正样本，这部分样本在训练过程中单个样本的损失函数较高，但是**该类占总体样本的比例较小**
- 难分负样本：错分成正样本的负样本，这部分样本在训练过程中单个样本的损失函数教高，但是该类占总体样本的比例教小

**1.正负样本不均衡**

​		以Faster RCNN为例，在RPN部分会生成20000个左右的Anchor，由于一张图中通常有10个左右的物体，**导致可能只有100个左右的Anchor会是正样本，正负样本比例约为1∶200，存在严重的不均衡。**

​		对于目标检测算法，主要需要关注的是对应着真实物体的**正样本**，在训练时会根据其loss来调整网络参数。相比之下，**负样本对应着图像的背景，如果有大量的负样本参与训练，则会淹没正样本的损失，从而降低网络收敛的效率与检测精度。**

目前，解决样本不均衡问题的一些思路：

​		机器学习中，解决样本不均衡问题主要有2种思路：数据角度和算法角度。从**数据角度**出发，有**扩大数据集、数据类别均衡采样**等方法。在算法层面，目标检测方法使用的方法主要有：

- Faster RCNN、SSD等算法在正负样本的筛选时，根据样本与真实物体的IoU大小，设置了**3∶1的正负样本比例**，这一点缓解了正负样本的不均衡，同时也对难易样本不均衡起到了作用。

- Faster RCNN在RPN模块中，**通过前景得分排序筛选出了2000个左右的候选框，这也会将大量的负样本与简单样本过滤掉，**缓解了前两个不均衡问题。

- **权重惩罚**：对于难易样本与类别间的不均衡，可以增大难样本与少类别的损失权重，从而增大模型对这些样本的惩罚，缓解不均衡问题。

- **数据增强**：从数据侧入手，可以在当前数据集上使用随机生成和添加扰动的方法，也可以利用网络爬虫数据等增加数据集的丰富性，从而缓解难易样本和类别间样本等不均衡问题，可以参考SSD的数据增强方法。

### 二、方法介绍

​		近年来，不少的研究者针对样本不均衡问题进行了深入研究，比较典型的有OHEM（在线困难样本挖掘）、S-OHEM、Focal Loss、GHM（梯度均衡化）。下面将详细介绍：  

#### 1.OHEM：在线难例挖掘

​		OHEM算法（online hard example miniing，发表于2016年的CVPR）主要是**针对训练过程中的困难样本自动选择**，其核心思想是**根据输入样本的损失进行筛选，筛选出困难样本（即对分类和检测影响较大的样本），然后将筛选得到的这些样本应用在随机梯度下降中训练。**

​		传统的Fast RCNN系列算法在正负样本选择的时候采用当前RoI与真实物体的IoU阈值比较的方法，这样容易忽略一些较为重要的难负样本，并且固定了正、负样本的比例与最大数量，显然不是最优的选择。以此为出发点，**OHEM将交替训练与SGD优化方法进行了结合，在每张图片的RoI中选择了较难的样本，实现了在线的难样本挖掘。**

![图片](https://mmbiz.qpic.cn/mmbiz_png/Z8w2ExrFgDwwQnhxYMbrQIkGyF0DUrWrS4IGqUrhSEkHqzvlUp6Sc0pFYW9uudhLkibWuwacdf4mG9PJ6meLD4w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

OHEM实现在线难样本挖掘的网络如上图所示。图中包含了两个相同的RCNN网络，上半部的a部分是**只可读的网络，只进行前向运算**；下半部的b网络**即可读也可写，需要完成前向计算与反向传播**。

 

在一个batch的训练中，基于Fast RCNN的OHEM算法可以分为以下5步：

（1）按照原始Fast RCNN算法，经过卷积提取网络与RoI Pooling**得到了每一张图像的RoI**。

（2）上半部的a网络对所有的RoI进行**前向计算**，得到**每一个RoI的损失**。

（3）对RoI的损失进行**排序**，进行一步**NMS操作，以去除掉重叠严重的RoI**，并**在筛选后的RoI中选择出固定数量损失较大的部分，作为难样本。**

（4）将筛选出的难样本**输入到可读写的b网络中**，进行前向计算，得到损失。

（5）利用b网络**得到的反向传播更新网络**，并将更新后的参数与上半部的a网络同步，完成一次迭代

**优点**：

1) 对于数据的类别不平衡问题不需要采用设置正负样本比例的方式来解决，这种**在线选择方式针对性更强**；

２) 随着**数据集的增大**，算法的提升更加明显；

**缺点**：

只保留loss较高的样本，完全忽略简单的样本，这**本质上是改变了训练时的输入分布（仅包含困难样本），这会导致模型在学习的时候失去对简单样本的判别能力**。

#### 2.**S-OHEM：基于loss分布采样的在线困难样本挖掘**

​		在OHEM中定义的多任务损失函数，在整个训练过程中各类损失具有相同的权重，这种方法**忽略了训练过程中不同损失类型的影响**，例如在训练期的后期，定位损失更为重要，因此OHEM**缺乏对定位精度的足够关注**。

S-OHEM算法采用了**分层抽样**的方法，**根据loss的分布抽样训练样本**。它的做法是：

首先将预设loss的**四个分段**：   

给定一个batch，先生成输入batch中**所有图像的候选RoI**，再将这些RoI送入到**Read only RoI网络**得到RoIs的损失，然后**将每个RoI计算损失并划分到上面四个分段中**，然后针对每个分段，通过排序筛选困难样本．再将经过筛选的RoIs送入反向传播，用于更新网络参数。

S-OHEM是基于OHEM的改进，如上图所示。网络分成两个部分：ConvNet和RoINet。RoINet又可看成两部分：**Read-only RoI Network和Standard RoI Network。**图中R表示向前传播的RoI的数量，B表示被馈送到反向传播的子采样的RoI的数量。S-OHEMiner**根据当前训练阶段的采样分布对region proposals进行抽样**。Read-only RoI Network只有forward操作，Standard RoI Network包括forward和backward操作，以hard example作为输入，计算损失并回传梯度。

RoINet的**两部分共享权重，可实现高效地分配内存**。在图中，蓝色箭头表示向前传播的过程，绿色箭头表示反向传播过程。

**优点**：



相比原生OHEM，S-OHEM考虑了**基于不同损失函数的分布来抽样选择困难样本，避免了仅使用高损失的样本来更新模型参数。**

**缺点**：

因为不同阶段，分类损失和定位损失的贡献不同，所以选择损失中的两个参数 **需要根据不同训练阶段进行改变**，当应用与不同数据集时，参数的选取也是不一样的。即**引入了额外的超参数**。

#### 3.**Focal loss：专注难样本**

 		当前一阶的物体检测算法，如SSD和YOLO等虽然实现了实时的速度，但精度始终无法与两阶的Faster RCNN相比。是什么阻碍了一阶算法的高精度呢？何凯明等人将其归咎于**正、负样本的不均衡**，并基于此提出了**新的损失函数Focal Loss及网络结构RetinaNet**，在与同期一阶网络速度相同的前提下，其检测精度比同期最优的二阶网络还要高。

​		对于SSD等一阶网络，由于其需要直接从所有的预选框中进行筛选，即使使用了固定正、负样本比例的方法，仍然效率低下，**简单的负样本仍然占据主要地位**，导致其精度不如两阶网络。为了解决一阶网络中样本的不均衡问题，何凯明等人**首先改善了分类过程中的交叉熵函数，提出了可以动态调整权重的Focal Loss**。

Focal Loss为了同时调节正、负样本与难易样本，提出了如下所示的损失函数。



![图片](https://mmbiz.qpic.cn/mmbiz_png/Z8w2ExrFgDwwQnhxYMbrQIkGyF0DUrWrvicVcbFBicaGnyJN6uxicreluiaJ4Oc3tTxTs0h3STDWfsedrfqbTv5fHA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



其中 ![图片](https://mmbiz.qpic.cn/mmbiz_svg/wcib2GksmGOnjuLbic8saoId6k5jrNhicPT5v2LzBlrunFmuIypr0u6v80QA3RmADHCytjQd7a9sbmlxnD7AVzQfYuiaCoRcbxDD/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 用于**控制正负样本的权重**，当其取比较小的值来降低负样本（多的那类样本）的权重； ![图片](https://mmbiz.qpic.cn/mmbiz_svg/wcib2GksmGOnjuLbic8saoId6k5jrNhicPTviaFLEbdMcIFMoR1nmPHf8gMtHRMfiaevxH7MibdEibx1Fvg9ibeAibVlcicFkZebMianVnQ/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 用于**控制难易样本的权重**，目的是**通过减少易分样本的权重，从而使得模型在训练的时候更加专注难分样本的学习。**文中通过批量实验统计得到当 ![图片](https://mmbiz.qpic.cn/mmbiz_svg/wcib2GksmGOnjuLbic8saoId6k5jrNhicPTVLjNc82av9E9m6x4JZ4kpxvBSdXTiboCz29ncxXygJTcztibFsUMYjibNGOvudRbnhU/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 时效果最好。



