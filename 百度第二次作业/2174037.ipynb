{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、深度学习基础知识\n",
    "**1、损失函数方法补充**\n",
    "\n",
    "\n",
    "**(1)0-1损失函数(zero-one loss)**\n",
    "\t\t  \n",
    "0-1损失是指预测值和目标值不相等为1， 否则为0:\n",
    "<br></br>\n",
    "<img src=\"https://www.zhihu.com/equation?tex=L+%28+Y+%2C+f+%28+X+%29+%29+%3D+%5Cleft%5C%7B+%5Cbegin%7Barray%7D+%7B+l+%7D+%7B+1+%2C+Y+%5Cneq+f+%28+X+%29+%7D+%5C%5C+%7B+0+%2C+Y+%3D+f+%28+X+%29+%7D+%5Cend%7Barray%7D+%5Cright.+++%5C%5C\" width=\"500\" hegiht=\"\">  \n",
    "</center>  \n",
    "<br></br>\n",
    "  \t特点：\n",
    "\n",
    "(1)0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用.\n",
    "\n",
    "(2)感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足\n",
    "<img src=\"https://www.zhihu.com/equation?tex=%7CY+-+f%28x%29%7C+%3C+T\" width=\"100\" hegiht=\"\">  时认为相等.\n",
    "\n",
    "\n",
    "**(2)绝对值损失函数**\n",
    "\n",
    "绝对值损失函数是计算预测值与目标值的差的绝对值：\n",
    "<br></br>\n",
    "<img src=\"https://www.zhihu.com/equation?tex=L%28Y%2C+f%28x%29%29+%3D+%7CY+-+f%28x%29%7C++%5C%5C\" width=\"500\" hegiht=\"\">  \n",
    "</center>  \n",
    "<br></br>\n",
    "\n",
    "**(3)负log对数损失函数**\n",
    "\n",
    "该 OP 对输入的预测结果和目标标签进行计算，返回负对数损失值。\n",
    "<br></br>\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/75c8a37a00484043979532e4af35afecb0c0f1762e674c07929b8b7ab94b89b7\" width=\"500\" hegiht=\"\">  \n",
    "</center>  \n",
    "<br></br>\n",
    "特点：\n",
    "\n",
    "(1) log对数损失函数能非常好的表征概率分布，在很多场景尤其是多分类，如果需要知道结果属于每个类别的置信度，那它非常适合。\n",
    "\n",
    "(2)健壮性不强，相比于hinge loss对噪声更敏感。\n",
    "\n",
    "(3)逻辑回归的损失函数就是log对数损失函数。\n",
    "\n",
    "```python\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "label = paddle.randn((10,1))\n",
    "prob = paddle.randn((10,1))\n",
    "cost = F.log_loss(input=prob, label=label)\n",
    "\n",
    "```\n",
    "\n",
    "**(4)指数损失函数（exponential loss）**\n",
    "\n",
    "指数损失函数的标准形式如下：\n",
    "<br></br>\n",
    "<img src=\"https://www.zhihu.com/equation?tex=L%28Y%7Cf%28X%29%29+%3D+exp%5B-yf%28x%29%5D++%5C%5C\" width=\"500\" hegiht=\"\">  \n",
    "</center>  \n",
    "<br></br>\n",
    "特点：\n",
    "\n",
    "(1)对离群点、噪声非常敏感。经常用在AdaBoost算法中。\n",
    "\n",
    "\n",
    "**(5)Hinge 损失函数**\n",
    "\n",
    "Hinge损失函数标准形式如下：\n",
    "<br></br>\n",
    "<img src=\"https://www.zhihu.com/equation?tex=L%28y%2C+f%28x%29%29+%3D+max%280%2C+1-yf%28x%29%29+++%5C%5C\" width=\"500\" hegiht=\"\">  \n",
    "</center>  \n",
    "<br></br>\t\n",
    "特点：\n",
    "\n",
    "(1)hinge损失函数表示如果被分类正确，损失为0，否则损失就为<img src=\"https://www.zhihu.com/equation?tex=1-yf%28x%29\" width=\"70\" hegiht=\"\">  。SVM就是使用这个损失函数。\n",
    "\n",
    "(2)一般的f(x) 是预测值，在-1到1之间， y是目标值(-1或1)。其含义是，f(x) 的值在-1和+1之间就可以了，并不鼓励 |f(x)|>1 ，即并不鼓励分类器过度自信，让某个正确分类的样本距离分割线超过1并不会有任何奖励，从而使分类器可以更专注于整体的误差。\n",
    "\n",
    "(3) 健壮性相对较高，对异常点、噪声不敏感，但它没太好的概率解释。\n",
    "```python\n",
    "def update_weights_Hinge(m1, m2, b, X1, X2, Y, learning_rate):\n",
    "    m1_deriv = 0\n",
    "    m2_deriv = 0\n",
    "    b_deriv = 0\n",
    "    N = len(X1)\n",
    "    for i in range(N):\n",
    "        # 计算偏导数\n",
    "        if Y[i]*(m1*X1[i] + m2*X2[i] + b) <= 1:\n",
    "            m1_deriv += -X1[i] * Y[i]\n",
    "            m2_deriv += -X2[i] * Y[i]\n",
    "            b_deriv += -Y[i]\n",
    "        # 否则偏导数为0\n",
    "    # 我们减去它，因为导数指向最陡的上升方向\n",
    "    m1 -= (m1_deriv / float(N)) * learning_rate\n",
    "    m2 -= (m2_deriv / float(N)) * learning_rate\n",
    "    b -= (b_deriv / float(N)) * learning_rate\n",
    "return m1, m2, b\n",
    "```\n",
    "\n",
    "**(6)感知损失(perceptron loss)函数**\n",
    "\n",
    "感知损失函数的标准形式如下：\n",
    "<br></br>\n",
    "<img src=\"https://www.zhihu.com/equation?tex=L%28y%2C+f%28x%29%29+%3D+max%280%2C+-f%28x%29%29++%5C%5C\" width=\"500\" hegiht=\"\">  \n",
    "</center>  \n",
    "<br></br>\t\n",
    "特点：\n",
    "\n",
    "(1)是Hinge损失函数的一个变种，Hinge loss对判定边界附近的点(正确端)惩罚力度很高。而perceptron loss只要样本的判定类别正确的话，它就满意，不管其判定边界的距离。它比Hinge loss简单，因为不是max-margin boundary，所以模型的泛化能力没 hinge loss强。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**3、池化方法补充**\n",
    "\n",
    "\n",
    "**(1)一般池化(General Pooling)**\n",
    "\t\t  \n",
    "<br></br>\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/28b614d1b58249e1b09b849fc63df3c957c80cc34acc4045b77c585285fb2262\" width=\"400\" hegiht=\"\">  \n",
    "</center>  \n",
    "<br></br>\n",
    "  \t\n",
    "   池化作用于图像中不重合的区域（与卷积操作不同），定义池化窗口的大小为sizeX，即图中红色正方形的边长，定义两个相邻池化窗口的水平位移 / 竖直位移为stride。一般池化由于每一池化窗口都是不重复的，所以sizeX=stride。\n",
    "   \n",
    " **(2)随机池化(Stochastic Pooling)**\n",
    " \n",
    " <br></br>\n",
    "<img src=\" https://ai-studio-static-online.cdn.bcebos.com/65d71436a1b542f6b9e5bae420b98e45533157499e0a46bb8485dc924866420e\" width=\"400\" hegiht=\"\"> \n",
    "</center>  \n",
    "<br></br>\n",
    "\n",
    "Stochastic pooling是一种简单有效的正则化CNN的方法，能够降低max pooling的过拟合现象，提高泛化能力。对于pooling层的输入，根据输入的多项式分布随机选择一个值作为输出。训练阶段和测试阶段的操作略有不同。\n",
    "\n",
    "训练阶段：\n",
    "\n",
    "1）前向传播：先将池化窗口中的元素全部除以它们的和，得到概率矩阵；再按照概率随机选中的方格的值，作为该区域池化后的值。\n",
    "\n",
    "2）反向传播：求导时，只需保留前向传播中已经被选中节点的位置的值，其它值都为0，类似max-pooling的反向传播。\n",
    "\n",
    "测试阶段：\n",
    "\n",
    "在测试时也使用Stochastic Pooling会对预测值引入噪音，降低性能。取而代之的是使用概率矩阵加权平均。比使用Average Pooling表现要好一些。在平均意义上，与Average Pooling近似，在局部意义上，服从Max Pooling准则。\n",
    "\n",
    "求值示例：[https://www.cnblogs.com/tornadomeet/archive/2013/11/19/3432093.html](http://)\n",
    "\n",
    " **(3)重叠池化(Overlapping Pooling)**\n",
    " \n",
    " 重叠池化，即相邻池化窗口之间会有重叠区域。如果定义池化窗口的大小为sizeX，定义两个相邻池化窗口的水平位移 / 竖直位移为stride，此时sizeX>stride。\n",
    "\n",
    "Alexnet中提出和使用，不仅可以提升预测精度，同时一定程度上可以减缓过拟合。相比于正常池化（步长s=2，窗口x=2），重叠池化(步长s=2，窗口x=3) 可以减少top-1, top-5的错误率分别为0.4% 和0.3%。\n",
    "\n",
    " **(4)混合池化(Mixed Pooling)**\n",
    " <br></br>\n",
    "<img src=\"https://img-blog.csdnimg.cn/20190413174230942.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI1NjE0Nzcz,size_16,color_FFFFFF,t_70\" width=\"500\" hegiht=\"\"> \n",
    "</center>  \n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**4、数据增强方法修改及补充**\n",
    "\n",
    "### 图像\n",
    "**(1)对比度拉升**\n",
    "\n",
    "采用了线性函数对图像的灰度值进行变换\n",
    "\n",
    "**(2)Gamma校正**\n",
    "\n",
    "采用了非线性函数（指数函数）对图像的灰度值进行变换\n",
    "\n",
    "这两种方式的实质是对感兴趣的图像区域进行展宽，对不感兴趣的背景区域进行压缩，从而达到图像增强的效果\n",
    "\n",
    "**(3)直方图均衡化**\n",
    "\n",
    "将原始图像的直方图通过积分概率密度函数转化为概率密度为1（理想情况）的图像，从而达到提高对比度的作用。直方图均衡化的实质也是一种特定区域的展宽，但是会导致整个图像向亮的区域变换。当原始图像给定时，对应的直方图均衡化的效果也相应的确定了。\n",
    "\n",
    "**(4)直方图规定化**\n",
    "\n",
    "针对直方图均衡化的存在的一些问题，将原始图像的直方图转化为规定的直方图的形式。一般目标图像的直方图的确定需要参考原始图像的直方图，并利用多高斯函数得到。\n",
    "\n",
    "**(5)同态滤波器**\n",
    "\n",
    "图像的灰度图像f(x,y)可以看做为入射光分量和反射光分量两部分组成：f(x,y)=i(x,y)r(x,y).入射光比较的均匀，随着空间位置变化比较小，占据低频分量段。反射光由于物体性质和结构特点不同从而反射强弱很不相同的光，随着空间位置的变化比较的剧烈。占据着高频分量。基于图像是由光照谱和反射谱结合而成的原理设计的。\n",
    "\n",
    "### 其他\n",
    "**色彩抖动**\n",
    "\n",
    "在实际工程中为了消除图像在不同背景中存在的差异性，通常会做一些色彩抖动操作，扩充数据集合。色彩抖动主要是在图像的颜色方面做增强，主要调整的是图像的亮度，饱和度和对比度。工程中不是任何数据集都适用，通常如果不同背景的图像较多，加入色彩抖动操作会有很好的提升。\n",
    "\n",
    "**几何变换类**\n",
    "\n",
    "几何变换类即对图像进行几何变换，包括翻转，旋转，裁剪，变形，缩放等各类操作。\n",
    "\n",
    "**颜色变换**\n",
    "\n",
    "包括噪声、模糊、颜色变换、擦除、填充等等。\n",
    "\n",
    "**GAN**\n",
    "\n",
    "通过生成对抗网络生成同类型的数据。比如生成汽车、人脸图片。通过图像风格迁移的手段，还可以生成同一物体再不同环境下的图片。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**5、图像分类方法综述**\n",
    "\n",
    "### 传统方法\n",
    "\n",
    "**SVM支持向量机**\n",
    "\n",
    "支持向量机（SVM）是一种强大而灵活的有监督机器学习算法是多维空间中超平面上不同类的表示。目标是分裂将数据集分成类，寻找最大边缘超平面。它建立了一个超平面或一组高维空间中的超平面和两类之间的良好分离是通过到任何类中最近的训练数据点距离最大的超平面。真正的力量该算法的性能取决于所使用的核函数。\n",
    "\n",
    "**KNN**\n",
    "\n",
    "K-近邻（K-NN）是一种非参数的惰性学习算法，用于分类和分类回归。该算法简单地依赖于特征向量和分类器之间的距离通过在k-最近的例子中找到最常见的类来获得未知的数据点。\n",
    "\n",
    "**BP 神经网络**\n",
    "\n",
    "BP（Back Propagation）网络是1986年由Rumelhart和McCelland为首的科学家小组提出，是一种按误差逆传播算法训练的多层前馈网络。它的学习规则是使用最速下降法，通过反向传播来不断调整网络的权值和阈值，使网络的误差平方和最小。BP神经网络模型拓扑结构包括输入层（input）、隐层(hide layer)和输出层(output layer)。\n",
    "\n",
    "### 深度学习方法\n",
    "\n",
    "**卷积神经网络**\n",
    "\n",
    "卷积神经网络（CNN，或ConvNet）是一种多层神经网络，旨在通过最少的预处理直接从像素图像中识别视觉模式。这是一个特殊的人工神经网络结构。它包括两个重要的元素，即卷积层和池化层。\n",
    "\n",
    "\n",
    "**迁移学习**\n",
    "\n",
    "转移学习是一种机器学习技术，首先在机器上训练神经网络模型与正在解决的问题类似的问题，并且存储在解决过程中获得的知识解决一个问题并将其应用于不同但相关的问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# MNIST手写数字识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### （一）准备数据\n",
    "\n",
    "(1)数据集介绍\n",
    "\n",
    "MNIST数据集包含60000个训练集和10000测试数据集。分为图片和标签，图片是28*28的像素矩阵，标签为0~9共10个数字。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/fc73217ae57f451a89badc801a903bb742e42eabd9434ecc8089efe19a66c076)\n",
    "\n",
    "(2)transform函数是定义了一个归一化标准化的标准\n",
    "\n",
    "(3)train_dataset和test_dataset\n",
    "\n",
    "paddle.vision.datasets.MNIST()中的mode='train'和mode='test'分别用于获取mnist训练集和测试集\n",
    "\n",
    "transform=transform参数则为归一化标准"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### （二）搭建网络\n",
    "-----\n",
    "本是一共使用两个网络inceptionV1和Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### inceptionV1\n",
    "\n",
    "Inception v1的网络，将1x1，3x3，5x5的conv和3x3的pooling，堆叠在一起，一方面增加了网络的width，另一方面增加了网络对尺度的适应性；\n",
    "\n",
    "由图： \t\n",
    "<br></br>\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/b2bfa4201a1b4e77bb343339785378af2523f7155b144177925745958db09712\" width=\"400\" hegiht=\"\">  \n",
    "</center>  \n",
    "<br></br>\n",
    "\n",
    "Inception v1的亮点：\n",
    "\n",
    "1.卷积层共有的一个功能，可以实现通道方向的降维和增维，至于是降还是增，取决于卷积层的通道数（滤波器个数），在Inception v1中1*1卷积用于降维，减少weights大小和feature map维度。\n",
    "\n",
    "2.1*1卷积特有的功能，由于1*1卷积只有一个参数，相当于对原始feature map做了一个scale，并且这个scale还是训练学出来的，无疑会对识别精度有提升。\n",
    "\n",
    "3.增加了网络的深度\n",
    "\n",
    "4.增加了网络的宽度\n",
    "\n",
    "5.同时使用了1*1，3*3，5*5的卷积，增加了网络对尺度的适应性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Resnet\n",
    "ResNet网络是参考了VGG19网络，在其基础上进行了修改，并通过短路机制加入了残差单元，如图5所示。变化主要体现在ResNet直接使用stride=2的卷积做下采样，并且用global average pool层替换了全连接层。ResNet的一个重要设计原则是：当feature map大小降低一半时，feature map的数量增加一倍，这保持了网络层的复杂度。从图5中可以看到，ResNet相比普通网络每两层间增加了短路机制，这就形成了残差学习，其中虚线表示feature map数量发生了改变。图5展示的34-layer的ResNet，还可以构建更深的网络如表1所示。从表中可以看到，对于18-layer和34-layer的ResNet，其进行的两层间的残差学习，当网络更深时，其进行的是三层间的残差学习，三层卷积核分别是1x1，3x3和1x1，一个值得注意的是隐含层的feature map数量是比较小的，并且是输出feature map数量的1/4。\n",
    "<br></br>\n",
    "<img src=\"https://pic2.zhimg.com/80/v2-7cb9c03871ab1faa7ca23199ac403bd9_720w.jpg\" width=\"400\" hegiht=\"\">  \n",
    "</center>  \n",
    "<br></br>\n",
    "\n",
    "而我们这次使用的是50层的resnet。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 优化器和损失函数\n",
    "\n",
    "优化器使用Adam\n",
    "损失函数为交叉熵损失函数\n",
    "\n",
    "------\n",
    "**Adam**\n",
    "\n",
    "Adam 是一种可以替代传统随机梯度下降过程的一阶优化算法，它能基于训练数据迭代地更新神经网络权重。\n",
    "\n",
    "2014年12月，Kingma和Lei Ba两位学者提出了Adam优化器，结合AdaGrad和RMSProp两种优化算法的优点。对梯度的一阶矩估计（First Moment Estimation，即梯度的均值）和二阶矩估计（Second Moment Estimation，即梯度的未中心化的方差）进行综合考虑，计算出更新步长。\n",
    "\n",
    "主要包含以下几个显著的优点：\n",
    "\n",
    "1. 实现简单，计算高效，对内存需求少\n",
    "\n",
    "2. 参数的更新不受梯度的伸缩变换影响\n",
    "\n",
    "3. 超参数具有很好的解释性，且通常无需调整或仅需很少的微调\n",
    "\n",
    "4. 更新的步长能够被限制在大致的范围内（初始学习率）\n",
    "\n",
    "5. 能自然地实现步长退火过程（自动调整学习率）\n",
    "\n",
    "6. 很适合应用于大规模的数据及参数的场景\n",
    "\n",
    "7. 适用于不稳定目标函数\n",
    "\n",
    "8. 适用于梯度稀疏或梯度存在很大噪声的问题\n",
    "\n",
    "综合Adam在很多情况下算作默认工作性能比较优秀的优化器。\n",
    "\n",
    "------\n",
    "**交叉熵损失函数**\n",
    "\n",
    "对数损失Log Loss ，也被称为交叉熵损失Cross-entropy Loss，是定义在概率分布的基础上的。它通常用于多项式(multinomia)logistic regression 和神经网络，还有在期望极大化算法(expectation-maximization)的一些变体中。\n",
    "\n",
    "对数损失用来度量分类器的预测输出的概率分布(predict_proba)和真实分布的差异，而不是去比较离散的类标签是否相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cache file /home/aistudio/.cache/paddle/dataset/mnist/train-images-idx3-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/train-images-idx3-ubyte.gz \n",
      "Begin to download\n",
      "\n",
      "Download finished\n",
      "Cache file /home/aistudio/.cache/paddle/dataset/mnist/train-labels-idx1-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/train-labels-idx1-ubyte.gz \n",
      "Begin to download\n",
      "........\n",
      "Download finished\n",
      "Cache file /home/aistudio/.cache/paddle/dataset/mnist/t10k-images-idx3-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/t10k-images-idx3-ubyte.gz \n",
      "Begin to download\n",
      "\n",
      "Download finished\n",
      "Cache file /home/aistudio/.cache/paddle/dataset/mnist/t10k-labels-idx1-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/t10k-labels-idx1-ubyte.gz \n",
      "Begin to download\n",
      "..\n",
      "Download finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 938/938 [==============================] - loss: 0.1428 - acc: 0.9242 - 153ms/step        \n",
      "Epoch 2/2\n",
      "step 938/938 [==============================] - loss: 0.1842 - acc: 0.9664 - 154ms/step        \n",
      "Eval begin...\n",
      "step 157/157 [==============================] - loss: 0.0181 - acc: 0.9773 - 39ms/step        \n",
      "Eval samples: 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.018119805], 'acc': 0.9773}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from paddle.vision.transforms import Compose, Normalize\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "from paddle.metric import Accuracy\r\n",
    "import random\r\n",
    "from paddle import fluid\r\n",
    "from visualdl import LogWriter\r\n",
    "\r\n",
    "log_writer=LogWriter(\"./data/log/train\") #log记录器\r\n",
    "\r\n",
    "\r\n",
    "transform = Compose([Normalize(mean=[127.5],\r\n",
    "                               std=[127.5],\r\n",
    "                               data_format='CHW')])\r\n",
    "#归一化\r\n",
    "\r\n",
    "#读取训练集 测试集数据\r\n",
    "train_dataset = paddle.vision.datasets.MNIST(mode='train', transform=transform)\r\n",
    "test_dataset = paddle.vision.datasets.MNIST(mode='test', transform=transform)\r\n",
    "\r\n",
    "class InceptionA(paddle.nn.Layer):  #作为网络一层\r\n",
    "    def __init__(self,in_channels):\r\n",
    "        super(InceptionA,self).__init__()\r\n",
    "        self.branch3x3_1=paddle.nn.Conv2D(in_channels,16,kernel_size=1) #第一个分支\r\n",
    "        self.branch3x3_2=paddle.nn.Conv2D( 16,24,kernel_size=3,padding=1)\r\n",
    "        self.branch3x3_3=paddle.nn.Conv2D(24,24,kernel_size=3,padding=1)\r\n",
    "\r\n",
    "        self.branch5x5_1=paddle.nn.Conv2D(in_channels, 16,kernel_size=1) #第二个分支\r\n",
    "        self.branch5x5_2=paddle.nn.Conv2D( 16,24,kernel_size=5,padding=2)\r\n",
    "\r\n",
    "        self.branch1x1=paddle.nn.Conv2D(in_channels, 16,kernel_size=1) #第三个分支\r\n",
    "\r\n",
    "        self.branch_pool=paddle.nn.Conv2D(in_channels,24,kernel_size= 1) #第四个分支\r\n",
    "\r\n",
    "    def forward(self,x):\r\n",
    "        #分支1处理过程\r\n",
    "        branch3x3= self.branch3x3_1(x)\r\n",
    "        branch3x3= self.branch3x3_2(branch3x3)\r\n",
    "        branch3x3= self.branch3x3_3(branch3x3)\r\n",
    "        #分支2处理过程\r\n",
    "        branch5x5=self.branch5x5_1(x)\r\n",
    "        branch5x5=self.branch5x5_2(branch5x5)\r\n",
    "        #分支3处理过程\r\n",
    "        branch1x1=self.branch1x1(x)\r\n",
    "        #分支4处理过程\r\n",
    "        branch_pool=F.avg_pool2d(x,kernel_size=3,stride=1,padding= 1)\r\n",
    "        branch_pool=self.branch_pool(branch_pool)\r\n",
    "        outputs=[branch1x1,branch5x5,branch3x3,branch_pool]     #将4个分支的输出拼接起来\r\n",
    "        return fluid.layers.concat(outputs,axis=1) #横着拼接， 共有24+24+16+24=88个通道\r\n",
    "\r\n",
    "class Net(paddle.nn.Layer):        #卷积，池化，inception，卷积，池化，inception，全连接\r\n",
    "    def __init__(self):\r\n",
    "        super(Net,self).__init__()\r\n",
    "        #定义两个卷积层\r\n",
    "        self.conv1=paddle.nn.Conv2D(1,10,kernel_size=5)\r\n",
    "        self.conv2=paddle.nn.Conv2D(88,20,kernel_size=5)\r\n",
    "        #Inception模块的输出均为88通道\r\n",
    "        self.incep1=InceptionA(in_channels=10 )\r\n",
    "        self.incep2=InceptionA(in_channels=20)\r\n",
    "        self.mp=paddle.nn.MaxPool2D(2)\r\n",
    "        self.fc=paddle.nn.Linear(1408,10) #5*5* 88 =2200，图像高*宽*通道数\r\n",
    "    def forward(self,x):\r\n",
    "        x=F.relu(self.mp(self.conv1(x)))# 卷积池化，relu  输出x为图像尺寸14*14*10\r\n",
    "        x =self.incep1(x)               #图像尺寸14*14*88\r\n",
    "\r\n",
    "        x =F.relu(self.mp(self.conv2(x)))# 卷积池化，relu  输出x为图像尺寸5*5*20\r\n",
    "        x = self.incep2(x)              #图像尺寸5*5*88\r\n",
    "\r\n",
    "        x = paddle.flatten(x, start_axis=1,stop_axis=-1)\r\n",
    "        x = self.fc(x)\r\n",
    "        return x\r\n",
    "model = paddle.Model(Net())   # 封装模型\r\n",
    "optim = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters()) # adam优化器\r\n",
    "\r\n",
    "# 配置模型\r\n",
    "model.prepare(\r\n",
    "    optim,\r\n",
    "    paddle.nn.CrossEntropyLoss(),\r\n",
    "    Accuracy()\r\n",
    "    )\r\n",
    "# 训练模型\r\n",
    "model.fit(train_dataset,epochs=2,batch_size=64,verbose=1)\r\n",
    "#评估\r\n",
    "model.evaluate(test_dataset, batch_size=64, verbose=1)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_id: 0, loss is: [8.038549], acc is: [0.09375]\n",
      "epoch: 0, batch_id: 200, loss is: [0.17661193], acc is: [0.90625]\n",
      "epoch: 0, batch_id: 400, loss is: [0.08071232], acc is: [0.96875]\n",
      "epoch: 0, batch_id: 600, loss is: [0.08213819], acc is: [0.96875]\n",
      "epoch: 0, batch_id: 800, loss is: [0.15711148], acc is: [0.9375]\n",
      "epoch: 1, batch_id: 0, loss is: [0.06838483], acc is: [0.96875]\n",
      "epoch: 1, batch_id: 200, loss is: [0.13941874], acc is: [0.953125]\n",
      "epoch: 1, batch_id: 400, loss is: [0.2525854], acc is: [0.953125]\n",
      "epoch: 1, batch_id: 600, loss is: [0.07762998], acc is: [0.984375]\n",
      "epoch: 1, batch_id: 800, loss is: [0.21090305], acc is: [0.9375]\n",
      "epoch: 2, batch_id: 0, loss is: [0.02737645], acc is: [0.984375]\n",
      "epoch: 2, batch_id: 200, loss is: [0.14992085], acc is: [0.984375]\n",
      "epoch: 2, batch_id: 400, loss is: [0.12653434], acc is: [0.953125]\n",
      "epoch: 2, batch_id: 600, loss is: [0.07890625], acc is: [0.96875]\n",
      "epoch: 2, batch_id: 800, loss is: [0.0716859], acc is: [0.96875]\n",
      "epoch: 3, batch_id: 0, loss is: [0.0043297], acc is: [1.]\n",
      "epoch: 3, batch_id: 200, loss is: [0.00976956], acc is: [1.]\n",
      "epoch: 3, batch_id: 400, loss is: [0.00391817], acc is: [1.]\n",
      "epoch: 3, batch_id: 600, loss is: [0.17953132], acc is: [0.953125]\n",
      "epoch: 3, batch_id: 800, loss is: [0.02471321], acc is: [0.984375]\n",
      "epoch: 4, batch_id: 0, loss is: [0.00848319], acc is: [1.]\n",
      "epoch: 4, batch_id: 200, loss is: [0.11103165], acc is: [0.96875]\n",
      "epoch: 4, batch_id: 400, loss is: [0.1320315], acc is: [0.953125]\n",
      "epoch: 4, batch_id: 600, loss is: [0.0846152], acc is: [0.984375]\n",
      "epoch: 4, batch_id: 800, loss is: [0.03339806], acc is: [0.984375]\n",
      "epoch: 5, batch_id: 0, loss is: [0.09055959], acc is: [0.96875]\n",
      "epoch: 5, batch_id: 200, loss is: [0.03097768], acc is: [0.984375]\n",
      "epoch: 5, batch_id: 400, loss is: [0.02069858], acc is: [0.984375]\n",
      "epoch: 5, batch_id: 600, loss is: [0.02296819], acc is: [0.984375]\n",
      "epoch: 5, batch_id: 800, loss is: [0.04789659], acc is: [0.96875]\n",
      "epoch: 6, batch_id: 0, loss is: [0.04462878], acc is: [0.984375]\n",
      "epoch: 6, batch_id: 200, loss is: [0.00070979], acc is: [1.]\n",
      "epoch: 6, batch_id: 400, loss is: [0.05100422], acc is: [0.984375]\n",
      "epoch: 6, batch_id: 600, loss is: [0.0143289], acc is: [0.984375]\n",
      "epoch: 6, batch_id: 800, loss is: [0.03134666], acc is: [0.984375]\n",
      "epoch: 7, batch_id: 0, loss is: [0.00357674], acc is: [1.]\n",
      "epoch: 7, batch_id: 200, loss is: [0.04037392], acc is: [0.984375]\n",
      "epoch: 7, batch_id: 400, loss is: [0.00309857], acc is: [1.]\n",
      "epoch: 7, batch_id: 600, loss is: [0.00198244], acc is: [1.]\n",
      "epoch: 7, batch_id: 800, loss is: [0.02160443], acc is: [1.]\n",
      "epoch: 8, batch_id: 0, loss is: [0.00072703], acc is: [1.]\n",
      "epoch: 8, batch_id: 200, loss is: [0.08212629], acc is: [0.96875]\n",
      "epoch: 8, batch_id: 400, loss is: [0.0149163], acc is: [1.]\n",
      "epoch: 8, batch_id: 600, loss is: [0.00219698], acc is: [1.]\n",
      "epoch: 8, batch_id: 800, loss is: [0.10704838], acc is: [0.953125]\n",
      "epoch: 9, batch_id: 0, loss is: [0.00747506], acc is: [1.]\n",
      "epoch: 9, batch_id: 200, loss is: [0.01648147], acc is: [1.]\n",
      "epoch: 9, batch_id: 400, loss is: [0.07135075], acc is: [0.984375]\n",
      "epoch: 9, batch_id: 600, loss is: [0.0258828], acc is: [0.984375]\n",
      "epoch: 9, batch_id: 800, loss is: [0.08392962], acc is: [0.96875]\n",
      "batch_id: 0, loss is: [0.00123495], acc is: [1.]\n",
      "batch_id: 20, loss is: [0.14454414], acc is: [0.96875]\n",
      "batch_id: 40, loss is: [0.14224531], acc is: [0.984375]\n",
      "batch_id: 60, loss is: [0.04787309], acc is: [0.984375]\n",
      "batch_id: 80, loss is: [0.00989278], acc is: [1.]\n",
      "batch_id: 100, loss is: [0.00045036], acc is: [1.]\n",
      "batch_id: 120, loss is: [4.0062594e-05], acc is: [1.]\n",
      "batch_id: 140, loss is: [0.24862386], acc is: [0.953125]\n",
      "正确率为：[1.]\n"
     ]
    }
   ],
   "source": [
    "from paddle.vision.transforms import Compose, Normalize\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "from paddle.metric import Accuracy\r\n",
    "import random\r\n",
    "from paddle import fluid\r\n",
    "from visualdl import LogWriter\r\n",
    "\r\n",
    "log_writer=LogWriter(\"./data/log/train\") #log记录器\r\n",
    "\r\n",
    "\r\n",
    "transform = Compose([Normalize(mean=[127.5],\r\n",
    "                               std=[127.5],\r\n",
    "                               data_format='CHW')])\r\n",
    "#归一化\r\n",
    "\r\n",
    "#读取训练集 测试集数据\r\n",
    "train_dataset = paddle.vision.datasets.MNIST(mode='train', transform=transform)\r\n",
    "test_dataset = paddle.vision.datasets.MNIST(mode='test', transform=transform)\r\n",
    "\r\n",
    "class InceptionA(paddle.nn.Layer):  #作为网络一层\r\n",
    "    def __init__(self,in_channels):\r\n",
    "        super(InceptionA,self).__init__()\r\n",
    "        self.branch3x3_1=paddle.nn.Conv2D(in_channels,16,kernel_size=1) #第一个分支\r\n",
    "        self.branch3x3_2=paddle.nn.Conv2D( 16,24,kernel_size=3,padding=1)\r\n",
    "        self.branch3x3_3=paddle.nn.Conv2D(24,24,kernel_size=3,padding=1)\r\n",
    "\r\n",
    "        self.branch5x5_1=paddle.nn.Conv2D(in_channels, 16,kernel_size=1) #第二个分支\r\n",
    "        self.branch5x5_2=paddle.nn.Conv2D( 16,24,kernel_size=5,padding=2)\r\n",
    "\r\n",
    "        self.branch1x1=paddle.nn.Conv2D(in_channels, 16,kernel_size=1) #第三个分支\r\n",
    "\r\n",
    "        self.branch_pool=paddle.nn.Conv2D(in_channels,24,kernel_size= 1) #第四个分支\r\n",
    "\r\n",
    "    def forward(self,x):\r\n",
    "        #分支1处理过程\r\n",
    "        branch3x3= self.branch3x3_1(x)\r\n",
    "        branch3x3= self.branch3x3_2(branch3x3)\r\n",
    "        branch3x3= self.branch3x3_3(branch3x3)\r\n",
    "        #分支2处理过程\r\n",
    "        branch5x5=self.branch5x5_1(x)\r\n",
    "        branch5x5=self.branch5x5_2(branch5x5)\r\n",
    "        #分支3处理过程\r\n",
    "        branch1x1=self.branch1x1(x)\r\n",
    "        #分支4处理过程\r\n",
    "        branch_pool=F.avg_pool2d(x,kernel_size=3,stride=1,padding= 1)\r\n",
    "        branch_pool=self.branch_pool(branch_pool)\r\n",
    "        outputs=[branch1x1,branch5x5,branch3x3,branch_pool]     #将4个分支的输出拼接起来\r\n",
    "        return fluid.layers.concat(outputs,axis=1) #横着拼接， 共有24+24+16+24=88个通道\r\n",
    "\r\n",
    "class Net(paddle.nn.Layer):        #卷积，池化，inception，卷积，池化，inception，全连接\r\n",
    "    def __init__(self):\r\n",
    "        super(Net,self).__init__()\r\n",
    "        #定义两个卷积层\r\n",
    "        self.conv1=paddle.nn.Conv2D(1,10,kernel_size=5)\r\n",
    "        self.conv2=paddle.nn.Conv2D(88,20,kernel_size=5)\r\n",
    "        #Inception模块的输出均为88通道\r\n",
    "        self.incep1=InceptionA(in_channels=10 )\r\n",
    "        self.incep2=InceptionA(in_channels=20)\r\n",
    "        self.mp=paddle.nn.MaxPool2D(2)\r\n",
    "        self.fc=paddle.nn.Linear(1408,10) #5*5* 88 =2200，图像高*宽*通道数\r\n",
    "    def forward(self,x):\r\n",
    "        x=F.relu(self.mp(self.conv1(x)))# 卷积池化，relu  输出x为图像尺寸14*14*10\r\n",
    "        x =self.incep1(x)               #图像尺寸14*14*88\r\n",
    "\r\n",
    "        x =F.relu(self.mp(self.conv2(x)))# 卷积池化，relu  输出x为图像尺寸5*5*20\r\n",
    "        x = self.incep2(x)              #图像尺寸5*5*88\r\n",
    "\r\n",
    "        x = paddle.flatten(x, start_axis=1,stop_axis=-1)\r\n",
    "        x = self.fc(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "#训练\r\n",
    "def train(model,Batch_size=64):\r\n",
    "    train_loader = paddle.io.DataLoader(train_dataset, batch_size=Batch_size, shuffle=True)\r\n",
    "    model.train()\r\n",
    "    iterator = 0\r\n",
    "    epochs = 10\r\n",
    "    total_steps = (int(50000//Batch_size)+1)*epochs\r\n",
    "    lr = paddle.optimizer.lr.PolynomialDecay(learning_rate=0.01,decay_steps=total_steps,end_lr=0.001)\r\n",
    "    optim = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())\r\n",
    "    # 用Adam作为优化函数\r\n",
    "    for epoch in range(epochs):\r\n",
    "        for batch_id, data in enumerate(train_loader()):\r\n",
    "            x_data = data[0]\r\n",
    "            y_data = data[1]\r\n",
    "            predicts = model(x_data)\r\n",
    "            loss = F.cross_entropy(predicts, y_data)\r\n",
    "            # 计算损失\r\n",
    "            acc = paddle.metric.accuracy(predicts, y_data)\r\n",
    "            loss.backward()\r\n",
    "            if batch_id % 200 == 0:\r\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}, acc is: {}\".format(epoch, batch_id, loss.numpy(), acc.numpy()))\r\n",
    "                log_writer.add_scalar(tag='acc',step=iterator,value=acc.numpy())\r\n",
    "                log_writer.add_scalar(tag='loss',step=iterator,value=loss.numpy())\r\n",
    "                iterator+=200\r\n",
    "            optim.step()\r\n",
    "            optim.clear_grad()\r\n",
    "        paddle.save(model.state_dict(),'./data/checkpoint/mnist_epoch{}'.format(epoch)+'.pdparams')\r\n",
    "        paddle.save(optim.state_dict(),'./data/checkpoint/mnist_epoch{}'.format(epoch)+'.pdopt')\r\n",
    "\r\n",
    "\r\n",
    "#测试\r\n",
    "def test(model):\r\n",
    "    # 加载测试数据集\r\n",
    "    test_loader = paddle.io.DataLoader(test_dataset, places=paddle.CPUPlace(), batch_size=64)\r\n",
    "    model.eval()\r\n",
    "    for batch_id, data in enumerate(test_loader()):\r\n",
    "        x_data = data[0]\r\n",
    "        y_data = data[1]\r\n",
    "        predicts = model(x_data)\r\n",
    "        # 获取预测结果\r\n",
    "        loss = F.cross_entropy(predicts, y_data)\r\n",
    "        acc = paddle.metric.accuracy(predicts, y_data)\r\n",
    "        if batch_id % 20 == 0:\r\n",
    "            print(\"batch_id: {}, loss is: {}, acc is: {}\".format(batch_id, loss.numpy(), acc.numpy()))\r\n",
    "\r\n",
    "#随机抽取100张图片进行测试\r\n",
    "def random_test(model,num=100):\r\n",
    "    select_id = random.sample(range(1, 10000), 100) #生成一百张测试图片的下标\r\n",
    "    test_loader = paddle.io.DataLoader(test_dataset, places=paddle.CPUPlace(), batch_size=64)\r\n",
    "    for batch_id, data in enumerate(test_loader()):\r\n",
    "        x_data = data[0]\r\n",
    "        label = data[1]\r\n",
    "    predicts = model(x_data)\r\n",
    "    #返回正确率\r\n",
    "    acc = paddle.metric.accuracy(predicts, label)\r\n",
    "    print(\"正确率为：{}\".format(acc.numpy()))\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    model = Net()\r\n",
    "    train(model)\r\n",
    "    test(model)\r\n",
    "    random_test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cache file /home/aistudio/.cache/paddle/dataset/mnist/train-images-idx3-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/train-images-idx3-ubyte.gz \n",
      "Begin to download\n",
      "\n",
      "Download finished\n",
      "Cache file /home/aistudio/.cache/paddle/dataset/mnist/train-labels-idx1-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/train-labels-idx1-ubyte.gz \n",
      "Begin to download\n",
      "........\n",
      "Download finished\n",
      "Cache file /home/aistudio/.cache/paddle/dataset/mnist/t10k-images-idx3-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/t10k-images-idx3-ubyte.gz \n",
      "Begin to download\n",
      "\n",
      "Download finished\n",
      "Cache file /home/aistudio/.cache/paddle/dataset/mnist/t10k-labels-idx1-ubyte.gz not found, downloading https://dataset.bj.bcebos.com/mnist/t10k-labels-idx1-ubyte.gz \n",
      "Begin to download\n",
      "..\n",
      "Download finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###: ResNet(\n",
      "  (conv1): Conv2D(1, 64, kernel_size=[7, 7], stride=[2, 2], padding=3, data_format=NCHW)\n",
      "  (bn1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2D(kernel_size=3, stride=2, padding=1)\n",
      "  (layer1): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv2D(64, 64, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "        (1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv2D(256, 64, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): BottleneckBlock(\n",
      "      (conv1): Conv2D(256, 64, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv2D(256, 128, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(128, 128, kernel_size=[3, 3], stride=[2, 2], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(128, 512, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2D(256, 512, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)\n",
      "        (1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv2D(512, 128, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(128, 512, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): BottleneckBlock(\n",
      "      (conv1): Conv2D(512, 128, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(128, 512, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): BottleneckBlock(\n",
      "      (conv1): Conv2D(512, 128, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(128, 512, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv2D(512, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(256, 256, kernel_size=[3, 3], stride=[2, 2], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2D(512, 1024, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)\n",
      "        (1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): BottleneckBlock(\n",
      "      (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): BottleneckBlock(\n",
      "      (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (4): BottleneckBlock(\n",
      "      (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (5): BottleneckBlock(\n",
      "      (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BottleneckBlock(\n",
      "      (conv1): Conv2D(1024, 512, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(512, 512, kernel_size=[3, 3], stride=[2, 2], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(512, 2048, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2D(1024, 2048, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)\n",
      "        (1): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckBlock(\n",
      "      (conv1): Conv2D(2048, 512, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(512, 512, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(512, 2048, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): BottleneckBlock(\n",
      "      (conv1): Conv2D(2048, 512, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "      (conv2): Conv2D(512, 512, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "      (conv3): Conv2D(512, 2048, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (bn3): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2D(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=10, dtype=float32)\n",
      ")\n",
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:641: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 938/938 [==============================] - loss: 0.3125 - acc: 0.9163 - 111ms/step         \n",
      "Epoch 2/2\n",
      "step 938/938 [==============================] - loss: 0.0046 - acc: 0.9684 - 113ms/step        \n",
      "Eval begin...\n",
      "step 157/157 [==============================] - loss: 0.0011 - acc: 0.9768 - 56ms/step        \n",
      "Eval samples: 10000\n"
     ]
    }
   ],
   "source": [
    "from paddle.vision.transforms import Compose, Normalize\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "from paddle.metric import Accuracy\r\n",
    "import random\r\n",
    "from paddle import fluid\r\n",
    "from visualdl import LogWriter\r\n",
    "from paddle.vision.models import ResNet\r\n",
    "from paddle.vision.models.resnet import BottleneckBlock, BasicBlock\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "log_writer=LogWriter(\"./data/log/train\") #log记录器\r\n",
    "\r\n",
    "\r\n",
    "transform = Compose([Normalize(mean=[127.5],\r\n",
    "                               std=[127.5],\r\n",
    "                               data_format='CHW')])\r\n",
    "#归一化\r\n",
    "\r\n",
    "#读取训练集 测试集数据\r\n",
    "train_dataset = paddle.vision.datasets.MNIST(mode='train', transform=transform)\r\n",
    "test_dataset = paddle.vision.datasets.MNIST(mode='test', transform=transform)\r\n",
    "\r\n",
    "# class Net(paddle.nn.Layer):\r\n",
    "#     def __init__(self):\r\n",
    "#         super(Net,self).__init__()\r\n",
    "#         self.layer=ResNet(BottleneckBlock, 50,10).conv1= paddle.nn.Conv2D(1,64,kernel_size=7,stride=2,padding=3)\r\n",
    "#         self.fc = paddle.nn.Linear(1000, 10)\r\n",
    "#     #网络的前向计算过程\r\n",
    "#     def forward(self,x):\r\n",
    "#         x=self.layer(x)\r\n",
    "#         print(x.shape)\r\n",
    "#         x=self.fc(x)\r\n",
    "#         return x\r\n",
    "\r\n",
    "# model = paddle.Model(Net())   # 封装模型\r\n",
    "model = resnet50 = ResNet(BottleneckBlock, 50,10)\r\n",
    "model.conv1= paddle.nn.Conv2D(1,64,kernel_size=7,stride=2,padding=3)\r\n",
    "# print(\"###:\",model)\r\n",
    "model = paddle.Model(model)   # 封装模型\r\n",
    "optim = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters()) # adam优化器\r\n",
    "\r\n",
    "# 配置模型\r\n",
    "model.prepare(\r\n",
    "    optim,\r\n",
    "    paddle.nn.CrossEntropyLoss(),\r\n",
    "    Accuracy()\r\n",
    "    )\r\n",
    "# 训练模型\r\n",
    "model.fit(train_dataset,epochs=2,batch_size=64,verbose=1)\r\n",
    "#评估\r\n",
    "model.evaluate(test_dataset, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_id: 0, loss is: [4.352549], acc is: [0.03125]\n",
      "epoch: 0, batch_id: 200, loss is: [0.17206733], acc is: [0.9375]\n",
      "epoch: 0, batch_id: 400, loss is: [0.37500668], acc is: [0.9375]\n",
      "epoch: 0, batch_id: 600, loss is: [0.05142215], acc is: [0.96875]\n",
      "epoch: 0, batch_id: 800, loss is: [0.02845992], acc is: [0.984375]\n",
      "epoch: 1, batch_id: 0, loss is: [0.12472294], acc is: [0.96875]\n",
      "epoch: 1, batch_id: 200, loss is: [0.02218988], acc is: [1.]\n",
      "epoch: 1, batch_id: 400, loss is: [0.0769475], acc is: [0.96875]\n",
      "epoch: 1, batch_id: 600, loss is: [0.07940727], acc is: [0.984375]\n",
      "epoch: 1, batch_id: 800, loss is: [2.2348113], acc is: [0.828125]\n",
      "epoch: 2, batch_id: 0, loss is: [0.12253131], acc is: [0.953125]\n",
      "epoch: 2, batch_id: 200, loss is: [0.04239178], acc is: [0.984375]\n",
      "epoch: 2, batch_id: 400, loss is: [0.26888686], acc is: [0.953125]\n",
      "epoch: 2, batch_id: 600, loss is: [0.05288016], acc is: [0.984375]\n",
      "epoch: 2, batch_id: 800, loss is: [0.00335404], acc is: [1.]\n",
      "epoch: 3, batch_id: 0, loss is: [0.03072129], acc is: [0.984375]\n",
      "epoch: 3, batch_id: 200, loss is: [0.02452387], acc is: [0.984375]\n",
      "epoch: 3, batch_id: 400, loss is: [0.06962978], acc is: [0.984375]\n",
      "epoch: 3, batch_id: 600, loss is: [0.06501038], acc is: [0.984375]\n",
      "epoch: 3, batch_id: 800, loss is: [0.01925229], acc is: [1.]\n",
      "epoch: 4, batch_id: 0, loss is: [0.08780574], acc is: [0.96875]\n",
      "epoch: 4, batch_id: 200, loss is: [1.2206304], acc is: [0.859375]\n",
      "epoch: 4, batch_id: 400, loss is: [0.06681633], acc is: [0.984375]\n",
      "epoch: 4, batch_id: 600, loss is: [0.09953395], acc is: [0.96875]\n",
      "epoch: 4, batch_id: 800, loss is: [0.01076876], acc is: [1.]\n",
      "epoch: 5, batch_id: 0, loss is: [0.2796191], acc is: [0.984375]\n",
      "epoch: 5, batch_id: 200, loss is: [0.42463142], acc is: [0.921875]\n",
      "epoch: 5, batch_id: 400, loss is: [0.01625043], acc is: [1.]\n",
      "epoch: 5, batch_id: 600, loss is: [0.02479116], acc is: [0.984375]\n",
      "epoch: 5, batch_id: 800, loss is: [0.1154367], acc is: [0.984375]\n",
      "epoch: 6, batch_id: 0, loss is: [0.002542], acc is: [1.]\n",
      "epoch: 6, batch_id: 200, loss is: [0.01092122], acc is: [1.]\n",
      "epoch: 6, batch_id: 400, loss is: [0.09240755], acc is: [0.96875]\n",
      "epoch: 6, batch_id: 600, loss is: [0.00513943], acc is: [1.]\n",
      "epoch: 6, batch_id: 800, loss is: [0.0324708], acc is: [0.984375]\n",
      "epoch: 7, batch_id: 0, loss is: [0.00517955], acc is: [1.]\n",
      "epoch: 7, batch_id: 200, loss is: [0.03241134], acc is: [0.984375]\n",
      "epoch: 7, batch_id: 400, loss is: [0.03085015], acc is: [1.]\n",
      "epoch: 7, batch_id: 600, loss is: [0.02870268], acc is: [0.984375]\n",
      "epoch: 7, batch_id: 800, loss is: [0.04011687], acc is: [0.984375]\n",
      "epoch: 8, batch_id: 0, loss is: [0.06196418], acc is: [0.96875]\n",
      "epoch: 8, batch_id: 200, loss is: [0.0255728], acc is: [0.984375]\n",
      "epoch: 8, batch_id: 400, loss is: [0.20592707], acc is: [0.9375]\n",
      "epoch: 8, batch_id: 600, loss is: [0.06191084], acc is: [0.984375]\n",
      "epoch: 8, batch_id: 800, loss is: [0.05252472], acc is: [0.984375]\n",
      "epoch: 9, batch_id: 0, loss is: [0.034944], acc is: [0.984375]\n",
      "epoch: 9, batch_id: 200, loss is: [0.04290599], acc is: [0.984375]\n",
      "epoch: 9, batch_id: 400, loss is: [0.00840936], acc is: [1.]\n",
      "epoch: 9, batch_id: 600, loss is: [0.06205343], acc is: [0.984375]\n",
      "epoch: 9, batch_id: 800, loss is: [0.02264239], acc is: [1.]\n",
      "batch_id: 0, loss is: [0.02776364], acc is: [0.984375]\n",
      "batch_id: 20, loss is: [0.11448706], acc is: [0.953125]\n",
      "batch_id: 40, loss is: [0.1217482], acc is: [0.96875]\n",
      "batch_id: 60, loss is: [0.06704508], acc is: [0.96875]\n",
      "batch_id: 80, loss is: [0.09237639], acc is: [0.96875]\n",
      "batch_id: 100, loss is: [0.00132418], acc is: [1.]\n",
      "batch_id: 120, loss is: [0.00300031], acc is: [1.]\n",
      "batch_id: 140, loss is: [0.00168684], acc is: [1.]\n",
      "正确率为：[1.]\n"
     ]
    }
   ],
   "source": [
    "from paddle.vision.transforms import Compose, Normalize\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "from paddle.metric import Accuracy\r\n",
    "import random\r\n",
    "from paddle import fluid\r\n",
    "from visualdl import LogWriter\r\n",
    "from paddle.vision.models import ResNet\r\n",
    "from paddle.vision.models.resnet import BottleneckBlock, BasicBlock\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "log_writer=LogWriter(\"./data/log/train\") #log记录器\r\n",
    "\r\n",
    "\r\n",
    "transform = Compose([Normalize(mean=[127.5],\r\n",
    "                               std=[127.5],\r\n",
    "                               data_format='CHW')])\r\n",
    "#归一化\r\n",
    "\r\n",
    "#读取训练集 测试集数据\r\n",
    "train_dataset = paddle.vision.datasets.MNIST(mode='train', transform=transform)\r\n",
    "test_dataset = paddle.vision.datasets.MNIST(mode='test', transform=transform)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def train(model,Batch_size=64):\r\n",
    "    train_loader = paddle.io.DataLoader(train_dataset, batch_size=Batch_size, shuffle=True)\r\n",
    "    model.train()\r\n",
    "    iterator = 0\r\n",
    "    epochs = 10\r\n",
    "    total_steps = (int(50000//Batch_size)+1)*epochs\r\n",
    "    lr = paddle.optimizer.lr.PolynomialDecay(learning_rate=0.01,decay_steps=total_steps,end_lr=0.001)\r\n",
    "    optim = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())\r\n",
    "    # 用Adam作为优化函数\r\n",
    "    for epoch in range(epochs):\r\n",
    "        for batch_id, data in enumerate(train_loader()):\r\n",
    "            x_data = data[0]\r\n",
    "            y_data = data[1]\r\n",
    "            predicts = model(x_data)\r\n",
    "            loss = F.cross_entropy(predicts, y_data)\r\n",
    "            # 计算损失\r\n",
    "            acc = paddle.metric.accuracy(predicts, y_data)\r\n",
    "            loss.backward()\r\n",
    "            if batch_id % 200 == 0:\r\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}, acc is: {}\".format(epoch, batch_id, loss.numpy(), acc.numpy()))\r\n",
    "                log_writer.add_scalar(tag='acc',step=iterator,value=acc.numpy())\r\n",
    "                log_writer.add_scalar(tag='loss',step=iterator,value=loss.numpy())\r\n",
    "                iterator+=200\r\n",
    "            optim.step()\r\n",
    "            optim.clear_grad()\r\n",
    "        paddle.save(model.state_dict(),'./data/checkpoint/mnist_epoch{}'.format(epoch)+'.pdparams')\r\n",
    "        paddle.save(optim.state_dict(),'./data/checkpoint/mnist_epoch{}'.format(epoch)+'.pdopt')\r\n",
    "\r\n",
    "\r\n",
    "#测试\r\n",
    "def test(model):\r\n",
    "    # 加载测试数据集\r\n",
    "    test_loader = paddle.io.DataLoader(test_dataset, places=paddle.CPUPlace(), batch_size=64)\r\n",
    "    model.eval()\r\n",
    "    for batch_id, data in enumerate(test_loader()):\r\n",
    "        x_data = data[0]\r\n",
    "        y_data = data[1]\r\n",
    "        predicts = model(x_data)\r\n",
    "        # 获取预测结果\r\n",
    "        loss = F.cross_entropy(predicts, y_data)\r\n",
    "        acc = paddle.metric.accuracy(predicts, y_data)\r\n",
    "        if batch_id % 20 == 0:\r\n",
    "            print(\"batch_id: {}, loss is: {}, acc is: {}\".format(batch_id, loss.numpy(), acc.numpy()))\r\n",
    "\r\n",
    "#随机抽取100张图片进行测试\r\n",
    "def random_test(model,num=100):\r\n",
    "    select_id = random.sample(range(1, 10000), 100) #生成一百张测试图片的下标\r\n",
    "    test_loader = paddle.io.DataLoader(test_dataset, places=paddle.CPUPlace(), batch_size=64)\r\n",
    "    for batch_id, data in enumerate(test_loader()):\r\n",
    "        x_data = data[0]\r\n",
    "        label = data[1]\r\n",
    "    predicts = model(x_data)\r\n",
    "    #返回正确率\r\n",
    "    acc = paddle.metric.accuracy(predicts, label)\r\n",
    "    print(\"正确率为：{}\".format(acc.numpy()))\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    model = resnet50 = ResNet(BottleneckBlock, 50,10)\r\n",
    "    model.conv1= paddle.nn.Conv2D(1,64,kernel_size=7,stride=2,padding=3)\r\n",
    "    train(model)\r\n",
    "    test(model)\r\n",
    "    random_test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
