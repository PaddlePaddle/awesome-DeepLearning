作业 一、深度学习发展历史 
1.深度学习的起源阶段 
      1943年，心里学家麦卡洛克和数学逻辑学家皮兹发表论文《神经活动中内在思想的逻辑演算》，提出了MP模型。
MP模型作为人工神经网络的起源，开创了人工神经网络的新时代，也奠定了神经网络模型的基础。 
      1949年，加拿大著名心理学家唐纳德·赫布在《行为的组织》中提出了一种基于无监督学习的规则——海布学习规则(Hebb Rule)。
海布学习规则与“条件反射”机理一致， 为以后的神经网络学习算法奠定了基础，具有重大的历史意义。
      20世纪50年代末，在MP模型和海布学习规则的研究基础上，美国科学家罗森布拉特发现了一种类似于人类学习过程的学习算法——感知机学习。 
并于1958年，正式提出了由两层神经元组成的神经网络，称之为“感知器”。感知器的提出吸引了大量科学家对人工神经网络研究的兴趣，对神经网络的发展具有里程碑式的意义。
     1969年，“AI之父”马文·明斯基和LOGO语言的创始人西蒙·派珀特共同编写了一本书籍《感知器》，在书中他们证明了单层感知器无法解决线性不可分问题（例如：异或问题）。
 由于这个致命的缺陷以及没有及时推广感知器到多层神经网络中，在20世纪70年代，人工神经网络进入了第一个寒冬期，人们对神经网络的研究也停滞了将近20年。
 2.深度学习的发展阶段 
      1982年，著名物理学家约翰·霍普菲尔德发明了Hopfield神经网络。Hopfield神经网络是一种结合存储系统和二元系统的循环神经网络。
 Hopfield网络也可以模拟人类的记忆，根据激活函数的选取不同，有连续型和离散型两种类型，分别用于优化计算和联想记忆。
但由于容易陷入局部最小值的缺陷，该算法并未在当时引起很大的轰动。 
      1986年，深度学习之父杰弗里·辛顿提出了一种适用于多层感知器的反向传播算法——BP算法。BP算法完美的解决了非线性分类问题，让人工神经网络再次的引起了人们广泛的关注。
但是由于八十年代计算机的硬件水平有限，如：运算能力跟不上，这就导致当神经网络的规模增大时，再使用BP算法会出现“梯度消失”的问题。这使得BP算法的发展受到了很大的限制。
再加上90年代中期，以SVM为代表的其它浅层机器学习算法被提出，并在分类、回归问题上均取得了很好的效果，其原理又明显不同于神经网络模型，
所以人工神经网络的发展再次进入了瓶颈期。 
3.深度学习的爆发阶段 
      2006年，杰弗里·辛顿以及他的学生鲁斯兰·萨拉赫丁诺夫正式提出了深度学习的概念。 
他们在世界顶级学术期刊《科学》发表的一篇文章中详细的给出了“梯度消失”问题的解决方案——通过无监督的学习方法逐层训练算法，再使用有监督的反向传播算法进行调优。
     2012年，在著名的ImageNet图像识别大赛中，杰弗里·辛顿领导的小组采用深度学习模型AlexNet一举夺冠。
AlexNet采用ReLU激活函数，从根本上解决了梯度消失问题，并采用GPU极大的提高了模型的运算速度。 
同年，由斯坦福大学著名的吴恩达教授和世界顶尖计算机专家Jeff Dean共同主导的深度神经网络——DNN技术在图像识别领域取得了惊人的成绩，
在ImageNet评测中成功的把错误率从26％降低到了15％。 深度学习算法在世界大赛的脱颖而出，也再一次吸引了学术界和工业界对于深度学习领域的关注。 
     2014年，Facebook基于深度学习技术的DeepFace项目，在人脸识别方面的准确率已经能达到97%以上，跟人类识别的准确率几乎没有差别。

二、人工智能、机器学习与深度学习区别与联系 
人工智能（Artificial Intelligence）——为机器赋予人的智能 
      我们目前能实现的，一般被称为“弱人工智能”（Narrow AI）。弱人工智能是能够与人一样，甚至比人更好地执行特定任务的技术。
例如，Pinterest上的图像分类；或者Facebook的人脸识别。 
机器学习—— 一种实现人工智能的方法 
      机器学习最基本的做法，是使用算法来解析数据、从中学习，然后对真实世界中的事件做出决策和预测。 
与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。  
深度学习——一种实现机器学习的技术 
     人工神经网络（Artificial Neural Networks）是早期机器学习中的一个重要的算法，历经数十年风风雨雨。 
神经网络的原理是受我们大脑的生理结构——互相交叉相连的神经元启发。但与大脑中一个神经元可以连接一定距离内的任意神经元不同，
人工神经网络具有离散的层、连接和数据传播的方向。

三、神经元、单层感知机、多层感知机 
     1.神经元 把树突到细胞核的阶段简化为线性加权的过程，其次把突触之间的信号传递简化为对求和结果的非线性变换.

     2.单层感知机 假设输入空间（特征空间）是 ，输入空间是Y={+1，-1}. 输入 表示实例的特征向量，对于应于输入空间（特征空间）的点；
输出表示实例的类别.由输入空间到输出空间的如下函数：称为感知机。其中，w和b为感知机模型参数，叫做权值或者权值向量，叫做偏置，表示w和x的内积，sign是符合函数

     3.多层感知机 多层感知机由感知机推广而来，最主要的特点是有多个神经元层，因此也叫深度神经网络(DNN: Deep Neural Networks)。

四、前向传播 
      所谓前向传播，就是给网络输入一个样本向量，该样本向量的各元素，经过各隐藏层的逐级加权求和+非线性激活，最终由输出层输出一个预测向量的过程。

五、反向传播 
       反向传播，适合于多层神经元网络的一种学习算法，它建立在梯度下降法的基础上。BP网络的输入输出关系实质上是一种映射关系：
 一个n输入m输出的BP神经网络所完成的功能是从n维欧氏空间向m维欧氏空间中一有限域的连续映射，这一映射具有高度非线性。 
它的信息处理能力来源于简单非线性函数的多次复合，因此具有很强的函数复现能力。这是BP算法得以应用的基础