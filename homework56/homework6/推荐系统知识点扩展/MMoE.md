### MMoE

多任务模型通过学习不同任务的联系和差异，可提高每个任务的学习效率和质量。多任务学习的的框架广泛采用 shared-bottom 的结构，不同任务间共用底部的隐层。这种结构本质上可以减少过拟合的风险，但是效果上可能受到任务差异和数据分布带来的影响。也有一些其他结构，比如两个任务的参数不共用，但是通过对不同任务的参数增加 L2 范数的限制；也有一些对每个任务分别学习一套隐层然后学习所有隐层的组合。和 shared-bottom 结构相比，这些模型对增加了针对任务的特定参数，在任务差异会影响公共参数的情况下对最终效果有提升。缺点就是模型增加了参数量所以需要更大的数据量来训练模型，而且模型更复杂并不利于在真实生产环境中实际部署使用。

因此，论文中提出了一个 Multi-gate Mixture-of-Experts ( MMoE ) 的多任务学习结构。MMoE 模型刻画了任务相关性，基于共享表示来学习特定任务的函数，避免了明显增加参数的缺点。

MMoE 模型的结构 ( 下图 c ) 基于广泛使用的 Shared-Bottom 结构 ( 下图 a ) 和 MoE 结构，其中图 ( b ) 是图 ( c ) 的一种特殊情况，下面依次介绍。

![](https://ai-studio-static-online.cdn.bcebos.com/7d8978c1f9bb42728138bccad3f3213e98207ebe79114fb39cae17644dea2f92)

在MMoE提出之前，多任务模型已经有许多经典架构被提出，其中绝大多数的优化都基于share-bottom架构，即不同的任务共享相同的feature或feature_map。

然而，这种架构极大地限制了模型表达的能力，为什么这么说？因为我们在共享特征的上层直接接入了多个目标的输出，而由于多个任务各自有不同的数据分布，也就是说我们对不同任务的输出具有一定的差异性，而相同的特征输入会极大地削弱模型的多任务输出表达而在某种程度上降低了多目标模型的泛化能力。

那么，如何去降低这种架构带来的影响，作者首先提出了One-gate MoE model，这种模型架构就在一定程度上解决了上述问题，即虽然说多个模型还是共享相同的输入特征，但是每个任务都利用"gate network"来区分特征表达的权重，从而提高了模型的表达能力。但是这种模型架构的"区分"还不是很大，毕竟输入的特征还是只有一个，于是作者受集成学习（ensemble learning）思想的影响，提出了multi-experts，即我们可以把单个的共享特征看做是一个弱学习器的输入，那么，根据集成思想，若干弱学习器的组合可以作为一个强学习器来对结果进行推理，再通过"gate network"就可以极大地提高多任务模型的表达能力了。

由图可以看出，我们在定义两个任务优化模型时，在特征输入阶段，进行特征转换，分别产生若干expert，作为我们模型的基学习器，然后在每个任务对应的输入，分别使用"gate network"来表征最后的结果输出。

从某种角度上来讲，MMoE的厉害之处在于它的expert可以定义成任意一种单独的模型，可以说，MMoE是一个框架，而非简简单单的一个模型！这就使得我们在expert上可以灵活自由地实现自己想要的模型设计。