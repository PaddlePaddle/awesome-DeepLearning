{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 LSTM结构\n",
    "LSTM是一种特殊的递归神经网络（RNN）。RNN由于自身结构的特点而非常适用于处理序列数据，其在自然语言处理，语音识别等序列数据处理上有非常成功的应用。LSTM的提出解决了RNN结构的“长依赖”问题。“长依赖”就是词语间跨越很长距离的关联关系。比如，当我们在前文知道某个人现在在北京时，后面推测该人在哪个国家时，会立马得到“他在中国”这个结论。在LSTM提出之前，RNN结构由于参数冲突（详见1.2）的原因，“长依赖”问题一直没有得到解决。不过，好在LSTM出现了。\n",
    "\n",
    "在一个标准的RNN结构中，网络的输出会与下一步的输入组合，形成新的输入一同送到下一步循环中进行训练。这一典型的自循环结构可以如下图表示：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/0fcdb6c5e1ef4e688e403a20a41863d487503258de5c471ea14e16b6be25e425)\n",
    "\n",
    "其中左边是真实的结构，右边是为了理解对各个时间步上的结构进行的切片展示。可以看到，每一个时间步上的输出都指向了下一个时间步的输入。这种对数据的处理反映了时序数据一种特点：后边的状态的数据会受到先前状态的影响。\n",
    "\n",
    "数据在图中A部分可以是经过一个非常简单的处理后被输出，如经过一个tanh层。但是这种简单的处理会面临一些复杂的问题，比如参数的学习会陷入一种无序的状态而不能收敛，甚至造成梯度爆炸而出现喜闻乐见的满屏NAN。LSTM的结构复杂一些，如下图表示：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b4b509dd74284320a954fef46015eaf47994ec6d21024bca8f4fe062e8dd763f)\n",
    "\n",
    "各个符号的含义如下：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/c10af4f60e574478a1148b9c326ab3c978af261b590d420189088ec43958b3bd)\n",
    "\n",
    "从上图我们首先可以看出，LSTM构造了一个自始至终都存在的状态$C_t$，称为细胞状态，如下图所示高亮部分显示。它的存在保证了学习到的信息能够被保留下来，而不会因为后续学习的干扰而被彻底改变。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/cd278551320b4745bdcf5292ed4b438d276c1ca08dc74695a42b15e3e01e70bd)\n",
    "\n",
    "同时，通过一种被称为门(Gate)的结构，细胞状态$C_t$可以进行更新。门结构其实就是一种点乘操作，将一个和$C_t$等长的状态变量和$C_t$相乘，如果状态变量的某值为0，则对应$C_t$部分与之相乘的结果也为0，这就像是一扇门拦住了$C_t$对应部分不让其通过一样，如上图浅色部分所示。在每个时间步上，$C_t$的状态通过以下方式进行更新：\n",
    "\n",
    "$$\n",
    "C_t=f_t * C_{t-1}+i_t * \\hat{C_t}\n",
    "$$\n",
    "\n",
    "我们可以将$f_t$和$i_t$视作两种门：$f_t$直接作用在$C_{t-1}$上，它控制了旧的细胞状态的哪一部分被保留，哪一部分被遗忘；$i_t$则作用在新的细胞状态$\\hat{C_t}$上，注意这个细胞状态上面的帽子，区别于旧的细胞状态，这个状态是根据输入而得到的，$i_t$控制了新的细胞状态哪一部分被保留，哪一部分被遗忘。可以看出，$f_t$的作用是忘记旧的知识，$i_t$的作用是学习新的知识，而新的知识来自于对新的输入的学习。下边给出了$f_t,i_t,\\hat{C_t}$是如何得到的：\n",
    "\n",
    "$$\n",
    "f_t = \\sigma\\left(W_f\\cdot\\left[h_{t-1},x_t\\right]+b_f\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "i_t = \\sigma\\left(W_i\\cdot\\left[h_{t-1},x_t\\right]+b_i\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{C_t}=tanh\\left(W_C\\cdot\\left[h_{t-1},x_t\\right]+b_C\\right)\n",
    "$$\n",
    "\n",
    "观察上述三个式子，我们可以发现，它们都是根据当前的输入而进行的操作，其中$f_t$根据当前的输入决定忘记什么，$i_t$根据当前的输入决定记住什么，而$\\hat{C_t}$便是对当前输入信息的提炼。明白了上面所述，便明白了为什么$f_t,i_t$中要用激活函数$\\sigma$，而$\\hat{C_t}$中要用激活函数$tanh$了。那么有些人会问，既然三种值的操作是一样的，为什么不将它们合并成一个值，类似于下式这样，岂不是参数更少而训练效率更高？其实这就是LSTM的创新之处。下式为什么不行呢？请参考1.2中的分析。\n",
    "\n",
    "$$\n",
    "C_t=tanh\\left(f\\left(C_t,W_C\\left[h_{t-1},x_t\\right]+b_C\\right)\\right)\n",
    "$$\n",
    "\n",
    "当细胞状态更新完以后，LSTM也不是直接将该细胞状态输入，而是像上边所述一样，又进行了一次选择性遗忘，套路也一样，式子和上边相同：\n",
    "\n",
    "$$\n",
    "o_t = \\sigma\\left(W_o\\left[h_{t-1},x_t\\right]+b_o\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = o_t * tanh\\left(C_t\\right)\n",
    "$$\n",
    "\n",
    "至此，LSTM在一个时间步上的动作便完成了，下一步将接收来自上一步的$h_t$和下一步的输入$x_{t+1}$继续重复上述操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 为何LSTM是这种结构？\n",
    "对于RNN结构的改进经历了很长的一段历史。在RNN结构被提出来以后，研究人员遭遇了很多困境，其中比较显著的问题是RNN结构的梯度传播问题。由于RNN独特的递归循环的结构，每次迭代中参数的更新都要在如何进行中做抉择：参数既要保留历史的信息，又要依据当前的新输入进行更替。在这个过程中会出现两种冲突：\n",
    "\n",
    "1. 输入参数冲突。假设我们当前要更新参数$w_{ji}$，其中$i$是输入的某维，$j$是待更新参数的某维。$j$会根据输入$i$来进行更新，以降低整体的误差。在这里，$i$一般不为0。在更新的过程中，$w_{ji}$会收到两种信号：（1）更新$w_{ji}$来储存新的输入信息；（2）保持$w_{ji}$不变以保护历史信息。这种冲突导致了RNN对输入非常敏感，一种轻微的数据扰动，可能就会造成梯度爆炸\n",
    "\n",
    "2. 输出参数冲突。现在我们假设当前更新参数$w_{kj}$，其中$k$是更新参数的某维，$j$是输出的某维。该参数需要提取输出$j$的信息，同时要避免$j$干扰参数$k$。同样地，$j$一般不为0。在参数更新中，更新信号表现为两种形式：（1）该信号希望参数$w_{kj}$得到$j$的信息；（2）该信号希望避免$j$的信息干扰$k$。\n",
    "\n",
    "简言之，递归循环的过程中，由于同时涉及到参数的保护和更新，而这种机制是不固定的，很难判断哪个参数需要更新，哪个参数需要保护，在训练过程中便很容易出现波动而使已经训练好的参数重新被扰乱。\n",
    "\n",
    "lstm的这种结构，就是为了解决上述问题。首先，为了解决梯度更新中梯度爆炸或者梯度消失的问题，引入了一个常数误差（Constant Error Carrousel, CEC），就是结构图示中，上边那个贯穿整个元胞的直线（$C_{t-1}->C_t$）；其次，引入一个输入门（Input Gate Unit）来决定哪些新的信息需要继承；最后，引入一个输出门（Output Gate Unit）来决定何种状态需要被输出。\n",
    "\n",
    "这种思路导致的结果是，信息的保存和更新这两个过程被分开了。信息的保存通过一个门结构来进行，信息的更新通过另一个门结构来进行，两者分别独享参数，从而避免了上述的两种冲突（当然，是否真正避免了，也不好说）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 用Paddle框架实现LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddle import fluid\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    \"\"\"\n",
    "    emb_dim: 词向量维度\n",
    "    vocab_size: 词典大小，不能小于训练数据中所有词的总数\n",
    "    num_layers: 隐含层的数量\n",
    "    hidden_size: 隐含层的大小\n",
    "    num_steps: LSTM 一次接收数据的最大长度，样本的timestamp\n",
    "    use_gpu: 是否使用gpu进行训练\n",
    "    dropout_prob: 如果大于0，就启用dropout，值在0-1区间\n",
    "    init_scale: 训练参数的初始化范围\n",
    "    lr：学习速率\n",
    "    vocab: 默认为None，占位，暂时没用\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_layers,\n",
    "                 hidden_size,\n",
    "                 num_steps,\n",
    "                 use_gpu=True,\n",
    "                 dropout_prob=None,\n",
    "                 init_scale=0.1,\n",
    "                 lr=0.001,\n",
    "                 vocab=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_steps = num_steps\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.use_gpu = use_gpu\n",
    "        self.init_scale = init_scale\n",
    "        self.vocab = vocab\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x, batch_size):\n",
    "        self.init_hidden = fluid.layers.data(name='init_hidden',\n",
    "                                             shape=[self.num_layers, batch_size, self.hidden_size],\n",
    "                                             append_batch_size=False)\n",
    "        self.init_cell = fluid.layers.data(name='init_cell',\n",
    "                                           shape=[self.num_layers, batch_size, self.hidden_size],\n",
    "                                           append_batch_size=False)\n",
    "        x_emb = fluid.embedding(input=x, size=[self.vocab_size, self.hidden_size],\n",
    "                                dtype='float32', is_sparse=False,\n",
    "                                param_attr=fluid.ParamAttr(\n",
    "                                    name='embedding_para',\n",
    "                                    initializer=fluid.initializer.UniformInitializer(\n",
    "                                        low=-self.init_scale, high=self.init_scale\n",
    "                                    )\n",
    "                                ))\n",
    "        x_emb = fluid.layers.reshape(x_emb, shape=[-1, self.num_steps, self.hidden_size])\n",
    "        if self.dropout_prob is not None and self.dropout_prob > 0.0:\n",
    "            x_emb = fluid.layers.dropout(x_emb, dropout_prob=self.dropout_prob,\n",
    "                                         dropout_implementation=\"upscale_in_train\")\n",
    "\n",
    "        rnn_out, last_hidden, last_cell = fluid.contrib.layers.basic_lstm(x_emb, self.init_hidden, self.init_cell,\n",
    "                                                                          self.hidden_size, self.num_layers,\n",
    "                                                                          dropout_prob=self.dropout_prob)\n",
    "        rnn_out = fluid.layers.reshape(rnn_out, shape=[-1, self.num_steps, self.hidden_size])\n",
    "        softmax_weight = fluid.layers.create_parameter(\n",
    "            [self.hidden_size, self.vocab_size],\n",
    "            dtype=\"float32\",\n",
    "            name=\"softmax_weight\",\n",
    "            default_initializer=fluid.initializer.UniformInitializer(\n",
    "                low=-self.init_scale, high=self.init_scale))\n",
    "        softmax_bias = fluid.layers.create_parameter(\n",
    "            [self.vocab_size],\n",
    "            dtype=\"float32\",\n",
    "            name='softmax_bias',\n",
    "            default_initializer=fluid.initializer.UniformInitializer(\n",
    "                low=-self.init_scale, high=self.init_scale))\n",
    "\n",
    "        proj = fluid.layers.matmul(rnn_out, softmax_weight)\n",
    "        proj = fluid.layers.elementwise_add(proj, softmax_bias)\n",
    "        proj = fluid.layers.reshape(proj, shape=[-1, self.vocab_size], inplace=True)\n",
    "        # 更新 init_hidden, init_cell\n",
    "        fluid.layers.assign(input=last_cell, output=self.init_cell)\n",
    "        fluid.layers.assign(input=last_hidden, output=self.init_hidden)\n",
    "        return proj, last_hidden, last_cell\n",
    "\n",
    "    def train(self, x, epochs=3, batch_size=32, log_interval=100):\n",
    "        \"\"\"\n",
    "        :param log_interval: 输出信息的间隔\n",
    "        :param x: 输入，一维list，文本需要经过编码\n",
    "        :param epochs: 训练回合数\n",
    "        :param batch_size: 训练batch大小\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        # 定义训练的program\n",
    "        main_program = fluid.default_main_program()\n",
    "        startup_program = fluid.default_startup_program()\n",
    "        train_loss, train_proj, self.last_hidden, self.last_cell, py_reader = self.build_train_model(main_program, startup_program)\n",
    "\n",
    "        # 定义测试的program, 写成全局的，以便留给测试函数\n",
    "        self.test_program = fluid.Program()\n",
    "        self.test_startup_program = fluid.Program()\n",
    "        self.test_loss, self.test_proj, _, _ = self.build_test_model(self.test_program, self.test_startup_program)\n",
    "        self.test_program = self.test_program.clone(for_test=True)\n",
    "\n",
    "        place = fluid.CUDAPlace(0) if self.use_gpu else fluid.CPUPlace()\n",
    "        self.exe = fluid.Executor(place)\n",
    "        self.exe.run(startup_program)\n",
    "\n",
    "        def data_gen():\n",
    "            batches = self.get_data_iter(x)\n",
    "            for batch in batches:\n",
    "                x_, y_ = batch\n",
    "                yield x_, y_\n",
    "\n",
    "        py_reader.decorate_tensor_provider(data_gen)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batch_times = []\n",
    "            epoch_start_time = time.time()\n",
    "            total_loss = 0\n",
    "            iters = 0\n",
    "            py_reader.start()\n",
    "            batch_id = 0\n",
    "            batch_start_time = time.time()\n",
    "            # 初始化init_hidden, init_cell\n",
    "            init_hidden = np.zeros((self.num_layers, self.batch_size, self.hidden_size), dtype='float32')\n",
    "            init_cell = np.zeros((self.num_layers, self.batch_size, self.hidden_size), dtype='float32')\n",
    "\n",
    "            data_len = len(x)\n",
    "            batch_len = data_len // self.batch_size\n",
    "            batch_num = (batch_len - 1) // self.num_steps\n",
    "\n",
    "            # 送入数据，抓取结果\n",
    "            try:\n",
    "                while True:\n",
    "                    # 送入数据\n",
    "                    data_feeds = {}\n",
    "                    data_feeds['init_hidden'] = init_hidden\n",
    "                    data_feeds['init_cell'] = init_cell\n",
    "                    fetch_outs = self.exe.run(main_program, feed=data_feeds,\n",
    "                                         fetch_list=[train_loss.name, self.last_hidden.name, self.last_cell.name])\n",
    "                    t_loss = np.array(fetch_outs[0])\n",
    "                    init_hidden = np.array(fetch_outs[1])\n",
    "                    init_cell = np.array(fetch_outs[2])\n",
    "\n",
    "                    total_loss += t_loss\n",
    "                    batch_time = time.time() - batch_start_time\n",
    "                    batch_times.append(batch_time)\n",
    "                    batch_start_time = time.time()\n",
    "\n",
    "                    batch_id += 1\n",
    "                    iters += self.num_steps\n",
    "                    if batch_id % log_interval == 0:\n",
    "                        ppl = np.exp(total_loss / iters)\n",
    "                        print(\"-- Epoch: %d - Batch: %d / %d - Cost Time: %.2f s -ETA: %.2f s- ppl: %.5f\"\n",
    "                              % (epoch + 1, batch_id, batch_num, sum(batch_times),\n",
    "                                 sum(batch_times) / batch_id * (batch_num - batch_id), ppl[0]))\n",
    "            except fluid.core.EOFException:\n",
    "                py_reader.reset()\n",
    "\n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            ppl = np.exp(total_loss / iters)\n",
    "            print(\"Epoch %d Done. Cost Time: %.2f s. ppl: %.5f.\" % (epoch + 1, epoch_time, ppl))\n",
    "\n",
    "    def evaluate(self, x):\n",
    "        \"\"\"\n",
    "        测试模型的效果\n",
    "        :param x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        eval_data_gen = self.get_data_iter(x)\n",
    "        total_loss = 0.0\n",
    "        iters = 0\n",
    "        # 初始化init_hidden, init_cell\n",
    "        init_hidden = np.zeros((self.num_layers, self.batch_size, self.hidden_size), dtype='float32')\n",
    "        init_cell = np.zeros((self.num_layers, self.batch_size, self.hidden_size), dtype='float32')\n",
    "\n",
    "        for batch_id, batch in enumerate(eval_data_gen):\n",
    "            x, y = batch\n",
    "            data_feeds = {}\n",
    "            data_feeds['init_hidden'] = init_hidden\n",
    "            data_feeds['init_cell'] = init_cell\n",
    "            data_feeds['x'] = x\n",
    "            data_feeds['y'] = y\n",
    "            fetch_outs = self.exe.run(self.test_program, feed=data_feeds,\n",
    "                                      fetch_list=[self.test_loss.name, self.last_hidden.name, self.last_cell.name])\n",
    "            cost_test = np.array(fetch_outs[0])\n",
    "            init_hidden = np.array(fetch_outs[1])\n",
    "            init_cell = np.array(fetch_outs[2])\n",
    "\n",
    "            total_loss += cost_test\n",
    "            iters += self.num_steps\n",
    "            ppl = np.exp(total_loss / iters)\n",
    "            print(\"-- Batch: %d - ppl: %.5f\" % (batch_id, ppl[0]))\n",
    "        print(\"ppl: %.5f\" % (ppl[0]))\n",
    "        return ppl\n",
    "\n",
    "    def get_data_iter(self, raw_data):\n",
    "        \"\"\"\n",
    "        处理原始文本，生成训练数据\n",
    "        对于RNN来说，一般为读取前n个词，然后预测下一个词，这里简化为每读一个词，预测下一个词。\n",
    "        由于LSTM考虑了长依赖，所以也可以做到读取n个词，预测下一个词\n",
    "        :param raw_data: 一个一维数组，list，\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        data_len = len(raw_data)\n",
    "        raw_data = np.asarray(raw_data, dtype='int64')\n",
    "        batch_len = data_len // self.batch_size\n",
    "        # 将一维数组变为二维数组，第一维是batch的数量，第二维是每个batch的数据，这里对后边不足batch_len的数据进行了裁剪，弃掉不用\n",
    "        data = raw_data[0:self.batch_size * batch_len].reshape((self.batch_size, batch_len))\n",
    "\n",
    "        # 为了保证每个batch最后一个词能够被预测，x的词最多被分到batch_len-1\n",
    "        batch_num = (batch_len - 1) // self.num_steps\n",
    "        for i in range(batch_num):\n",
    "            x = np.copy(data[:, i * self.num_steps:(i + 1) * self.num_steps])\n",
    "            y = np.copy(data[:, i * self.num_steps + 1:(i + 1) * self.num_steps + 1])\n",
    "            x = x.reshape((-1, self.num_steps, 1))\n",
    "            y = y.reshape((-1, 1))\n",
    "            yield x, y\n",
    "\n",
    "    def build_train_model(self, main_program, startup_program):\n",
    "        \"\"\"\n",
    "        读取数据，构建网络\n",
    "        :param main_program:\n",
    "        :param startup_program:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with fluid.program_guard(main_program, startup_program):\n",
    "            feed_shapes = [[self.batch_size, self.num_steps, 1],\n",
    "                           [self.batch_size * self.num_steps, 1]]\n",
    "            py_reader = fluid.layers.py_reader(capacity=64, shapes=feed_shapes, dtypes=['int64', 'int64'])\n",
    "            x, y = fluid.layers.read_file(py_reader)\n",
    "            # 使用unique_name.guard创建变量空间，以便在test时共享参数\n",
    "            with fluid.unique_name.guard():\n",
    "                proj, last_hidden, last_cell = self.forward(x, self.batch_size)\n",
    "\n",
    "                loss = self.get_loss(proj, y)\n",
    "                optimizer = fluid.optimizer.Adam(learning_rate=self.lr,\n",
    "                                                 grad_clip=fluid.clip.GradientClipByGlobalNorm(clip_norm=1000))\n",
    "                optimizer.minimize(loss)\n",
    "\n",
    "                # 不知道有什么用，先写上\n",
    "                #loss.persistable = True\n",
    "                #proj.persistable = True\n",
    "                #last_cell.persistable = True\n",
    "                #last_hidden.persistable = True\n",
    "\n",
    "                return loss, proj, last_hidden, last_cell, py_reader\n",
    "\n",
    "    def build_test_model(self, main_program, startup_program):\n",
    "        \"\"\"\n",
    "        验证模型效果\n",
    "        :param main_program:\n",
    "        :param startup_program:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with fluid.program_guard(main_program, startup_program):\n",
    "            x = fluid.layers.data(name='x', shape=[self.batch_size, self.num_steps, 1], dtype='int64', append_batch_size=False)\n",
    "            y = fluid.layers.data(name='y', shape=[self.batch_size * self.num_steps, 1], dtype='int64', append_batch_size=False)\n",
    "            # 使用unique_name.guard创建变量空间，和train共享参数\n",
    "            with fluid.unique_name.guard():\n",
    "                proj, last_hidden, last_cell = self.forward(x, self.batch_size)\n",
    "                loss = self.get_loss(proj, y)\n",
    "\n",
    "                # 不知道有什么用，先写上\n",
    "                #loss.persistable = True\n",
    "                #proj.persistable = True\n",
    "                #last_cell.persistable = True\n",
    "                #last_hidden.persistable = True\n",
    "\n",
    "                return loss, proj, last_hidden, last_cell\n",
    "\n",
    "    def get_loss(self, proj, y):\n",
    "        loss = fluid.layers.softmax_with_cross_entropy(logits=proj, label=y, soft_label=False)\n",
    "        loss = fluid.layers.reshape(loss, shape=[-1, self.num_steps])\n",
    "        loss = fluid.layers.reduce_mean(loss, dim=[0])\n",
    "        loss = fluid.layers.reduce_sum(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    将文本中的特定字符串做修改和替换处理\n",
    "    :param string:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9:(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\":\", \" : \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def build_vocab(sentences, EOS='</eos>'):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    # vocabulary_inv=['<PAD/>', 'the', ....]\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    # vocabulary = {'<PAD/>': 0, 'the': 1, ',': 2, 'a': 3, 'and': 4, ..}\n",
    "    vocabulary = {x: i+1 for i, x in enumerate(vocabulary_inv)}\n",
    "    vocabulary[EOS] = 0\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def file_to_ids(src_file, src_vocab):\n",
    "    \"\"\"\n",
    "    将文章单词序列转化成词典id序列\n",
    "    :param src_file:\n",
    "    :param src_vocab:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    src_data = []\n",
    "    for line in src_file:\n",
    "        ids = [src_vocab[w] for w in line if w in src_vocab]\n",
    "        src_data += ids + [0]\n",
    "    return src_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text = list(open(\"text8\", \"r\").readlines())\n",
    "x_text = [clean_str(sent) for sent in x_text]\n",
    "vocabulary, vocabulary_inv = build_vocab(x_text)\n",
    "x_text = file_to_ids(x_text, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 训练 \n",
    "训练的结果与数据的吻合程度用困惑度指标ppl来衡量，参考了[基于LSTM的语言模型实现](https://aistudio.baidu.com/aistudio/projectdetail/592038)。ppl的值即e为底，平均交叉熵损失为指数的幂指数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-23 22:28:03,461-WARNING: paddle.fluid.layers.py_reader() may be deprecated in the near future. Please use paddle.fluid.io.DataLoader.from_generator() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch: 1 - Batch: 100 / 1562 - Cost Time: 4.34 s -ETA: 63.41 s- ppl: 11.76008\n",
      "-- Epoch: 1 - Batch: 200 / 1562 - Cost Time: 8.81 s -ETA: 60.02 s- ppl: 9.82037\n",
      "-- Epoch: 1 - Batch: 300 / 1562 - Cost Time: 13.23 s -ETA: 55.66 s- ppl: 8.90764\n",
      "-- Epoch: 1 - Batch: 400 / 1562 - Cost Time: 17.70 s -ETA: 51.42 s- ppl: 8.29910\n",
      "-- Epoch: 1 - Batch: 500 / 1562 - Cost Time: 22.38 s -ETA: 47.54 s- ppl: 7.92295\n",
      "-- Epoch: 1 - Batch: 600 / 1562 - Cost Time: 27.72 s -ETA: 44.45 s- ppl: 7.64613\n",
      "-- Epoch: 1 - Batch: 700 / 1562 - Cost Time: 32.12 s -ETA: 39.56 s- ppl: 7.45674\n",
      "-- Epoch: 1 - Batch: 800 / 1562 - Cost Time: 36.51 s -ETA: 34.77 s- ppl: 7.28143\n",
      "-- Epoch: 1 - Batch: 900 / 1562 - Cost Time: 41.14 s -ETA: 30.26 s- ppl: 7.15504\n",
      "-- Epoch: 1 - Batch: 1000 / 1562 - Cost Time: 45.60 s -ETA: 25.62 s- ppl: 7.03732\n",
      "-- Epoch: 1 - Batch: 1100 / 1562 - Cost Time: 50.01 s -ETA: 21.00 s- ppl: 6.93446\n",
      "-- Epoch: 1 - Batch: 1200 / 1562 - Cost Time: 54.44 s -ETA: 16.42 s- ppl: 6.85130\n",
      "-- Epoch: 1 - Batch: 1300 / 1562 - Cost Time: 58.82 s -ETA: 11.85 s- ppl: 6.76774\n",
      "-- Epoch: 1 - Batch: 1400 / 1562 - Cost Time: 63.22 s -ETA: 7.31 s- ppl: 6.69398\n",
      "-- Epoch: 1 - Batch: 1500 / 1562 - Cost Time: 67.64 s -ETA: 2.80 s- ppl: 6.62512\n",
      "Epoch 1 Done. Cost Time: 71.40 s. ppl: 6.58101.\n",
      "-- Epoch: 2 - Batch: 100 / 1562 - Cost Time: 4.44 s -ETA: 64.86 s- ppl: 5.69574\n",
      "-- Epoch: 2 - Batch: 200 / 1562 - Cost Time: 8.84 s -ETA: 60.20 s- ppl: 5.66448\n",
      "-- Epoch: 2 - Batch: 300 / 1562 - Cost Time: 13.24 s -ETA: 55.70 s- ppl: 5.65923\n",
      "-- Epoch: 2 - Batch: 400 / 1562 - Cost Time: 17.78 s -ETA: 51.66 s- ppl: 5.59913\n",
      "-- Epoch: 2 - Batch: 500 / 1562 - Cost Time: 22.33 s -ETA: 47.43 s- ppl: 5.58417\n",
      "-- Epoch: 2 - Batch: 600 / 1562 - Cost Time: 26.87 s -ETA: 43.08 s- ppl: 5.57558\n",
      "-- Epoch: 2 - Batch: 700 / 1562 - Cost Time: 31.33 s -ETA: 38.58 s- ppl: 5.58567\n",
      "-- Epoch: 2 - Batch: 800 / 1562 - Cost Time: 35.82 s -ETA: 34.12 s- ppl: 5.57449\n",
      "-- Epoch: 2 - Batch: 900 / 1562 - Cost Time: 40.36 s -ETA: 29.68 s- ppl: 5.57888\n",
      "-- Epoch: 2 - Batch: 1000 / 1562 - Cost Time: 45.67 s -ETA: 25.67 s- ppl: 5.57147\n",
      "-- Epoch: 2 - Batch: 1100 / 1562 - Cost Time: 50.29 s -ETA: 21.12 s- ppl: 5.56401\n",
      "-- Epoch: 2 - Batch: 1200 / 1562 - Cost Time: 54.81 s -ETA: 16.53 s- ppl: 5.56481\n",
      "-- Epoch: 2 - Batch: 1300 / 1562 - Cost Time: 59.31 s -ETA: 11.95 s- ppl: 5.55441\n",
      "-- Epoch: 2 - Batch: 1400 / 1562 - Cost Time: 63.86 s -ETA: 7.39 s- ppl: 5.54430\n",
      "-- Epoch: 2 - Batch: 1500 / 1562 - Cost Time: 68.29 s -ETA: 2.82 s- ppl: 5.53270\n",
      "Epoch 2 Done. Cost Time: 71.19 s. ppl: 5.52337.\n",
      "-- Epoch: 3 - Batch: 100 / 1562 - Cost Time: 4.51 s -ETA: 65.94 s- ppl: 5.39640\n",
      "-- Epoch: 3 - Batch: 200 / 1562 - Cost Time: 9.05 s -ETA: 61.66 s- ppl: 5.40248\n",
      "-- Epoch: 3 - Batch: 300 / 1562 - Cost Time: 13.59 s -ETA: 57.17 s- ppl: 5.39704\n",
      "-- Epoch: 3 - Batch: 400 / 1562 - Cost Time: 18.23 s -ETA: 52.97 s- ppl: 5.34842\n",
      "-- Epoch: 3 - Batch: 500 / 1562 - Cost Time: 23.56 s -ETA: 50.05 s- ppl: 5.33977\n",
      "-- Epoch: 3 - Batch: 600 / 1562 - Cost Time: 28.07 s -ETA: 45.01 s- ppl: 5.33564\n",
      "-- Epoch: 3 - Batch: 700 / 1562 - Cost Time: 32.54 s -ETA: 40.07 s- ppl: 5.34963\n",
      "-- Epoch: 3 - Batch: 800 / 1562 - Cost Time: 37.01 s -ETA: 35.25 s- ppl: 5.34571\n",
      "-- Epoch: 3 - Batch: 900 / 1562 - Cost Time: 41.38 s -ETA: 30.44 s- ppl: 5.35640\n",
      "-- Epoch: 3 - Batch: 1000 / 1562 - Cost Time: 45.82 s -ETA: 25.75 s- ppl: 5.35715\n",
      "-- Epoch: 3 - Batch: 1100 / 1562 - Cost Time: 50.24 s -ETA: 21.10 s- ppl: 5.35674\n",
      "-- Epoch: 3 - Batch: 1200 / 1562 - Cost Time: 54.72 s -ETA: 16.51 s- ppl: 5.36241\n",
      "-- Epoch: 3 - Batch: 1300 / 1562 - Cost Time: 59.24 s -ETA: 11.94 s- ppl: 5.35790\n",
      "-- Epoch: 3 - Batch: 1400 / 1562 - Cost Time: 63.78 s -ETA: 7.38 s- ppl: 5.35406\n",
      "-- Epoch: 3 - Batch: 1500 / 1562 - Cost Time: 69.08 s -ETA: 2.86 s- ppl: 5.34906\n",
      "Epoch 3 Done. Cost Time: 71.91 s. ppl: 5.34205.\n"
     ]
    }
   ],
   "source": [
    "lstm_test = LSTM(vocab_size=len(vocabulary), num_layers=1, hidden_size=100, num_steps=20, use_gpu=False, dropout_prob=0.2, init_scale=0.1, lr=0.01)\n",
    "lstm_test.train(x_text[:1000000], epochs=3, batch_size=32, log_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Batch: 0 - ppl: 6.15266\n",
      "-- Batch: 1 - ppl: 5.13366\n",
      "-- Batch: 2 - ppl: 4.71580\n",
      "-- Batch: 3 - ppl: 4.71751\n",
      "-- Batch: 4 - ppl: 4.62988\n",
      "-- Batch: 5 - ppl: 4.60868\n",
      "-- Batch: 6 - ppl: 4.58287\n",
      "ppl: 4.58287\n"
     ]
    }
   ],
   "source": [
    "ppl = lstm_test.evaluate(x_text[1000000:1005000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
