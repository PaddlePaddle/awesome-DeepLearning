# 7月13日作业

## 实例归一化

### 一、主要概念

​		通常随着网络加深,非线性变换前的输入值分布会逐渐偏移,整体分布往非线性函数取值区间的两端靠近,导致反向传播到低层神经网络时梯度消失, 这是训练深度神经网络收敛越来越慢的主要原因, 因此需要利用归一化技术避免这一问题。如图1中所示。

​		实例归一化作为归一化技术的一种,起初Ulyanov等人将其应用在图像风格转换领域,相比批量归一化能迁移更多的纹理和细节至目标图像，实例归一化对一组数据中每个样本分别做规范化操作,即将数据分布强行变换到均值为0方差为1的标准正态分布,算法如图2所示, 使数据落在非线性函数比较敏感的区域, 这样输入发生较小变化也会使损失函数产生较大变化,从而避免梯度消失的问题。对于一组数据格式为N×C×H×W(其中N为batch,C为channel,H×W为size),由于仅对每个样本单独做规范化操作, 因此当N 变小时需要增大H×W , 即提高数据的尺寸, 避免数据量过小造成网络难以收敛。

### 二、算法流程



- 沿着通道计算每张图的均值u

- 沿着通道计算每张图的方差σ^2

- 对x做归一化，x’=(x-u)/开根号(σ^2+ε)

- 加入缩放和平移变量γ和β ,归一化后的值，y=γx’+β

  ### 三、算法作用

  ​		实例归一化有很多优点: 梯度变大也意味着收敛速度变快, 从而加快了模型的训练速度; 可以设置更大的学习率; 降低训练过程中发生梯度爆炸或梯度消失的可能;对于卷积核初始化参数不是很敏感;在一定程度上可以避免过拟合等。

  ### 四、算法应用

  #### 1.图像风格转换

  ​		在Gatys等人的IST算法中，他们提出的策略是通过L-BFGS算法优化生成图片，风格图片以及内容图片再VGG-19上生成的Feature Map的均方误差。这种策略由于Feature Map的像素点数量过于多导致了优化起来非常消耗时间以及内存。IN的作者Ulyanov等人同在2016年提出了Texture network（图3），

  ​		图3中的生成器网络（Generator Network）是一个由卷积操作构成的全卷积网络，在原始的Texture Network中，生成器使用的操作包括卷积，池化，上采样以及**BN**。但是作者发现当训练生成器网络网络时，使用的样本数越少（例如16个），得到的效果越好。但是我们知道BN并不适用于样本数非常少的环境中，因此作者提出了IN，一种不受限于批量大小的算法专门用于Texture Network中的生成器网络。

  ​		Ulyanov等人的工作证明了训练一个生成器网络g(x,z)是可以应用于给输入图像x添加x0的风格的，再现Gatys等人得研究结果。在这儿，风格图x0是固定的，生成器g被训练作将风格应用到任意一个输入图像x中。变量z是一个随机种子，可以被用来获取样品结果。

  ​		函数g是一个卷积神经网络，从实例中学习得出的，这里一个例子就是一个内容图像xt t=1~n, 学习解决这个问题：

  ​		zt~N(0,1)是从一个高斯分布的独立同分布的样本，loss L采用预训练的CNN去从一个x0图像中获取特征，内容图像xt，风格化后的图像g(xt, zt)，跟前文说的一样比较他们的统计特征。

  ​		因为生成器网络g很快速，作者Ulyanov等人发现用过多的训练集去训练它往往会获得更差的质量，尤其，一个网络被用16张图片训练，往往会比用1000张图片训练产生更好的结果，我们最重要的发现就是，由于每次卷积钱都用0填充图像的边缘，即使使用更复杂的边缘填充算法，它也不可能解决这个问题，最终，最好的结果表现在Ulyanov的论文中，是从非常少量的训练集中，提前很早停止得来的。我们推测，训练的目标对于神经网络来说学起来过于生硬。

  ​		一个简单的观测是：一般来说，图像风格化的结果不应该取决于内容图的对比度（见fig 2）事实上，风格上的差距被设计用来将元素从风格图转移到内容图，因此风格化后图的对比度应该是近似于风格图的对比度的。因此，生成器网络必须忽视内容图的对比度信息，问题就在于，对比度正则化是不是可以有效，通过结合在标准的CNN块中，或者说直接实现在结构中。

  ​		Ulyanov 和Johnson都用了卷积、池化、上采样和批量正则化。在实践中，学习一个高度非线性化的对比度正则化函数作为层的组合，是非常困难的。想知道为什么的话，让x作为一个输入的tensor：

  ​		包含了一批图像（一共T个），让Xtijk表示它的第tijk个元素，k和j跨空间维度，i是特征通道（如果是RGB图像的话就是颜色通道）t是图像在Batch中的索引。那么一个简单的对比度正则化方程可以就此给出：

  ​		现在还不清楚，这样的函数是如何能被实现为一系列Relu层和卷积操作的。

  ​		从另一方面来说，Ulyanov等人提的生成器网络是包含一个正则化层的，是batch-normalization，关键的不同点在于，后者（batch-normalization）是把正规化用在 一整个Batch的图像上，而不是针对单个图像。

  ​		为了结合特定实例正规化和批量正规化的影响，我们建议，以实例正规化来取代后者（实例正规化也被称为“对比度正规划”）层：

  ​		我们把Batch-normalization用instance-normalization来替换， 所有在生成器网络中的的batch-normalization都这样替换。这样做能阻止实例特定的均值和协方差简化学习的过程。和batch-normalization不同， instance-normalization层在测试时的表现也挺好。

  

  