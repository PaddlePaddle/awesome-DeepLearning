{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、深度学习基础知识\n",
    "\n",
    "**1、深度学习发展历史**\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/6c463e1c2f0e428a94e28f6754a077fe6a188b87b0b24854ae6088546572b390\">\n",
    "</center>  \n",
    "<br></br>\n",
    "  \n",
    "  我们将深度学习分为三个历史阶段：\n",
    " \n",
    "  \n",
    "    起源阶段：\n",
    "    1943年，心里学家麦卡洛克和数学逻辑学家皮兹发表论文《神经活动中内在思想的逻辑演算》，提出了MP模型。MP模型是模仿神经元的结构和工作原理，构成出的一个基于神经网络的数学模型，本质上是一种“模拟人类大脑”的神经元模型。MP模型作为人工神经网络的起源，开创了人工神经网络的新时代，也奠定了神经网络模型的基础。\n",
    "    \n",
    "    1949年，加拿大著名心理学家唐纳德·赫布在《行为的组织》中提出了一种基于无监督学习的规则——海布学习规则(Hebb Rule)。为以后的神经网络学习算法奠定了基础，具有重大的历史意义。\n",
    "    \n",
    "    20世纪50年代末，在MP模型和海布学习规则的研究基础上，美国科学家罗森布拉特发现了一种类似于人类学习过程的学习算法——感知机学习。并于1958年，正式提出了由两层神经元组成的神经网络，称之为“感知器”。感知器本质上是一种线性模型，可以对输入的训练集数据进行二分类，且能够在训练集中自动更新权值。\n",
    " \n",
    "但由于异或问题这类线性不可分问题的无法解决，人工智能迎来第一个寒冬。\n",
    "\t  \n",
    "    发展阶段：\n",
    "    1982年，著名物理学家约翰·霍普菲尔德发明了Hopfield神经网络。Hopfield神经网络是一种结合存储系统和二元系统的循环神经网络。Hopfield网络也可以模拟人类的记忆，根据激活函数的选取不同，有连续型和离散型两种类型，分别用于优化计算和联想记忆。但由于容易陷入局部最小值的缺陷，该算法并未在当时引起很大的轰动。\n",
    "\n",
    "    1986年，深度学习之父杰弗里·辛顿提出了一种适用于多层感知器的反向传播算法——BP算法。BP算法在传统神经网络正向传播的基础上，增加了误差的反向传播过程。反向传播过程不断地调整神经元之间的权值和阈值，直到输出的误差达到减小到允许的范围之内，或达到预先设定的训练次数为止。BP算法完美的解决了非线性分类问题，让人工神经网络再次的引起了人们广泛的关注。\n",
    "    \n",
    "但随后因为算力的限制和“梯度消失”等问题的出现，以及SVM等浅层学习的算法出现，神经网络的研究再次进入瓶颈期。\n",
    "\n",
    "\t  爆发阶段：\n",
    "     2006年，杰弗里·辛顿以及他的学生鲁斯兰·萨拉赫丁诺夫正式提出了深度学习的概念。他们在世界顶级学术期刊《科学》发表的一篇文章中详细的给出了“梯度消失”问题的解决方案——通过无监督的学习方法逐层训练算法，再使用有监督的反向传播算法进行调优。该深度学习方法的提出，立即在学术圈引起了巨大的反响，以斯坦福大学、多伦多大学为代表的众多世界知名高校纷纷投入巨大的人力、财力进行深度学习领域的相关研究。而后又在迅速蔓延到工业界中.\n",
    "\n",
    " \t  2012年，在著名的ImageNet图像识别大赛中，杰弗里·辛顿领导的小组采用深度学习模型AlexNet一举夺冠。AlexNet采用ReLU激活函数，从根本上解决了梯度消失问题，并采用GPU极大的提高了模型的运算速度。同年，由斯坦福大学著名的吴恩达教授和世界顶尖计算机专家Jeff Dean共同主导的深度神经网络——DNN技术在图像识别领域取得了惊人的成绩，在ImageNet评测中成功的把错误率从26％降低到了15％。深度学习算法在世界大赛的脱颖而出，也再一次吸引了学术界和工业界对于深度学习领域的关注。\n",
    "\n",
    "\t  2014年，Facebook基于深度学习技术的DeepFace项目，在人脸识别方面的准确率已经能达到97%以上，跟人类识别的准确率几乎没有差别。这样的结果也再一次证明了深度学习算法在图像识别方面的一骑绝尘。\n",
    "\n",
    "\t  2016年，随着谷歌公司基于深度学习开发的AlphaGo以4:1的比分战胜了国际顶尖围棋高手李世石，深度学习的热度一时无两。后来，AlphaGo又接连和众多世界级围棋高手过招，均取得了完胜。这也证明了在围棋界，基于深度学习技术的机器人已经超越了人类。\n",
    "-------\n",
    "参考https://zhuanlan.zhihu.com/p/34472753"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**2、人工智能、机器学习、深度学习有什么区别和联系?**\n",
    "\n",
    "三者依次为包含关系：人工智能包含机器学习，机器学习是人工智能的一个分支，也是人工智能的核心；深度学习是机器学习的一个新兴的领域，在算力和数据库的发展支持下得到了快速的发展。概括来说，人工智能、机器学习和深度学习覆盖的技术范畴是逐层递减的。人工智能是最宽泛的概念。机器学习是当前比较有效的一种实现人工智能的方式。深度学习是机器学习算法中最热门的一个分支，近些年取得了显著的进展，并替代了大多数传统机器学习算法。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/ea3e79b897d44d41aac068587be5fa72e430630c57ac4906ae6d622b36c8d976\" width=\"200\" hegiht=\"\">  \n",
    "</center>  \n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**3、神经元、单层感知机多层感知机**\n",
    "\n",
    "**神经元**：神经网络中最基本的成分是神经元(neuron)模型，即上述所说的“简单单元”。在生物神经网络中，每个神经元与其他神经元相连，当它“兴奋”时，就会向相连的神经元发送化学物质，从而改变这些神经元内的电位；如果某神经元的电位超过了一个“阈值”，那么它就会被激活，即“兴奋”起来，向其他神经元发送化学物质。将其抽象为数学模型如下图所示：\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/952e51f216674a0a896ada986f651354d5cccf8f00a549d287ccc5ebae53494a\" width=\"300\" hegiht=\"\">  \n",
    "</center>  \n",
    "<br></br>\n",
    "\n",
    "即通过对n个输入信号，通过带权重的连接（connection）进行传递，将总的输入求和通过“激活函数”处理产生输出。\n",
    "\n",
    "**单层感知机**：感知机是基础的线性二分类模型（即输出为两个状态），由两层神经元组成。感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元。且激活函数为总的输入与阈值进行比较，从而产生两种输出，用作二分类。\n",
    "\n",
    "**多层感知机**： 多层感知机（MLP，Multilayer Perceptron）也叫人工神经网络（ANN，Artificial Neural Network），除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构，如下图：\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/09da825ad24f4c9c9d27df0cdc2745f95aac68ed876649c7b0c309764213bd53\" width=\"400\" hegiht=\"\">    \n",
    "</center>  \n",
    "<br></br>\n",
    "\n",
    "MLP并没有规定隐层的数量，因此可以根据各自的需求选择合适的隐层层数。且对于输出层神经元的个数也没有限制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**4、什么是前向传播(包含图文示例)？**\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/bf2a50271b4749cc998f6fb0b94e6abfb98ce5d6426a400b894bbf438d8aa358\" width=\"400\" hegiht=\"\">     \n",
    "</center>  \n",
    "<br></br>\n",
    "\n",
    "如图所示，这里讲得已经很清楚了，前向传播的思想比较简单。\n",
    "举个例子，假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。\n",
    "最终不断的通过这种方法一层层的运算，得到输出层结果。\n",
    "\n",
    "对于前向传播来说，不管维度多高，其过程都可以用如下公式表示：$a^2=σ(z^2)=σ(a^1*W^2+b^2)$\n",
    "\n",
    "其中，上标代表层数，星号表示卷积，b表示偏置项bias，σ表示激活函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**5、什么是反向传播(包含图文示例)？**\n",
    "\n",
    "BP算法(即误差反向传播算法)适合于多层神经元网络的一种学习算法，它建立在梯度下降法的基础上。\n",
    "\n",
    "BP算法的学习过程由正向传播过程和反向传播过程组成。在正向传播过程中，输入信息通过输入层经隐含层，逐层处理并传向输出层。如果在输出层得不到期望的输出值，则取输出与期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯量，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。\n",
    "\n",
    "具体过程如下：\n",
    "\n",
    "假设有一个网络：\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/018b27f765fb4213b18b84385a17f1885630ecf9b180457c9c0d19f9e7aef010\" width=\"400\" hegiht=\"\">     \n",
    "</center>  \n",
    "<br></br>\n",
    "\n",
    "其中，每个神经元有两个单元，第一个单元是对其输入信号和对应的权重的乘积求和，第二单元就是激活函数，它的输出就是整个神经元的输出。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/6ffcd780faa54e5da407d9dddc872d4d29f647b7c23f44618fb0b6cf43b74648\" width=\"400\" hegiht=\"\">     \n",
    "</center>  \n",
    "\n",
    "开始输入\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/e681ef8e1b3a472190218d7730f7a91451ba51b810ce4407803508b13a7b3f3e\" width=\"400\" hegiht=\"\">     \n",
    "</center> \n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/2bdbc014486d4657a193788be89ed150577365dc2cc74ac0bb5b7f362f13e579\" width=\"400\" hegiht=\"\">     \n",
    "</center> \n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/cf5e84a7121a47e5bf609680cc3de31d0fbfe270cadf47609f790808f877cd12\" width=\"400\" hegiht=\"\">     \n",
    "</center> \n",
    "\n",
    "然后信号传播到输出层，信号加权求和然后通过激活函数计算，输出最终结果。\n",
    "\n",
    "以上就是前向传播的过程，接下来是计算误差的向前传播时，每个神经元所贡献的误差，反向传播的名字也由此而来。\n",
    "\n",
    "神经网络模型的预测结果是y，而真实值是z，那么最终的误差为 δ = z − y。\n",
    "在之前，几乎不可能直接计算中间层的神经元的误差，因为它们的输出值是未知的，而反向传播算法解决了这个问题，从最终结果的误差，一步步反推中间神经元的误差。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/4991a183aef54f9aae49d133c442321abc04107882e74eabafd00ab23c555032\" width=\"400\" hegiht=\"\">     \n",
    "</center> \n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/e900474efe3940d68817f8bb66b1bdee48620a654ced47ab9330b47bd3092a0b\" width=\"400\" hegiht=\"\">     \n",
    "</center>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/8b4244926584465ca9a3dc55ff9a9e3634d9eca8a32a47e287d43099220dac0b\" width=\"400\" hegiht=\"\">     \n",
    "</center>\n",
    "\n",
    "当每个神经元的误差都被计算出来后，神经元之间连接的权重就可以被更新了。这里的 η 表示学习速率，是人为设置的一个参数。这里可以理解为，新的权重，在之前的基础上，在神经元的梯度方向上前进一小步。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/f3ddf3ca562c4ba2b198e22ed16bd5fe4dfaf31f1d0d49e9b9e57477b0f27b00\" width=\"400\" hegiht=\"\">     \n",
    "</center>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/c274f2671c4a4793b4b25b0eb4baee1a8794824af34b4c0b8bc563b8ed0cf64e\" width=\"400\" hegiht=\"\">     \n",
    "</center>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/468c04af64524d718e480a975dfc7ef25e99aefd2afa4f3981b5b457cd78c9c1\" width=\"400\" hegiht=\"\">     \n",
    "</center>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/14d8ae141af449b1b9c98e17c71db5f0d42deaec002c4c6cbca3facfd9d37940\" width=\"400\" hegiht=\"\">     \n",
    "</center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 房价预测任务\n",
    "\n",
    "## 线性回归模型\n",
    "\n",
    "假设房价和影响因素之间能够用线性关系来描述：\n",
    "\n",
    "$$y = {\\sum_{j=1}^Mx_j w_j} + b$$\n",
    "\n",
    "模型的求解即是通过数据拟合出每个$w_j$和$b$。其中，$w_j$和$b$分别表示该线性模型的权重和偏置。一维情况下，$w_j$ 和 $b$ 是直线的斜率和截距。\n",
    "\n",
    "线性回归模型使用均方误差作为损失函数（Loss），用以衡量预测房价和真实房价的差异，公式如下：\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat{Y_i} - {Y_i})^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据处理\n",
    "\n",
    "数据处理包含五个部分：数据导入、数据形状变换、数据集划分、数据归一化处理和封装`load data`函数。数据预处理后，才能被模型调用。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "* 本教程中的代码都可以在AI Studio上直接运行，Print结果都是基于程序真实运行的结果。\n",
    "* 由于是真实案例，代码之间存在依赖关系，因此需要读者逐条、全部运行，否则会导致命令执行报错。\n",
    "\n",
    "------\n",
    "\n",
    "### 读入数据\n",
    "\n",
    "通过如下代码读入数据，了解下波士顿房价的数据集结构，数据存放在本地目录下housing.data文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\r\n",
    "\r\n",
    "# zip_src: 需要解压的文件路径\r\n",
    "# dst_dir: 解压后文件存放路径\r\n",
    "def unzip_file(zip_src, dst_dir):\r\n",
    "\tr = zipfile.is_zipfile(zip_src)\r\n",
    "\tif r:\r\n",
    "\t\tfz = zipfile.ZipFile(zip_src, 'r')\r\n",
    "\t\tfor file in fz.namelist():\r\n",
    "\t\t\tfz.extract(file, dst_dir)\r\n",
    "\telse:\r\n",
    "\t\tprint('This is not a zip file !!!')\r\n",
    "\r\n",
    "\r\n",
    "unzip_file('./data/data269/房价预测.zip', './data/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 98.87, 599.  ],\n",
       "       [ 68.74, 450.  ],\n",
       "       [ 89.24, 440.  ],\n",
       "       ...,\n",
       "       [ 89.  , 735.  ],\n",
       "       [ 59.53, 360.  ],\n",
       "       [ 97.  , 600.  ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'data/data/房价预测/data/data.txt'\r\n",
    "data = np.loadtxt(fname, delimiter=',',dtype='float64')\r\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据集划分\n",
    "\n",
    "将数据集划分成训练集和测试集，其中训练集用于确定模型的参数，测试集用于评判模型的效果。为什么要对数据集进行拆分，而不能直接应用于模型训练呢？这与学生时代的授课和考试关系比较类似，如 **图4** 所示。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/a1c845a50e28474d9aa72028edfea33f1a3deca1d54d40ec94ba366d3a18c408\" width=\"600\" hegiht=\"\" ></center>\n",
    "<center><br>图4：训练集和测试集拆分的意义</br></center>\n",
    "<br></br>\n",
    "\n",
    "上学时总有一些自作聪明的同学，平时不认真学习，考试前临阵抱佛脚，将习题死记硬背下来，但是成绩往往并不好。因为学校期望学生掌握的是知识，而不仅仅是习题本身。另出新的考题，才能鼓励学生努力去掌握习题背后的原理。同样我们期望模型学习的是任务的本质规律，而不是训练数据本身，模型训练未使用的数据，才能更真实的评估模型的效果。\n",
    "\n",
    "在本案例中，我们将80%的数据用作训练集，20%用作测试集，实现代码如下。通过打印训练集的形状，可以发现共有870个样本，每个样本含有1个特征和1个预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = 0.8\r\n",
    "offset = int(data.shape[0] * ratio)\r\n",
    "training_data = data[:offset]\r\n",
    "test_data = data[offset:]\r\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    fname = 'data/data/房价预测/data/data.txt'\r\n",
    "    data = np.loadtxt(fname, delimiter=',',dtype='float64')\r\n",
    "    \r\n",
    "   \r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算训练集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\r\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\r\n",
    "    feature_num = 2\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        #print(maximums[i], minimums[i], avgs[i])\r\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\r\n",
    "    \r\n",
    "     # 因为数据为一个特征所以我们需要加一个偏置作为一个恒定输入\r\n",
    "    a = data[:,0]\r\n",
    "    b = np.ones(870)/5\r\n",
    "    c = np.c_[a,b]\r\n",
    "    d = data[:,1]\r\n",
    "    data = np.c_[c,d]\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36767373, 0.2       ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取数据\r\n",
    "training_data, test_data = load_data()\r\n",
    "b = np.ones(870)\r\n",
    "x = training_data[:, :-1]\r\n",
    "y = training_data[:, -1:]\r\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型设计\n",
    "\n",
    "模型设计是深度学习模型关键要素之一，也称为网络结构设计，相当于模型的假设空间，即实现模型“前向计算”（从输入到输出）的过程。\n",
    "\n",
    "如果将输入特征和输出预测值均以向量表示，输入特征$x$有2个分量，$y$有1个分量，那么参数权重的形状（shape）是$2\\times1$。假设我们以如下任意数字赋值参数做初始化：\n",
    "$$w=[0.1, 0.2]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0, point:[0.00075471 0.4004477 ], loss:0.045452453517784853\n",
      "iter:50, point:[0.00074502 0.41482487], loss:0.044625116870524534\n",
      "iter:100, point:[0.00073552 0.4289173 ], loss:0.043830226918054466\n",
      "iter:150, point:[0.00072621 0.44273063], loss:0.04306651115781323\n",
      "iter:200, point:[0.00071708 0.45627038], loss:0.042332746992561905\n",
      "iter:250, point:[0.00070813 0.46954198], loss:0.041627759773184767\n",
      "iter:300, point:[0.00069937 0.48255072], loss:0.0409504209182478\n",
      "iter:350, point:[0.00069077 0.49530183], loss:0.040299646107304896\n",
      "iter:400, point:[0.00068235 0.50780039], loss:0.03967439354505939\n",
      "iter:450, point:[0.00067409 0.52005142], loss:0.03907366229360215\n",
      "iter:500, point:[0.00066599 0.53205982], loss:0.038496490670056435\n",
      "iter:550, point:[0.00065806 0.54383038], loss:0.03794195470706413\n",
      "iter:600, point:[0.00065028 0.55536783], loss:0.03740916667364924\n",
      "iter:650, point:[0.00064266 0.56667677], loss:0.0368972736540904\n",
      "iter:700, point:[0.00063519 0.57776174], loss:0.03640545618252751\n",
      "iter:750, point:[0.00062786 0.58862717], loss:0.035932926931116785\n",
      "iter:800, point:[0.00062068 0.5992774 ], loss:0.035478929449634024\n",
      "iter:850, point:[0.00061365 0.6097167 ], loss:0.035042736954508394\n",
      "iter:900, point:[6.06747568e-04 6.19949255e-01], loss:0.034623651165348227\n",
      "iter:950, point:[5.99986574e-04 6.29979148e-01], loss:0.03422100118709626\n",
      "iter:1000, point:[5.93359484e-04 6.39810396e-01], loss:0.033834142436024825\n",
      "iter:1050, point:[5.86863644e-04 6.49446934e-01], loss:0.033462455607851424\n",
      "iter:1100, point:[5.80496457e-04 6.58892617e-01], loss:0.033105345686323256\n",
      "iter:1150, point:[5.74255373e-04 6.68151227e-01], loss:0.0327622409906832\n",
      "iter:1200, point:[5.68137895e-04 6.77226468e-01], loss:0.03243259226049252\n",
      "iter:1250, point:[5.62141576e-04 6.86121971e-01], loss:0.03211587177634523\n",
      "iter:1300, point:[5.56264015e-04 6.94841297e-01], loss:0.0318115725150665\n",
      "iter:1350, point:[5.50502860e-04 7.03387934e-01], loss:0.03151920733804273\n",
      "iter:1400, point:[5.44855807e-04 7.11765303e-01], loss:0.03123830821138382\n",
      "iter:1450, point:[5.39320595e-04 7.19976756e-01], loss:0.030968425456669436\n",
      "iter:1500, point:[5.33895009e-04 7.28025579e-01], loss:0.03070912703107956\n",
      "iter:1550, point:[5.28576879e-04 7.35914993e-01], loss:0.03045999783575716\n",
      "iter:1600, point:[5.23364075e-04 7.43648156e-01], loss:0.030220639051295607\n",
      "iter:1650, point:[5.18254512e-04 7.51228161e-01], loss:0.02999066749928714\n",
      "iter:1700, point:[5.13246145e-04 7.58658042e-01], loss:0.029769715028910283\n",
      "iter:1750, point:[5.08336970e-04 7.65940773e-01], loss:0.0295574279275742\n",
      "iter:1800, point:[5.03525023e-04 7.73079268e-01], loss:0.029353466354676592\n",
      "iter:1850, point:[4.98808378e-04 7.80076382e-01], loss:0.02915750379756859\n",
      "iter:1900, point:[4.94185146e-04 7.86934917e-01], loss:0.02896922654885573\n",
      "iter:1950, point:[4.89653479e-04 7.93657618e-01], loss:0.02878833320419833\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Network(object):\r\n",
    "    def __init__(self, num_of_weights):\r\n",
    "        # 随机产生w的初始值\r\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\r\n",
    "        np.random.seed(0)\r\n",
    "        self.w = np.random.randn(num_of_weights, 1)\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        self.w[0]=0\r\n",
    "        z = np.dot(x, self.w)\r\n",
    "        return z\r\n",
    "    \r\n",
    "    def loss(self, z, y):\r\n",
    "        error = z - y\r\n",
    "        num_samples = error.shape[0]\r\n",
    "        cost = error * error\r\n",
    "        cost = np.sum(cost) / num_samples\r\n",
    "        return cost\r\n",
    "    \r\n",
    "    def gradient(self, x, y):\r\n",
    "        z = self.forward(x)\r\n",
    "        N = x.shape[0]\r\n",
    "        gradient_w = 1. / N * np.sum((z-y) * x, axis=0)\r\n",
    "        gradient_w = gradient_w[:, np.newaxis]\r\n",
    "        return gradient_w\r\n",
    "    \r\n",
    "    def update(self, gradient_w,eta = 0.01):\r\n",
    "        self.w = self.w - eta * gradient_w\r\n",
    "\r\n",
    "        \r\n",
    "    def train(self, x, y, iterations=1000, eta=0.01):\r\n",
    "        points = []\r\n",
    "        losses = []\r\n",
    "        for i in range(iterations):\r\n",
    "            points.append(self.w)\r\n",
    "            z = self.forward(x)\r\n",
    "            L = self.loss(z, y)\r\n",
    "            gradient_w = self.gradient(x, y)\r\n",
    "            self.update(gradient_w, eta)\r\n",
    "            losses.append(L)\r\n",
    "            if i % 50 == 0:\r\n",
    "                print(f'iter:{i}, point:{self.w[:,0]}, loss:{L}')\r\n",
    "        return points, losses\r\n",
    "\r\n",
    "# 获取数据\r\n",
    "train_data, test_data = load_data()\r\n",
    "x = train_data[:, :-1]\r\n",
    "y = train_data[:, -1:]\r\n",
    "# 创建网络\r\n",
    "net = Network(2)\r\n",
    "num_iterations=2000\r\n",
    "# 启动训练\r\n",
    "points, losses = net.train(x, y, iterations=num_iterations, eta=0.01)\r\n",
    "\r\n",
    "# 画出损失函数的变化趋势\r\n",
    "plot_x = np.arange(num_iterations)\r\n",
    "plot_y = np.array(losses)\r\n",
    "plt.plot(plot_x, plot_y)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / iter   0, loss = 0.3592\n",
      "Epoch   0 / iter   1, loss = 0.3061\n",
      "Epoch   0 / iter   2, loss = 0.2902\n",
      "Epoch   0 / iter   3, loss = 0.2916\n",
      "Epoch   0 / iter   4, loss = 0.3011\n",
      "Epoch   0 / iter   5, loss = 0.2894\n",
      "Epoch   0 / iter   6, loss = 0.2820\n",
      "Epoch   1 / iter   0, loss = 0.2887\n",
      "Epoch   1 / iter   1, loss = 0.2710\n",
      "Epoch   1 / iter   2, loss = 0.2675\n",
      "Epoch   1 / iter   3, loss = 0.2687\n",
      "Epoch   1 / iter   4, loss = 0.2684\n",
      "Epoch   1 / iter   5, loss = 0.2720\n",
      "Epoch   1 / iter   6, loss = 0.2540\n",
      "Epoch   2 / iter   0, loss = 0.2398\n",
      "Epoch   2 / iter   1, loss = 0.2593\n",
      "Epoch   2 / iter   2, loss = 0.2365\n",
      "Epoch   2 / iter   3, loss = 0.2479\n",
      "Epoch   2 / iter   4, loss = 0.2568\n",
      "Epoch   2 / iter   5, loss = 0.2344\n",
      "Epoch   2 / iter   6, loss = 0.2139\n",
      "Epoch   3 / iter   0, loss = 0.2272\n",
      "Epoch   3 / iter   1, loss = 0.2221\n",
      "Epoch   3 / iter   2, loss = 0.2148\n",
      "Epoch   3 / iter   3, loss = 0.2152\n",
      "Epoch   3 / iter   4, loss = 0.2301\n",
      "Epoch   3 / iter   5, loss = 0.2045\n",
      "Epoch   3 / iter   6, loss = 0.1984\n",
      "Epoch   4 / iter   0, loss = 0.2151\n",
      "Epoch   4 / iter   1, loss = 0.1936\n",
      "Epoch   4 / iter   2, loss = 0.1902\n",
      "Epoch   4 / iter   3, loss = 0.1991\n",
      "Epoch   4 / iter   4, loss = 0.2018\n",
      "Epoch   4 / iter   5, loss = 0.1756\n",
      "Epoch   4 / iter   6, loss = 0.1822\n",
      "Epoch   5 / iter   0, loss = 0.1825\n",
      "Epoch   5 / iter   1, loss = 0.1785\n",
      "Epoch   5 / iter   2, loss = 0.1818\n",
      "Epoch   5 / iter   3, loss = 0.1794\n",
      "Epoch   5 / iter   4, loss = 0.1685\n",
      "Epoch   5 / iter   5, loss = 0.1642\n",
      "Epoch   5 / iter   6, loss = 0.1664\n",
      "Epoch   6 / iter   0, loss = 0.1608\n",
      "Epoch   6 / iter   1, loss = 0.1686\n",
      "Epoch   6 / iter   2, loss = 0.1640\n",
      "Epoch   6 / iter   3, loss = 0.1513\n",
      "Epoch   6 / iter   4, loss = 0.1567\n",
      "Epoch   6 / iter   5, loss = 0.1610\n",
      "Epoch   6 / iter   6, loss = 0.1387\n",
      "Epoch   7 / iter   0, loss = 0.1433\n",
      "Epoch   7 / iter   1, loss = 0.1472\n",
      "Epoch   7 / iter   2, loss = 0.1416\n",
      "Epoch   7 / iter   3, loss = 0.1484\n",
      "Epoch   7 / iter   4, loss = 0.1426\n",
      "Epoch   7 / iter   5, loss = 0.1412\n",
      "Epoch   7 / iter   6, loss = 0.1320\n",
      "Epoch   8 / iter   0, loss = 0.1426\n",
      "Epoch   8 / iter   1, loss = 0.1276\n",
      "Epoch   8 / iter   2, loss = 0.1244\n",
      "Epoch   8 / iter   3, loss = 0.1370\n",
      "Epoch   8 / iter   4, loss = 0.1254\n",
      "Epoch   8 / iter   5, loss = 0.1291\n",
      "Epoch   8 / iter   6, loss = 0.1180\n",
      "Epoch   9 / iter   0, loss = 0.1119\n",
      "Epoch   9 / iter   1, loss = 0.1256\n",
      "Epoch   9 / iter   2, loss = 0.1181\n",
      "Epoch   9 / iter   3, loss = 0.1199\n",
      "Epoch   9 / iter   4, loss = 0.1189\n",
      "Epoch   9 / iter   5, loss = 0.1159\n",
      "Epoch   9 / iter   6, loss = 0.1128\n",
      "Epoch  10 / iter   0, loss = 0.1130\n",
      "Epoch  10 / iter   1, loss = 0.1098\n",
      "Epoch  10 / iter   2, loss = 0.1147\n",
      "Epoch  10 / iter   3, loss = 0.1074\n",
      "Epoch  10 / iter   4, loss = 0.1056\n",
      "Epoch  10 / iter   5, loss = 0.0958\n",
      "Epoch  10 / iter   6, loss = 0.1056\n",
      "Epoch  11 / iter   0, loss = 0.1018\n",
      "Epoch  11 / iter   1, loss = 0.1032\n",
      "Epoch  11 / iter   2, loss = 0.1018\n",
      "Epoch  11 / iter   3, loss = 0.1019\n",
      "Epoch  11 / iter   4, loss = 0.0919\n",
      "Epoch  11 / iter   5, loss = 0.0960\n",
      "Epoch  11 / iter   6, loss = 0.0924\n",
      "Epoch  12 / iter   0, loss = 0.0927\n",
      "Epoch  12 / iter   1, loss = 0.0888\n",
      "Epoch  12 / iter   2, loss = 0.1022\n",
      "Epoch  12 / iter   3, loss = 0.0934\n",
      "Epoch  12 / iter   4, loss = 0.0842\n",
      "Epoch  12 / iter   5, loss = 0.0838\n",
      "Epoch  12 / iter   6, loss = 0.0890\n",
      "Epoch  13 / iter   0, loss = 0.0956\n",
      "Epoch  13 / iter   1, loss = 0.0879\n",
      "Epoch  13 / iter   2, loss = 0.0762\n",
      "Epoch  13 / iter   3, loss = 0.0824\n",
      "Epoch  13 / iter   4, loss = 0.0759\n",
      "Epoch  13 / iter   5, loss = 0.0887\n",
      "Epoch  13 / iter   6, loss = 0.0788\n",
      "Epoch  14 / iter   0, loss = 0.0840\n",
      "Epoch  14 / iter   1, loss = 0.0711\n",
      "Epoch  14 / iter   2, loss = 0.0823\n",
      "Epoch  14 / iter   3, loss = 0.0771\n",
      "Epoch  14 / iter   4, loss = 0.0795\n",
      "Epoch  14 / iter   5, loss = 0.0756\n",
      "Epoch  14 / iter   6, loss = 0.0730\n",
      "Epoch  15 / iter   0, loss = 0.0754\n",
      "Epoch  15 / iter   1, loss = 0.0718\n",
      "Epoch  15 / iter   2, loss = 0.0701\n",
      "Epoch  15 / iter   3, loss = 0.0691\n",
      "Epoch  15 / iter   4, loss = 0.0717\n",
      "Epoch  15 / iter   5, loss = 0.0717\n",
      "Epoch  15 / iter   6, loss = 0.0755\n",
      "Epoch  16 / iter   0, loss = 0.0746\n",
      "Epoch  16 / iter   1, loss = 0.0733\n",
      "Epoch  16 / iter   2, loss = 0.0693\n",
      "Epoch  16 / iter   3, loss = 0.0636\n",
      "Epoch  16 / iter   4, loss = 0.0602\n",
      "Epoch  16 / iter   5, loss = 0.0635\n",
      "Epoch  16 / iter   6, loss = 0.0677\n",
      "Epoch  17 / iter   0, loss = 0.0554\n",
      "Epoch  17 / iter   1, loss = 0.0703\n",
      "Epoch  17 / iter   2, loss = 0.0585\n",
      "Epoch  17 / iter   3, loss = 0.0676\n",
      "Epoch  17 / iter   4, loss = 0.0647\n",
      "Epoch  17 / iter   5, loss = 0.0608\n",
      "Epoch  17 / iter   6, loss = 0.0658\n",
      "Epoch  18 / iter   0, loss = 0.0687\n",
      "Epoch  18 / iter   1, loss = 0.0618\n",
      "Epoch  18 / iter   2, loss = 0.0600\n",
      "Epoch  18 / iter   3, loss = 0.0580\n",
      "Epoch  18 / iter   4, loss = 0.0543\n",
      "Epoch  18 / iter   5, loss = 0.0559\n",
      "Epoch  18 / iter   6, loss = 0.0585\n",
      "Epoch  19 / iter   0, loss = 0.0567\n",
      "Epoch  19 / iter   1, loss = 0.0601\n",
      "Epoch  19 / iter   2, loss = 0.0543\n",
      "Epoch  19 / iter   3, loss = 0.0577\n",
      "Epoch  19 / iter   4, loss = 0.0608\n",
      "Epoch  19 / iter   5, loss = 0.0488\n",
      "Epoch  19 / iter   6, loss = 0.0562\n",
      "Epoch  20 / iter   0, loss = 0.0630\n",
      "Epoch  20 / iter   1, loss = 0.0484\n",
      "Epoch  20 / iter   2, loss = 0.0513\n",
      "Epoch  20 / iter   3, loss = 0.0558\n",
      "Epoch  20 / iter   4, loss = 0.0581\n",
      "Epoch  20 / iter   5, loss = 0.0508\n",
      "Epoch  20 / iter   6, loss = 0.0468\n",
      "Epoch  21 / iter   0, loss = 0.0586\n",
      "Epoch  21 / iter   1, loss = 0.0523\n",
      "Epoch  21 / iter   2, loss = 0.0469\n",
      "Epoch  21 / iter   3, loss = 0.0455\n",
      "Epoch  21 / iter   4, loss = 0.0496\n",
      "Epoch  21 / iter   5, loss = 0.0485\n",
      "Epoch  21 / iter   6, loss = 0.0556\n",
      "Epoch  22 / iter   0, loss = 0.0515\n",
      "Epoch  22 / iter   1, loss = 0.0505\n",
      "Epoch  22 / iter   2, loss = 0.0464\n",
      "Epoch  22 / iter   3, loss = 0.0526\n",
      "Epoch  22 / iter   4, loss = 0.0445\n",
      "Epoch  22 / iter   5, loss = 0.0491\n",
      "Epoch  22 / iter   6, loss = 0.0464\n",
      "Epoch  23 / iter   0, loss = 0.0444\n",
      "Epoch  23 / iter   1, loss = 0.0538\n",
      "Epoch  23 / iter   2, loss = 0.0471\n",
      "Epoch  23 / iter   3, loss = 0.0380\n",
      "Epoch  23 / iter   4, loss = 0.0469\n",
      "Epoch  23 / iter   5, loss = 0.0461\n",
      "Epoch  23 / iter   6, loss = 0.0510\n",
      "Epoch  24 / iter   0, loss = 0.0438\n",
      "Epoch  24 / iter   1, loss = 0.0421\n",
      "Epoch  24 / iter   2, loss = 0.0453\n",
      "Epoch  24 / iter   3, loss = 0.0411\n",
      "Epoch  24 / iter   4, loss = 0.0485\n",
      "Epoch  24 / iter   5, loss = 0.0468\n",
      "Epoch  24 / iter   6, loss = 0.0473\n",
      "Epoch  25 / iter   0, loss = 0.0485\n",
      "Epoch  25 / iter   1, loss = 0.0455\n",
      "Epoch  25 / iter   2, loss = 0.0439\n",
      "Epoch  25 / iter   3, loss = 0.0441\n",
      "Epoch  25 / iter   4, loss = 0.0396\n",
      "Epoch  25 / iter   5, loss = 0.0372\n",
      "Epoch  25 / iter   6, loss = 0.0452\n",
      "Epoch  26 / iter   0, loss = 0.0412\n",
      "Epoch  26 / iter   1, loss = 0.0392\n",
      "Epoch  26 / iter   2, loss = 0.0430\n",
      "Epoch  26 / iter   3, loss = 0.0471\n",
      "Epoch  26 / iter   4, loss = 0.0396\n",
      "Epoch  26 / iter   5, loss = 0.0436\n",
      "Epoch  26 / iter   6, loss = 0.0404\n",
      "Epoch  27 / iter   0, loss = 0.0484\n",
      "Epoch  27 / iter   1, loss = 0.0361\n",
      "Epoch  27 / iter   2, loss = 0.0436\n",
      "Epoch  27 / iter   3, loss = 0.0388\n",
      "Epoch  27 / iter   4, loss = 0.0386\n",
      "Epoch  27 / iter   5, loss = 0.0431\n",
      "Epoch  27 / iter   6, loss = 0.0366\n",
      "Epoch  28 / iter   0, loss = 0.0345\n",
      "Epoch  28 / iter   1, loss = 0.0371\n",
      "Epoch  28 / iter   2, loss = 0.0436\n",
      "Epoch  28 / iter   3, loss = 0.0401\n",
      "Epoch  28 / iter   4, loss = 0.0400\n",
      "Epoch  28 / iter   5, loss = 0.0398\n",
      "Epoch  28 / iter   6, loss = 0.0426\n",
      "Epoch  29 / iter   0, loss = 0.0386\n",
      "Epoch  29 / iter   1, loss = 0.0379\n",
      "Epoch  29 / iter   2, loss = 0.0368\n",
      "Epoch  29 / iter   3, loss = 0.0430\n",
      "Epoch  29 / iter   4, loss = 0.0462\n",
      "Epoch  29 / iter   5, loss = 0.0335\n",
      "Epoch  29 / iter   6, loss = 0.0346\n",
      "Epoch  30 / iter   0, loss = 0.0394\n",
      "Epoch  30 / iter   1, loss = 0.0380\n",
      "Epoch  30 / iter   2, loss = 0.0391\n",
      "Epoch  30 / iter   3, loss = 0.0366\n",
      "Epoch  30 / iter   4, loss = 0.0317\n",
      "Epoch  30 / iter   5, loss = 0.0393\n",
      "Epoch  30 / iter   6, loss = 0.0407\n",
      "Epoch  31 / iter   0, loss = 0.0351\n",
      "Epoch  31 / iter   1, loss = 0.0401\n",
      "Epoch  31 / iter   2, loss = 0.0358\n",
      "Epoch  31 / iter   3, loss = 0.0388\n",
      "Epoch  31 / iter   4, loss = 0.0366\n",
      "Epoch  31 / iter   5, loss = 0.0387\n",
      "Epoch  31 / iter   6, loss = 0.0338\n",
      "Epoch  32 / iter   0, loss = 0.0364\n",
      "Epoch  32 / iter   1, loss = 0.0417\n",
      "Epoch  32 / iter   2, loss = 0.0371\n",
      "Epoch  32 / iter   3, loss = 0.0334\n",
      "Epoch  32 / iter   4, loss = 0.0352\n",
      "Epoch  32 / iter   5, loss = 0.0379\n",
      "Epoch  32 / iter   6, loss = 0.0320\n",
      "Epoch  33 / iter   0, loss = 0.0398\n",
      "Epoch  33 / iter   1, loss = 0.0286\n",
      "Epoch  33 / iter   2, loss = 0.0366\n",
      "Epoch  33 / iter   3, loss = 0.0348\n",
      "Epoch  33 / iter   4, loss = 0.0359\n",
      "Epoch  33 / iter   5, loss = 0.0385\n",
      "Epoch  33 / iter   6, loss = 0.0353\n",
      "Epoch  34 / iter   0, loss = 0.0332\n",
      "Epoch  34 / iter   1, loss = 0.0396\n",
      "Epoch  34 / iter   2, loss = 0.0385\n",
      "Epoch  34 / iter   3, loss = 0.0338\n",
      "Epoch  34 / iter   4, loss = 0.0328\n",
      "Epoch  34 / iter   5, loss = 0.0308\n",
      "Epoch  34 / iter   6, loss = 0.0367\n",
      "Epoch  35 / iter   0, loss = 0.0339\n",
      "Epoch  35 / iter   1, loss = 0.0320\n",
      "Epoch  35 / iter   2, loss = 0.0408\n",
      "Epoch  35 / iter   3, loss = 0.0349\n",
      "Epoch  35 / iter   4, loss = 0.0290\n",
      "Epoch  35 / iter   5, loss = 0.0311\n",
      "Epoch  35 / iter   6, loss = 0.0402\n",
      "Epoch  36 / iter   0, loss = 0.0366\n",
      "Epoch  36 / iter   1, loss = 0.0372\n",
      "Epoch  36 / iter   2, loss = 0.0303\n",
      "Epoch  36 / iter   3, loss = 0.0329\n",
      "Epoch  36 / iter   4, loss = 0.0344\n",
      "Epoch  36 / iter   5, loss = 0.0330\n",
      "Epoch  36 / iter   6, loss = 0.0339\n",
      "Epoch  37 / iter   0, loss = 0.0295\n",
      "Epoch  37 / iter   1, loss = 0.0308\n",
      "Epoch  37 / iter   2, loss = 0.0304\n",
      "Epoch  37 / iter   3, loss = 0.0347\n",
      "Epoch  37 / iter   4, loss = 0.0396\n",
      "Epoch  37 / iter   5, loss = 0.0407\n",
      "Epoch  37 / iter   6, loss = 0.0294\n",
      "Epoch  38 / iter   0, loss = 0.0365\n",
      "Epoch  38 / iter   1, loss = 0.0345\n",
      "Epoch  38 / iter   2, loss = 0.0327\n",
      "Epoch  38 / iter   3, loss = 0.0293\n",
      "Epoch  38 / iter   4, loss = 0.0277\n",
      "Epoch  38 / iter   5, loss = 0.0352\n",
      "Epoch  38 / iter   6, loss = 0.0367\n",
      "Epoch  39 / iter   0, loss = 0.0355\n",
      "Epoch  39 / iter   1, loss = 0.0383\n",
      "Epoch  39 / iter   2, loss = 0.0289\n",
      "Epoch  39 / iter   3, loss = 0.0294\n",
      "Epoch  39 / iter   4, loss = 0.0395\n",
      "Epoch  39 / iter   5, loss = 0.0283\n",
      "Epoch  39 / iter   6, loss = 0.0298\n",
      "Epoch  40 / iter   0, loss = 0.0334\n",
      "Epoch  40 / iter   1, loss = 0.0365\n",
      "Epoch  40 / iter   2, loss = 0.0333\n",
      "Epoch  40 / iter   3, loss = 0.0280\n",
      "Epoch  40 / iter   4, loss = 0.0300\n",
      "Epoch  40 / iter   5, loss = 0.0336\n",
      "Epoch  40 / iter   6, loss = 0.0325\n",
      "Epoch  41 / iter   0, loss = 0.0330\n",
      "Epoch  41 / iter   1, loss = 0.0373\n",
      "Epoch  41 / iter   2, loss = 0.0280\n",
      "Epoch  41 / iter   3, loss = 0.0272\n",
      "Epoch  41 / iter   4, loss = 0.0321\n",
      "Epoch  41 / iter   5, loss = 0.0368\n",
      "Epoch  41 / iter   6, loss = 0.0307\n",
      "Epoch  42 / iter   0, loss = 0.0321\n",
      "Epoch  42 / iter   1, loss = 0.0292\n",
      "Epoch  42 / iter   2, loss = 0.0335\n",
      "Epoch  42 / iter   3, loss = 0.0268\n",
      "Epoch  42 / iter   4, loss = 0.0297\n",
      "Epoch  42 / iter   5, loss = 0.0291\n",
      "Epoch  42 / iter   6, loss = 0.0430\n",
      "Epoch  43 / iter   0, loss = 0.0329\n",
      "Epoch  43 / iter   1, loss = 0.0342\n",
      "Epoch  43 / iter   2, loss = 0.0330\n",
      "Epoch  43 / iter   3, loss = 0.0246\n",
      "Epoch  43 / iter   4, loss = 0.0366\n",
      "Epoch  43 / iter   5, loss = 0.0282\n",
      "Epoch  43 / iter   6, loss = 0.0316\n",
      "Epoch  44 / iter   0, loss = 0.0325\n",
      "Epoch  44 / iter   1, loss = 0.0262\n",
      "Epoch  44 / iter   2, loss = 0.0365\n",
      "Epoch  44 / iter   3, loss = 0.0312\n",
      "Epoch  44 / iter   4, loss = 0.0305\n",
      "Epoch  44 / iter   5, loss = 0.0248\n",
      "Epoch  44 / iter   6, loss = 0.0378\n",
      "Epoch  45 / iter   0, loss = 0.0270\n",
      "Epoch  45 / iter   1, loss = 0.0295\n",
      "Epoch  45 / iter   2, loss = 0.0308\n",
      "Epoch  45 / iter   3, loss = 0.0335\n",
      "Epoch  45 / iter   4, loss = 0.0349\n",
      "Epoch  45 / iter   5, loss = 0.0286\n",
      "Epoch  45 / iter   6, loss = 0.0333\n",
      "Epoch  46 / iter   0, loss = 0.0382\n",
      "Epoch  46 / iter   1, loss = 0.0274\n",
      "Epoch  46 / iter   2, loss = 0.0318\n",
      "Epoch  46 / iter   3, loss = 0.0286\n",
      "Epoch  46 / iter   4, loss = 0.0276\n",
      "Epoch  46 / iter   5, loss = 0.0286\n",
      "Epoch  46 / iter   6, loss = 0.0338\n",
      "Epoch  47 / iter   0, loss = 0.0303\n",
      "Epoch  47 / iter   1, loss = 0.0298\n",
      "Epoch  47 / iter   2, loss = 0.0298\n",
      "Epoch  47 / iter   3, loss = 0.0272\n",
      "Epoch  47 / iter   4, loss = 0.0334\n",
      "Epoch  47 / iter   5, loss = 0.0335\n",
      "Epoch  47 / iter   6, loss = 0.0304\n",
      "Epoch  48 / iter   0, loss = 0.0344\n",
      "Epoch  48 / iter   1, loss = 0.0303\n",
      "Epoch  48 / iter   2, loss = 0.0286\n",
      "Epoch  48 / iter   3, loss = 0.0311\n",
      "Epoch  48 / iter   4, loss = 0.0247\n",
      "Epoch  48 / iter   5, loss = 0.0292\n",
      "Epoch  48 / iter   6, loss = 0.0348\n",
      "Epoch  49 / iter   0, loss = 0.0347\n",
      "Epoch  49 / iter   1, loss = 0.0270\n",
      "Epoch  49 / iter   2, loss = 0.0314\n",
      "Epoch  49 / iter   3, loss = 0.0296\n",
      "Epoch  49 / iter   4, loss = 0.0245\n",
      "Epoch  49 / iter   5, loss = 0.0284\n",
      "Epoch  49 / iter   6, loss = 0.0360\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\r\n",
    "class Network(object):\r\n",
    "    def __init__(self, num_of_weights):\r\n",
    "        # 随机产生w的初始值\r\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\r\n",
    "        #np.random.seed(0)\r\n",
    "        self.w = np.random.randn(num_of_weights, 1)\r\n",
    "\r\n",
    "    # 前向传播过程\r\n",
    "    def forward(self, x):\r\n",
    "        z = np.dot(x, self.w)\r\n",
    "        return z\r\n",
    "    \r\n",
    "    # 均方差损失函数\r\n",
    "    def loss(self, z, y):\r\n",
    "        error = z - y\r\n",
    "        num_samples = error.shape[0]\r\n",
    "        cost = error * error\r\n",
    "        cost = np.sum(cost) / num_samples\r\n",
    "        return cost\r\n",
    "    \r\n",
    "    # 梯度下降法\r\n",
    "    def gradient(self, x, y):\r\n",
    "        z = self.forward(x)\r\n",
    "        N = x.shape[0]\r\n",
    "        gradient_w = 1. / N * np.sum((z-y) * x, axis=0)\r\n",
    "        gradient_w = gradient_w[:, np.newaxis]\r\n",
    "       \r\n",
    "        return gradient_w\r\n",
    "    \r\n",
    "    # 参数更新公式\r\n",
    "    def update(self, gradient_w, eta = 0.01):\r\n",
    "        self.w = self.w - eta * gradient_w\r\n",
    "        \r\n",
    "            \r\n",
    "    # 训练过程            \r\n",
    "    def train(self, training_data, num_epoches, batch_size=10, eta=0.01):\r\n",
    "        n = len(training_data)\r\n",
    "        losses = []\r\n",
    "        for epoch_id in range(num_epoches):\r\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机的打乱，\r\n",
    "            # 然后再按每次取batch_size条数据的方式取出\r\n",
    "            np.random.shuffle(training_data)\r\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\r\n",
    "            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]\r\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "                #print(self.w.shape)\r\n",
    "                #print(self.b)\r\n",
    "                x = mini_batch[:, :-1]\r\n",
    "                y = mini_batch[:, -1:]\r\n",
    "                a = self.forward(x)\r\n",
    "                loss = self.loss(a, y)\r\n",
    "                gradient_w= self.gradient(x, y)\r\n",
    "                self.update(gradient_w, eta)\r\n",
    "                losses.append(loss)\r\n",
    "                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\r\n",
    "                                 format(epoch_id, iter_id, loss))\r\n",
    "        \r\n",
    "        return losses\r\n",
    "\r\n",
    "# 获取数据\r\n",
    "train_data, test_data = load_data()\r\n",
    "\r\n",
    "# 创建网络\r\n",
    "net = Network(2)\r\n",
    "# 启动训练\r\n",
    "losses = net.train(train_data, num_epoches=50, batch_size=100, eta=0.05)\r\n",
    "\r\n",
    "# 画出损失函数的变化趋势\r\n",
    "plot_x = np.arange(len(losses))\r\n",
    "plot_y = np.array(losses)\r\n",
    "plt.plot(plot_x, plot_y)\r\n",
    "plt.show()\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# paddlepaddle实现房价预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "代码中参数含义如下：\n",
    "\n",
    "* paddle：飞桨的主库，paddle 根目录下保留了常用API的别名，当前包括：paddle.tensor、paddle.framework目录下的所有API。\n",
    "\n",
    "* paddle.nn：组网相关的API，例如 Linear 、卷积 Conv2D 、 循环神经网络 LSTM 、损失函数 CrossEntropyLoss 、 激活函数 ReLU 等。\n",
    "\n",
    "* Linear：神经网络的全连接层函数，即包含所有输入权重相加的基本神经元结构。在房价预测任务中，使用只有一层的神经网络（全连接层）来实现线性回归模型。\n",
    "\n",
    "* paddle.nn.functional：与paddle.nn一样，包含组网相关的API，例如Linear、激活函数ReLu等。两者下的同名模块功能相同，运行性能也基本一致。 但是，paddle.nn下的模块均是类，每个类下可以自带模块参数；paddle.nn.functional下的模块均是函数，需要手动传入模块计算需要的参数。在实际使用中，卷积、全连接层等层本身具有可学习的参数，建议使用paddle.nn模块，而激活函数、池化等操作没有可学习参数，可以考虑直接使用paddle.nn.functional下的函数代替。\n",
    "<br></br>\n",
    "------\n",
    "\n",
    "**说明：**\n",
    "\n",
    "飞桨支持两种深度学习建模编写方式，更方便调试的动态图模式和性能更好并便于部署的静态图模式。\n",
    "\n",
    "* 动态图模式（命令式编程范式，类比Python）：解析式的执行方式。用户无需预先定义完整的网络结构，每写一行网络代码，即可同时获得计算结果。\n",
    "* 静态图模式（声明式编程范式，类比C++）：先编译后执行的方式。用户需预先定义完整的网络结构，再对网络结构进行编译优化后，才能执行获得计算结果。\n",
    "\n",
    "飞桨框架2.0及之后的版本，默认使用动态图模式进行编码，同时提供了全面完备的动转静支持。开发者仅需添加一个装饰器（ to_static ），飞桨会自动将动态图的程序，转换为静态图的program，并使用该program训练并可保存静态模型以实现推理部署。\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加载飞桨、Numpy和相关类库\r\n",
    "import paddle\r\n",
    "from paddle.nn import Linear\r\n",
    "import paddle.nn.functional as F\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import random\r\n",
    "import paddle.fluid as fluid\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    fname = 'data/data/房价预测/data/data.txt'\r\n",
    "    data = np.loadtxt(fname, delimiter=',',dtype='float32')\r\n",
    "    \r\n",
    "   \r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算训练集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\r\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\r\n",
    "    feature_num = 2\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        #print(maximums[i], minimums[i], avgs[i])\r\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\r\n",
    "    \r\n",
    "    #  # 因为数据为一个特征所以我们需要加一个偏置作为一个恒定输入\r\n",
    "    # a = data[:,0]\r\n",
    "    # b = np.ones(870)/5\r\n",
    "    # c = np.c_[a,b]\r\n",
    "    # d = data[:,1]\r\n",
    "    # data = np.c_[c,d]\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "模型定义的实质是定义线性回归的网络结构，飞桨建议通过创建Python类的方式完成模型网络的定义，该类需要继承paddle.nn.Layer父类，并且在类中定义``init``函数和``forward``函数。``forward``函数是框架指定实现前向计算逻辑的函数，程序在调用模型实例时会自动执行forward方法。在``forward``函数中使用的网络层需要在``init``函数中声明。\n",
    "\n",
    "实现过程分如下两步：\n",
    "\n",
    "1. **定义init函数**：在类的初始化函数中声明每一层网络的实现函数。在房价预测模型中，只需要定义一层全连接层，模型结构和使用Python和Numpy构建神经网络模型》章节模型保持一致。\n",
    "1. **定义forward函数**：构建神经网络结构，实现前向计算过程，并返回预测结果，在本任务中返回的是房价预测结果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Regressor(paddle.nn.Layer):\r\n",
    "\r\n",
    "    # self代表类的实例自身\r\n",
    "    def __init__(self):\r\n",
    "        # 初始化父类中的一些参数\r\n",
    "        super(Regressor, self).__init__()\r\n",
    "        \r\n",
    "        # 定义一层全连接层，输入维度是13，输出维度是1\r\n",
    "        \r\n",
    "        self.fc1 = Linear(in_features=1, out_features=4)\r\n",
    "        \r\n",
    "        self.fc2 = Linear(in_features=4, out_features=1)\r\n",
    "    \r\n",
    "    # 网络的前向计算\r\n",
    "    def forward(self, inputs):\r\n",
    "        \r\n",
    "        middle2 = self.fc1(inputs)\r\n",
    "        paddle.to_tensor(middle2)\r\n",
    "        m = paddle.nn.ReLU()\r\n",
    "        out = m(middle2)\r\n",
    "        x = self.fc2(out)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练配置\n",
    "\n",
    "训练配置过程包含四步，如 **图2** 所示：\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/96075d4df5ae4e01ac1491ebf176fa557bd122b646ba49238f65c9b38a98cab4\" width=\"700\" hegiht=\"\" ></center>\n",
    "<center><br>图2：训练配置流程示意图</br></center>\n",
    "<br></br>\n",
    "\n",
    "1. 声明定义好的回归模型Regressor实例，并将模型的状态设置为训练。\n",
    "1. 使用load_data函数加载训练数据和测试数据。\n",
    "1. 设置优化算法和学习率，优化算法采用随机梯度下降SGD，学习率设置为0.01。\n",
    "\n",
    "训练配置代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 声明定义好的线性回归模型\r\n",
    "model = Regressor()\r\n",
    "# 开启模型训练模式\r\n",
    "model.train()\r\n",
    "# 加载数据\r\n",
    "training_data, test_data = load_data()\r\n",
    "# 定义优化算法，使用随机梯度下降SGD\r\n",
    "# 学习率设置为0.01\r\n",
    "opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "------\n",
    "\n",
    "**说明：**\n",
    "\n",
    "模型实例有两种状态：训练状态``.train()``和预测状态``.eval()``。训练时要执行正向计算和反向传播梯度两个过程，而预测时只需要执行正向计算，为模型指定运行状态，有两点原因：\n",
    "\n",
    "1. 部分高级的算子（例如Dropout和BatchNorm，在计算机视觉的章节会详细介绍）在两个状态执行的逻辑不同；\n",
    "1. 从性能和存储空间的考虑，预测状态时更节省内存(无需记录反向梯度)，性能更好。\n",
    "\n",
    "------\n",
    "\n",
    "在基于Python实现神经网络模型的案例中，我们为实现梯度下降编写了大量代码，而使用飞桨框架只需要定义SGD就可以实现优化器设置，大大简化了这个过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练过程\n",
    "\n",
    "训练过程采用二层循环嵌套方式：\n",
    "\n",
    "- **内层循环：** 负责整个数据集的一次遍历，采用分批次方式（batch）。假设数据集样本数量为1000，一个批次有10个样本，则遍历一次数据集的批次数量是1000/10=100，即内层循环需要执行100次。\n",
    "\n",
    "        for iter_id, mini_batch in enumerate(mini_batches):\n",
    "\n",
    "- **外层循环：** 定义遍历数据集的次数，通过参数EPOCH_NUM设置。\n",
    "\n",
    "        for epoch_id in range(EPOCH_NUM):\n",
    "\n",
    "------\n",
    "**说明**:\n",
    "\n",
    "batch的取值会影响模型训练效果。batch过大，会增大内存消耗和计算时间，且训练效果并不会明显提升（因为每次参数只向梯度反方向移动一小步，所以方向没必要特别精确）；batch过小，每个batch的样本数据将没有统计意义，计算的梯度方向可能偏差较大。由于房价预测模型的训练数据集较小，我们将batch为设置10。\n",
    "\n",
    "------\n",
    "\n",
    "每次内层循环都需要执行如下四个步骤，如 **图3** 所示，计算过程与使用Python编写模型完全一致。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/8154cf612a024a3f9144b4e31f59568ef9ad59c155b344919221d63bb9ccfcc8\" width=\"700\" hegiht=\"\" ></center>\n",
    "<center><br>图3：内循环计算过程</br></center>\n",
    "<br></br>\n",
    "\n",
    "1. 数据准备：将一个批次的数据先转换成np.array格式，再转换成paddle内置tensor格式。\n",
    "1. 前向计算：将一个批次的样本数据灌入网络中，计算输出结果。\n",
    "1. 计算损失函数：以前向计算结果和真实房价作为输入，通过损失函数square_error_cost API计算出损失函数值（Loss）。飞桨所有的API接口都有完整的说明和使用案例，在后续教程中我们会详细介绍API的查阅方法。\n",
    "1. 反向传播：执行梯度反向传播``backward``函数，即从后到前逐层计算每一层的梯度，并根据设置的优化算法更新参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss is: [0.19077912]\n",
      "epoch: 0, iter: 20, loss is: [0.26821977]\n",
      "epoch: 0, iter: 40, loss is: [0.10774108]\n",
      "epoch: 0, iter: 60, loss is: [0.06238294]\n",
      "epoch: 1, iter: 0, loss is: [0.02389851]\n",
      "epoch: 1, iter: 20, loss is: [0.10332851]\n",
      "epoch: 1, iter: 40, loss is: [0.05110644]\n",
      "epoch: 1, iter: 60, loss is: [0.07717873]\n",
      "epoch: 2, iter: 0, loss is: [0.06780779]\n",
      "epoch: 2, iter: 20, loss is: [0.03951957]\n",
      "epoch: 2, iter: 40, loss is: [0.05023772]\n",
      "epoch: 2, iter: 60, loss is: [0.04566764]\n",
      "epoch: 3, iter: 0, loss is: [0.05174584]\n",
      "epoch: 3, iter: 20, loss is: [0.13672993]\n",
      "epoch: 3, iter: 40, loss is: [0.0810004]\n",
      "epoch: 3, iter: 60, loss is: [0.08541711]\n",
      "epoch: 4, iter: 0, loss is: [0.05637588]\n",
      "epoch: 4, iter: 20, loss is: [0.11330146]\n",
      "epoch: 4, iter: 40, loss is: [0.04999761]\n",
      "epoch: 4, iter: 60, loss is: [0.0425316]\n",
      "epoch: 5, iter: 0, loss is: [0.0291314]\n",
      "epoch: 5, iter: 20, loss is: [0.03193913]\n",
      "epoch: 5, iter: 40, loss is: [0.02953169]\n",
      "epoch: 5, iter: 60, loss is: [0.07679995]\n",
      "epoch: 6, iter: 0, loss is: [0.06592248]\n",
      "epoch: 6, iter: 20, loss is: [0.02056373]\n",
      "epoch: 6, iter: 40, loss is: [0.07544459]\n",
      "epoch: 6, iter: 60, loss is: [0.05759854]\n",
      "epoch: 7, iter: 0, loss is: [0.03330588]\n",
      "epoch: 7, iter: 20, loss is: [0.03034755]\n",
      "epoch: 7, iter: 40, loss is: [0.01952718]\n",
      "epoch: 7, iter: 60, loss is: [0.07346249]\n",
      "epoch: 8, iter: 0, loss is: [0.02260392]\n",
      "epoch: 8, iter: 20, loss is: [0.05158296]\n",
      "epoch: 8, iter: 40, loss is: [0.03655307]\n",
      "epoch: 8, iter: 60, loss is: [0.01882508]\n",
      "epoch: 9, iter: 0, loss is: [0.08805832]\n",
      "epoch: 9, iter: 20, loss is: [0.0167549]\n",
      "epoch: 9, iter: 40, loss is: [0.03234318]\n",
      "epoch: 9, iter: 60, loss is: [0.047973]\n"
     ]
    }
   ],
   "source": [
    "EPOCH_NUM = 10   # 设置外层循环次数\r\n",
    "BATCH_SIZE = 10  # 设置batch大小\r\n",
    "\r\n",
    "# 定义外层循环\r\n",
    "for epoch_id in range(EPOCH_NUM):\r\n",
    "    # 在每轮迭代开始之前，将训练数据的顺序随机的打乱\r\n",
    "    np.random.shuffle(training_data)\r\n",
    "    # 将训练数据进行拆分，每个batch包含10条数据\r\n",
    "    mini_batches = [training_data[k:k+BATCH_SIZE] for k in range(0, len(training_data), BATCH_SIZE)]\r\n",
    "    # 定义内层循环\r\n",
    "    for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "        x = np.array(mini_batch[:, :-1]) # 获得当前批次训练数据\r\n",
    "        y = np.array(mini_batch[:, -1:]) # 获得当前批次训练标签（真实房价）\r\n",
    "        # 将numpy数据转为飞桨动态图tensor形式\r\n",
    "        house_features = paddle.to_tensor(x)\r\n",
    "        prices = paddle.to_tensor(y)\r\n",
    "        \r\n",
    "        # 前向计算\r\n",
    "        predicts = model(house_features)\r\n",
    "        \r\n",
    "        # 计算损失\r\n",
    "        loss = F.square_error_cost(predicts, label=prices)\r\n",
    "        avg_loss = paddle.mean(loss)\r\n",
    "        if iter_id%20==0:\r\n",
    "            print(\"epoch: {}, iter: {}, loss is: {}\".format(epoch_id, iter_id, avg_loss.numpy()))\r\n",
    "        \r\n",
    "        # 反向传播\r\n",
    "        avg_loss.backward()\r\n",
    "        # 最小化loss,更新参数\r\n",
    "        opt.step()\r\n",
    "        # 清除梯度\r\n",
    "        opt.clear_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 保存并测试模型\n",
    "\n",
    "### 保存模型\n",
    "\n",
    "将模型当前的参数数据``model.state_dict()``保存到文件中（通过参数指定保存的文件名 LR_model），以备预测或校验的程序调用，代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存成功，模型参数保存在LR_model.pdparams中\n"
     ]
    }
   ],
   "source": [
    "# 保存模型参数，文件名为LR_model.pdparams\r\n",
    "paddle.save(model.state_dict(), 'LR_model.pdparams')\r\n",
    "print(\"模型保存成功，模型参数保存在LR_model.pdparams中\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "理论而言，直接使用模型实例即可完成预测，而本教程中预测的方式为什么是先保存模型，再加载模型呢？这是因为在实际应用中，训练模型和使用模型往往是不同的场景。模型训练通常使用大量的线下服务器（不对外向企业的客户/用户提供在线服务），而模型预测则通常使用线上提供预测服务的服务器，或者将已经完成的预测模型嵌入手机或其他终端设备中使用。因此本教程的讲解方式更贴合真实场景的使用方法。\n",
    "\n",
    "回顾下基于飞桨实现的房价预测模型，实现效果与之前基于Python实现的模型没有区别，但两者的实现成本有天壤之别。飞桨的愿景是用户只需要了解模型的逻辑概念，不需要关心实现细节，就能搭建强大的模型。\n",
    "\n",
    "### 测试模型\n",
    "\n",
    "下面我们选择一条数据样本，测试下模型的预测效果。测试过程和在应用场景中使用模型的过程一致，主要可分成如下三个步骤：\n",
    "\n",
    "1. 配置模型预测的机器资源。本案例默认使用本机，因此无需写代码指定。\n",
    "1. 将训练好的模型参数加载到模型实例中。由两个语句完成，第一句是从文件中读取模型参数；第二句是将参数内容加载到模型。加载完毕后，需要将模型的状态调整为``eval()``（校验）。上文中提到，训练状态的模型需要同时支持前向计算和反向传导梯度，模型的实现较为臃肿，而校验和预测状态的模型只需要支持前向计算，模型的实现更加简单，性能更好。\n",
    "1. 将待预测的样本特征输入到模型中，打印输出的预测结果。\n",
    "\n",
    "通过``load_one_example``函数实现从数据集中抽一条样本作为测试样本，具体实现代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_one_example():\r\n",
    "    # 从上边已加载的测试集中，随机选择一条作为测试数据\r\n",
    "    idx = np.random.randint(0, test_data.shape[0])\r\n",
    "    idx = -10\r\n",
    "    one_data, label = test_data[idx, :-1], test_data[idx, -1]\r\n",
    "    # 修改该条数据shape为[1,13]\r\n",
    "    one_data =  one_data.reshape([1,-1])\r\n",
    "\r\n",
    "    return one_data, label\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2c02b4d79248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 对结果做反归一化处理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mavg_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# 对label数据做反归一化处理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mavg_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_values' is not defined"
     ]
    }
   ],
   "source": [
    "# 参数为保存模型参数的文件地址\r\n",
    "model_dict = paddle.load('LR_model.pdparams')\r\n",
    "model.load_dict(model_dict)\r\n",
    "model.eval()\r\n",
    "\r\n",
    "# 参数为数据集的文件地址\r\n",
    "one_data, label = load_one_example()\r\n",
    "# 将数据转为动态图的variable格式 \r\n",
    "one_data = paddle.to_tensor(one_data)\r\n",
    "predict = model(one_data)\r\n",
    "\r\n",
    "# 对结果做反归一化处理\r\n",
    "predict = predict * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "# 对label数据做反归一化处理\r\n",
    "label = label * (max_values[-1] - min_values[-1]) + avg_values[-1]\r\n",
    "\r\n",
    "print(\"Inference result is {}, the corresponding label is {}\".format(predict.numpy(), label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
