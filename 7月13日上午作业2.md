# 7月13日上午作业2

## 可变形卷积v1

#### 一、提出背景

​		如何有效地对几何图形的变化进行建模一直是一个挑战，大体上有两种处理方法：（1）构建一个包含各种变化的数据集，其本质是数据扩增（2）使用具有形变不变性的特征和算法（如SIFT）。这两种方法都有很大的局限性：几何形变被假设是固定和已知的，这是一种先验信息，用这些已知的形变去处理未知的形变是不合理的；手工设计的特征或算法无法应对过度复杂的形变，即使该形变是已知的。近年来，CNNs在计算机视觉领域取得了飞速的发展和进步，在图像分类，语义分割，目标检测领域都有很好的应用。然后鉴于CNNs固定的几何结构，导致对几何形变的建模受到限制。本文提出了两个新模块来提升CNNs的形变建模能力，称为“deformable convolution”和“deformable ROI pooling”,这两个方法都是基于在模块中增加额外偏移量的空间采样位置和从目标任务中学习到偏移量且不需要额外的监督。这些新的模块可以很容易的取代现有CNNs的普通模块并且利用反向传播进行端到端的训练，产生可变形的卷积神经网络。该方法在语义分割和目标检测上有很好的表现。

#### 二、可变形卷积v1及其应用

CNNs对大型，未知形状变换的建模存在固有的缺陷，这种缺陷来源于CNNs模块固有的几何结构：卷积单元对输入特征图的固定位置进行采样；池化层以固定的比例进行池化；即使是ROI pooling也是将ROI分割到固定的bin中去。这些特性是有影响的，例如，在同一层Conv中，所有的激活单元的感受野是一样的，但由于不同位置可能对应着不同尺度或变形的物体，因此对尺度或者感受野大小进行自适应是进行精确定位所需要的。为了解决或者减轻这个问题，本文提出了两种新的模块，可变形卷积（deformable conv）和可变形感兴趣区域池化（deformable ROI Pooling）,来提高对形变的建模能力。这两个模块都是基于一个平行网络学习offset（偏移），使得卷积核在input feature map的采样点发生偏移，集中于我们感兴趣的区域或者目标。通过研究发现，标准卷积中的规则格点采样是导致网络难以适应几何形变的“罪魁祸首”，为了削弱这个限制，对卷积核中每个采样点的位置都增加了一个偏移变量，可以实现在当前位置附近随意采样而不局限于之前的规则格点。如下图所示，是常见的采样点和可变形卷积采样的对比

![img](https://pic2.zhimg.com/80/v2-cc3a4124a8ff451d330e7aefba1d19d5_720w.jpg)


（a）是常见的3x3卷积核的采样方式，（b）是采样可变形卷积，加上偏移量之后的采样点的变化，其中（c）(d)是可变形卷积的特殊形式

- **Networks**

- - **Deformable Convolution**

**（1）**首先声明一点，在可变形卷积中，可变形卷积操作和池化操作都是2维的，都是在同一channel上进行的，常规的卷积操作主要可以分为两部分：（1）在输入的feature map上使用规则网格R进行采样；（2）进行加权运算，R定义了感受野的大小和扩张

![img](https://pic3.zhimg.com/80/v2-6e6aaf7edc36192d934b5b2af9465202_720w.png)


对于在输出的feature map上的每个位置P0,通过下列式子进行计算：

![img](https://pic1.zhimg.com/80/v2-730bba50b3b9ef48cb01705982470d74_720w.jpg)


其中，Pn是对R中所列位置的枚举
**（2）**可变形卷积的操作是不同的，在可变形网络的操作中，常规的规则网格R通过增加一个偏移量进行扩张，同样的位置P0变为：

![img](https://pic4.zhimg.com/80/v2-0e97bd304ed01fbad6b345975949624f_720w.jpg)


现在，采样的位置变成了不规则位置，由于偏移量△Pn通常是小数，因此我们通过双线性插值法进行实现，公式为：

![img](https://pic2.zhimg.com/80/v2-c4a2238abba03fa396bffd4d3f6659ad_720w.jpg)

##### **可变形卷积的可视化**



![img](https://pic4.zhimg.com/80/v2-ac536e8bd9430f9c06347e59b8a75bf3_720w.jpg)

我们可以从上图看到，可以看到当绿色点在目标上时，红色点所在区域也集中在目标位置，并且基本能够覆盖不同尺寸的目标，因此经过可变形卷积，我们可以更好地提取出感兴趣物体的完整特征，效果是非常不错的。

#### 三、可变形卷积和标准卷积的区别

- deformable convolution，在常规的卷积中增加2D偏移值，可以将采样区域自由变形，如图1(d)。偏移值是可学习的，通过额外的卷积层获得，取决于输入的特征。
- deformable RoI pooling，为RoI pooling中的每个bin添加一个偏移值做整体的移动，能够自适应不同形状目标的局部定位。同样，偏移值是可学习的，取决于输入特征和RoI区域

## 可变形卷积v2

#### 一、提出背景

DCN v1听起来不错，但其实也有问题：我们的可变形卷积有可能*引入了无用的上下文（区域）来干扰我们的特征提取*，这显然会降低算法的表现。作者也做了一个实验进行对比说明：

![img](https://pic4.zhimg.com/80/v2-d030ab506fbed8bed5c1fd426c3e604f_720w.jpg)

我们可以看到虽然DCN v1更能覆盖整个物体，但是同时也会引入一些无关的背景，这造成了干扰，所以作者提出了三个解决方法：

（1）*More Deformable Conv Layers（使用更多的可变形卷积）*。

（2）*Modulated Deformable Modules（在DCNv1基础（添加offset）上添加每个采样点的权重）*

（3）*R-CNN Feature Mimicking（模拟R-CNN的feature）。*

#### 二、可变形卷积v2及其应用

##### **使用更多的可变形卷积**

在DCN v1中只在conv 5中使用了三个可变形卷积，在DCN v2中把conv3到conv5都换成了可变形卷积，提高算法对几何形变的建模能力。

![img](https://pic2.zhimg.com/80/v2-a4e0056b833385e2ea37522f89a86d7d_720w.jpg)

##### **在DCNv1基础（添加offset）上添加每个采样点的权重**

我们知道在DCN v1中的卷积是添加了一个offset![[公式]](https://www.zhihu.com/equation?tex=%5CDelta%7BP_n%7D):

![img](https://pic4.zhimg.com/80/v2-7a06b5893d8008a7f7219f204803da8f_720w.jpg)

为了解决引入了一些无关区域的问题，在DCN v2中我们不只添加每一个采样点的偏移，还添加了一个权重系数![[公式]](https://www.zhihu.com/equation?tex=%5CDelta%7Bm_k%7D)，来区分我们引入的区域是否为我们感兴趣的区域，假如这个采样点的区域我们不感兴趣，则把权重学习为0即可：

![img](https://pic1.zhimg.com/80/v2-4d7662db5e10d0c857a57e9cc24bcfd4_720w.jpg)

*总的来说，DCN v1中引入的offset是要寻找有效信息的区域位置，DCN v2中引入权重系数是要给找到的这个位置赋予权重，这两方面保证了有效信息的准确提取*。

##### **R-CNN Feature Mimicking**

作者发现把R-CNN和Faster RCNN的classification score结合起来可以提升performance，说明R-CNN学到的focus在物体上的feature可以解决无关上下文的问题。但是增加额外的R-CNN会使inference速度变慢很多。DCNV2里的解决方法是*把R-CNN当做teacher network，让DCN V2的ROIPooling之后的feature去模拟R-CNN的feature*，类似知识蒸馏的做法，下面会具体展开：

![img](https://pic2.zhimg.com/80/v2-6b19538af42e6613ef3b463a3f7c69b5_720w.jpg)

左边的网络为主网络（Faster RCNN），右边的网络为子网络（RCNN）。实现上大致是用主网络训练过程中得到的RoI去裁剪原图，然后将裁剪到的图resize到224×224大小作为子网络的输入，这部分最后提取的特征和主网络输出的1024维特征作为feature mimicking loss的输入，用来约束这2个特征的差异（通过一个余弦相似度计算，如下图所示），同时子网络通过一个分类损失进行监督学习，因为并不需要回归坐标，所以没有回归损失。在inference阶段仅有主网络部分，因此这个操作不会在inference阶段增加计算成本。

![img](https://pic3.zhimg.com/80/v2-a33bd318fe6ea7eeec959cfe61d27392_720w.jpg)

再用直白一点的话说，*因为RCNN这个子网络的输入就是RoI在原输入图像上裁剪出来的图像，因此不存在RoI以外区域信息的干扰，这就使得RCNN这个网络训练得到的分类结果更加可靠，以此通过一个损失函数监督主网络Faster RCNN的分类支路训练就能够使网络提取到更多RoI内部特征，而不是自己引入的外部特征。*

总的loss由三部分组成：mimic loss + R-CNN classification loss + Faster-RCNN loss.

##### **DCN v2可视化**

通过实验结果我们也可以看到DCN v2更能集中在物体的完整有效的区域：

![img](https://pic1.zhimg.com/80/v2-6c0892c628a65f7e0a2a3548eed25b58_720w.jpg)