# 深度学习基础知识

18200100218   刘士杰  飞桨四队

## 一.深度学习发展历史

### 1.起源阶段

1943年，由神经科学家麦卡洛克(W.S.McCilloch) 和数学家皮兹（W.Pitts）\*在《数学生物物理学公告》上发表论文《神经活动中内在思想的逻辑演算》（A Logical Calculus of the Ideas Immanent in Nervous Activity）。建立了神经网络和数学模型，称为MCP模型。所谓\*MCP模型，其实是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，也就诞生了所谓的“模拟大脑”，人工神经网络的大门由此开启。

1958年，计算机科学家罗森布拉特（ Rosenblatt）提出了两层神经元组成的神经网络，称之为“感知器”(Perceptrons)。第一次将MCP用于机器学习（machine learning）分类(classification)。“感知器”算法算法使用MCP模型对输入的多维数据进行二分类，且能够使用梯度下降法从训练样本中自动学习更新权值。1962年,该方法被证明为能够收敛，理论与实践效果引起第一次神经网络的浪潮。

1969年，纵观科学发展史，无疑都是充满曲折的，深度学习也毫不例外。 1969年，美国数学家及人工智能先驱 Marvin Minsky 在其著作中证明了感知器本质上是一种线性模型（linear model），只能处理线性分类问题，就连最简单的XOR（亦或）问题都无法正确分类。这等于直接宣判了感知器的死刑，神经网络的研究也陷入了将近20年的停滞。

1986年，由神经网络之父 Geoffrey Hinton 在1986年发明了适用于多层感知器（MLP）的BP（Backpropagation）算法，并采用Sigmoid进行非线性映射，有效解决了非线性分类和学习的问题。该方法引起了神经网络的第二次热潮。

90年代时期，1991年BP算法被指出存在梯度消失问题，也就是说在误差梯度后项传递的过程中，后层梯度以乘性方式叠加到前层，由于Sigmoid函数的饱和特性，后层梯度本来就小，误差梯度传到前层时几乎为0，因此无法对前层进行有效的学习，该问题直接阻碍了深度学习的进一步发展。

此外90年代中期，支持向量机算法诞生（SVM算法）等各种浅层机器学习模型被提出，SVM也是一种有监督的学习模型，应用于模式识别，分类以及回归分析等。支持向量机以统计学为基础，和神经网络有明显的差异，支持向量机等算法的提出再次阻碍了深度学习的发展。

### 2.发展阶段

2006年，加拿大多伦多大学教授、机器学习领域泰斗、神经网络之父—— Geoffrey Hinton 和他的学生 Ruslan Salakhutdinov 在顶尖学术刊物《科学》上发表了一篇文章，该文章提出了深层网络训练中梯度消失问题的解决方案：无监督预训练对权值进行初始化+有监督训练微调。斯坦福大学、纽约大学、加拿大蒙特利尔大学等成为研究深度学习的重镇，至此开启了深度学习在学术界和工业界的浪潮。

2011年，ReLU激活函数被提出，该激活函数能够有效的抑制梯度消失问题。2011年以来，微软首次将DL应用在语音识别上，取得了重大突破。微软研究院和Google的语音识别研究人员先后采用DNN技术降低语音识别错误率20％~30％，是语音识别领域十多年来最大的突破性进展。2012年，DNN技术在图像识别领域取得惊人的效果，在ImageNet评测上将错误率从26％降低到15％。在这一年，DNN还被应用于制药公司的DrugeActivity预测问题，并获得世界最好成绩。

### 3.爆发阶段

2012年，Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络AlexNet一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。

2013、2014、2015、2016年，通过ImageNet图像识别比赛，DL的网络结构，训练方法，GPU硬件的不断进步，促使其在其他领域也在不断的征服战场。

2016年3月，由谷歌（Google）旗下DeepMind公司开发的AlphaGo(基于深度学习)与围棋世界冠军、职业九段棋手李世石进行围棋人机大战，以4比1的总比分获胜；2016年末2017年初，该程序在中国棋类网站上以“大师”（Master）为注册帐号与中日韩数十位围棋高手进行快棋对决，连续60局无一败绩；2017年5月，在中国乌镇围棋峰会上，它与排名世界第一的世界围棋冠军柯洁对战，以3比0的总比分获胜。围棋界公认阿尔法围棋的棋力已经超过人类职业围棋顶尖水平。





## 二.人工智能，机器学习和深度学习之间有什么区别和联系

这三者之间是一个包含的关系，人工智能是一个相当宽泛的概念，它也是非常前言的综合学科，其涉及计算机科学、统计学、脑神经学和社会科学等学科，其中，机器学习在近年来逐渐火热，而实现机器学习又以深度学习为最，即这三者之间是一个逐级包含的关系。其关系可如下图展示。

<img src="C:\Users\刘士杰\AppData\Roaming\Typora\typora-user-images\image-20210710151205484.png" alt="image-20210710151205484" style="zoom:150%;" />

另外，为实现深度学习，近年来，神经网络成为其最为火爆的工具，比如卷积神经网络，递归神经网络等等，成为一个非常高效的工具。其与另外三者的交叉也如下。

![image-20210710151333202](C:\Users\刘士杰\AppData\Roaming\Typora\typora-user-images\image-20210710151333202.png)







## 三.神经元，单层感知机，多层感知机

### 1.神经元

![image-20210710152025122](C:\Users\刘士杰\AppData\Roaming\Typora\typora-user-images\image-20210710152025122.png)

一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。突触之间的交流通过神经递质实现。

下面对上面的这个模型进行抽象处理。首先考虑到神经元结构有多个树突，一个轴突可将其抽象为下图的黑箱结构：

​      ![img](file:///C:\Users\刘士杰\AppData\Local\Temp\ksohtml2368\wps1.jpg)

### 2.单层感知机

将神经元模型中的黑箱进一步细化以及符号化就可以得到单层感知机模型，示意图如下：

![image-20210710152222317](C:\Users\刘士杰\AppData\Roaming\Typora\typora-user-images\image-20210710152222317.png)

可以看到，感知机的基本模型包括：

**输入**： 实际可能回比这更多,此处添加了一个偏置1，是为了平衡线性加权函数总是过零点的问题。

**权值**：对应于每个输入都有一个加权的权值。

**激活函数**：激活函数f对应于一个非线性函数，其选择有很多，如sigmoid,tanh函数等等。

**输出**：由激活激活函数进行处理后的结果，往往是区分度较大的非连续值用于分类。

单层感知机缺点是无法解决“异或”问题

### 3.多层感知机

为了弥补单层感知机无法解决“异或”问题的缺陷，多层感知机出现了，多层感知机就是含有至少一

个隐藏层的由全连接层组成的神经网络，且在整个神经网络中除了输入层之外，每一层都经过激活

函数进行非线性变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。参考下图的单隐

藏层的MLP结构：

![image-20210710152522453](C:\Users\刘士杰\AppData\Roaming\Typora\typora-user-images\image-20210710152522453.png)

多层感知器（MLP，Multilayer Perceptron）是一种前馈人工神经网络模型，其将输入的多个数据集映射到单一的输出的数据集上。







## 四.什么是前向传播

前向传播可用下图的网络结构来表示

![image-20210710152818952](C:\Users\刘士杰\AppData\Roaming\Typora\typora-user-images\image-20210710152818952.png)

例如，假设上一层结点i,j,k,…等一些结点与本层的结点w有连接，那么结点w的值怎么算呢？就是通过上一层的i,j,k等结点以及对应的连接权值进行加权和运算，最终结果再加上一个偏置项（图中为了简单省略了），最后在通过一个非线性函数（即激活函数），如ReLu，sigmoid等函数，最后得到的结果就是本层结点w的输出。

最终不断的通过这种方法一层层的运算，得到输出层结果。

对于前向传播来说，不管维度多高，其过程都可以用如下公式表示：

a2 = σ(z2) = σ(a1 * W2 +b2)







## 五.什么是反向传播

BP（backpropagaion）算法，即误差反向传播算法，适合于多层神经元网络的一种学习算法，它建立在梯度下降法的基础上。BP网络的输入输出关系实质上是一种映射关系：一个n输入m输出的BP神经网络所完成的功能是从n维欧氏空间向m维欧氏空间中一有限域的连续映射，这一映射具有高度非线性。它的信息处理能力来源于简单非线性函数的多次复合，因此具有很强的函数复现能力。这是BP算法得以应用的基础。

其整个的算法过程如下：

**1.输入数据；**

**2初始化参数w,b；**

**3.前向传播计算预测值；**

**4.利用损失函数计算损失；**

**5.反向传播逐级更新梯度，即更新参数w,b；**

经过多次的反向传播即可得到我们的理想参数。



