YOLO v1
YOLO把目标检测看作一个回归问题，直接用一个网络进行分类和框回归。YOLO网络借鉴了GoogLeNet分类网络结构。不同的是，YOLO未使用inception module，而是使用1x1卷积层（此处1x1卷积层的存在是为了跨通道信息整合）和3x3卷积层简单替代。卷积层提取特征，全连接层预测类别和框位置回归，共24个卷积层，2个全连接层。

实现方法：
1.将一幅图像分成SxS个网格(grid cell)，如果某个object的中心 落在这个网格中，则这个网格就负责预测这个object。
2.每个网络需要预测B个BBox的位置信息和confidence（置信度）信息，一个BBox对应着四个位置信息和一个confidence信息。confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息：其中如果有object落在一个grid cell里，第一项取1，否则取0。 第二项是预测的bounding box和实际的groundtruth之间的IoU值。
3.每个bounding box要预测(x,y,w,h)和confidence共5个值，每个网格还要预测一个类别信息，记为C类。则SxS个网格，每个网格要预测B个bounding box还要预测C个categories。输出就是SxSx(5 * B + C)的一个tensor。（注意：class信息是针对每个网格的，confidence信息是针对每个bounding box的。）
4.在test的时候，每个网格预测的class信息和bounding box预测的confidence信息相乘，就得到每个bounding box的class-specific confidence score。
5.得到每个box的class-specific confidence score以后，设置阈值，滤掉得分低的boxes，对保留的boxes进行NMS处理，就得到最终的检测结果。

优点：
速度快。看作一个回归问题，不需要复杂的pipeline。对图像有全局理解。用整个图像的特征去预测bbox，而不是像RCNN，只能候选框的特征预测bbox。候选框的数量少很多，仅7 * 7 * 2=49个。而RCNN的selectlive search有2000个，计算量大。

缺点：
每个网格只预测2个bbox，限制了模型预测物体的数量。多次下采样，边界框预测所使用的特征是相对粗糙的特征。

YOLO v2
YOLO v2相对v1版本，在继续保持处理速度的基础上，进行了如下改进：
1.网络采用DarkNet-19主干网络的升级：使用全局平均池化，每个特征图得到1个值，再用全连接会少很多参数。
2.Batch Normalization：v1中也大量用了Batch Normalization，同时在定位层后边用了dropout，v2中取消了dropout，在卷积层全部使用Batch Normalization。BN能够给模型收敛带来显著地提升，同时也消除了其他形式正则化的必要，提高模型泛化能力。
3.使用anchors：借鉴faster R-CNN和SSD，对于一个中心点，使用多个anchor，得到多个bounding box，每个bounding box包含4个位置坐标参数(x,y,w,h)和21个类别概率信息。而在yolov1中，每个grid（对应anchor），仅预测一次类别，而且只有两个bounding box来进行坐标预测。v1中直接在卷积层之后使用全连接层预测bbox的坐标。v2借鉴Faster R-CNN的思想预测bbox的偏移.移除了全连接层,并且删掉了一个pooling层使特征的分辨率更大一些。v1中每张图片预测7x7x2=98个box,而v2加上Anchor Boxes能预测13X13X5(5+20)个box。
4.去掉全连接层：和SSD一样，模型中只包含卷积和平均池化层（平均池化是为了变为一维向量，做softmax分类）。这样做一方面是由于物体检测中的目标，只是图片中的一个区块，它是局部感受野，没必要做全连接。而是为了输入不同尺寸的图片，如果采用全连接，则只能输入固定大小图片了。
5.YOLO v2的损失函数跟YOLO v1差别不大，唯一的差别就是关于bbox的w和h的损失去掉了根号。

YOLO v3
YOLO v3的模型比之前的模型复杂了不少，可以通过改变模型结构的大小来权衡速度与精度。
简而言之，YOLO v3的先验检测（Prior detection）系统将分类器或定位器重新用于执行检测任务。他们将模型应用于图像的多个位置和尺度。而那些评分较高的区域就可以视为检测结果。此外，相对于其它目标检测方法，我们使用了完全不同的方法。我们将一个单神经网络应用于整张图像，该网络将图像划分为不同的区域，因而预测每一块区域的边界框和概率，这些边界框会通过预测的概率加权。我们的模型相比于基于分类器的系统有一些优势。它在测试时会查看整个图像，所以它的预测利用了图像中的全局信息。与需要数千张单一目标图像的R-CNN不同，它通过单一网络评估进行预测。这令YOLO v3非常快，一般它比R-CNN快1000倍、比Fast R-CNN快100倍。

改进之处在于：
1.多尺度预测：YOLO v3借鉴了FPN的思想，从不同尺度提取特征。相比YOLO v2，YOLO v3提取最后3层特征图，不仅在每个特征图上分别独立做预测，同时通过将小特征图上采样到与大的特征图相同大小，然后与大的特征图拼接做进一步预测。用维度聚类的思想聚类出9种尺度的anchor box，将9种尺度的anchor box均匀的分配给3种尺度的特征图。
2.更好的基础分类网络：结合残差思想，提取更深层次的语义信息。仍然使用连续的3×3和1×1的卷积层。通过上采样对三个不同尺度做预测。如将8 * 8的特征图上采样和16 * 16的特征图相加再次计算，这样可以预测出更小的物体。采用了步长为2的卷积层代替pooling层，因为池化层会丢失信息。
3.分类器不使用Softmax，分类损失采用binary cross-entropy loss（二分类交叉损失熵）。

YOLO v4
相比于YOLO v3，总结一下YOLOv4框架：
Backbone：CSPDarknet53
Neck：SPP，PAN
Head：YOLO v3
YOLOv4 = CSPDarknet53 + SPP + PAN + YOLO v3

改进之处在于：
1.backbone由Darknet53变成CSPDarknet53。
2.学习率的不同：在YOLO v3中，学习率一般都是使用衰减学习率，就是每学习多少epoch，学习率减少固定的值。在YOLO v4中，学习率的变化使用了一种比较新的方法——学习率余弦退火衰减。
3.激活函数不同：YOLO v3使用的leaky relu激活函数，YOLO v4使用的mish激活函数。

YOLO v5
相比于YOLO v4，YOLO v5在原理性方法没有太多改进。但是在速度与模型大小上比YOLO v4有较大提升，可以认为是通过模型裁剪后的工程化应用（即推理速度和准确率增加、模型尺寸减小）。改进之处在于：
1.输入端：Mosaic数据增强、自适应锚框计算：yolov5会进行三种数据增强：缩放，色彩空间调整和马赛克增强。其中马赛克增强是通过将四张图像进行随机缩放、随机裁剪、随机分布方式进行拼接，小目标的检测效果得到提升。yolov5还将初始化anchor的功能嵌入到代码中，每次训练数据集之前，都会自动计算该数据集最合适的Anchor尺寸，该功能可以在代码中设置超参数进行关闭。
2.Backbone：Focus结构，CSP结构：yolov5中设计了两种CSP结构，CSP1_X应用于BackBone主干网络，另一种CSP_2X结构则应用于Neck中。在yolov5中，原始的图像输入Focus结构，采用切片操作，先变成的特征图，再经过一次32个卷积核的卷积操作，最终变成的特征图，focus结构减少了FLOPs，提高了速度，但对于模型的精度mAP没有提升。
3.Neck：采用FPN+PAN结构：yolov5的Neck结构中，采用借鉴CSPnet设计的CSP2_X结构，加强网络特征融合的能力。
4.Prediction：损失函数采用GIOU_Loss。
