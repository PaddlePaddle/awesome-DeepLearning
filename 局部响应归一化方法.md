## 局部响应归一化方法

### 概念

##### 侧抑制 #####

在生物神经学有一个概念叫做侧抑制(lateral inhibition)，侧抑制指的是相近的[神经元彼此之间发生的抑制作用，即在某个神经元受到刺激而产生兴奋时，再刺激相近的神经元，则后者所发生的兴奋对前者产生的抑制作用。

##### LNR(Local Response Normalization)

归一化的目的的 “抑制” ，局部响应归一化借鉴侧抑制的思想来实现局部抑制。

LRN是一种提高深度学习准确度的技术方法。LRN一般是在激活、池化函数后的一种方法。

在ALexNet中，提出了LRN层，对局部神经元的活动创建竞争机制，**使其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。**

### 算法流程

##### 计算公式

$$
b_{x,y}^{i} = \frac{a_{x,y}^{i}}{(k+\alpha\sum_{j=max(0, i-n/2)}^{min(N-1, i+n/2)}(a_{x,y}^{j})^2)^{\beta}}
$$

$$
b_{x,y}^{i}是归一化后的值，i是通道的位置，代表更新第几个通道的值，x与y代表待更新像素的位置；
a_{x,y}^{i}是输入值，即激活函数Relu对a的输出值；
k, \alpha, \beta, n/2都是自定义的系数；
N是总通道数。
$$

##### 公式详解

这个公式中的a表示卷积层（包括卷积操作和池化操作）后的输出结果，这个输出结果的结构是一个四维数组[batch,height,width,channel]，batch就是批次数(每一批为一张图片)，height就是图片高度，width就是图片宽度，channel就是通道数可以理解成一批图片中的某一个图片经过卷积操作后输出的神经元个数(或是理解成处理后的图片深度)。

ai(x,y)表示在这个输出结构中的一个位置[a,b,c,d]，可以理解成在某一张图中的某一个通道下的某个高度和某个宽度位置的点，即第a张图的第d个通道下的高度为b宽度为c的点。

公式中的N表示通道数(channel)。a,n/2,k,α,β分别表示函数中的input,depth_radius,bias,alpha,beta，其中n/2,k,α,β都是自定义的，特别注意一下∑叠加的方向是沿着通道方向的，即每个点值的平方和是沿着a中的第3维channel方向的，也就是一个点同方向的前面n/2个通道（最小为第0个通道）和后n/2个通道（最大为第d-1个通道）的点的平方和(共n+1个点)。我们把input当成是d个3维的矩阵，即把input的通道数当作3维矩阵的个数，叠加的方向也是在通道方向。

### 作用

1.归一化有助于快速收敛；

2.对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。

### 应用场景

LRN归一化首次应用在AlexNet模型中。LRN通过在相邻卷积核生成的feature map之间引入竞争，从而有些本来在feature map中显著的特征在A中更显著，而在相邻的其他feature map中被抑制，这样让不同卷积核产生的feature map之间的相关性变小。
