```python
!pip install -U d2l
!python -m pip install paddlepaddle -i https://mirror.baidu.com/pypi/simple
```

# 3.2 çº¿æ€§å›å½’ä»0å¼€å§‹å®ç°


```python
import random
# torch
import torch
from d2l import torch as d2l
# paddle
import numpy as np
import paddle
import matplotlib.pyplot as plt
```

## 3.2.1ç”Ÿæˆæ•°æ®é›†

è¿™é‡Œæ ¹æ®å¸¦æœ‰å™ªå£°çš„çº¿æ€§æ¨¡å‹æ„é€ ä¸€ä¸ªäººé€ æ•°æ®é›†ã€‚ç„¶åä½¿ç”¨è¿™ä¸ªæœ‰é™æ ·æœ¬çš„æ•°æ®é›†æ¢å¤æ­¤æ¨¡å‹çš„å‚æ•°ã€‚ä¸ºäº†æ–¹ä¾¿å¯è§†åŒ–ï¼Œè¿™é‡Œé‡‡ç”¨äº†ä½ç»´æ•°æ®ã€‚

ä¸‹é¢ä»£ç ä¸­ï¼Œç”Ÿæˆäº†ä¸€ä¸ªåŒ…å«1000ä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·çš„ä¸¤ä¸ªç‰¹å¾ã€‚åˆæˆæ•°æ®é›†æ˜¯ä¸€ä¸ªçŸ©é˜µï¼š

$$X\in R^{100 \times 2}$$

ä½¿ç”¨çº¿æ€§æ¨¡å‹å‚æ•°$w=[2,-3.4]^T$ã€$b=4.2$å’Œå™ªå£°é¡¹$\varepsilon$ç”Ÿæˆæ•°æ®é›†åŠå…¶æ ‡ç­¾ï¼š
$$y=Xw+b+\varepsilon$$

ä½ å¯ä»¥å°†$\varepsilon$è§†ä¸ºæ•è·ç‰¹å¾å’Œæ ‡ç­¾æ—¶çš„æ½œåœ¨è§‚æµ‹è¯¯å·®ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬è®¤ä¸ºæ ‡å‡†å‡è®¾æˆç«‹ï¼Œå³$\varepsilon$æœä»å‡å€¼ä¸º0çš„æ­£æ€åˆ†å¸ƒã€‚ ä¸ºäº†ç®€åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬å°†æ ‡å‡†å·®è®¾ä¸º0.01ã€‚ä¸‹é¢çš„ä»£ç ç”Ÿæˆåˆæˆæ•°æ®é›†ã€‚

#### torchç‰ˆ


```python
def synthetic_data(w, b, num_examples):
    """ç”Ÿæˆ y = Xw + b + å™ªå£°ã€‚"""
    X = torch.normal(0, 1, (num_examples, len(w)))
    print(X.shape)
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
print('features:', features[0], '\nlabel:', labels[0])
```


```python
d2l.set_figsize()
d2l.plt.scatter(features[:, (1)].detach().numpy(),
                labels.detach().numpy(), 1);
```

#### d2lç‰ˆ


```python
def synthetic_data(w, b, num_examples):
    """ç”Ÿæˆ y = Xw + b + å™ªå£°ã€‚"""
    X = np.random.normal(0, 1, (num_examples, len(w)))
    print(X.shape, 'X')
    y = np.dot(X, w) + b
    print(y.shape, 'y')
    y += np.random.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))

d2l_w = np.array([2, -3.4])
print(d2l_w.shape, 'd2l_w')
d2l_b = 4.2
d2l_features, d2l_labels = synthetic_data(d2l_w, d2l_b, 1000)
```


```python
plt.scatter(d2l_features[:, (1)], d2l_labels, 1);
```

#### paddleç‰ˆ


```python
paddle.utils.run_check()
```


```python
def synthetic_data_paddle(w, b, num_examples):
  """Generate y = Xw + b + noise."""
  X = np.random.normal(0, 1, (num_examples, len(w)))
  y = np.dot(X, w) + b
  y += np.random.normal(0, 0.01, y.shape)
  return X, y.reshape((-1, 1))

paddle_w = np.array([2, -3.4])
# paddle_w = paddle.to_tensor([2, -3.4])
paddle_b = 4.2
paddle_features, paddle_labels = synthetic_data_paddle(paddle_w, paddle_b, 1000)
print('paddle features:', paddle_features[0], '\nlabel:', paddle_labels[0])
```


```python
plt.scatter(paddle_features[:, (1)], paddle_labels, 1);
```

## 3.2.2 è¯»å–æ•°æ®é›†

è®­ç»ƒæ¨¡å‹æ—¶è¦å¯¹æ•°æ®éå†ï¼Œæ¯æ¬¡æŠ½å–ä¸€å°æ‰¹é‡æ ·æœ¬ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ¥æ›´æ–°æ¨¡å‹ã€‚ç”±äºæ­¤è¿‡ç¨‹æ˜¯è®­ç»ƒæœºå™¨å­¦ä¹ ç®—æ³•çš„åŸºç¡€ï¼Œæ‰€ä»¥æœ‰å¿…è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°èƒ½å¤Ÿæ‰“ä¹±æ•°æ®é›†ä¸­çš„æ ·æœ¬å¹¶ä»¥å°æ‰¹é‡çš„æ–¹å¼è·å–æ•°æ®ã€‚

åœ¨ä¸‹é¢ä»£ç ä¸­ï¼Œå®šä¹‰äº†ä¸€ä¸ª`data_iter`å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥æ”¶æ‰¹é‡å¤§å°ã€ç‰¹å¾çŸ©é˜µå’Œæ ‡ç­¾å‘é‡ä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆå¤§å°ä¸º`batch_size`çš„å°æ‰¹é‡ï¼Œæ¯ä¸ªå°æ‰¹é‡åŒ…å«ä¸€ç»„ç‰¹å¾å’Œæ ‡ç­¾ã€‚

#### torchç‰ˆ


```python
def data_iter(batch_size, features, labels):
    print(batch_size)
    print(features.shape)
    print(labels.shape)
    num_examples = len(features)
    print(num_examples)
    indices = list(range(num_examples))
    # è¿™äº›æ ·æœ¬æ˜¯éšæœºè¯»å–çš„ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡ºåº
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(indices[i:min(i +
                                                   batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
```

#### paddleç‰ˆ


```python
def paddle_data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # è¿™äº›æ ·æœ¬æ˜¯éšæœºè¯»å–çš„ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡ºåº
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = np.array(indices[i:min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
```

é€šå¸¸ï¼Œæˆ‘ä»¬ä½¿ç”¨åˆç†å¤§å°çš„å°æ‰¹é‡æ¥åˆ©ç”¨GPUç¡¬ä»¶çš„ä¼˜åŠ¿ï¼Œå› ä¸ºGPUåœ¨å¹¶è¡Œå¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚æ¯ä¸ªæ ·æœ¬éƒ½å¯ä»¥å¹¶è¡Œåœ°è¿›è¡Œæ¨¡å‹è®¡ç®—ï¼Œä¸”æ¯ä¸ªæ ·æœ¬æŸå¤±å‡½æ•°çš„æ¢¯åº¦ä¹Ÿå¯ä»¥è¢«å¹¶è¡Œåœ°è®¡ç®—ï¼ŒGPUå¯ä»¥åœ¨å¤„ç†å‡ ç™¾ä¸ªæ ·æœ¬æ—¶ï¼Œæ‰€èŠ±è´¹çš„æ—¶é—´ä¸æ¯”å¤„ç†ä¸€ä¸ªæ ·æœ¬æ—¶å¤šå¤ªå¤šã€‚

è®©æˆ‘ä»¬ç›´è§‚æ„Ÿå—ä¸€ä¸‹ã€‚è¯»å–ç¬¬ä¸€ä¸ªå°æ‰¹é‡æ•°æ®æ ·æœ¬å¹¶æ‰“å°ã€‚æ¯ä¸ªæ‰¹é‡çš„ç‰¹å¾ç»´åº¦è¯´æ˜äº†æ‰¹é‡å¤§å°å’Œè¾“å…¥ç‰¹å¾æ•°ã€‚ åŒæ ·çš„ï¼Œæ‰¹é‡çš„æ ‡ç­¾å½¢çŠ¶ä¸`batch_size`ç›¸ç­‰ã€‚

#### torchç‰ˆ


```python
batch_size = 10

for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break
```

#### paddleç‰ˆ


```python
batch_size = 10

for paddle_X, paddle_y in paddle_data_iter(batch_size, paddle_features, paddle_labels):
    print(paddle_X, '\n', paddle_y)
    break
```

å½“æˆ‘ä»¬è¿è¡Œè¿­ä»£æ—¶ï¼Œæˆ‘ä»¬ä¼šè¿ç»­åœ°è·å¾—ä¸åŒçš„å°æ‰¹é‡ï¼Œç›´è‡³éå†å®Œæ•´ä¸ªæ•°æ®é›†ã€‚ ä¸Šé¢å®ç°çš„è¿­ä»£å¯¹äºæ•™å­¦æ¥è¯´å¾ˆå¥½ï¼Œä½†å®ƒçš„æ‰§è¡Œæ•ˆç‡å¾ˆä½ï¼Œå¯èƒ½ä¼šåœ¨å®é™…é—®é¢˜ä¸Šé™·å…¥éº»çƒ¦ã€‚ ä¾‹å¦‚ï¼Œå®ƒè¦æ±‚æˆ‘ä»¬å°†æ‰€æœ‰æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œå¹¶æ‰§è¡Œå¤§é‡çš„éšæœºå†…å­˜è®¿é—®ã€‚ åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å®ç°çš„å†…ç½®è¿­ä»£å™¨æ•ˆç‡è¦é«˜å¾—å¤šï¼Œå®ƒå¯ä»¥å¤„ç†å­˜å‚¨åœ¨æ–‡ä»¶ä¸­çš„æ•°æ®å’Œé€šè¿‡æ•°æ®æµæä¾›çš„æ•°æ®ã€‚

## 3.2.3 åˆå§‹åŒ–æ¨¡å‹å‚æ•°

åœ¨å¼€å§‹ç”¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ¨¡å‹å‚æ•°ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆæœ‰ä¸€äº›å‚æ•°ã€‚ åœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä»å‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º0.01çš„æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·éšæœºæ•°æ¥åˆå§‹åŒ–æƒé‡ï¼Œå¹¶å°†åç½®åˆå§‹åŒ–ä¸º0ã€‚

#### torchç‰ˆ


```python
w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
print(w)
print(b)
```

#### paddleç‰ˆ


```python
# paddle_w = paddle.nn.initializer.Normal(0, 0.01)
# paddle_w.stop_gradient = False
# paddle_b = paddle.zeros(shape=[1])
# paddle_b.stop_gradient = False
paddle_w = np.random.normal(0, 0.01, (2, 1))
paddle_b = np.zeros(1)
print(paddle_w)
print(paddle_b)
```

åœ¨åˆå§‹åŒ–å‚æ•°ä¹‹åï¼Œæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯æ›´æ–°è¿™äº›å‚æ•°ï¼Œç›´åˆ°è¿™äº›å‚æ•°è¶³å¤Ÿæ‹Ÿåˆæˆ‘ä»¬çš„æ•°æ®ã€‚ æ¯æ¬¡æ›´æ–°éƒ½éœ€è¦è®¡ç®—æŸå¤±å‡½æ•°å…³äºæ¨¡å‹å‚æ•°çš„æ¢¯åº¦ã€‚æœ‰äº†è¿™ä¸ªæ¢¯åº¦ï¼Œæˆ‘ä»¬å°±å¯ä»¥å‘å‡å°æŸå¤±çš„æ–¹å‘æ›´æ–°æ¯ä¸ªå‚æ•°ã€‚ å› ä¸ºæ‰‹åŠ¨è®¡ç®—æ¢¯åº¦å¾ˆæ¯ç‡¥è€Œä¸”å®¹æ˜“å‡ºé”™ï¼Œæ‰€ä»¥æ²¡æœ‰äººä¼šæ‰‹åŠ¨è®¡ç®—æ¢¯åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨ 2.5èŠ‚ ä¸­å¼•å…¥çš„è‡ªåŠ¨å¾®åˆ†æ¥è®¡ç®—æ¢¯åº¦ã€‚

## 3.2.4 å®šä¹‰æ¨¡å‹
æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¿…é¡»å®šä¹‰æ¨¡å‹ï¼Œå°†æ¨¡å‹çš„è¾“å…¥å’Œå‚æ•°åŒæ¨¡å‹çš„è¾“å‡ºå…³è”èµ·æ¥ã€‚ å›æƒ³ä¸€ä¸‹ï¼Œè¦è®¡ç®—çº¿æ€§æ¨¡å‹çš„è¾“å‡ºï¼Œæˆ‘ä»¬åªéœ€è®¡ç®—è¾“å…¥ç‰¹å¾  ğ—  å’Œæ¨¡å‹æƒé‡ ğ° çš„çŸ©é˜µ-å‘é‡ä¹˜æ³•ååŠ ä¸Šåç½® ğ‘ ã€‚æ³¨æ„ï¼Œä¸Šé¢çš„ ğ—ğ°  æ˜¯ä¸€ä¸ªå‘é‡ï¼Œè€Œ ğ‘ æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚å›æƒ³ä¸€ä¸‹ 2.1.3èŠ‚ ä¸­æè¿°çš„å¹¿æ’­æœºåˆ¶ã€‚å½“æˆ‘ä»¬ç”¨ä¸€ä¸ªå‘é‡åŠ ä¸€ä¸ªæ ‡é‡æ—¶ï¼Œæ ‡é‡ä¼šè¢«åŠ åˆ°å‘é‡çš„æ¯ä¸ªåˆ†é‡ä¸Šã€‚

#### torchç‰ˆ


```python
def linreg(X, w, b):
    """çº¿æ€§å›å½’æ¨¡å‹ã€‚"""
    return torch.matmul(X, w) + b
```

#### paddleç‰ˆ


```python
def paddle_linreg(X, w, b):
    """çº¿æ€§å›å½’æ¨¡å‹ã€‚"""
    # return paddle.matmul(X, w) + b
    return np.dot(X, w) + b
```

## 3.2.5 å®šä¹‰æŸå¤±å‡½æ•°
å› ä¸ºè¦æ›´æ–°æ¨¡å‹ã€‚éœ€è¦è®¡ç®—æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Œæ‰€ä»¥æˆ‘ä»¬åº”è¯¥å…ˆå®šä¹‰æŸå¤±å‡½æ•°ã€‚ è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ 3.1èŠ‚ ä¸­æè¿°çš„å¹³æ–¹æŸå¤±å‡½æ•°ã€‚ åœ¨å®ç°ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°†çœŸå®å€¼yçš„å½¢çŠ¶è½¬æ¢ä¸ºå’Œé¢„æµ‹å€¼y_hatçš„å½¢çŠ¶ç›¸åŒã€‚


```python
def squared_loss(y_hat, y):
    """å‡æ–¹æŸå¤±ã€‚"""
    return (y_hat - y.reshape(y_hat.shape))**2 / 2
```

## 3.2.6 å®šä¹‰ä¼˜åŒ–ç®—æ³•

æ­£å¦‚æˆ‘ä»¬åœ¨ 3.1èŠ‚ ä¸­è®¨è®ºçš„ï¼Œçº¿æ€§å›å½’æœ‰è§£æè§£ã€‚ç„¶è€Œï¼Œè¿™æ˜¯ä¸€æœ¬å…³äºæ·±åº¦å­¦ä¹ çš„ä¹¦ï¼Œè€Œä¸æ˜¯ä¸€æœ¬å…³äºçº¿æ€§å›å½’çš„ä¹¦ã€‚ ç”±äºè¿™æœ¬ä¹¦ä»‹ç»çš„å…¶ä»–æ¨¡å‹éƒ½æ²¡æœ‰è§£æè§£ï¼Œä¸‹é¢æˆ‘ä»¬å°†åœ¨è¿™é‡Œä»‹ç»å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™çš„å·¥ä½œç¤ºä¾‹ã€‚

åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œä½¿ç”¨ä»æ•°æ®é›†ä¸­éšæœºæŠ½å–çš„ä¸€ä¸ªå°æ‰¹é‡ï¼Œç„¶åæ ¹æ®å‚æ•°è®¡ç®—æŸå¤±çš„æ¢¯åº¦ã€‚æ¥ä¸‹æ¥ï¼Œæœç€å‡å°‘æŸå¤±çš„æ–¹å‘æ›´æ–°æˆ‘ä»¬çš„å‚æ•°ã€‚ ä¸‹é¢çš„å‡½æ•°å®ç°å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™æ›´æ–°ã€‚è¯¥å‡½æ•°æ¥å—æ¨¡å‹å‚æ•°é›†åˆã€å­¦ä¹ é€Ÿç‡å’Œæ‰¹é‡å¤§å°ä½œä¸ºè¾“å…¥ã€‚æ¯ä¸€æ­¥æ›´æ–°çš„å¤§å°ç”±å­¦ä¹ é€Ÿç‡lrå†³å®šã€‚ å› ä¸ºæˆ‘ä»¬è®¡ç®—çš„æŸå¤±æ˜¯ä¸€ä¸ªæ‰¹é‡æ ·æœ¬çš„æ€»å’Œï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨æ‰¹é‡å¤§å°ï¼ˆbatch_sizeï¼‰æ¥å½’ä¸€åŒ–æ­¥é•¿ï¼Œè¿™æ ·æ­¥é•¿å¤§å°å°±ä¸ä¼šå–å†³äºæˆ‘ä»¬å¯¹æ‰¹é‡å¤§å°çš„é€‰æ‹©ã€‚

#### torchç‰ˆ


```python
def sgd(params, lr, batch_size): 
    """å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ã€‚"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

#### paddleç‰ˆ


```python
def pddle_sgd(params, lr, batch_size):
    """å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ã€‚"""
    with paddle.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

## 3.2.7 è®­ç»ƒ

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æ¨¡å‹è®­ç»ƒæ‰€æœ‰éœ€è¦çš„è¦ç´ ï¼Œå¯ä»¥å®ç°ä¸»è¦çš„è®­ç»ƒè¿‡ç¨‹éƒ¨åˆ†äº†ã€‚ ç†è§£è¿™æ®µä»£ç è‡³å…³é‡è¦ï¼Œå› ä¸ºåœ¨æ•´ä¸ªæ·±åº¦å­¦ä¹ çš„èŒä¸šç”Ÿæ¶¯ä¸­ï¼Œä½ ä¼šä¸€éåˆä¸€éåœ°çœ‹åˆ°å‡ ä¹ç›¸åŒçš„è®­ç»ƒè¿‡ç¨‹ã€‚ åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬è¯»å–ä¸€å°æ‰¹é‡è®­ç»ƒæ ·æœ¬ï¼Œå¹¶é€šè¿‡æˆ‘ä»¬çš„æ¨¡å‹æ¥è·å¾—ä¸€ç»„é¢„æµ‹ã€‚ è®¡ç®—å®ŒæŸå¤±åï¼Œæˆ‘ä»¬å¼€å§‹åå‘ä¼ æ’­ï¼Œå­˜å‚¨æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚æœ€åï¼Œæˆ‘ä»¬è°ƒç”¨ä¼˜åŒ–ç®—æ³• sgd æ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚

æ¥ä¸‹æ¥ï¼Œå°†æ‰§è¡Œä¸€ä¸‹å¾ªç¯ï¼š

- åˆå§‹åŒ–å‚æ•°
- é‡å¤ï¼Œç›´åˆ°å®Œæˆ
  - è®¡ç®—æ¢¯åº¦ $g \leftarrow \partial_{(w,b)}\frac{1}{\vert B\vert}\sum_{i \in B}l(x^{(i)},y^{(i)},w,b)$
  - æ›´æ–°å‚æ•°$(w,b) \leftarrow (w,b) \leftarrow \eta g$ 

åœ¨æ¯ä¸ªè¿­ä»£å‘¨æœŸï¼ˆepochï¼‰ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨`data_iter`å‡½æ•°éå†æ•´ä¸ªæ•°æ®é›†ï¼Œå¹¶å°†è®­ç»ƒæ•°æ®é›†ä¸­æ‰€æœ‰æ ·æœ¬éƒ½ä½¿ç”¨ä¸€æ¬¡ï¼ˆå‡è®¾æ ·æœ¬æ•°èƒ½å¤Ÿè¢«æ‰¹é‡å¤§å°æ•´é™¤ï¼‰ã€‚è¿™é‡Œçš„è¿­ä»£å‘¨æœŸä¸ªæ•°`num_epochs`å’Œå­¦ä¹ ç‡`lr`éƒ½æ˜¯è¶…å‚æ•°ï¼Œåˆ†åˆ«è®¾ä¸º3å’Œ0.03ã€‚è®¾ç½®è¶…å‚æ•°å¾ˆæ£˜æ‰‹ï¼Œéœ€è¦é€šè¿‡åå¤è¯•éªŒè¿›è¡Œè°ƒæ•´ã€‚ æˆ‘ä»¬ç°åœ¨å¿½ç•¥è¿™äº›ç»†èŠ‚ï¼Œä»¥åä¼šåœ¨ 2èŠ‚ ä¸­è¯¦ç»†ä»‹ç»ã€‚

#### torchç‰ˆ


```python
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)  # `X`å’Œ`y`çš„å°æ‰¹é‡æŸå¤±
        # å› ä¸º`l`å½¢çŠ¶æ˜¯(`batch_size`, 1)ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚`l`ä¸­çš„æ‰€æœ‰å…ƒç´ è¢«åŠ åˆ°ä¸€èµ·ï¼Œ
        # å¹¶ä»¥æ­¤è®¡ç®—å…³äº[`w`, `b`]çš„æ¢¯åº¦
        l.sum().backward()
        sgd([w, b], lr, batch_size)  # ä½¿ç”¨å‚æ•°çš„æ¢¯åº¦æ›´æ–°å‚æ•°
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

å› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯è‡ªå·±åˆæˆçš„æ•°æ®é›†ï¼Œæ‰€ä»¥æˆ‘ä»¬çŸ¥é“çœŸæ­£çš„å‚æ•°æ˜¯ä»€ä¹ˆã€‚ å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ¯”è¾ƒçœŸå®å‚æ•°å’Œé€šè¿‡è®­ç»ƒå­¦åˆ°çš„å‚æ•°æ¥è¯„ä¼°è®­ç»ƒçš„æˆåŠŸç¨‹åº¦ã€‚äº‹å®ä¸Šï¼ŒçœŸå®å‚æ•°å’Œé€šè¿‡è®­ç»ƒå­¦åˆ°çš„å‚æ•°ç¡®å®éå¸¸æ¥è¿‘ã€‚


```python
print(f'wçš„ä¼°è®¡è¯¯å·®: {true_w - w.reshape(true_w.shape)}')
print(f'bçš„ä¼°è®¡è¯¯å·®: {true_b - b}')
```

æ³¨æ„ï¼Œæˆ‘ä»¬ä¸åº”è¯¥æƒ³å½“ç„¶åœ°è®¤ä¸ºæˆ‘ä»¬èƒ½å¤Ÿå®Œç¾åœ°æ¢å¤å‚æ•°ã€‚ åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¸å¤ªå…³å¿ƒæ¢å¤çœŸæ­£çš„å‚æ•°ï¼Œè€Œæ›´å…³å¿ƒé‚£äº›èƒ½é«˜åº¦å‡†ç¡®é¢„æµ‹çš„å‚æ•°ã€‚ å¹¸è¿çš„æ˜¯ï¼Œå³ä½¿æ˜¯åœ¨å¤æ‚çš„ä¼˜åŒ–é—®é¢˜ä¸Šï¼Œéšæœºæ¢¯åº¦ä¸‹é™é€šå¸¸ä¹Ÿèƒ½æ‰¾åˆ°éå¸¸å¥½çš„è§£ã€‚å…¶ä¸­ä¸€ä¸ªåŸå› æ˜¯ï¼Œåœ¨æ·±åº¦ç½‘ç»œä¸­å­˜åœ¨è®¸å¤šå‚æ•°ç»„åˆèƒ½å¤Ÿå®ç°é«˜åº¦ç²¾ç¡®çš„é¢„æµ‹ã€‚

#### paddleç‰ˆ


```python
lr = 0.03
num_epochs = 3
net = paddle_linreg
loss = squared_loss

for epoch in range(num_epochs):
    for paddle_X, paddle_y in paddle_data_iter(batch_size, features, labels):
        l = loss(net(paddle_X, paddle_w, paddle_b), paddle_y)  # `paddle_X`å’Œ`paddle_y`çš„å°æ‰¹é‡æŸå¤±
        # å› ä¸º`l`å½¢çŠ¶æ˜¯(`batch_size`, 1)ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚`l`ä¸­çš„æ‰€æœ‰å…ƒç´ è¢«åŠ åˆ°ä¸€èµ·ï¼Œ
        # å¹¶ä»¥æ­¤è®¡ç®—å…³äº[`paddle_w`, `paddle_b`]çš„æ¢¯åº¦
        l.sum().backward()
        paddle_sgd([paddle_w, paddle_b], lr, batch_size)  # ä½¿ç”¨å‚æ•°çš„æ¢¯åº¦æ›´æ–°å‚æ•°
    with paddle.no_grad():
        paddle_train_l = loss(net(paddle_features, paddle_w, paddle_b), padle_labels)
        print(f'epoch {epoch + 1}, loss {float(pddle_train_l.mean()):f}')
```
