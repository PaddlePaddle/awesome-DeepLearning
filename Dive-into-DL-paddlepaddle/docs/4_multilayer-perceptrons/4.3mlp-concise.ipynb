{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 多层感知机的简洁实现\n",
    ":label:`sec_mlp_concise`\n",
    "\n",
    "正如你所期待的，我们可以(**通过高级API更简洁地实现多层感知机**)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeiWu-GU\\anaconda3\\envs\\pte\\lib\\site-packages\\ipykernel\\pylab\\backend_inline.py:164: DeprecationWarning: `configure_inline_support` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.configure_inline_support()`\n",
      "  configure_inline_support(ip, backend)\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from paddle import nn\n",
    "from d2l import torch as d2l\n",
    "import pd2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## 模型\n",
    "\n",
    "与softmax回归的简洁实现（:numref:`sec_softmax_concise`）相比，唯一的区别是我们添加了2个全连接层（之前我们只添加了1个全连接层）。第一层是[**隐藏层**]，它(**包含256个隐藏单元，并使用了ReLU激活函数**)。第二层是输出层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(784, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 10))\n",
    "\n",
    "\n",
    "for layer in net:\n",
    "    if type(layer) == nn.Linear:\n",
    "        weight_attr = paddle.framework.ParamAttr(initializer=paddle.nn.initializer.Normal(mean=0.0, std=0.01))\n",
    "        layer.weight_attr=weight_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "[**训练过程**]的实现与我们实现softmax回归时完全相同，这种模块化设计使我们能够将与和模型架构有关的内容独立出来。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size, lr, num_epochs = 256, 0.1, 10\n",
    "loss = nn.CrossEntropyLoss()\n",
    "trainer = paddle.optimizer.SGD(parameters=net.parameters(), learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = pd2l.load_data_fashion_mnist(batch_size)\n",
    "# d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 我们可以使用高级API更简洁地实现多层感知机。\n",
    "* 对于相同的分类问题，多层感知机的实现与softmax回归的实现相同，只是多层感知机的实现里增加了带有激活函数的隐藏层。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 尝试添加不同数量的隐藏层（也可以修改学习率）。怎么样设置效果最好？\n",
    "1. 尝试不同的激活函数。哪个效果最好？\n",
    "1. 尝试不同的方案来初始化权重。什么方法效果最好？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1802)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pte",
   "language": "python",
   "name": "pte"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
