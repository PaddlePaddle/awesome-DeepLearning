# 深度学习基础题

## 基于层次softmax训练词向量

### 词向量

词向量（Word embedding），又叫Word嵌入式自然语言处理（NLP）中的一组语言建模和特征学习技术的统称，其中来自词汇表的单词或短语被映射到实数的向量。 从概念上讲，它涉及从每个单词一维的空间到具有更低维度的连续向量空间的数学嵌入。

### softmax函数

softmax函数，又称**归一化指数函数。**它是二分类函数sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。下图展示了softmax的计算方法：

![](\images\1.png)

softmax函数具有“**两向量的‘亲密度’转条件概率**”的功能：

![](\images\2.png)

### 基于层次softmax训练词向量

#### 分层softmax

Hierarchical softmax （H-Softmax）是由Morin和Bengio[3]受到二叉树的启发而提出。H-Softmax本质上是用层级关系替代了扁平化的softmax层，如图1所示，每个叶子节点表示一个词语。于是，计算单个词语概率值的计算过程被拆解为一系列的概率计算，这样可以避免对所有词语进行标准化计算。用H-Softmax替换softmax层之后，词语的预测速度可以提升至少50倍，速度的提升对于低延时要求的实时系统至关重要，比如谷歌新推出的消息应用Allo。

我们可以把原来的softmax看做深度为1的树，词表V中的每一个词语表示一个叶子节点。计算一个词语的softmax概率需要对|V|个节点的概率值做标准化。如果把softmax改为二叉树结构，每个word表示叶子节点，那么只需要沿着通向该词语的叶子节点的路径搜索，而不需要考虑其它的节点。

平衡二叉树的深度是log2(|V|)，因此，最多只需要计算log2(|V|)个节点就能得到目标词语的概率值。注意，得到的概率值已经经过了标准化，因为二叉树所有叶子节点组成一个概率分布，所有叶子节点的概率值总和等于1。我们可以简单地验证一下，在图1的根节点（Node o）处，两个分枝的概率和必须为1。之后的每个节点，它的两个子节点的概率值之和等于节点本身的概率值。因为整条搜索路径没有概率值的损失，所以最底层所有叶子节点的概率值之和必定等于1，hierarchical softmax定义了词表V中所有词语的标准化概率分布。

具体说来，当遍历树的时候，我们需要能够计算左侧分枝或是右侧分枝的概率值。为此，给每个节点分配一个向量表示。与常规的softmax做法不同，这里不是给每个输出词语w生成词向量v’w，而是给每个节点n计算一个向量v’n。总共有|V|-1个节点，每个节点都有自己独一无二的向量表示，H-Softmax方法用到的参数与常规的softmax几乎一样。于是，在给定上下文c时，就能够计算节点n左右两个分枝的概率。

假设已知出现了词语“the”、“dog”、“and”、“the”，则出现词语“cat”的概率值就是在节点1向左偏的概率值、在节点2向右偏的概率以及在节点5向右偏的概率值的乘积。Hugo Lachorelle在他的视频教程中给了更详细的解释。Rong[7]的文章也详细地解释了这些概念，并推导了H-Softmax。

显然，树形的结构非常重要。若我们让模型在各个节点的预测更方便，比如路径相近的节点概率值也相近，那么凭直觉系统的性能肯定还会提升。沿着这个思路，Morin和Bengio使用WordNet的同义词集作为树簇。然而性能依旧不如常规的softmax方法。Mnih和Hinton[8]将聚类算法融入到树形结构的学习过程，递归地将词集分为两个集合，效果终于和softmax方法持平，计算量有所减小。

值得注意的是，此方法只是加速了训练过程，因为我们可以提前知道将要预测的词语（以及其搜索路径）。在测试过程中，被预测词语是未知的，仍然无法避免计算所有词语的概率值。

在实践中，一般不用“左节点”和“右节点”，而是给每个节点赋一个索引向量，这个向量表示该节点的搜索路径。如图2所示，如果约定该位为0表示向左搜索，该位为1表示向右搜索，那词语“cat”的向量就是011。

上文中提到平衡二叉树的深度不超过log2(|V|)。若词表的大小是|V|=10000，那么搜索路径的平均长度就是13.3。因此，词表中的每个词语都能表示为一个平均长度为13.3比特的向量，即信息量为13.3比特。

#### 分片softmax

Chen等人在论文中介绍了一种传统softmax层的变换形式，称作Differentiated Softmax (D-Softmax)。D-Softmax基于的假设是并不是所有词语都需要相同数量的参数：多次出现的高频词语需要更多的参数去拟合，而较少见的词语就可以用较少的参数。

传统的softmax层用到了dx|V|的稠密矩阵来存放输出的词向量表示v′w∈ℝd，论文中采用了稀疏矩阵。他们将词向量v′w按照词频分块，每块区域的向量维度各不相同。分块数量和对应的维度是超参数，可以根据需要调整。
图3中，A区域的词向量维度是dA（这个分块是高频词语，向量的维度较高），B和C区域的词向量维度分别是dB和dC。其余空白区域的值为0。

隐藏层h的输出被视为是各个分块的级联，比如图3中h层的输出是由三个长度分别为dA、dB、dC的向量级联而成。D-Softmax只需计算各个向量段与h对应位置的内积，而不需整个矩阵和向量参与计算。

由于大多数的词语只需要相对较少的参数，计算softmax的复杂度得到降低，训练速度因此提升。相对于H-Softmax方法，D-Softmax的优化方法在测试阶段仍然有效。Chen在2015年的论文中提到D-Softmax是测试阶段最快的方法，同时也是准确率最高的之一。但是，由于低频词语的参数较少，D-Softmax对这部分数据的建模能力较弱。

#### CNN-softmax

传统softmax层的另一种改进是受到Kim[3]的论文启发，Kim对输入词向量vw采用了字符级别的CNN模型。相反，Jozefowicz在2016年将同样的方法用于输出词向量v′w，并将这种方法称为CNN-Softmax。如图4所示，如果我们在输入端和输出端加上CNN模型，输出端CNN生成的向量v′w与输入端CNN生成的向量必然不相同，因为输入和输出的词向量矩阵就不一样。
尽管这个方法仍需要计算常规softmax的标准化，但模型的参数却大大的减少：只需要保留CNN模型的参数，不需要保存整个词向量矩阵dx|V|。在测试阶段，输出词向量v′w可以提前计算，所以性能损失不大。

但是，由于字符串是在连续空间内表示，而且模型倾向于用平滑函数将字符串映射到词向量，因此基于字符的模型往往无法区分拼写相似但是含义不同的词语。为了消除上述影响，论文的作者增加了一个矫正因数，显著地缩小了CNN-Softmax与传统方法的性能差距。通过调整矫正因数的维度，作者可以方便地取舍模型的大小与计算性能。

论文的作者还提到，前一层h的输出可以传入字符级别的LSTM模型，这个模型每次预测输出词语的一个字母。但是，这个方法的性能并不令人满意。Ling等人在2014年的论文中采用类类似的方法来解决机器翻译任务，取得了不错的效果。

## LSTM解决NLP任务

### LSTM

长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。

![](C:\Users\VULCAN\Desktop\基础知识\基础知识1\images\4.jpg)

### 序列到类别

长期、短期记忆全卷积神经网络(LSTM-FCN)和注意力LSTM-FCN (ALSTM-FCN)在旧的加州大学河滨分校(UCR)时间序列存储库中对时间序列信号进行分类的任务中表现出了最先进的性能。然而，对于LSTM-FCN和ALSTM-FCN为何表现良好，目前还没有研究。在本文中，我们对LSTM-FCN和ALSTM-FCN进行了一系列烧蚀试验(3627个实验)，以便更好地理解模型及其各个子模块。对ALSTM-FCN和LSTM-FCN的烧蚀试验结果表明，联合使用时，这两种材料的烧蚀效果较好。使用Wilcoxson符号秩检验比较了两种z归一化技术，即单独对每个样本进行z归一化和对整个数据集进行z归一化，以显示性能上的统计差异。此外，我们还通过比较维度洗牌与LSTM-FCN在不应用维度洗牌时的性能，了解维度洗牌对LSTM-FCN的影响。最后，我们证明了LSTM- fcn的性能，当LSTM块被一个GRU、基本RNN和密集块替换时。

### 序列到序列

序列到序列模型发展到今天，根据不同任务有着不同的变体。这里将介绍最简单最基本的版本，其他版本也是在这一基础版本上进行的修改和变化，了解了最基本的框架之后，再看别的模型就没有太大问题了。

序列到序列任务往往具有以下两个特点：

1.输入输出时不定长的。比如说想要构建一个聊天机器人，你的对话和他的回复长度都是不定的。

2.输入输出元素之间是具有顺序关系的。不同的顺序，得到的结果应该是不同的，比如“不开心”和“开心不”这两个短语的意思是不同的。

为了解决以上两个问题，我们想到了使用RNN类模型来做，主要使用的LSTM。

下面对模型的整体框架进行介绍。

模型主要由两个部分组成，一个编码器（encoder）和一个解码器（decoder）。

编码器和解码器一般都是由RNN类网络构成，常用LSTM。这是由于使用RNN可以自适应输入输出，这里就不多提了。后面的网络都会以LSTM为例进行说明。

通信领域，编码器（Encoder）指的是将信号进行编制，转换成容易传输的形式。

而在这里，主要指的是将句子编码成一个能够映射出句子大致内容的**固定长度的向量**。

![](C:\Users\VULCAN\Desktop\基础知识\基础知识1\images\5.jpg)

如图4，对于投入到的每个RNN展开的节点，我们将会得到一个输出层输出和一个隐含层输出，我们最终需要使用到的是最后一个输入节点的隐含层输出。这里面最后一个隐含节点的输出蕴含了前面所有节点的输入内容。

解码器（Decoder），这里就是将由编码器得到的固定长度的向量再还原成对应的序列数据，一般使用和编码器同样的结构，也是一个RNN类的网络。

实际操作过程会将由编码器得到的定长向量传递给解码器，解码器节点会使用这个向量作为隐藏层输入和一个开始标志位作为当前位置的输入。得到的输出向量能够映射成为我们想要的输出结果，并且会将映射输出的向量传递给下一个展开的RNN节点。

每一次展开，会使用上一个节点的映射输出所对应的向量来投入到下一个输入中去，直到遇到停止标志位，即停止展开。

![](\images\6.jpg)

模型的训练主要需要考虑三个部分：**编码器的输入**，**解码器的输入**，**以及解码器的输出**。

序列任务的数据集主要分为原序列和转换序列，比如说对话任务中，编码器的LSTM投入的输入为 are you free tomorrow四个词所对应的向量，而对起始的隐含层输入不做具体要求。

解码器的第一个输入节点位置为开始的标志位，如图5的<START>。

而后面跟进的输入在训练阶段一般有两种选择。一种是投入正确输出的输入。也被称为teacher force输入，即每一个节点的输入都为上一个节点正确的输出。

以下图6为例，我们需要的正确输出为W X Y Z，那么我们不论实际输出为多少，我们都将输入W X Y Z。

![](\images\7.jpg)

在这种情况进行训练，可以避免前面一个节点出错导致后面连续累计错误。并且在早期，使用这种方式训练，能够使训练更容易。

然而，在实际生成测试的过程中，我们不可能知道正确的数据是什么，因而难以避免这种错误累计的情况。所以产生了按照上一个节点输出作为下一个节点输出的训练情况。上面的图5就是表达的这种情况，这也是真正实际生成的方式。我们往往会在训练的后期加入这种投入的方式。结合二者进行训练，这样可以使得生成的效果更好。

对于输出部分没有什么特殊的，一般会将RNN节点所对应的输出通过一个全连接神经映射到字典维度大小的向量。再通过交叉熵来量化生成的分布和实际分布之间的差异，通过反向传播进而训练整个网络。

序列到序列模型看似非常完美，但是实际使用的过程中仍然会遇到一些问题。比如说句子长度过长，会产生梯度消失的问题。由于使用的是最后的一个隐含层输出的定长向量，那么对于远靠近末端的单词，“记忆”得会越深刻，而远远离的单词则会被逐渐稀释掉。面对这些问题，也有对应的一些解决方案比如加入attention，将句子倒向输入等。关于attention等内容将会在后面的文章进行介绍。