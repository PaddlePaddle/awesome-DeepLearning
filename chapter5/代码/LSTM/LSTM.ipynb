{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.fluid as fluid\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "import paddle.dataset.imikolov as imikolov# 导入imikolov数据集\r\n",
    "from paddle.text.datasets import Imikolov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn.functional as F\r\n",
    "from paddle.nn import LSTM, Embedding, Dropout, Linear\r\n",
    "from paddle.io import Dataset, BatchSampler, DataLoader\r\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cache file /home/aistudio/.cache/paddle/dataset/imikolov/imikolov%2Fsimple-examples.tgz not found, downloading https://dataset.bj.bcebos.com/imikolov%2Fsimple-examples.tgz \n",
      "Begin to download\n",
      "\n",
      "Download finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585\n"
     ]
    }
   ],
   "source": [
    "# 取词表\r\n",
    "word_idx=imikolov.build_dict(min_word_freq=200) #min_word_freq=50\r\n",
    "print(len(word_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PredictNextWord(paddle.nn.Layer):# 预测下一个词\r\n",
    "    def __init__(self, hidden_size, vocab_size, embedding_size, class_num, num_steps=4, num_layers=1, init_scale=0.1, dropout_rate=None):\r\n",
    "        # 1.hidden_size，表示embedding-size，hidden和cell向量的维度\r\n",
    "        # 2.vocab_size，模型可以考虑的词表大小\r\n",
    "        # 3.embedding_size，表示词向量的维度\r\n",
    "        # 4.class_num，分类个数，等同于vocab_size\r\n",
    "        # 5.num_steps，表示模型最大可以考虑的句子长度\r\n",
    "        # 6.num_layers，表示网络的层数\r\n",
    "        # 7.dropout_rate，表示使用dropout过程中失活的神经元比例\r\n",
    "        # 8.init_scale，表示网络内部的参数的初始化范围\r\n",
    "        \r\n",
    "        super(PredictNextWord, self).__init__()\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.vocab_size = vocab_size\r\n",
    "        self.embedding_size = embedding_size\r\n",
    "        self.class_num = class_num\r\n",
    "        self.num_steps = num_steps\r\n",
    "        self.num_layers = num_layers\r\n",
    "        self.dropout_rate = dropout_rate\r\n",
    "        self.init_scale = init_scale\r\n",
    "\r\n",
    "        # embedding 将词转化为词向量\r\n",
    "        self.embedding = paddle.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, sparse=False, \r\n",
    "                                    weight_attr=paddle.ParamAttr(initializer=paddle.nn.initializer.Uniform(low=-init_scale, high=init_scale)))\r\n",
    "\r\n",
    "        # 构建LSTM模型\r\n",
    "        self.simple_lstm_rnn = paddle.nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers)\r\n",
    "        self.cls_fc = paddle.nn.Linear(in_features=self.num_steps*self.hidden_size, out_features=self.class_num)\r\n",
    "        # dropout\r\n",
    "        self.dropout_layer = paddle.nn.Dropout(p=self.dropout_rate, mode='upscale_in_train')\r\n",
    "\r\n",
    "\r\n",
    "    # forwad函数为模型前向计算的函数\r\n",
    "    def forward(self, inputs):\r\n",
    "        batch_size = inputs.shape[0]\r\n",
    "\r\n",
    "        # 定义LSTM的初始hidden和cell\r\n",
    "        init_hidden_data = np.zeros(\r\n",
    "            (self.num_layers, batch_size, self.hidden_size), dtype='float32')\r\n",
    "        init_cell_data = np.zeros(\r\n",
    "            (self.num_layers, batch_size, self.hidden_size), dtype='float32')\r\n",
    "        init_hidden = paddle.to_tensor(init_hidden_data)#hidden\r\n",
    "        init_cell = paddle.to_tensor(init_cell_data)#cell\r\n",
    "\r\n",
    "        x_emb = self.embedding(inputs)\r\n",
    "        x_emb = paddle.reshape(x_emb, shape=[-1, self.num_steps, self.embedding_size])#get embedding\r\n",
    "\r\n",
    "        # dropout\r\n",
    "        if self.dropout_rate is not None and self.dropout_rate > 0.0:\r\n",
    "            x_emb = self.dropout_layer(x_emb)\r\n",
    "\r\n",
    "\r\n",
    "        # 使用LSTM网络，把每个句子转换为语义向量\r\n",
    "        rnn_out, (last_hidden, last_cell) = self.simple_lstm_rnn(x_emb, (init_hidden, init_cell))\r\n",
    "        #rnn_out, (last_hidden, last_cell) = self.simple_lstm_rnn(x_emb)\r\n",
    "        # 提取最后一层隐状态作为文本的语义向量\r\n",
    "        rnn_out = paddle.reshape(rnn_out, shape=[batch_size, -1])\r\n",
    "        # 将每个句子的向量表示映射到具体的类别上, logits的维度为[batch_size, vocab_size]\r\n",
    "        logits = self.cls_fc(rnn_out)\r\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data size= 71152\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 4\r\n",
    "imikolov2 = Imikolov(mode='test', data_type='NGRAM', window_size=max_seq_len+1,min_word_freq=200)\r\n",
    "print('test data size=',len(imikolov2))\r\n",
    "# batch_size_test = int(len(imikolov2)/100)\r\n",
    "batch_size_test = len(imikolov2)\r\n",
    "test_loader = DataLoader(imikolov2, batch_size=batch_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(model):# 测试\r\n",
    "    model.eval()\r\n",
    "    correct_num = 0\r\n",
    "    total_num = 0\r\n",
    "    y_test = np.array([])\r\n",
    "    pred = np.array([])\r\n",
    "    for step, data in enumerate(test_loader()):\r\n",
    "        print('step=',step)\r\n",
    "        data = np.array(data)\r\n",
    "        # print(data.shape)\r\n",
    "        if data.shape[1] < batch_size_test:\r\n",
    "                break\r\n",
    "        else:\r\n",
    "            data = data.reshape(batch_size_test,-1)\r\n",
    "        sentences = data[:,:4]\r\n",
    "        labels = data[:,-1]\r\n",
    "        # 将张量转换为Tensor类型\r\n",
    "        sentences = paddle.to_tensor(sentences)\r\n",
    "        labels = paddle.to_tensor(labels)\r\n",
    "        logits = model(sentences)\r\n",
    "        labels = labels.numpy()\r\n",
    "\r\n",
    "\r\n",
    "        probs = F.softmax(logits)\r\n",
    "        probs = probs.numpy()\r\n",
    "        probs = probs.argmax(axis=1)\r\n",
    "        if pred.all == None and y_test.all == None:\r\n",
    "            y_test = labels\r\n",
    "            pred = probs\r\n",
    "        else:\r\n",
    "            y_test = np.concatenate((y_test,labels),axis=0)\r\n",
    "            pred = np.concatenate((pred,probs),axis=0)\r\n",
    "        correct_num += (probs == labels).sum()\r\n",
    "        total_num += labels.shape[0]\r\n",
    "    accuracy = float(correct_num/total_num)\r\n",
    "\r\n",
    "    print(\"Accuracy: %.4f\" % accuracy)\r\n",
    "    print('y_test=', y_test)\r\n",
    "    print('pred=', pred)\r\n",
    "    accuracy = metrics.accuracy_score(y_test, pred)\r\n",
    "    overall_precison = metrics.precision_score(y_test, pred, average=\"micro\")\r\n",
    "    average_precison = metrics.precision_score(y_test, pred, average=\"macro\")\r\n",
    "    overall_recall = metrics.recall_score(y_test, pred, average=\"micro\")\r\n",
    "    average_recall = metrics.recall_score(y_test, pred, average=\"macro\")\r\n",
    "    print('accuracy = ', accuracy)\r\n",
    "    print('overall_precison = ', overall_precison)\r\n",
    "    print('average_precison = ', average_precison)\r\n",
    "    print('overall_recall = ', overall_recall)\r\n",
    "    print('average_recall = ', average_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size= 803522\n"
     ]
    }
   ],
   "source": [
    "# 定义训练参数\r\n",
    "epoch_num = 10\r\n",
    "batch_size = 32\r\n",
    "learning_rate = 0.02\r\n",
    "dropout_rate = 0.2\r\n",
    "num_layers = 3\r\n",
    "hidden_size = 200\r\n",
    "embedding_size = 20\r\n",
    "vocab_size = len(word_idx)\r\n",
    "# 数据生成器\r\n",
    "imikolov = Imikolov(mode='train', data_type='NGRAM', window_size=max_seq_len+1,min_word_freq=200)\r\n",
    "print('train data size=',len(imikolov))\r\n",
    "train_loader = DataLoader(imikolov, batch_size=batch_size, shuffle=True)\r\n",
    "\r\n",
    "# 使用GPU\r\n",
    "paddle.set_device('gpu:0')\r\n",
    "\r\n",
    "next_word_predicter = PredictNextWord(hidden_size, vocab_size, embedding_size, class_num=vocab_size, num_steps=max_seq_len, num_layers=num_layers, dropout_rate=dropout_rate)\r\n",
    "\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.999, parameters= next_word_predicter.parameters()) # , beta1=0.9, beta2=0.999,\r\n",
    "\r\n",
    "# 定义训练函数\r\n",
    "losses = []\r\n",
    "steps = []\r\n",
    "def train(model):# TRAIN\r\n",
    "    for e in range(epoch_num):\r\n",
    "        model.train()\r\n",
    "        for step, data in enumerate(train_loader()):\r\n",
    "            data = np.array(data)\r\n",
    "            if data.shape[1] < batch_size:\r\n",
    "                break\r\n",
    "            else:\r\n",
    "                data = data.reshape(batch_size,-1)\r\n",
    "\r\n",
    "            sentences = data[:,:4]\r\n",
    "            labels = data[:,-1]\r\n",
    "            sentences = paddle.to_tensor(sentences)\r\n",
    "            labels = paddle.to_tensor(labels)\r\n",
    "            # 前向计算，将数据feed进模型，并得到预测的情感标签和损失\r\n",
    "            logits = model(sentences)\r\n",
    "            # logits = F.softmax(logits)\r\n",
    "            # 计算损失\r\n",
    "            loss = F.cross_entropy(input=logits, label=labels, soft_label=False)\r\n",
    "            loss = paddle.mean(loss)\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            optimizer.clear_grad()\r\n",
    "            if step % 1000 == 0:\r\n",
    "                losses.append(loss.numpy()[0])\r\n",
    "                steps.append(step)\r\n",
    "                # 打印当前loss数值\r\n",
    "                print(\"epoch %d, step %d, loss %.3f\" % (e+1, step, loss.numpy()[0]))\r\n",
    "        evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 0, loss 6.371\n",
      "epoch 1, step 1000, loss 4.402\n",
      "epoch 1, step 2000, loss 4.119\n",
      "epoch 1, step 3000, loss 4.697\n",
      "epoch 1, step 4000, loss 4.205\n"
     ]
    }
   ],
   "source": [
    "train(next_word_predicter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
