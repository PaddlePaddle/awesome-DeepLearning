{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 波士顿房价预测任务\n",
    "\n",
    "上一节我们初步认识了神经网络的基本概念（如神经元、多层连接、前向计算、计算图）和模型结构三要素（模型假设、评价函数和优化算法）。本节将以“波士顿房价预测”任务为例，向读者介绍使用Python语言和Numpy库来构建神经网络模型的思考过程和操作方法。\n",
    "\n",
    "波士顿房价预测是一个经典的机器学习任务，类似于程序员世界的“Hello World”。和大家对房价的普遍认知相同，波士顿地区的房价受诸多因素影响。该数据集统计了13种可能影响房价的因素和该类型房屋的均价，期望构建一个基于13个因素进行房价预测的模型，如 **图1** 所示。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/abce0cb2a92f4e679c6855cfa520491597171533a0b0447e8d51d904446e213e\" width=\"500\" hegiht=\"\" ></center>\n",
    "<center><br>图1：波士顿房价影响因素示意图</br></center>\n",
    "<br></br>\n",
    "\n",
    "对于预测问题，可以根据预测输出的类型是连续的实数值，还是离散的标签，区分为回归任务和分类任务。因为房价是一个连续值，所以房价预测显然是一个回归任务。下面我们尝试用最简单的线性回归模型解决这个问题，并用神经网络来实现这个模型。\n",
    "\n",
    "## 线性回归模型\n",
    "\n",
    "假设房价和各影响因素之间能够用线性关系来描述：\n",
    "\n",
    "$$y = {\\sum_{j=1}^Mx_j w_j} + b$$\n",
    "\n",
    "模型的求解即是通过数据拟合出每个$w_j$和$b$。其中，$w_j$和$b$分别表示该线性模型的权重和偏置。一维情况下，$w_j$ 和 $b$ 是直线的斜率和截距。\n",
    "\n",
    "线性回归模型使用均方误差作为损失函数（Loss），用以衡量预测房价和真实房价的差异，公式如下：\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n(\\hat{Y_i} - {Y_i})^{2}$$\n",
    "\n",
    "------\n",
    "**思考：**\n",
    "\n",
    "为什么要以均方误差作为损失函数？即将模型在每个训练样本上的预测误差加和，来衡量整体样本的准确性。这是因为损失函数的设计不仅仅要考虑“合理性”，同样需要考虑“易解性”，这个问题在后面的内容中会详细阐述。\n",
    "\n",
    "------\n",
    "\n",
    "## 线性回归模型的神经网络结构\n",
    "\n",
    "神经网络的标准结构中每个神经元由加权和与非线性变换构成，然后将多个神经元分层的摆放并连接形成神经网络。线性回归模型可以认为是神经网络模型的一种极简特例，是一个只有加权和、没有非线性变换的神经元（无需形成网络），如 **图2** 所示。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/f9117a5a34d44b1eab85147e62b4e6295e485e48d79d4a03adaa14a447ffd230\" width=\"300\" hegiht=\"\" ></center>\n",
    "<center><br>图2：线性回归模型的神经网络结构</br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 构建波士顿房价预测任务的神经网络模型\n",
    "\n",
    "深度学习不仅实现了模型的端到端学习，还推动了人工智能进入工业大生产阶段，产生了标准化、自动化和模块化的通用框架。不同场景的深度学习模型具备一定的通用性，五个步骤即可完成模型的构建和训练，如 **图3** 所示。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/12fdca24a3b94166a9e8c815ef4b0e4ddfec541f3a024a4392f1fc17fa186c7b\" width=\"800\" hegiht=\"\" ></center>\n",
    "<center><br>图3：构建神经网络/深度学习模型的基本步骤</br></center>\n",
    "<br></br>\n",
    "\n",
    "正是由于深度学习的建模和训练的过程存在通用性，在构建不同的模型时，只有模型三要素不同，其它步骤基本一致，深度学习框架才有用武之地。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据处理\n",
    "\n",
    "数据处理包含五个部分：数据导入、数据形状变换、数据集划分、数据归一化处理和封装`load data`函数。数据预处理后，才能被模型调用。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "\n",
    "* 本教程中的代码都可以在AI Studio上直接运行，Print结果都是基于程序真实运行的结果。\n",
    "* 由于是真实案例，代码之间存在依赖关系，因此需要读者逐条、全部运行，否则会导致命令执行报错。\n",
    "\n",
    "------\n",
    "\n",
    "### 读入数据\n",
    "\n",
    "通过如下代码读入数据，了解下波士顿房价的数据集结构，数据存放在本地目录下housing.data文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 / iter   0, loss = 12.0712\n",
      "Epoch   0 / iter   1, loss = 1.3205\n",
      "Epoch   0 / iter   2, loss = 1.0575\n",
      "Epoch   0 / iter   3, loss = 0.8349\n",
      "Epoch   0 / iter   4, loss = 0.3143\n",
      "Epoch   1 / iter   0, loss = 0.7816\n",
      "Epoch   1 / iter   1, loss = 0.6912\n",
      "Epoch   1 / iter   2, loss = 1.1913\n",
      "Epoch   1 / iter   3, loss = 0.8817\n",
      "Epoch   1 / iter   4, loss = 2.0175\n",
      "Epoch   2 / iter   0, loss = 1.3745\n",
      "Epoch   2 / iter   1, loss = 0.7925\n",
      "Epoch   2 / iter   2, loss = 1.1347\n",
      "Epoch   2 / iter   3, loss = 1.1520\n",
      "Epoch   2 / iter   4, loss = 0.2685\n",
      "Epoch   3 / iter   0, loss = 1.0636\n",
      "Epoch   3 / iter   1, loss = 0.7018\n",
      "Epoch   3 / iter   2, loss = 0.8979\n",
      "Epoch   3 / iter   3, loss = 0.8220\n",
      "Epoch   3 / iter   4, loss = 0.3975\n",
      "Epoch   4 / iter   0, loss = 0.7171\n",
      "Epoch   4 / iter   1, loss = 0.7610\n",
      "Epoch   4 / iter   2, loss = 0.8034\n",
      "Epoch   4 / iter   3, loss = 0.7683\n",
      "Epoch   4 / iter   4, loss = 0.3259\n",
      "Epoch   5 / iter   0, loss = 0.8355\n",
      "Epoch   5 / iter   1, loss = 0.5845\n",
      "Epoch   5 / iter   2, loss = 0.5383\n",
      "Epoch   5 / iter   3, loss = 0.6477\n",
      "Epoch   5 / iter   4, loss = 0.9474\n",
      "Epoch   6 / iter   0, loss = 0.5218\n",
      "Epoch   6 / iter   1, loss = 0.4951\n",
      "Epoch   6 / iter   2, loss = 0.6562\n",
      "Epoch   6 / iter   3, loss = 0.8057\n",
      "Epoch   6 / iter   4, loss = 0.2393\n",
      "Epoch   7 / iter   0, loss = 0.6685\n",
      "Epoch   7 / iter   1, loss = 0.6986\n",
      "Epoch   7 / iter   2, loss = 0.4896\n",
      "Epoch   7 / iter   3, loss = 0.5872\n",
      "Epoch   7 / iter   4, loss = 0.7271\n",
      "Epoch   8 / iter   0, loss = 1.0442\n",
      "Epoch   8 / iter   1, loss = 1.2555\n",
      "Epoch   8 / iter   2, loss = 1.1099\n",
      "Epoch   8 / iter   3, loss = 0.9905\n",
      "Epoch   8 / iter   4, loss = 1.6156\n",
      "Epoch   9 / iter   0, loss = 1.3516\n",
      "Epoch   9 / iter   1, loss = 0.5498\n",
      "Epoch   9 / iter   2, loss = 0.4402\n",
      "Epoch   9 / iter   3, loss = 0.5063\n",
      "Epoch   9 / iter   4, loss = 0.0101\n",
      "Epoch  10 / iter   0, loss = 0.4014\n",
      "Epoch  10 / iter   1, loss = 0.3358\n",
      "Epoch  10 / iter   2, loss = 0.4704\n",
      "Epoch  10 / iter   3, loss = 0.4019\n",
      "Epoch  10 / iter   4, loss = 1.9399\n",
      "Epoch  11 / iter   0, loss = 12.0665\n",
      "Epoch  11 / iter   1, loss = 0.7323\n",
      "Epoch  11 / iter   2, loss = 0.4738\n",
      "Epoch  11 / iter   3, loss = 0.7960\n",
      "Epoch  11 / iter   4, loss = 2.1549\n",
      "Epoch  12 / iter   0, loss = 3.9354\n",
      "Epoch  12 / iter   1, loss = 0.6146\n",
      "Epoch  12 / iter   2, loss = 0.6364\n",
      "Epoch  12 / iter   3, loss = 0.8949\n",
      "Epoch  12 / iter   4, loss = 0.1942\n",
      "Epoch  13 / iter   0, loss = 1.0268\n",
      "Epoch  13 / iter   1, loss = 0.4567\n",
      "Epoch  13 / iter   2, loss = 0.3401\n",
      "Epoch  13 / iter   3, loss = 0.4343\n",
      "Epoch  13 / iter   4, loss = 0.3264\n",
      "Epoch  14 / iter   0, loss = 0.4632\n",
      "Epoch  14 / iter   1, loss = 0.3318\n",
      "Epoch  14 / iter   2, loss = 0.3297\n",
      "Epoch  14 / iter   3, loss = 0.3471\n",
      "Epoch  14 / iter   4, loss = 0.3317\n",
      "Epoch  15 / iter   0, loss = 0.3161\n",
      "Epoch  15 / iter   1, loss = 0.3427\n",
      "Epoch  15 / iter   2, loss = 0.2730\n",
      "Epoch  15 / iter   3, loss = 0.2785\n",
      "Epoch  15 / iter   4, loss = 0.0994\n",
      "Epoch  16 / iter   0, loss = 0.3708\n",
      "Epoch  16 / iter   1, loss = 0.2571\n",
      "Epoch  16 / iter   2, loss = 0.2897\n",
      "Epoch  16 / iter   3, loss = 0.2919\n",
      "Epoch  16 / iter   4, loss = 0.3086\n",
      "Epoch  17 / iter   0, loss = 0.4399\n",
      "Epoch  17 / iter   1, loss = 0.3259\n",
      "Epoch  17 / iter   2, loss = 0.2732\n",
      "Epoch  17 / iter   3, loss = 0.2734\n",
      "Epoch  17 / iter   4, loss = 0.2591\n",
      "Epoch  18 / iter   0, loss = 0.8363\n",
      "Epoch  18 / iter   1, loss = 0.8706\n",
      "Epoch  18 / iter   2, loss = 0.4580\n",
      "Epoch  18 / iter   3, loss = 0.4568\n",
      "Epoch  18 / iter   4, loss = 0.6048\n",
      "Epoch  19 / iter   0, loss = 1.7472\n",
      "Epoch  19 / iter   1, loss = 2.4244\n",
      "Epoch  19 / iter   2, loss = 1.0014\n",
      "Epoch  19 / iter   3, loss = 0.3101\n",
      "Epoch  19 / iter   4, loss = 0.3071\n",
      "Epoch  20 / iter   0, loss = 0.2878\n",
      "Epoch  20 / iter   1, loss = 0.1695\n",
      "Epoch  20 / iter   2, loss = 0.1823\n",
      "Epoch  20 / iter   3, loss = 0.2293\n",
      "Epoch  20 / iter   4, loss = 0.2817\n",
      "Epoch  21 / iter   0, loss = 0.3205\n",
      "Epoch  21 / iter   1, loss = 0.3066\n",
      "Epoch  21 / iter   2, loss = 0.2260\n",
      "Epoch  21 / iter   3, loss = 0.2882\n",
      "Epoch  21 / iter   4, loss = 0.2835\n",
      "Epoch  22 / iter   0, loss = 0.1921\n",
      "Epoch  22 / iter   1, loss = 0.2288\n",
      "Epoch  22 / iter   2, loss = 0.1872\n",
      "Epoch  22 / iter   3, loss = 0.3350\n",
      "Epoch  22 / iter   4, loss = 0.6793\n",
      "Epoch  23 / iter   0, loss = 2.3048\n",
      "Epoch  23 / iter   1, loss = 0.9874\n",
      "Epoch  23 / iter   2, loss = 0.2567\n",
      "Epoch  23 / iter   3, loss = 0.1803\n",
      "Epoch  23 / iter   4, loss = 0.1044\n",
      "Epoch  24 / iter   0, loss = 0.1428\n",
      "Epoch  24 / iter   1, loss = 0.1320\n",
      "Epoch  24 / iter   2, loss = 0.1435\n",
      "Epoch  24 / iter   3, loss = 0.1254\n",
      "Epoch  24 / iter   4, loss = 0.0622\n",
      "Epoch  25 / iter   0, loss = 0.1109\n",
      "Epoch  25 / iter   1, loss = 0.1366\n",
      "Epoch  25 / iter   2, loss = 0.1300\n",
      "Epoch  25 / iter   3, loss = 0.1476\n",
      "Epoch  25 / iter   4, loss = 0.1874\n",
      "Epoch  26 / iter   0, loss = 0.2977\n",
      "Epoch  26 / iter   1, loss = 0.2795\n",
      "Epoch  26 / iter   2, loss = 0.1569\n",
      "Epoch  26 / iter   3, loss = 0.1104\n",
      "Epoch  26 / iter   4, loss = 0.0873\n",
      "Epoch  27 / iter   0, loss = 0.3288\n",
      "Epoch  27 / iter   1, loss = 0.3314\n",
      "Epoch  27 / iter   2, loss = 0.3889\n",
      "Epoch  27 / iter   3, loss = 0.2776\n",
      "Epoch  27 / iter   4, loss = 0.1918\n",
      "Epoch  28 / iter   0, loss = 0.1295\n",
      "Epoch  28 / iter   1, loss = 0.1390\n",
      "Epoch  28 / iter   2, loss = 0.0934\n",
      "Epoch  28 / iter   3, loss = 0.1015\n",
      "Epoch  28 / iter   4, loss = 0.0399\n",
      "Epoch  29 / iter   0, loss = 0.1037\n",
      "Epoch  29 / iter   1, loss = 0.0848\n",
      "Epoch  29 / iter   2, loss = 0.1361\n",
      "Epoch  29 / iter   3, loss = 0.1314\n",
      "Epoch  29 / iter   4, loss = 0.0959\n",
      "Epoch  30 / iter   0, loss = 0.1372\n",
      "Epoch  30 / iter   1, loss = 0.2018\n",
      "Epoch  30 / iter   2, loss = 0.3979\n",
      "Epoch  30 / iter   3, loss = 0.8767\n",
      "Epoch  30 / iter   4, loss = 1.1544\n",
      "Epoch  31 / iter   0, loss = 0.9199\n",
      "Epoch  31 / iter   1, loss = 0.0781\n",
      "Epoch  31 / iter   2, loss = 0.1396\n",
      "Epoch  31 / iter   3, loss = 0.1011\n",
      "Epoch  31 / iter   4, loss = 0.1111\n",
      "Epoch  32 / iter   0, loss = 0.6540\n",
      "Epoch  32 / iter   1, loss = 0.3673\n",
      "Epoch  32 / iter   2, loss = 0.2777\n",
      "Epoch  32 / iter   3, loss = 0.1269\n",
      "Epoch  32 / iter   4, loss = 0.3903\n",
      "Epoch  33 / iter   0, loss = 0.1976\n",
      "Epoch  33 / iter   1, loss = 0.2111\n",
      "Epoch  33 / iter   2, loss = 0.2297\n",
      "Epoch  33 / iter   3, loss = 0.1792\n",
      "Epoch  33 / iter   4, loss = 0.1445\n",
      "Epoch  34 / iter   0, loss = 0.2714\n",
      "Epoch  34 / iter   1, loss = 0.3205\n",
      "Epoch  34 / iter   2, loss = 0.1826\n",
      "Epoch  34 / iter   3, loss = 0.2700\n",
      "Epoch  34 / iter   4, loss = 0.1581\n",
      "Epoch  35 / iter   0, loss = 0.1896\n",
      "Epoch  35 / iter   1, loss = 0.1661\n",
      "Epoch  35 / iter   2, loss = 0.1497\n",
      "Epoch  35 / iter   3, loss = 0.1283\n",
      "Epoch  35 / iter   4, loss = 0.0725\n",
      "Epoch  36 / iter   0, loss = 0.1062\n",
      "Epoch  36 / iter   1, loss = 0.1056\n",
      "Epoch  36 / iter   2, loss = 0.0836\n",
      "Epoch  36 / iter   3, loss = 0.1312\n",
      "Epoch  36 / iter   4, loss = 0.4115\n",
      "Epoch  37 / iter   0, loss = 0.9691\n",
      "Epoch  37 / iter   1, loss = 0.4920\n",
      "Epoch  37 / iter   2, loss = 0.3766\n",
      "Epoch  37 / iter   3, loss = 0.1393\n",
      "Epoch  37 / iter   4, loss = 0.1153\n",
      "Epoch  38 / iter   0, loss = 0.1127\n",
      "Epoch  38 / iter   1, loss = 0.1486\n",
      "Epoch  38 / iter   2, loss = 0.1289\n",
      "Epoch  38 / iter   3, loss = 0.1136\n",
      "Epoch  38 / iter   4, loss = 0.0695\n",
      "Epoch  39 / iter   0, loss = 0.0883\n",
      "Epoch  39 / iter   1, loss = 0.0702\n",
      "Epoch  39 / iter   2, loss = 0.0898\n",
      "Epoch  39 / iter   3, loss = 0.0960\n",
      "Epoch  39 / iter   4, loss = 0.2594\n",
      "Epoch  40 / iter   0, loss = 1.0782\n",
      "Epoch  40 / iter   1, loss = 2.7554\n",
      "Epoch  40 / iter   2, loss = 0.4472\n",
      "Epoch  40 / iter   3, loss = 0.6543\n",
      "Epoch  40 / iter   4, loss = 0.6297\n",
      "Epoch  41 / iter   0, loss = 0.1630\n",
      "Epoch  41 / iter   1, loss = 0.1613\n",
      "Epoch  41 / iter   2, loss = 0.1588\n",
      "Epoch  41 / iter   3, loss = 0.1772\n",
      "Epoch  41 / iter   4, loss = 0.2194\n",
      "Epoch  42 / iter   0, loss = 0.1786\n",
      "Epoch  42 / iter   1, loss = 0.1140\n",
      "Epoch  42 / iter   2, loss = 0.1233\n",
      "Epoch  42 / iter   3, loss = 0.1004\n",
      "Epoch  42 / iter   4, loss = 0.0339\n",
      "Epoch  43 / iter   0, loss = 0.0837\n",
      "Epoch  43 / iter   1, loss = 0.0776\n",
      "Epoch  43 / iter   2, loss = 0.1335\n",
      "Epoch  43 / iter   3, loss = 0.1286\n",
      "Epoch  43 / iter   4, loss = 0.0562\n",
      "Epoch  44 / iter   0, loss = 0.1161\n",
      "Epoch  44 / iter   1, loss = 0.0974\n",
      "Epoch  44 / iter   2, loss = 0.1002\n",
      "Epoch  44 / iter   3, loss = 0.1093\n",
      "Epoch  44 / iter   4, loss = 0.1381\n",
      "Epoch  45 / iter   0, loss = 0.0663\n",
      "Epoch  45 / iter   1, loss = 0.0777\n",
      "Epoch  45 / iter   2, loss = 0.0733\n",
      "Epoch  45 / iter   3, loss = 0.0672\n",
      "Epoch  45 / iter   4, loss = 0.0244\n",
      "Epoch  46 / iter   0, loss = 0.0868\n",
      "Epoch  46 / iter   1, loss = 0.0827\n",
      "Epoch  46 / iter   2, loss = 0.0665\n",
      "Epoch  46 / iter   3, loss = 0.0517\n",
      "Epoch  46 / iter   4, loss = 0.0838\n",
      "Epoch  47 / iter   0, loss = 0.1520\n",
      "Epoch  47 / iter   1, loss = 0.2054\n",
      "Epoch  47 / iter   2, loss = 0.4845\n",
      "Epoch  47 / iter   3, loss = 0.7135\n",
      "Epoch  47 / iter   4, loss = 0.4576\n",
      "Epoch  48 / iter   0, loss = 0.3190\n",
      "Epoch  48 / iter   1, loss = 0.0899\n",
      "Epoch  48 / iter   2, loss = 0.1030\n",
      "Epoch  48 / iter   3, loss = 0.0408\n",
      "Epoch  48 / iter   4, loss = 0.0052\n",
      "Epoch  49 / iter   0, loss = 0.0884\n",
      "Epoch  49 / iter   1, loss = 0.0603\n",
      "Epoch  49 / iter   2, loss = 0.0655\n",
      "Epoch  49 / iter   3, loss = 0.0599\n",
      "Epoch  49 / iter   4, loss = 0.0275\n",
      "取倒数第十行数据测试实际值为 19.7 预测值为 [20.82701642]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "datafile = './work/housing.data'\r\n",
    "data = np.fromfile(datafile, sep=' ')\r\n",
    "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS',\r\n",
    "                 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\r\n",
    "feature_num = len(feature_names)\r\n",
    "data = data.reshape([data.shape[0] // feature_num, feature_num])\r\n",
    "\r\n",
    "ratio = 0.8\r\n",
    "offset = int(data.shape[0] * ratio)\r\n",
    "training_data = data[:offset]\r\n",
    "\r\n",
    "maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), training_data.sum(axis=0)\r\n",
    "\r\n",
    "\r\n",
    "for i in range(feature_num):\r\n",
    "    data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\r\n",
    "\r\n",
    "\r\n",
    "def load_data():\r\n",
    "    # 从文件导入数据\r\n",
    "    datafile = './work/housing.data'\r\n",
    "    data = np.fromfile(datafile, sep=' ')\r\n",
    "\r\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\r\n",
    "    feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT',\r\n",
    "                     'MEDV']\r\n",
    "    feature_num = len(feature_names)\r\n",
    "\r\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\r\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\r\n",
    "\r\n",
    "    # 将原数据集拆分成训练集和测试集\r\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\r\n",
    "    # 测试集和训练集必须是没有交集的\r\n",
    "    ratio = 0.8\r\n",
    "    offset = int(data.shape[0] * ratio)\r\n",
    "    training_data = data[:offset]\r\n",
    "\r\n",
    "    # 计算训练集的最大值，最小值，平均值\r\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), training_data.sum(axis=0) / \\\r\n",
    "                               training_data.shape[0]\r\n",
    "\r\n",
    "    # 对数据进行归一化处理\r\n",
    "    for i in range(feature_num):\r\n",
    "        # print(maximums[i], minimums[i], avgs[i])\r\n",
    "        data[:, i] = (data[:, i] - minimums[i]) / (maximums[i] - minimums[i])\r\n",
    "\r\n",
    "    # 训练集和测试集的划分比例\r\n",
    "    training_data = data[:offset]\r\n",
    "    test_data = data[offset:]\r\n",
    "    return training_data, test_data, minimums[13], maximums[13]\r\n",
    "\r\n",
    "\r\n",
    "class Network(object):\r\n",
    "    def __init__(self, num_of_weights):\r\n",
    "        # 随机产生w的初始值\r\n",
    "        # 为了保持程序每次运行结果的一致性，此处设置固定的随机数种子\r\n",
    "        self.w1 = np.random.randn(num_of_weights, num_of_weights)\r\n",
    "        self.b1 = 0.\r\n",
    "        self.w2 = np.random.randn(num_of_weights, 1)\r\n",
    "        self.b2 = 0.\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        z1 = np.dot(x, self.w1) + self.b1\r\n",
    "        z1relu = np.maximum(z1, 0)\r\n",
    "        z = np.dot(z1relu, self.w2) + self.b2\r\n",
    "        return z, z1relu, z1\r\n",
    "\r\n",
    "    def loss(self, z, y):\r\n",
    "\r\n",
    "        error = z - y\r\n",
    "\r\n",
    "        num_samples = error.shape[0]\r\n",
    "        cost = error * error\r\n",
    "        cost = np.sum(cost) / num_samples\r\n",
    "        return cost\r\n",
    "\r\n",
    "    def gradient(self, x, y):\r\n",
    "        z, z1relu, z1 = self.forward(x)\r\n",
    "        N = x.shape[0]\r\n",
    "        gradient_w2 = 1. / N * np.sum((z - y) * z1relu, axis=0)\r\n",
    "        gradient_w2 = gradient_w2[:, np.newaxis]\r\n",
    "        gradient_b2 = 1. / N * np.sum(z - y)\r\n",
    "        gradient_w1 = 1. / N * np.sum((z - y) * z1relu * x, axis=0)\r\n",
    "        gradient_w1 = gradient_w1[:, np.newaxis]\r\n",
    "        gradient_b1 = 1. / N * np.sum((z - y) * (y - z1relu))\r\n",
    "\r\n",
    "        return gradient_w1, gradient_b1, gradient_w2, gradient_b2\r\n",
    "\r\n",
    "    def update(self, gradient_w1, gradient_b1, gradient_w2, gradient_b2, eta=0.01):\r\n",
    "        self.w1 = self.w1 - eta * gradient_w1\r\n",
    "        self.b1 = self.b1 - eta * gradient_b1\r\n",
    "        self.w2 = self.w2 - eta * gradient_w2\r\n",
    "        self.b2 = self.b2 - eta * gradient_b2\r\n",
    "\r\n",
    "    def train(self, training_data, num_epochs, batch_size=10, eta=0.01):\r\n",
    "        n = len(training_data)\r\n",
    "        losses = []\r\n",
    "        for epoch_id in range(num_epochs):\r\n",
    "            # 在每轮迭代开始之前，将训练数据的顺序随机打乱\r\n",
    "            # 然后再按每次取batch_size条数据的方式取出\r\n",
    "            np.random.shuffle(training_data)\r\n",
    "            # 将训练数据进行拆分，每个mini_batch包含batch_size条的数据\r\n",
    "            mini_batches = [training_data[k:k + batch_size] for k in range(0, n, batch_size)]\r\n",
    "            for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "                # print(self.w.shape)\r\n",
    "                # print(self.b)\r\n",
    "                x = mini_batch[:, :-1]\r\n",
    "                y = mini_batch[:, -1:]\r\n",
    "                a, a1, a2 = self.forward(x)\r\n",
    "                loss = self.loss(a, y)\r\n",
    "                gradient_w1, gradient_b1, gradient_w2, gradient_b2 = self.gradient(x, y)\r\n",
    "                self.update(gradient_w1, gradient_b1, gradient_w2, gradient_b2, eta)\r\n",
    "                losses.append(loss)\r\n",
    "                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.\r\n",
    "                      format(epoch_id, iter_id, loss))\r\n",
    "\r\n",
    "        return losses\r\n",
    "\r\n",
    "\r\n",
    "# 获取数据\r\n",
    "train_data, test_data, min, max = load_data()\r\n",
    "\r\n",
    "# 创建网络\r\n",
    "net = Network(13)\r\n",
    "# 启动训练\r\n",
    "losses = net.train(train_data, num_epochs=50, batch_size=100, eta=0.1)\r\n",
    "\r\n",
    "# 画出损失函数的变化趋势\r\n",
    "plot_x = np.arange(len(losses))\r\n",
    "plot_y = np.array(losses)\r\n",
    "plt.plot(plot_x, plot_y)\r\n",
    "plt.show()\r\n",
    "\r\n",
    "\r\n",
    "x = test_data[-10, -1]\r\n",
    "pred, pred1, pred2 = net.forward(test_data[-10, :-1])\r\n",
    "repred = (pred * max) - (min * (pred - 1))\r\n",
    "retest = (x * max) - (min * (x - 1))\r\n",
    "print(\"取倒数第十行数据测试实际值为\", retest, \"预测值为\", repred)\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
