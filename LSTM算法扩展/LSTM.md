## 1.1 $LSTM$结构

$LSTM$是一种特殊的递归神经网络（$RNN$）。$RNN$由于自身结构的特点而非常适用于处理序列数据，其在自然语言处理，语音识别等序列数据处理上有非常成功的应用。$LSTM$的提出解决了$RNN$结构的“长依赖”问题。“长依赖”就是词语间跨越很长距离的关联关系。比如，当我们在前文知道某个人现在在北京时，后面推测该人在哪个国家时，会立马得到“他在中国”这个结论。在$LSTM$提出之前，$RNN$结构由于参数冲突（详见1.2）的原因，“长依赖”问题一直没有得到解决。不过，好在$LSTM$出现了。

在一个标准的$RNN$结构中，网络的输出会与下一步的输入组合，形成新的输入一同送到下一步循环中进行训练。这一典型的自循环结构可以如下图表示：

<div align="center">
    <a>
    	<img src="https://ai-studio-static-online.cdn.bcebos.com/0fcdb6c5e1ef4e688e403a20a41863d487503258de5c471ea14e16b6be25e425">  
    </a>
</div>

其中左边是真实的结构，右边是为了理解对各个时间步上的结构进行的切片展示。可以看到，每一个时间步上的输出都指向了下一个时间步的输入。这种对数据的处理反映了时序数据一种特点：后边的状态的数据会受到先前状态的影响。

数据在图中A部分可以是经过一个非常简单的处理后被输出，如经过一个$tanh$层。但是这种简单的处理会面临一些复杂的问题，比如参数的学习会陷入一种无序的状态而不能收敛，甚至造成梯度爆炸而出现喜闻乐见的满屏NAN。$LSTM$的结构复杂一些，如下图表示：

<div align="center">
    <a>
    	<img src="https://ai-studio-static-online.cdn.bcebos.com/b4b509dd74284320a954fef46015eaf47994ec6d21024bca8f4fe062e8dd763f">  
    </a>
</div>

各个符号的含义如下：

<div align="center">
    <a>
    	<img src="https://ai-studio-static-online.cdn.bcebos.com/c10af4f60e574478a1148b9c326ab3c978af261b590d420189088ec43958b3bd">  
    </a>
</div>

从上图我们首先可以看出，$LSTM$构造了一个自始至终都存在的状态 $ C_t $，称为细胞状态，如下图所示高亮部分显示。它的存在保证了学习到的信息能够被保留下来，而不会因为后续学习的干扰而被彻底改变。

<div align="center">
    <a>
    	<img src="https://ai-studio-static-online.cdn.bcebos.com/cd278551320b4745bdcf5292ed4b438d276c1ca08dc74695a42b15e3e01e70bd">  
    </a>
</div>

同时，通过一种被称为门(Gate)的结构，细胞状态$C_t$可以进行更新。门结构其实就是一种点乘操作，将一个和$C_t$等长的状态变量和$C_t$相乘，如果状态变量的某值为0，则对应$C_t$部分与之相乘的结果也为0，这就像是一扇门拦住了$C_t$对应部分不让其通过一样，如上图浅色部分所示。在每个时间步上，$C_t$的状态通过以下方式进行更新：

$$
C_t=f_t * C_{t-1}+i_t * \hat{C_t}
$$
我们可以将$f_t$和$i_t$视作两种门：$f_t$直接作用在$C_{t-1}$上，它控制了旧的细胞状态的哪一部分被保留，哪一部分被遗忘；$i_t$则作用在新的细胞状态$\hat{C_t}$上，注意这个细胞状态上面的帽子，区别于旧的细胞状态，这个状态是根据输入而得到的，$i_t$控制了新的细胞状态哪一部分被保留，哪一部分被遗忘。可以看出，$f_t$的作用是忘记旧的知识，$i_t$的作用是学习新的知识，而新的知识来自于对新的输入的学习。下边给出了$f_t,i_t,\hat{C_t}$是如何得到的：

$$
f_t = \sigma\left(W_f\cdot\left[h_{t-1},x_t\right]+b_f\right)
$$

$$
i_t = \sigma\left(W_i\cdot\left[h_{t-1},x_t\right]+b_i\right)
$$

$$
\hat{C_t}=tanh\left(W_C\cdot\left[h_{t-1},x_t\right]+b_C\right)
$$
观察上述三个式子，我们可以发现，它们都是根据当前的输入而进行的操作，其中$f_t$根据当前的输入决定忘记什么，$i_t$根据当前的输入决定记住什么，而$\hat{C_t}$便是对当前输入信息的提炼。明白了上面所述，便明白了为什么$f_t,i_t$中要用激活函数$\sigma$，而$\hat{C_t}$中要用激活函数$tanh$了。那么有些人会问，既然三种值的操作是一样的，为什么不将它们合并成一个值，类似于下式这样，岂不是参数更少而训练效率更高？其实这就是$LSTM$的创新之处。下式为什么不行呢？请参考1.2中的分析。
$$
C_t=tanh\left(f\left(C_t,W_C\left[h_{t-1},x_t\right]+b_C\right)\right)
$$
当细胞状态更新完以后，$LSTM$也不是直接将该细胞状态输入，而是像上边所述一样，又进行了一次选择性遗忘，套路也一样，式子和上边相同：
$$
o_t = \sigma\left(W_o\left[h_{t-1},x_t\right]+b_o\right)
$$

$$
h_t = o_t * tanh\left(C_t\right)
$$

至此，$LSTM$在一个时间步上的动作便完成了，下一步将接收来自上一步的$h_t$和下一步的输入$x_{t+1}$继续重复上述操作。



## 1.2 为何$LSTM$是这种结构？

对于$RNN$结构的改进经历了很长的一段历史。在$RNN$结构被提出来以后，研究人员遭遇了很多困境，其中比较显著的问题是$RNN$结构的梯度传播问题。由于$RNN$独特的递归循环的结构，每次迭代中参数的更新都要在如何进行中做抉择：参数既要保留历史的信息，又要依据当前的新输入进行更替。在这个过程中会出现两种冲突：

1. 输入参数冲突。假设我们当前要更新参数$w_{ji}$，其中$i$是输入的某维，$j$是待更新参数的某维。$j$会根据输入$i$来进行更新，以降低整体的误差。在这里，$i$一般不为0。在更新的过程中，$w_{ji}$会收到两种信号：（1）更新$w_{ji}$来储存新的输入信息；（2）保持$w_{ji}$不变以保护历史信息。这种冲突导致了$RNN$对输入非常敏感，一种轻微的数据扰动，可能就会造成梯度爆炸

2. 输出参数冲突。现在我们假设当前更新参数$w_{kj}$，其中$k$是更新参数的某维，$j$是输出的某维。该参数需要提取输出$j$的信息，同时要避免$j$干扰参数$k$。同样地，$j$一般不为0。在参数更新中，更新信号表现为两种形式：（1）该信号希望参数$w_{kj}$得到$j$的信息；（2）该信号希望避免$j$的信息干扰$k$。

简言之，递归循环的过程中，由于同时涉及到参数的保护和更新，而这种机制是不固定的，很难判断哪个参数需要更新，哪个参数需要保护，在训练过程中便很容易出现波动而使已经训练好的参数重新被扰乱。

$LSTM$的这种结构，就是为了解决上述问题。首先，为了解决梯度更新中梯度爆炸或者梯度消失的问题，引入了一个常数误差（Constant Error Carrousel, CEC），就是结构图示中，上边那个贯穿整个元胞的直线（$C_{t-1}->C_t$）；其次，引入一个输入门（Input Gate Unit）来决定哪些新的信息需要继承；最后，引入一个输出门（Output Gate Unit）来决定何种状态需要被输出。

这种思路导致的结果是，信息的保存和更新这两个过程被分开了。信息的保存通过一个门结构来进行，信息的更新通过另一个门结构来进行，两者分别独享参数，从而避免了上述的两种冲突。
