{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encoding=utf8\r\n",
    "import io\r\n",
    "import os\r\n",
    "import re\r\n",
    "import sys\r\n",
    "import random\r\n",
    "import requests\r\n",
    "from collections import OrderedDict \r\n",
    "import math\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "from paddle.nn import Embedding\r\n",
    "import paddle.nn.functional as F\r\n",
    "import tarfile\r\n",
    "from paddle.nn import LSTM, Embedding, Dropout, Linear\r\n",
    "import paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#应用库内数据，下载速度慢，所以直接用url下到工作区\r\n",
    "#train_ds, test_ds = paddlenlp.datasets.load_dataset(\"ptb\", splits=(\"train\", \"test\"))\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "def download():\r\n",
    "    # 通过python的requests类，下载存储在\r\n",
    "    # http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\r\n",
    "    corpus_url = \"http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\"\r\n",
    "    web_request = requests.get(corpus_url)\r\n",
    "    corpus = web_request.content\r\n",
    "\r\n",
    "    # 将下载的文件写在当前目录的aclImdb_v1.tar.gz文件内\r\n",
    "    with open(\"./aclImdb_v1.tar.gz\", \"wb\") as f:\r\n",
    "        f.write(corpus)\r\n",
    "    f.close()\r\n",
    "\r\n",
    "download()\"\"\"\r\n",
    "#也慢，所以在本地下载数据后，挂载到云上\r\n",
    "#因为此前的代码根据API中的数据格式进行处理，和本地的数据格式不一样，在此处构建loadtext转变格式\r\n",
    "def loadtxt(path):\r\n",
    "    file = open(path)\r\n",
    "    lines=[]\r\n",
    "    for line in file.readlines():\r\n",
    "        line = line.strip(\" \\n \")\r\n",
    "        lines.append(line)\r\n",
    "    data=[]\r\n",
    "    for sentence in lines:\r\n",
    "        one_data={}\r\n",
    "        one_data['sentence']=sentence\r\n",
    "        data.append(one_data)\r\n",
    "    return data\r\n",
    "\r\n",
    "train_path='ptb.train.txt'\r\n",
    "test_path='ptb.test.txt'\r\n",
    "\r\n",
    "train_ds = loadtxt(train_path)\r\n",
    "test_ds =loadtxt(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "数据集信息一览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter'}\n",
      "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter\n",
      "42068\n",
      "3761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"for i in range(len(test_ds)):\\n    print(len(train_ds[i]['sentence']))\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.array(train_ds[0]))\r\n",
    "print(train_ds[0]['sentence']) #字典\r\n",
    "print(len(train_ds))\r\n",
    "print(len(test_ds))\r\n",
    "\r\n",
    "\"\"\"for i in range(len(test_ds)):\r\n",
    "    print(len(train_ds[i]['sentence']))\"\"\"\r\n",
    "#句长不等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aer',\n",
       " 'banknote',\n",
       " 'berlitz',\n",
       " 'calloway',\n",
       " 'centrust',\n",
       " 'cluett',\n",
       " 'fromstein',\n",
       " 'gitano',\n",
       " 'guterman',\n",
       " 'hydro-quebec',\n",
       " 'ipo',\n",
       " 'kia',\n",
       " 'memotec',\n",
       " 'mlx',\n",
       " 'nahb',\n",
       " 'punts',\n",
       " 'rake',\n",
       " 'regatta',\n",
       " 'rubens',\n",
       " 'sim',\n",
       " 'snack-food',\n",
       " 'ssangyong',\n",
       " 'swapo',\n",
       " 'wachter']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=train_ds[0]['sentence']\r\n",
    "#sentence.strip().lower()\r\n",
    "sentence.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter'], ['pierre', '<unk>', 'n', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'n'], ['mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group'], ['rudolph', '<unk>', 'n', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate'], ['a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of', 'workers', 'exposed', 'to', 'it', 'more', 'than', 'n', 'years', 'ago', 'researchers', 'reported']]\n",
      "[['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter'], ['pierre', '<unk>', 'n', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'n'], ['mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group'], ['rudolph', '<unk>', 'n', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate'], ['a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of', 'workers', 'exposed', 'to', 'it', 'more', 'than', 'n', 'years', 'ago', 'researchers', 'reported']]\n"
     ]
    }
   ],
   "source": [
    "def data_preprocess(dataset):\r\n",
    "    data_set = []\r\n",
    "    for i in range(len(dataset)):\r\n",
    "        # 这里有一个小trick是把所有的句子转换为小写，从而减小词表的大小\r\n",
    "        # 一般来说这样的做法有助于效果提升\r\n",
    "        sentence=train_ds[i]['sentence']\r\n",
    "        sentence = sentence.strip().lower()\r\n",
    "        sentence = sentence.split(\" \")\r\n",
    "        \r\n",
    "        data_set.append(sentence)\r\n",
    "    return data_set\r\n",
    "\r\n",
    "train_corpus = data_preprocess(train_ds)\r\n",
    "test_corpus = data_preprocess(test_ds)\r\n",
    "print(train_corpus[:5])\r\n",
    "print(test_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totoally 10000 different words in the corpus\n",
      "word <unk>, its id 3, its word freq 32481\n",
      "word [pad], its id 1, its word freq 10000000000\n",
      "word the, its id 2, its word freq 50770\n",
      "word n, its id 3, its word freq 32481\n",
      "word of, its id 4, its word freq 24400\n",
      "word to, its id 5, its word freq 23638\n",
      "word a, its id 6, its word freq 21196\n",
      "word in, its id 7, its word freq 18000\n",
      "word and, its id 8, its word freq 17474\n",
      "word 's, its id 9, its word freq 9784\n"
     ]
    }
   ],
   "source": [
    "# 构造词典，统计每个词的频率，并根据频率将每个词转换为一个整数id\r\n",
    "def build_dict(corpus):\r\n",
    "    word_freq_dict = dict()\r\n",
    "    for sentence in corpus:\r\n",
    "        for word in sentence:\r\n",
    "            if word not in word_freq_dict:\r\n",
    "                word_freq_dict[word] = 0\r\n",
    "            word_freq_dict[word] += 1\r\n",
    "\r\n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)\r\n",
    "    \r\n",
    "    word2id_dict = dict()\r\n",
    "    word2id_freq = dict()\r\n",
    "\r\n",
    "    # 一般来说，我们把unk和pad放在词典前面，给他们一个比较小的id，这样比较方便记忆，并且易于后续扩展词表\r\n",
    "    word2id_dict['<unk>'] = 0\r\n",
    "    word2id_freq[0] = 1e10\r\n",
    "\r\n",
    "    word2id_dict['[pad]'] = 1\r\n",
    "    word2id_freq[1] = 1e10\r\n",
    "\r\n",
    "    for word, freq in word_freq_dict:\r\n",
    "        word2id_dict[word] = len(word2id_dict)\r\n",
    "        word2id_freq[word2id_dict[word]] = freq\r\n",
    "\r\n",
    "    return word2id_freq, word2id_dict\r\n",
    "\r\n",
    "word2id_freq, word2id_dict = build_dict(train_corpus)\r\n",
    "vocab_size = len(word2id_freq)\r\n",
    "print(\"there are totoally %d different words in the corpus\" % vocab_size)\r\n",
    "for _, (word, word_id) in zip(range(10), word2id_dict.items()):\r\n",
    "    print(\"word %s, its id %d, its word freq %d\" % (word, word_id, word2id_freq[word_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42068 tokens in the corpus\n",
      "[[9970, 9971, 9972, 9973, 9974, 9975, 9976, 9977, 9978, 9979, 9980, 9981, 9982, 9983, 9984, 9985, 9986, 9987, 9988, 9989, 9990, 9991, 9992, 9993], [8569, 3, 3, 72, 393, 33, 2116, 2, 146, 19, 6, 8570, 275, 407, 3], [23, 3, 13, 141, 4, 3, 5278, 2, 3055, 1581, 96], [7232, 3, 3, 72, 393, 8, 337, 141, 4, 2468, 657, 2158, 949, 24, 521, 6, 8570, 275, 4, 39, 303, 437, 3661], [6, 941, 4, 3143, 495, 262, 5, 137, 5882, 4219, 5883, 30, 986, 6, 240, 755, 4, 1013, 2765, 211, 6, 96, 4, 427, 4060, 5, 14, 45, 55, 3, 72, 195, 1233, 220]]\n",
      "[[9970, 9971, 9972, 9973, 9974, 9975, 9976, 9977, 9978, 9979, 9980, 9981, 9982, 9983, 9984, 9985, 9986, 9987, 9988, 9989, 9990, 9991, 9992, 9993], [8569, 3, 3, 72, 393, 33, 2116, 2, 146, 19, 6, 8570, 275, 407, 3], [23, 3, 13, 141, 4, 3, 5278, 2, 3055, 1581, 96], [7232, 3, 3, 72, 393, 8, 337, 141, 4, 2468, 657, 2158, 949, 24, 521, 6, 8570, 275, 4, 39, 303, 437, 3661], [6, 941, 4, 3143, 495, 262, 5, 137, 5882, 4219, 5883, 30, 986, 6, 240, 755, 4, 1013, 2765, 211, 6, 96, 4, 427, 4060, 5, 14, 45, 55, 3, 72, 195, 1233, 220]]\n"
     ]
    }
   ],
   "source": [
    "# 把语料转换为id序列\r\n",
    "def convert_corpus_to_id(corpus, word2id_dict):\r\n",
    "    data_set = []\r\n",
    "    for sentence in corpus:\r\n",
    "        # 将句子中的词逐个替换成id，如果句子中的词不在词表内，则替换成'<unk>'\r\n",
    "        # 这里需要注意，一般来说我们可能需要查看一下test-set中，句子'<unk>'的比例，\r\n",
    "        # 如果存在过多'<unk>'的情况，那就说明我们的训练数据不足或者切分存在巨大偏差，需要调整\r\n",
    "        sentence = [word2id_dict[word] if word in word2id_dict \\\r\n",
    "                    else word2id_dict['<unk>'] for word in sentence]    \r\n",
    "        data_set.append(sentence)\r\n",
    "    return data_set\r\n",
    "\r\n",
    "train_corpus = convert_corpus_to_id(train_corpus, word2id_dict)\r\n",
    "test_corpus = convert_corpus_to_id(test_corpus, word2id_dict)\r\n",
    "print(\"%d tokens in the corpus\" % len(train_corpus))\r\n",
    "print(train_corpus[:5])\r\n",
    "print(test_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "在PTB数据集中，每个句子并非随机抽取的文本，而是在上下文之间有关联的内容。语言模型为了利用上下文信息，必须将前面句子的信息传递到后面的句子。为了实现这个目标，在PTB上下文有关联的数据集上，通常采用另一种batching方法。\n",
    "\n",
    "将长序列切割为固定长度的子序列。循环神经网络在处理完一个子序列后，它最终的隐藏状态将复制到下一个序列中作为初始值，这样在前向计算时，效果等同于一次性顺序地读取了整个文档；而在反向传播时，梯度则只在每个子序列内部传播。\n",
    "\n",
    "为了利用计算时的并行能力，希望每一次计算可以对多个句子进行并行处理，同时又要尽量保证batch之间的上下文连续。解决方案是，先将整个文档分成若干连续段落，再让batch中的每一个位置负责其中一段。例如，如果batch大小时4，则先将这个文档平均分成4个子序列，让batch中的每一个位置负责其中一个子序列，这样每个子文档内部的所有数据仍可以被顺序处理。\n",
    "\n",
    "下面的代码从文件中读取数据，并按上面介绍的方案将数据整理成batch。由于PTB数据集比较小，因此可以直接将这个数据集一次性读入内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 20               # 训练数据batch的大小\r\n",
    "TRAIN_NUM_STEP = 35                 # 训练数据截断长度\r\n",
    "\r\n",
    "#生成保存上下文信息的总文本id_list，用它来构造训练batch\r\n",
    "def make_id_list(corpus):\r\n",
    "    id_list=[]\r\n",
    "    for sentence in corpus:\r\n",
    "        for word in sentence:\r\n",
    "            id_list.append(word)\r\n",
    "    return id_list\r\n",
    "\r\n",
    "#train_id_list=make_id_list(test_corpus)\r\n",
    "#print(train_id_list[:50])\r\n",
    "\r\n",
    "def make_batch(id_list, batch_size, num_step):\r\n",
    "    # 计算总的batch数量，每个batch包含的单词数量是batch_size * num_step\r\n",
    "    num_batches = (len(id_list) - 1) // (batch_size * num_step)\r\n",
    "    # 将数据整理成一个维度为[batch_size, num_batches * num_step]的二维数组\r\n",
    "    data = np.array(id_list[: num_batches * batch_size * num_step])\r\n",
    "    data = np.reshape(data, [batch_size, num_batches * num_step])\r\n",
    "    # 沿着第二个维度将数据切分成num_batches个batch,存入一个数组\r\n",
    "    data_batches = np.split(data, num_batches, axis=1)\r\n",
    "\r\n",
    "    # 重复上述操作，但是每个位置向右移动一位，这里得到的是RNN每一步输出所需要预测的下一个单词\r\n",
    "    label = np.array(id_list[1: num_batches * batch_size * num_step + 1])\r\n",
    "    label = np.reshape(label, [batch_size, num_batches * num_step])\r\n",
    "    label_batches = np.split(label, num_batches, axis=1)\r\n",
    "    # 返回一个长度为num_batches的数组，其中每一项包含一个data矩阵和一个label矩阵\r\n",
    "    return list(zip(data_batches, label_batches))\r\n",
    "\r\n",
    "\r\n",
    "train_id_list=make_id_list(train_corpus)\r\n",
    "train_batch=make_batch(train_id_list,TRAIN_BATCH_SIZE,TRAIN_NUM_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1267\n",
      "2\n",
      "20\n",
      "35\n",
      "[9970 9971 9972 9973 9974 9975 9976 9977 9978 9979 9980 9981 9982 9983\n",
      " 9984 9985 9986 9987 9988 9989 9990 9991 9992 9993 8569    3    3   72\n",
      "  393   33 2116    2  146   19    6]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_batch))#113个batch\r\n",
    "\r\n",
    "print(len(train_batch[0]))#每一个batch有2个矩阵：数据和标签\r\n",
    "\r\n",
    "print(len(train_batch[0][0]))#数据矩阵有20个句子，标签矩阵中有20个词矩阵\r\n",
    "\r\n",
    "print(len(train_batch[0][0][0]))#一个句子中有35个词\r\n",
    "\r\n",
    "print(train_batch[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9970 9971 9972 9973 9974 9975 9976 9977 9978 9979 9980 9981 9982 9983\n",
      " 9984 9985 9986 9987 9988 9989 9990 9991 9992 9993 8569    3    3   72\n",
      "  393   33 2116    2  146   19    6]\n",
      "[9971 9972 9973 9974 9975 9976 9977 9978 9979 9980 9981 9982 9983 9984\n",
      " 9985 9986 9987 9988 9989 9990 9991 9992 9993 8569    3    3   72  393\n",
      "   33 2116    2  146   19    6 8570]\n",
      "[1569  106  138  767   14   13 1513   18 1446    2  844  236    2 1384\n",
      "    5 1273    7 1635 1089 3842   17  380 1352    4  207    2 2600    4\n",
      "    2  261   13    5  335    2    3]\n",
      "[ 106  138  767   14   13 1513   18 1446    2  844  236    2 1384    5\n",
      " 1273    7 1635 1089 3842   17  380 1352    4  207    2 2600    4    2\n",
      "  261   13    5  335    2    3   16]\n"
     ]
    }
   ],
   "source": [
    "print(train_batch[0][0][0])#第一个batch中的第一个句子\r\n",
    "\r\n",
    "print(train_batch[0][1][0])#第一个batch中输出预测的标签矩阵\r\n",
    "\r\n",
    "print(train_batch[0][0][1])\r\n",
    "\r\n",
    "print(train_batch[0][1][1])\r\n",
    "\r\n",
    "#标签矩阵的最后一个值，就是对应输入句子的真实next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "class WordPredictor(paddle.nn.Layer):\r\n",
    "    \r\n",
    "    def __init__(self, hidden_size, vocab_size, embedding_size, prediction_num=10000, num_steps=35, num_layers=1, init_scale=0.1, dropout_rate=None):\r\n",
    "        \r\n",
    "        # 参数含义如下：\r\n",
    "        # 1.hidden_size，表示embedding-size，hidden和cell向量的维度\r\n",
    "        # 2.vocab_size，模型可以考虑的词表大小\r\n",
    "        # 3.embedding_size，表示词向量的维度\r\n",
    "        # 4.prediction_num,预测输出词的数量，每次预测输出对10000个词的概率\r\n",
    "        # 5.num_steps，表示这个情感分析模型最大可以考虑的句子长度\r\n",
    "        # 6.num_layers，表示网络的层数\r\n",
    "        # 7.dropout_rate，表示使用dropout过程中失活的神经元比例\r\n",
    "        # 8.init_scale，表示网络内部的参数的初始化范围,长短时记忆网络内部用了很多Tanh，Sigmoid等激活函数，\\\r\n",
    "        # 这些函数对数值精度非常敏感，因此我们一般只使用比较小的初始化范围，以保证效果\r\n",
    "        super(WordPredictor, self).__init__()\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.vocab_size = vocab_size\r\n",
    "        self.embedding_size = embedding_size\r\n",
    "        self.prediction_num = prediction_num\r\n",
    "        self.num_steps = num_steps\r\n",
    "        self.num_layers = num_layers\r\n",
    "        self.dropout_rate = dropout_rate\r\n",
    "        self.init_scale = init_scale\r\n",
    "       \r\n",
    "        # 声明一个LSTM模型，用来把每个句子抽象成向量\r\n",
    "        self.simple_lstm_rnn = paddle.nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers)\r\n",
    "\r\n",
    "        # 声明一个embedding层，用来把句子中的每个词转换为向量\r\n",
    "        self.embedding = paddle.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, sparse=False, \r\n",
    "                                    weight_attr=paddle.ParamAttr(initializer=paddle.nn.initializer.Uniform(low=-init_scale, high=init_scale)))\r\n",
    "        \r\n",
    "        # 声明使用上述语义向量映射到具体词时所需要使用的线性层\r\n",
    "        self.pred_fc = paddle.nn.Linear(in_features=self.hidden_size, out_features=self.prediction_num, \r\n",
    "                             weight_attr=None, bias_attr=None)\r\n",
    "        \r\n",
    "        self.embedding_out = paddle.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, sparse=False, \r\n",
    "                                    weight_attr=paddle.ParamAttr(initializer=paddle.nn.initializer.Uniform(low=-init_scale, high=init_scale)))\r\n",
    "\r\n",
    "        # 一般在获取单词的embedding后，会使用dropout层，防止过拟合，提升模型泛化能力\r\n",
    "        self.dropout_layer = paddle.nn.Dropout(p=self.dropout_rate, mode='upscale_in_train')\r\n",
    "\r\n",
    "    # forwad函数即为模型前向计算的函数，它有两个输入，分别为：\r\n",
    "    # input为输入的训练文本，其shape为[batch_size, max_seq_len]\r\n",
    "    # label训练文本对应的情感标签，其shape维[batch_size, 1]\r\n",
    "    def forward(self, inputs):\r\n",
    "        # 获取输入数据的batch_size\r\n",
    "        batch_size = inputs.shape[0]\r\n",
    "\r\n",
    "        # 本实验默认使用1层的LSTM，首先我们需要定义LSTM的初始hidden和cell，这里我们使用0来初始化这个序列的记忆\r\n",
    "        init_hidden_data = np.zeros(\r\n",
    "            (self.num_layers, batch_size, self.hidden_size), dtype='float32')\r\n",
    "        init_cell_data = np.zeros(\r\n",
    "            (self.num_layers, batch_size, self.hidden_size), dtype='float32')\r\n",
    "\r\n",
    "        # 将这些初始记忆转换为飞桨可计算的向量，并且设置stop_gradient=True，避免这些向量被更新，从而影响训练效果\r\n",
    "        init_hidden = paddle.to_tensor(init_hidden_data)\r\n",
    "        init_hidden.stop_gradient = True\r\n",
    "        init_cell = paddle.to_tensor(init_cell_data)\r\n",
    "        init_cell.stop_gradient = True\r\n",
    "\r\n",
    "        # 对应以上第2步，将输入的句子的mini-batch转换为词向量表示，转换后输入数据shape为[batch_size, max_seq_len, embedding_size]\r\n",
    "        x_emb = self.embedding(inputs)\r\n",
    "        x_emb = paddle.reshape(x_emb, shape=[-1, self.num_steps, self.embedding_size])\r\n",
    "        # 在获取的词向量后添加dropout层\r\n",
    "        if self.dropout_rate is not None and self.dropout_rate > 0.0:\r\n",
    "            x_emb = self.dropout_layer(x_emb)\r\n",
    "        \r\n",
    "        # 对应以上第3步，使用LSTM网络，把每个句子转换为语义向量\r\n",
    "        # 返回的last_hidden即为最后一个时间步的输出，其shape为[self.num_layers, batch_size, hidden_size]\r\n",
    "        rnn_out, (last_hidden, last_cell) = self.simple_lstm_rnn(x_emb, (init_hidden, init_cell))\r\n",
    "        \r\n",
    "        # 提取最后一层隐状态作为文本的语义向量，其shape为[batch_size, hidden_size]\r\n",
    "        last_hidden = paddle.reshape(last_hidden[-1], shape=[-1, self.hidden_size])\r\n",
    "\r\n",
    "        # 对应以上第4步，将每个句子的向量表示映射到具体的词上, logits的维度为[batch_size, 10000]\r\n",
    "        logits = self.pred_fc(last_hidden)\r\n",
    "        \r\n",
    "\r\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练和验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:55: DeprecationWarning: \n",
      "Warning:\n",
      "API \"paddle.nn.functional.loss.softmax_with_cross_entropy\" is deprecated since 2.0.0, and will be removed in future versions. Please use \"paddle.nn.functional.cross_entropy\" instead.\n",
      "reason: Please notice that behavior of \"paddle.nn.functional.softmax_with_cross_entropy\" and \"paddle.nn.functional.cross_entropy\" is different. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, loss 7.937\n",
      "step 200, loss 7.471\n",
      "step 300, loss 7.219\n",
      "step 400, loss 6.024\n",
      "step 500, loss 5.731\n",
      "step 600, loss 6.277\n",
      "step 700, loss 6.498\n",
      "step 800, loss 6.513\n",
      "step 900, loss 6.056\n",
      "step 1000, loss 6.101\n",
      "step 1100, loss 8.410\n",
      "step 1200, loss 6.955\n",
      "step 1300, loss 6.290\n",
      "step 1400, loss 5.890\n",
      "step 1500, loss 6.334\n",
      "step 1600, loss 6.209\n",
      "step 1700, loss 4.862\n",
      "step 1800, loss 5.621\n",
      "step 1900, loss 6.028\n",
      "step 2000, loss 5.741\n",
      "step 2100, loss 5.978\n",
      "step 2200, loss 5.775\n",
      "step 2300, loss 5.538\n",
      "step 2400, loss 5.722\n",
      "step 2500, loss 6.435\n",
      "step 2600, loss 5.310\n",
      "step 2700, loss 5.776\n",
      "step 2800, loss 6.395\n",
      "step 2900, loss 6.455\n",
      "step 3000, loss 6.220\n",
      "step 3100, loss 5.336\n",
      "step 3200, loss 5.688\n",
      "step 3300, loss 6.471\n",
      "step 3400, loss 5.965\n",
      "step 3500, loss 6.319\n",
      "step 3600, loss 5.569\n",
      "step 3700, loss 5.505\n",
      "step 3800, loss 4.938\n",
      "step 3900, loss 4.927\n",
      "step 4000, loss 5.030\n",
      "step 4100, loss 4.996\n",
      "step 4200, loss 5.127\n",
      "step 4300, loss 5.208\n",
      "step 4400, loss 4.576\n",
      "step 4500, loss 4.822\n",
      "step 4600, loss 4.933\n",
      "step 4700, loss 4.373\n",
      "step 4800, loss 4.244\n",
      "step 4900, loss 5.349\n",
      "step 5000, loss 5.212\n",
      "step 5100, loss 4.904\n",
      "step 5200, loss 5.173\n",
      "step 5300, loss 4.026\n",
      "step 5400, loss 5.478\n",
      "step 5500, loss 3.402\n",
      "step 5600, loss 5.140\n",
      "step 5700, loss 4.488\n",
      "step 5800, loss 5.349\n",
      "step 5900, loss 4.267\n",
      "step 6000, loss 4.658\n",
      "step 6100, loss 4.279\n",
      "step 6200, loss 4.877\n",
      "step 6300, loss 4.977\n",
      "step 6400, loss 4.091\n",
      "step 6500, loss 3.405\n",
      "step 6600, loss 5.439\n",
      "step 6700, loss 5.662\n",
      "step 6800, loss 4.634\n",
      "step 6900, loss 4.579\n",
      "step 7000, loss 4.238\n",
      "step 7100, loss 4.844\n",
      "step 7200, loss 3.647\n",
      "step 7300, loss 4.353\n",
      "step 7400, loss 5.271\n",
      "step 7500, loss 4.317\n",
      "step 7600, loss 4.599\n",
      "step 7700, loss 3.757\n",
      "step 7800, loss 4.901\n",
      "step 7900, loss 5.203\n",
      "step 8000, loss 4.444\n",
      "step 8100, loss 4.114\n",
      "step 8200, loss 4.331\n",
      "step 8300, loss 4.669\n",
      "step 8400, loss 4.550\n",
      "step 8500, loss 4.227\n",
      "step 8600, loss 4.384\n",
      "step 8700, loss 4.298\n",
      "step 8800, loss 5.461\n",
      "step 8900, loss 4.474\n",
      "step 9000, loss 4.324\n",
      "step 9100, loss 5.332\n",
      "step 9200, loss 4.162\n",
      "step 9300, loss 4.587\n",
      "step 9400, loss 4.242\n",
      "step 9500, loss 3.829\n",
      "step 9600, loss 4.034\n",
      "step 9700, loss 5.134\n",
      "step 9800, loss 3.889\n",
      "step 9900, loss 3.538\n",
      "step 10000, loss 4.163\n",
      "step 10100, loss 3.100\n",
      "step 10200, loss 3.705\n",
      "step 10300, loss 4.178\n",
      "step 10400, loss 4.380\n",
      "step 10500, loss 3.443\n",
      "step 10600, loss 3.941\n",
      "step 10700, loss 4.090\n",
      "step 10800, loss 4.126\n",
      "step 10900, loss 3.921\n",
      "step 11000, loss 3.508\n",
      "step 11100, loss 3.322\n",
      "step 11200, loss 3.350\n",
      "step 11300, loss 4.516\n",
      "step 11400, loss 2.914\n",
      "step 11500, loss 2.795\n",
      "step 11600, loss 4.946\n",
      "step 11700, loss 2.662\n",
      "step 11800, loss 3.585\n",
      "step 11900, loss 3.458\n",
      "step 12000, loss 3.854\n",
      "step 12100, loss 3.345\n",
      "step 12200, loss 3.519\n",
      "step 12300, loss 3.798\n",
      "step 12400, loss 3.577\n",
      "step 12500, loss 3.342\n",
      "step 12600, loss 3.597\n",
      "step 12700, loss 3.450\n",
      "step 12800, loss 4.079\n",
      "step 12900, loss 4.071\n",
      "step 13000, loss 3.904\n",
      "step 13100, loss 4.189\n",
      "step 13200, loss 3.909\n",
      "step 13300, loss 3.250\n",
      "step 13400, loss 2.521\n",
      "step 13500, loss 3.169\n",
      "step 13600, loss 4.094\n",
      "step 13700, loss 4.059\n",
      "step 13800, loss 3.417\n",
      "step 13900, loss 3.197\n",
      "step 14000, loss 3.051\n",
      "step 14100, loss 3.288\n",
      "step 14200, loss 3.512\n",
      "step 14300, loss 4.119\n",
      "step 14400, loss 3.756\n",
      "step 14500, loss 3.062\n",
      "step 14600, loss 3.357\n",
      "step 14700, loss 2.586\n",
      "step 14800, loss 4.366\n",
      "step 14900, loss 3.922\n",
      "step 15000, loss 3.192\n",
      "step 15100, loss 3.784\n",
      "step 15200, loss 3.072\n",
      "step 15300, loss 3.743\n",
      "step 15400, loss 4.608\n",
      "step 15500, loss 4.428\n",
      "step 15600, loss 3.453\n",
      "step 15700, loss 3.389\n",
      "step 15800, loss 2.537\n",
      "step 15900, loss 3.148\n",
      "step 16000, loss 3.139\n",
      "step 16100, loss 3.773\n",
      "step 16200, loss 3.788\n",
      "step 16300, loss 3.322\n",
      "step 16400, loss 4.220\n",
      "step 16500, loss 3.199\n",
      "step 16600, loss 4.148\n",
      "step 16700, loss 3.871\n",
      "step 16800, loss 4.719\n",
      "step 16900, loss 3.700\n",
      "step 17000, loss 3.224\n",
      "step 17100, loss 3.972\n",
      "step 17200, loss 3.166\n",
      "step 17300, loss 4.810\n",
      "step 17400, loss 3.604\n",
      "step 17500, loss 3.467\n",
      "step 17600, loss 3.354\n",
      "step 17700, loss 3.231\n",
      "step 17800, loss 2.569\n",
      "step 17900, loss 3.353\n",
      "step 18000, loss 3.560\n",
      "step 18100, loss 2.738\n",
      "step 18200, loss 3.942\n",
      "step 18300, loss 2.807\n",
      "step 18400, loss 3.674\n",
      "step 18500, loss 2.818\n",
      "step 18600, loss 3.646\n",
      "step 18700, loss 4.507\n",
      "step 18800, loss 2.957\n",
      "step 18900, loss 2.529\n",
      "step 19000, loss 3.960\n",
      "step 19100, loss 3.647\n",
      "step 19200, loss 3.174\n",
      "step 19300, loss 2.839\n",
      "step 19400, loss 3.434\n",
      "step 19500, loss 3.455\n",
      "step 19600, loss 3.479\n",
      "step 19700, loss 3.281\n",
      "step 19800, loss 3.620\n",
      "step 19900, loss 3.346\n",
      "step 20000, loss 4.068\n",
      "step 20100, loss 2.628\n",
      "step 20200, loss 3.527\n",
      "step 20300, loss 3.192\n",
      "step 20400, loss 3.260\n",
      "step 20500, loss 3.711\n",
      "step 20600, loss 3.908\n",
      "step 20700, loss 3.374\n",
      "step 20800, loss 2.771\n",
      "step 20900, loss 3.138\n",
      "step 21000, loss 2.961\n",
      "step 21100, loss 3.548\n",
      "step 21200, loss 2.546\n",
      "step 21300, loss 3.769\n",
      "step 21400, loss 3.492\n",
      "step 21500, loss 3.762\n",
      "step 21600, loss 3.617\n",
      "step 21700, loss 3.179\n",
      "step 21800, loss 3.170\n",
      "step 21900, loss 3.630\n",
      "step 22000, loss 3.304\n",
      "step 22100, loss 5.018\n",
      "step 22200, loss 2.266\n",
      "step 22300, loss 2.541\n",
      "step 22400, loss 2.717\n",
      "step 22500, loss 3.869\n",
      "step 22600, loss 3.610\n",
      "step 22700, loss 2.834\n",
      "step 22800, loss 3.793\n",
      "step 22900, loss 2.750\n",
      "step 23000, loss 3.541\n",
      "step 23100, loss 3.743\n",
      "step 23200, loss 4.217\n",
      "step 23300, loss 3.305\n",
      "step 23400, loss 3.723\n",
      "step 23500, loss 4.302\n",
      "step 23600, loss 2.609\n",
      "step 23700, loss 3.377\n",
      "step 23800, loss 3.269\n",
      "step 23900, loss 4.505\n",
      "step 24000, loss 3.690\n",
      "step 24100, loss 2.708\n",
      "step 24200, loss 3.515\n",
      "step 24300, loss 2.285\n",
      "step 24400, loss 2.985\n",
      "step 24500, loss 2.870\n",
      "step 24600, loss 2.376\n",
      "step 24700, loss 3.107\n",
      "step 24800, loss 3.818\n",
      "step 24900, loss 3.519\n",
      "step 25000, loss 4.699\n",
      "step 25100, loss 3.605\n",
      "step 25200, loss 3.917\n",
      "step 25300, loss 4.485\n"
     ]
    }
   ],
   "source": [
    "# 定义训练参数\r\n",
    "epoch_num = 20\r\n",
    "batch_size = 20\r\n",
    "\r\n",
    "learning_rate = 0.01\r\n",
    "dropout_rate = 0.2\r\n",
    "num_layers = 1\r\n",
    "hidden_size = 256\r\n",
    "embedding_size = 256\r\n",
    "max_seq_len = 35\r\n",
    "vocab_size = len(word2id_freq)\r\n",
    "\r\n",
    "# 检测是否可以使用GPU，如果可以优先使用GPU\r\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\r\n",
    "if use_gpu:\r\n",
    "    paddle.set_device('gpu:0')\r\n",
    "\r\n",
    "# 实例化模型\r\n",
    "word_predictor = WordPredictor(hidden_size, vocab_size, embedding_size,  num_steps=max_seq_len, num_layers=num_layers, dropout_rate=dropout_rate)\r\n",
    "\r\n",
    "# 指定优化策略，更新模型参数\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.999, parameters= word_predictor.parameters()) \r\n",
    "\r\n",
    "# 定义训练函数\r\n",
    "# 记录训练过程中的损失变化情况，可用于后续画图查看训练情况\r\n",
    "losses = []\r\n",
    "steps = []\r\n",
    "step = 0\r\n",
    "\r\n",
    "def train(model):\r\n",
    "    # 开启模型训练模式\r\n",
    "    model.train()\r\n",
    "    step = 0\r\n",
    "    epoch_num = 20\r\n",
    "    # 建立训练数据生成器，每次迭代生成一个batch，每个batch包含训练文本和文本对应的预测输出\r\n",
    "    train_id_list=make_id_list(train_corpus)\r\n",
    "    train_loader = make_batch(train_id_list,batch_size,max_seq_len)\r\n",
    "    \r\n",
    "    while(epoch_num>0):\r\n",
    "        epoch_num=epoch_num-1\r\n",
    "        for batch_data in (train_loader):\r\n",
    "            sentences = batch_data[0]\r\n",
    "            labels = np.zeros([batch_size,len(word2id_freq)])\r\n",
    "            for i in range(batch_size):\r\n",
    "                labels[i][batch_data[1][i][max_seq_len-1]] = 1\r\n",
    "            # 获取数据，并将张量转换为Tensor类型\r\n",
    "            sentences = paddle.to_tensor(sentences)\r\n",
    "            labels=labels.astype(\"float32\")\r\n",
    "            labels = paddle.to_tensor(labels)\r\n",
    "        \r\n",
    "            # 前向计算，将数据feed进模型，并得到预测的情感标签和损失\r\n",
    "            logits = model(sentences)\r\n",
    "\r\n",
    "            # 计算损失\r\n",
    "            loss = F.softmax_with_cross_entropy(logits, label=labels,soft_label=True)\r\n",
    "            loss = paddle.mean(loss)\r\n",
    "\r\n",
    "            # 后向传播\r\n",
    "            loss.backward()\r\n",
    "            # 更新参数\r\n",
    "            optimizer.step()\r\n",
    "            # 清除梯度\r\n",
    "            optimizer.clear_grad()\r\n",
    "            step+=1\r\n",
    "            if step % 100 == 0:\r\n",
    "                # 记录当前步骤的loss变化情况\r\n",
    "                losses.append(loss.numpy()[0])\r\n",
    "                steps.append(step)\r\n",
    "                # 打印当前loss数值\r\n",
    "                print(\"step %d, loss %.3f\" % (step, loss.numpy()[0]))\r\n",
    "\r\n",
    "#训练模型\r\n",
    "train(word_predictor)\r\n",
    "\r\n",
    "# 保存模型，包含两部分：模型参数和优化器参数\r\n",
    "model_name = \"word_predictor\"\r\n",
    "# 保存训练好的模型参数\r\n",
    "paddle.save(word_predictor.state_dict(), \"{}.pdparams\".format(model_name))\r\n",
    "# 保存优化器参数，方便后续模型继续训练\r\n",
    "paddle.save(optimizer.state_dict(), \"{}.pdopt\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2248187\n",
      "2.0698133\n",
      "2.5195918\n",
      "2.6731122\n",
      "2.781458\n",
      "2.5506375\n",
      "2.5229526\n",
      "2.7695343\n",
      "1.7921125\n",
      "1.5301162\n",
      "2.0458202\n",
      "2.1600127\n",
      "2.2089536\n",
      "2.5916903\n",
      "2.1193178\n",
      "2.291356\n",
      "2.5017333\n",
      "1.8197747\n",
      "2.2765024\n",
      "2.448466\n",
      "2.7207272\n",
      "2.235856\n",
      "2.9058704\n",
      "0.9083152\n",
      "3.3780913\n",
      "2.2738767\n",
      "3.034109\n",
      "2.802174\n",
      "2.070563\n",
      "2.9802952\n",
      "1.7074177\n",
      "3.0309887\n",
      "2.3154252\n",
      "2.7516403\n",
      "2.2980306\n",
      "3.0308218\n",
      "3.2759545\n",
      "3.2880511\n",
      "2.392037\n",
      "2.1802344\n",
      "2.2167194\n",
      "1.8243294\n",
      "2.8874636\n",
      "2.8581574\n",
      "2.51129\n",
      "3.2798755\n",
      "3.4761224\n",
      "2.9465032\n",
      "2.477037\n",
      "3.7621474\n",
      "2.1338117\n",
      "2.5772882\n",
      "2.4724739\n",
      "3.0121753\n",
      "2.6000204\n",
      "2.514338\n",
      "2.7832594\n",
      "1.8919666\n",
      "2.7685692\n",
      "3.1906834\n",
      "3.1233087\n",
      "2.7859933\n",
      "2.7595596\n",
      "2.7127957\n",
      "2.506654\n",
      "2.4585955\n",
      "2.9025521\n",
      "2.6337223\n",
      "2.4671838\n",
      "1.5749327\n",
      "2.1385121\n",
      "3.3348565\n",
      "2.655046\n",
      "2.8515453\n",
      "2.7485979\n",
      "3.485397\n",
      "2.5329626\n",
      "2.4860525\n",
      "2.958406\n",
      "1.9912493\n",
      "2.4883237\n",
      "3.0432968\n",
      "2.9127862\n",
      "2.5964115\n",
      "2.359677\n",
      "3.1654375\n",
      "2.3604064\n",
      "2.2860062\n",
      "3.3965492\n",
      "3.0826578\n",
      "2.788619\n",
      "1.7307892\n",
      "2.231455\n",
      "2.20822\n",
      "1.539428\n",
      "2.6197343\n",
      "2.5535545\n",
      "1.8368229\n",
      "2.9541156\n",
      "2.929738\n",
      "3.0342336\n",
      "3.0388548\n",
      "2.7176104\n",
      "1.9063581\n",
      "2.3601303\n",
      "2.9478476\n",
      "2.133565\n",
      "3.2009275\n",
      "1.3748679\n",
      "1.9462886\n",
      "2.2982428\n",
      "1.9109396\n",
      "2.4500203\n"
     ]
    }
   ],
   "source": [
    "def test(model):\r\n",
    "    # 开启模型测试模式，在该模式下，网络不会进行梯度更新\r\n",
    "    model.eval()\r\n",
    "    batch_size=20\r\n",
    "    max_seq_len=35\r\n",
    "    # 建立训练数据生成器，每次迭代生成一个batch，每个batch包含训练文本和文本对应的预测输出\r\n",
    "    test_id_list=make_id_list(test_corpus)\r\n",
    "    test_loader = make_batch(test_id_list,batch_size,max_seq_len)\r\n",
    "    for batch_data in (test_loader):\r\n",
    "        sentences = batch_data[0]\r\n",
    "        labels = np.zeros([batch_size,len(word2id_freq)])\r\n",
    "        for i in range(batch_size):\r\n",
    "            labels[i][batch_data[1][i][max_seq_len-1]] = 1\r\n",
    "        # 获取数据，并将张量转换为Tensor类型\r\n",
    "        sentences = paddle.to_tensor(sentences)\r\n",
    "        labels=labels.astype(\"float32\")\r\n",
    "        labels = paddle.to_tensor(labels)\r\n",
    "    \r\n",
    "        # 前向计算，将数据feed进模型，并得到预测的词\r\n",
    "        logits = model(sentences)\r\n",
    "\r\n",
    "        loss = F.cross_entropy(logits, label=labels,soft_label=True)\r\n",
    "        loss = paddle.mean(loss)\r\n",
    "        print(loss.numpy()[0])\r\n",
    "\r\n",
    "test(word_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "任务基本完成。\n",
    "\n",
    "还需要改进的地方：\n",
    "\n",
    "输出转化为词，计算accuracy，校订损失（有闲再搞）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
