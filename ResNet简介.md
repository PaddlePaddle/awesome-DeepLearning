### ResNet简介

残差神经网络ResNet (Residual Neural Network)131是由微软研究院的何凯明博士等四名华人学者提出的深度学习模块,通过其设计的残差模块成功训练出了深度可达152层的神经网络，基于该残差模块的深度神经网络在 ILSVRC2015&COCO的比赛中夺得ImageNet检测、ImageNet定位、COCO检测以及COCO分割上的第一名。

在以往神经网络的训练中研究者意识到了深度的重要性,因此用层数更深的网络来拟合数据样本成为一个可供选择项。但由此出现了一个问题，当网络层数过高时就会出现梯度消失和爆炸的问题，严重阻碍了网络的收敛。虽然，通过标准初始化和中间归一化层很大程度上解决了梯度消失和爆炸的问题，使得数十层的神经网络能够收敛，但随着网络层数的增加，模型精度变得饱和，然后会迅速退化，导致网络性能下降。由此，何凯明博士等人设计提出的残差神经网络应运而生。[9]

### ResNet理论

由此,何凯明等人提出在网络中引入一个残差框架来解决网络由于深度增加而导致性能下降的问题。假设一段神经网络的输入层数据用x表征，输出层的期望输出结果为H(x)，故H(x)是输入数据通过复杂网络后的潜在映射，其映射也关系极为复杂难以表征。换个思考方式，直接将输入x传到输出层作为初始结果，即通过如下图所示的短连接操作，如此一来需要学习的目标函数就转换为F(x)=H(x)-x，残差结构 ResNet相当于将学习目标转变了,不再是学习一个输入到输出的完整映射关系，而是最优解H(x)和输入全等映射x的差值，即残差F(x)=H(x)-x，[9]神经模块如图所示。

![img](file:///C:\Users\shenj\AppData\Local\Temp\ksohtml27524\wps1.jpg) 

（图一）

深度残差网络的基本组成单元是残差单元,残差单元一般由卷积Conv层,批处理归一化 Batchnorm层和非线性激活函数Relu共同构成.图一给出了原始残差单元的示意图,令第1个残差单元的输入为x,那么下一层的输出为:

![img](file:///C:\Users\shenj\AppData\Local\Temp\ksohtml27524\wps2.png)

其中，![img](file:///C:\Users\shenj\AppData\Local\Temp\ksohtml27524\wps3.png)是残差函数，![img](file:///C:\Users\shenj\AppData\Local\Temp\ksohtml27524\wps4.png)是该残差函数对应的权重函数，![img](file:///C:\Users\shenj\AppData\Local\Temp\ksohtml27524\wps5.png)是非线性激活函数ReLU[4]。

深度残差网络由多个残差学习单元堆叠而成.给定输入的图像数据,深度残差网络首先将输入数据依次送入卷积层Conv、非线性激活函数层Relu和批处理归一化层 Batchnorm;然后将处理的结果进一步送入到多个残差单元,再经过批处理归一化层BN和多个全连接层;最后得到输出结果。